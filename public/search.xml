<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HBase Java Admin API]]></title>
    <url>%2Fhbase-java-admin-api.html</url>
    <content type="text"><![CDATA[HBase 使用 Java 语言开发，因而 HBase 原生提供了一个 Java 语言客户端。这篇文章介绍 HBase Admin API，包括创建、启用、禁用、删除表等。如果项目使用 Maven 进行依赖管理，只需添加如下依赖即可以使用 Java 客户端访问 HBase 集群：&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.1.6&lt;/version&gt;&lt;/dependency&gt; 需要注意的是，客户端版本和 HBase 版本需要保持一致，否则可能会遇到不兼容的问题。 如果要是遇到 Protobuf 等类冲突时，可以使用 HBase 提供的一个非常方便的 Jar：&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-shaded-client&lt;/artifactId&gt; &lt;version&gt;2.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-shaded-server&lt;/artifactId&gt; &lt;version&gt;2.1.6&lt;/version&gt;&lt;/dependency&gt; HBASE-13517 hbase-shaded-client 和 hbase-shaded-server 是在无法以其他方式解决依赖冲突的场景下使用的。在没有冲突的情况下，我们应首选：hbase-client 和 hbase-server。不要在协处理器内部使用 hbase-shaded-server或 hbase-shaded-client，因为这样可能会发生不好的事情。 1. 连接HBase构建一个 Configuration 示例，该示例包含了一些客户端配置，最重要的必须配置是 HBase 集群的 ZooKeeper 地址与端口。ConnectionFactory 根据 Configuration 示例创建一个 Connection 对象，该 Connection 对象线程安全，封装了连接到 HBase 集群所需要的所有信息，如元数据缓存等。由于 Connection 开销比较大，类似于关系数据库的连接池，因此实际使用中会将该 Connection 缓存起来重复使用： public class HBaseConn &#123; private static final HBaseConn INSTANCE = new HBaseConn(); private static Configuration config; private static Connection conn; private HBaseConn() &#123; try &#123; if (config == null) &#123; config = HBaseConfiguration.create(); config.set("hbase.zookeeper.quorum", "127.0.0.1:2181"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取连接 * @return */ private Connection getConnection() &#123; if (conn == null || conn.isClosed()) &#123; try &#123; conn = ConnectionFactory.createConnection(config); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return conn; &#125; /** * 关闭连接 */ private void closeConnection() &#123; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 获取连接 * @return */ public static Connection create() &#123; return INSTANCE.getConnection(); &#125; /** * 关闭连接 */ public static void close() &#123; INSTANCE.closeConnection(); &#125;&#125; 2. 创建表可以使用 Admin 类的 createTable() 方法在 HBase 中创建表。此类属于 org.apache.hadoop.hbase.client 包中。Admin 类需要通过 Connection 对象来获取。使用 TableDescriptorBuilder 来构建表名以及列族。然后使用 Admin 类的 createTable() 方法创建表：public static void create(String name, String... columnFamilies) throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); TableName tableName = TableName.valueOf(name); if (admin.tableExists(tableName)) &#123; System.out.println("table " + tableName + " already exists"); return; &#125; TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName); for (String cf : columnFamilies) &#123; builder.setColumnFamily(ColumnFamilyDescriptorBuilder.of(cf)); &#125; admin.createTable(builder.build()); System.out.println("create table " + tableName + " success");&#125; HTableDescriptor、HColumnDescriptor 在 2.0.0 版本开始废弃，并在 3.0.0 版本中移除：HBaseAdmin admin = new HBaseAdmin(conf);HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(name));for (String cf : columnFamilies) &#123; tableDescriptor.addFamily(new HColumnDescriptor(cf));&#125;admin.createTable(tableDescriptor); 3. 判断表是否存在要使用 HBase Shell 验证表是否存在，可以使用 exist 命令。同样，使用 Java API，我们可以调用 Admin 类的 tableExists() 方法来验证表是否存在：public static boolean exists(Admin admin, String name) throws IOException &#123; boolean result = admin.tableExists(TableName.valueOf(name)); return result;&#125; 4. 禁用表如果要禁用表，可以使用 Admin 类的 disableTable() 方法。在禁用表之前，我们需要先验证表是否已禁用，可以使用 Admin 类的 isTableDisabled() 方法来验证表是否禁用：public static void disable(String name) throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); TableName tableName = TableName.valueOf(name); if (!admin.tableExists(tableName)) &#123; System.out.println("table " + tableName + " not exists"); return; &#125; boolean isDisabled = admin.isTableDisabled(tableName); if (!isDisabled) &#123; System.out.println("disable table " + name); admin.disableTable(tableName); &#125;&#125; 5. 启用表如果要启用表，可以使用 Admin 类的 enableTable() 方法。在启用表之前，我们需要先验证表是否已启用，可以使用 Admin 类的 isTableEnabled() 方法来验证表是否启用：public static void enable(String name) throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); TableName tableName = TableName.valueOf(name); if (!admin.tableExists(tableName)) &#123; System.out.println("table " + tableName + " not exists"); return; &#125; boolean isEnabled = admin.isTableEnabled(tableName); if (!isEnabled) &#123; System.out.println("enable table " + name); admin.enableTable(tableName); &#125;&#125; 6. 删除表如果要删除表，可以使用 Admin 类中的 deleteTable() 方法删除表。在删除表之前，我们需要先验证表是否已被禁用：public static void delete(String name) throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); TableName tableName = TableName.valueOf(name); if (!admin.tableExists(tableName)) &#123; System.out.println("table " + tableName + " not exists"); return; &#125; // 禁用表 admin.disableTable(tableName); // 删除表 admin.deleteTable(tableName); System.out.println("delete table " + name);&#125; 7. 添加列族如果添加列族，可以使用 Admin 类的 addColumnFamily() 方法将列族添加到表中。addColumnFamily() 方法需要提供表名以及 ColumnFamilyDescriptor 类对象：public static void addColumnFamily(String name, String cf) throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); TableName tableName = TableName.valueOf(name); if (!admin.tableExists(tableName)) &#123; System.out.println("table " + tableName + " not exists"); return; &#125; ColumnFamilyDescriptor desc = ColumnFamilyDescriptorBuilder.of(cf); admin.addColumnFamily(tableName, desc); System.out.println("add column family " + cf);&#125; 8. 删除列族如果要删除列族，可以使用 Admin 类的 deleteColumnFamily() 方法将列族从表中删除。deleteColumnFamily() 方法需要提供表名以及 ColumnFamilyDescriptor 类对象：public static void deleteColumnFamily(String name, String cf) throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); TableName tableName = TableName.valueOf(name); if (!admin.tableExists(tableName)) &#123; System.out.println("table " + tableName + " not exists"); return; &#125; admin.deleteColumnFamily(tableName, cf.getBytes()); System.out.println("delete column family " + cf);&#125; 9. 停止实例化 Admin 类并调用 shutdown() 方法来停止 HBase：public static void shutdown () throws IOException &#123; Connection connection = HBaseConn.create(); Admin admin = connection.getAdmin(); admin.shutdown();&#125; 欢迎关注我的公众号和博客：]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase Shell 命令]]></title>
    <url>%2Fhbase-shell-and-commands.html</url>
    <content type="text"><![CDATA[HBase 提供了一个非常方便的命令行交互工具 HBase Shell。通过 HBase Shell 可以创建表，也可以增删查数据，同时集群的管理、状态查看等也可以通过 HBase shell 实现。 HBase Shell 用法： 确保用 HBase Shell 对所有名称使用双引号，例如表名和列名。 逗号分隔命令参数。 在输入要运行的命令之后，键入。 在表的创建和更改中，我们使用配置字典，它们是Ruby哈希。看起来像：{‘key1’ =&gt; ‘value1’, ‘key2’ =&gt; ‘value2’, …}。 1. 连接HBase Shell通过使用以下命令，我们可以通过 Shell 连接到正在运行的 HBase：./bin/hbase shell 键入 help 然后回车可以查看 Shell 命令以及参数列表：hbase(main):001:0&gt; helpHBase Shell, version 2.1.6, rba26a3e1fd5bda8a84f99111d9471f62bb29ed1d, Mon Aug 26 20:40:38 CST 2019Type &apos;help &quot;COMMAND&quot;&apos;, (e.g. &apos;help &quot;get&quot;&apos; -- the quotes are necessary) for help on a specific command.Commands are grouped. Type &apos;help &quot;COMMAND_GROUP&quot;&apos;, (e.g. &apos;help &quot;general&quot;&apos;) for help on a command group.COMMAND GROUPS: Group name: general Commands: processlist, status, table_help, version, whoami... 如果想退出 Shell，可以通过键入 exit 退出：hbase(main):002:0&gt; exit 2. 常规命令在 Hbase 中，有如下常规命令： status version whoami 2.1 status可以使用 status 命令展示 HBase 集群的系统状态的详细，例如服务器数量：hbase(main):001:0&gt; status1 active master, 0 backup masters, 1 servers, 0 dead, 3.0000 average loadTook 0.3686 seconds 我们还可以传递特定参数，具体取决于我们想了解系统的哪些详细状态。参数可以为 summary，simple，detailed 或 replication。默认不填为 summary。下面我们展示了如何将不同的参数传递给 status 命令：hbase&gt; statushbase&gt; status &apos;simple&apos;hbase&gt; status &apos;summary&apos;hbase&gt; status &apos;detailed&apos;hbase&gt; status &apos;replication&apos;hbase&gt; status &apos;replication&apos;, &apos;source&apos;hbase&gt; status &apos;replication&apos;, &apos;sink&apos; 2.2 version可以使用 version 命令展示当前使用的 HBase 版本：hbase(main):005:0&gt; version2.1.6, rba26a3e1fd5bda8a84f99111d9471f62bb29ed1d, Mon Aug 26 20:40:38 CST 2019 2.3 whoami可以使用 whoami 命令展示当前 HBase 用户：hbase(main):010:0&gt; whoamismartsi (auth:SIMPLE) groups: staff, everyone, localaccounts, _appserverusr, admin, _appserveradm, _lpadmin, com.apple.sharepoint.group.1, _appstore, _lpoperator, _developer, _analyticsusers, com.apple.access_ftp, com.apple.access_screensharing, com.apple.access_ssh 3. DDL命令数据定义语言(Data Definition Language, DDL)，包括数据库表的创建、修改、删除等语句。 在 Hbase 中，有如下数据定义命令： Create Exists Describe List Disable Disable_all Is_disabled Enable Enable_all Is_enabled Alter Drop 3.1 Create可以使用 create 命令创建表，必须指定表名和列族名，以及可选的命名空间参数。列族名可以是一个简单的字符串，也可以是包含 NAME 属性的字典：hbase(main):002:0&gt; create &apos;t1&apos;, &apos;f1&apos;, &apos;f2&apos;, &apos;f3&apos;Created table t1Took 2.7450 seconds=&gt; Hbase::Table - t1 上述命令也可以通过字典方式创建：create &apos;t1&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;&#125;, &#123;NAME =&gt; &apos;f3&apos;&#125; 上面命令均没有指定命名空间，默认为 default。使用如下命令在 ns1 命名空间下创建 t1 表：hbase(main):026:0&gt; create &apos;ns1:t1&apos;, &apos;f1&apos;, &apos;f2&apos;, &apos;f3&apos;Created table ns1:t1Took 2.4030 seconds=&gt; Hbase::Table - ns1:t1 命名空间是表的逻辑分组，类似于关系数据库系统中的数据库。这种抽象为多租户相关功能奠定了基础。具体使用可以查阅HBase 命名空间 Namespace 3.2 Exists可以使用 exists 命令判断表是否存在：hbase&gt; exists &apos;t1&apos;hbase&gt; exists &apos;ns1:t1&apos; 例如，使用如下命令查看表 t1 是否存在：hbase(main):003:0&gt; exists &apos;t1&apos;Table t1 does existTook 0.0914 seconds=&gt; truehbase(main):029:0&gt; exists &apos;ns1:t1&apos;Table ns1:t1 does existTook 0.1863 seconds=&gt; true 3.3 Describe可以使用 describe 命令查看表信息：hbase&gt; describe &apos;t1&apos;hbase&gt; describe &apos;ns1:t1&apos; 或者简写为：hbase&gt; desc &apos;t1&apos;hbase&gt; desc &apos;ns1:t1&apos; 例如，使用如下命令查看表 t1 的具体信息：hbase(main):043:0&gt; describe &apos;t1&apos;Table t1 is ENABLEDt1COLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; &apos;f1&apos;, VERSIONS =&gt; &apos;1&apos;, EVICT_BLOCKS_ON_CLOSE =&gt; &apos;false&apos;, NEW_VERSION_BEHAVIOR =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, CACHE_DATA_ON_WRITE =&gt; &apos;false&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, CACHE_INDEX_ON_WRITE =&gt; &apos;false&apos;, IN_MEMORY =&gt; &apos;false&apos;, CACHE_BLOOMS_ON_WRITE =&gt; &apos;false&apos;, PREFETCH_BLOCKS_ON_OPEN =&gt; &apos;false&apos;, COMPRESSION =&gt; &apos;NONE&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;&#125;...Took 0.0319 seconds 可以看到在建表的时候没有指定任何属性，但是 HBase 默认会给表设置一些属性： VERSIONS：HBase 对表的数据行可以保留多个数据版本，以时间戳来区分。VERSIONS 表示对该表应该保留多少个数据版本。 KEEP_DELETED_CELLS：保留删除的数据。这意味着可以通过 Get 或 Scan 操作获取已经被删除的数据（如果数据删除后经过了一次主压缩，那么这些删除的数据也会被清理）。需要注意的是，如果开启了集群间复制，则这个属性必须设置为 true，否则可能导致数据复制失败。 DATA_BLOCK_ENCODING：数据块编码。用类似压缩算法的编码形式来节省存储空间，主要是针对行键。默认情况下不启用数据块编码。 TTL：数据有效时长。超过有效时长的数据在主压缩的时候会被删除。 BLOOMFILTER：布隆过滤器。数据查询 Scan 操作的时候用来排除待扫描的 StoreFile 文件。 REPLICATION_SCOPE：集群间数据复制开关。当集群间数据复制配置好之后 REPLICATION_SCOPE = 1 的表会开启复制。默认为 0，表示不开启复制。 COMPRESSION：压缩方式。HBase 提供了多种压缩方式来在数据存储到磁盘之前压缩以减少存储空间。 BLOCKSIZE：HBase 读取数据的最小单元。设置过大会导致读取很多不需要的数据，过小则会产生过多的索引文件，默认为 64 KB。 3.4 List可以使用 list 命令查看 HBase 中用户自定义的表，也可以使用正则表达式查询：hbase&gt; listhbase&gt; list &apos;abc.*&apos;hbase&gt; list &apos;ns:abc.*&apos;hbase&gt; list &apos;ns:.*&apos; 例如，使用如下命令查看所有的表：hbase(main):045:0&gt; listTABLEns1:t1ns1:testt1test4 row(s)Took 0.0075 seconds=&gt; [&quot;ns1:t1&quot;, &quot;ns1:test&quot;, &quot;t1&quot;, &quot;test&quot;] 3.5 Disable可以使用 disable 命令禁用表：hbase(main):050:0&gt; disable &apos;test&apos;Took 0.7409 secondshbase(main):051:0&gt; disable &apos;ns1:test&apos;Took 0.4331 seconds 3.6 Disable_all可以使用 disable_all 命令禁用满足正则表达式条件的表：hbase&gt; disable_all &apos;t.*&apos;hbase&gt; disable_all &apos;ns:t.*&apos;hbase&gt; disable_all &apos;ns:.*&apos; 3.7 Is_disabled可以使用 is_disabled 命令查看表是否被禁用：hbase&gt; is_disabled &apos;t1&apos;hbase&gt; is_disabled &apos;ns1:t1&apos; 例如，使用如下命令查看表 t1 是否被禁用：hbase(main):009:0&gt; is_disabled &apos;t1&apos;trueTook 0.0269 seconds=&gt; 1 3.8 Enable可以使用 enable 命令启用表：hbase&gt; enable &apos;t1&apos;hbase&gt; enable &apos;ns1:t1&apos; 3.9 Enable_all可以使用 enable_all 命令启用满足正则表达式条件的表：hbase&gt; enable_all &apos;t.*&apos;hbase&gt; enable_all &apos;ns:t.*&apos;hbase&gt; enable_all &apos;ns:.*&apos; 3.10 Is_enabled可以使用 is_enabled 命令查看表是否启用：hbase&gt; is_enabled &apos;t1&apos;hbase&gt; is_enabled &apos;ns1:t1&apos; 例如，使用如下命令查看表 t1 是否启用：hbase(main):004:0&gt; is_enabled &apos;t1&apos;falseTook 0.0242 seconds=&gt; false 3.11 Alter可以使用 alter 命令对 Hbase 的表以及列族进行修改，如新增一个列族、修改表属性，增加协处理器等等。修改表的模式之前需要先将表下线（禁用），然后执行修改命令，再上线（启用）。例如，使用如下命令修改表列族 f1 可以最多保留5个版本的数据：hbase(main):012:0&gt; alter &apos;t1&apos;, NAME =&gt; &apos;f1&apos;, VERSIONS =&gt; 5Updating all regions with the new schema...All regions updated.Done.Took 1.3834 seconds 我们也可以同时操作多个列族：hbase&gt; alter &apos;t1&apos;, &apos;f1&apos;, &#123;NAME =&gt; &apos;f2&apos;, IN_MEMORY =&gt; true&#125;, &#123;NAME =&gt; &apos;f3&apos;, VERSIONS =&gt; 5&#125; 我们可以使用如下方法删除表 ns1:t1 中的 f3 列族：hbase&gt; alter &apos;ns1:t1&apos;, NAME =&gt; &apos;f3&apos;, METHOD =&gt; &apos;delete&apos;hbase&gt; alter &apos;ns1:t1&apos;, &apos;delete&apos; =&gt; &apos;f3&apos; 3.12 drop可以使用 drop 命令删除表，但是表必须首先要禁用表：hbase(main):052:0&gt; drop &apos;t1&apos;Took 0.2300 secondshbase(main):053:0&gt; drop &apos;ns1:t1&apos;Took 0.2357 seconds 4. DML命令数据操纵语言(Data Manipulation Language, DML)，包括数据的修改、查询、删除等语句。 在 Hbase 中，有如下数据操纵命令： Put Get Scan Count Append Delete Deleteall Truncate 4.1 Put可以使用 put 命令将一行数据插入到 HBase 表中：put &lt;table&gt;, &lt;rowkey&gt;, &lt;列族:列标示符&gt;, &lt;值&gt; 例如，使用如下命令分别在表 t1、ns1:t1 插入一行数据：put &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;, &apos;this is first value&apos;put &apos;ns1:t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;, &apos;this is first value of ns1&apos; 4.2 Get可以使用 get 命令获取 HBase 表的一行或者一个单元的内容：hbase&gt; get &apos;ns1:t1&apos;, &apos;r1&apos;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;TIMERANGE =&gt; [ts1, ts2]&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;c1&apos;&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; [&apos;c1&apos;, &apos;c2&apos;, &apos;c3&apos;]&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;c1&apos;, TIMESTAMP =&gt; ts1&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;c1&apos;, TIMERANGE =&gt; [ts1, ts2], VERSIONS =&gt; 4&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;c1&apos;, TIMESTAMP =&gt; ts1, VERSIONS =&gt; 4&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:abc&apos;)&quot;&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;c2&apos;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, [&apos;c1&apos;, &apos;c2&apos;]hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;c1&apos;, ATTRIBUTES =&gt; &#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;c1&apos;, AUTHORIZATIONS =&gt; [&apos;PRIVATE&apos;,&apos;SECRET&apos;]&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;CONSISTENCY =&gt; &apos;TIMELINE&apos;&#125;hbase&gt; get &apos;t1&apos;, &apos;r1&apos;, &#123;CONSISTENCY =&gt; &apos;TIMELINE&apos;, REGION_REPLICA_ID =&gt; 1&#125; 例如，使用如下命令可以获取表 t1 的一条记录：hbase(main):028:0&gt; get &apos;ns1:t1&apos;, &apos;r1&apos;COLUMN CELL f1:c1 timestamp=1573986505024, value=this is third value of ns11 row(s)Took 0.0299 seconds Get 也可以支持获取多个版本的数据，但是需要修改表的 VERSIONS 属性以支持多版本：hbase(main):019:0&gt; alter &apos;ns1:t1&apos;, NAME =&gt; &apos;f1&apos;, VERSIONS =&gt; 3Updating all regions with the new schema...1/1 regions updated.Done.Took 1.8057 secondshbase(main):027:0&gt; get &apos;ns1:t1&apos;, &apos;r1&apos;, &#123;COLUMN =&gt; &apos;f1:c1&apos;, VERSIONS =&gt; 3&#125;COLUMN CELL f1:c1 timestamp=1573986505024, value=this is third value of ns1 f1:c1 timestamp=1573986499590, value=this is second value of ns1 f1:c1 timestamp=1573986493844, value=this is first value of ns11 row(s)Took 0.0070 seconds 4.3 Scan可以使用 scan 命令来扫描表的数据。该命令是 HBase 数据查询命令中最复杂的命令，需要特别注意查询的数据量，以免由于扫描数据过大导致 HBase 集群出现响应延迟：hbase(main):042:0&gt; scan &apos;ns1:t1&apos;ROW COLUMN+CELL r1 column=f1:c1, timestamp=1573994164276, value=this is first value of r1 r2 column=f1:c1, timestamp=1573994095092, value=this is first value of r2 r3 column=f1:c1, timestamp=1573994102776, value=this is first value of r3 r4 column=f1:c1, timestamp=1573994109851, value=this is first value of r4 r5 column=f1:c1, timestamp=1573994116959, value=this is first value of r5 r6 column=f1:c1, timestamp=1573994123621, value=this is first value of r6 r7 column=f1:c1, timestamp=1573994130600, value=this is first value of r7 r8 column=f1:c1, timestamp=1573994138110, value=this is first value of r88 row(s)Took 0.0137 seconds 我们也可以通过指定时间区间获取某个时刻的数据：hbase(main):043:0&gt; scan &apos;ns1:t1&apos;, &#123;TIMERANGE =&gt; [1573994102776, 1573994130600]&#125;ROW COLUMN+CELL r3 column=f1:c1, timestamp=1573994102776, value=this is first value of r3 r4 column=f1:c1, timestamp=1573994109851, value=this is first value of r4 r5 column=f1:c1, timestamp=1573994116959, value=this is first value of r5 r6 column=f1:c1, timestamp=1573994123621, value=this is first value of r64 row(s)Took 0.0786 seconds 我们也可以从 r6 行键开始获取 2 条记录：hbase(main):002:0&gt; scan &apos;ns1:t1&apos;, &#123;COLUMNS =&gt; &apos;f1:c1&apos;, LIMIT =&gt; 2, STARTROW =&gt; &apos;r6&apos;&#125;ROW COLUMN+CELL r6 column=f1:c1, timestamp=1573994123621, value=this is first value of r6 r7 column=f1:c1, timestamp=1573994130600, value=this is first value of r72 row(s)Took 0.4751 seconds 4.4 Count可以使用 count 命令统计表的行数：hbase(main):005:0&gt; count &apos;ns1:t1&apos;8 row(s)Took 0.0726 seconds=&gt; 8 此操作可能需要花费很长的时间，因为会使用 $HADOOP_HOME/bin/hadoop/jarhbase.jar rowcount 命令来运行计数 MapReduce 作业。 默认情况下每 1000 行展示当前统计的计数，也可以通过 INTERVAL 属性重新指定计数间隔，如下每 2 行展示一次计数：hbase(main):007:0&gt; count &apos;ns1:t1&apos;, INTERVAL =&gt; 2Current count: 2, row: r2Current count: 4, row: r4Current count: 6, row: r6Current count: 8, row: r88 row(s)Took 0.0200 seconds=&gt; 8 4.5 Append可以使用 append 命令向指定的单元上在原先值上追加新值：hbase&gt; append &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos; 例如，使用如下命令在原先的值后面追加, append a new value：hbase(main):047:0&gt; get &apos;ns1:t1&apos;, &apos;r1&apos;COLUMN CELL f1:c1 timestamp=1573994164276, value=this is first value of r11 row(s)Took 0.0475 secondshbase(main):049:0&gt; append &apos;ns1:t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;, &apos;, append a new value&apos;CURRENT VALUE = this is first value of r1, append a new valueTook 0.0119 secondshbase(main):050:0&gt; get &apos;ns1:t1&apos;, &apos;r1&apos;COLUMN CELL f1:c1 timestamp=1573996248316, value=this is first value of r1, append a new value1 row(s)Took 0.0171 seconds 4.6 Delete可以使用 delete 命令删除某列数据。如果我们指定了时间戳，那么我们只会删除对应版本的单元数据：hbase(main):027:0&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;, &apos;f1:c1:value1&apos;Took 0.0029 secondshbase(main):028:0&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c2&apos;, &apos;f1:c2:value1&apos;Took 0.0032 secondshbase(main):029:0&gt; delete &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;Took 0.0040 secondshbase(main):030:0&gt; scan &apos;t1&apos;ROW COLUMN+CELL r1 column=f1:c2, timestamp=1577537013630, value=f1:c2:value11 row(s)Took 0.0097 secondshbase(main):031:0&gt; delete &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c2&apos;Took 0.0056 secondshbase(main):032:0&gt; scan &apos;t1&apos;ROW COLUMN+CELL0 row(s)Took 0.0035 seconds 4.7 Deleteall可以使用 deleteall 命令删除整行数据：hbase(main):033:0&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;, &apos;f1:c1:value1&apos;Took 0.0030 secondshbase(main):034:0&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c2&apos;, &apos;f1:c2:value1&apos;Took 0.0044 secondshbase(main):035:0&gt; deleteall &apos;t1&apos;, &apos;r1&apos;Took 0.0038 secondshbase(main):036:0&gt; scan &apos;t1&apos;ROW COLUMN+CELL0 row(s)Took 0.0036 seconds 4.8 Truncate可以使用 truncate 命令清空整个表的数据：hbase(main):037:0&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;f1:c1&apos;, &apos;f1:c1:value1&apos;Took 0.0053 secondshbase(main):038:0&gt; put &apos;t1&apos;, &apos;r2&apos;, &apos;f1:c1&apos;, &apos;f1:c1:value2&apos;Took 0.0031 secondshbase(main):039:0&gt; put &apos;t1&apos;, &apos;r3&apos;, &apos;f1:c1&apos;, &apos;f1:c1:value3&apos;Took 0.0037 secondshbase(main):040:0&gt; scan &apos;t1&apos;ROW COLUMN+CELL r1 column=f1:c1, timestamp=1577537153001, value=f1:c1:value1 r2 column=f1:c1, timestamp=1577537167747, value=f1:c1:value2 r3 column=f1:c1, timestamp=1577537173985, value=f1:c1:value33 row(s)Took 0.0072 secondshbase(main):041:0&gt; truncate &apos;t1&apos;Truncating &apos;t1&apos; table (it may take a while):Disabling table...Truncating table...Took 2.6141 secondshbase(main):042:0&gt; scan &apos;t1&apos;ROW COLUMN+CELL0 row(s)Took 0.7199 seconds 欢迎关注我的公众号和博客： 原文:HBase Shell &amp; Commands – Usage &amp; Starting HBase Shell]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis中Scan命令的基本用法]]></title>
    <url>%2Fbasic-usage-of-scan-command-in-redis.html</url>
    <content type="text"><![CDATA[1. 概述SCAN 命令以及比较相近的 SSCAN、HSCAN 和 ZSCAN 命令都用于增量迭代数据集元素： SCAN 命令用于迭代当前数据库中的数据库键。 SSCAN 命令用于迭代集合(Set)中的元素。 HSCAN 命令用于迭代哈希(Hash)中的字段以及对应的值。 ZSCAN 命令用于迭代有序集合(Sorted Set)中的元素以及对应的得分。 由于这些命令都可以增量迭代，每次调用都只会返回少量元素，所以这些命令可以用于生产环境中，不用担心像使用 KEYS、SMEMBERS 命令带来的问题。在键或元素的大数据集上调用这些命令可能会长时间（甚至几秒钟）阻塞服务器。像 SMEMBERS 这样的阻塞命令能够在给定的时间内提供数据集中所有的元素，但 SCAN 系列命令仅对返回的元素提供有限的保证，因为数据集在我们增量迭代时可能会发生改变。 SCAN，SSCAN，HSCAN 以及 ZSCAN 命令工作原理都非常类似，因此这篇文章会涵盖这四个命令。区别在于 SSCAN，HSCAN 以及 ZSCAN 命令，第一个参数是保存 Set，Hash或 Sorted Set 值的键的名称。SCAN命令不需要任何键名参数，因为它会迭代当前数据库中所有的键，因此迭代的对象是数据库本身。 2. 基本用法SCAN 是基于游标的迭代器。这意味着在每次调用该命令时，服务器都会返回一个更新后的新游标，用户需要在下一次调用中将这个新游标作为 SCAN 命令的游标参数。当 SCAN 命令的游标参数被设置为 0 时， 服务器将开始一次新的迭代，而当服务器向用户返回的新游标为 0 时会终止迭代。以下是 SCAN 迭代的示例：redis 127.0.0.1:6379&gt; scan 01) &quot;17&quot;2) 1) &quot;key:12&quot; 2) &quot;key:8&quot; 3) &quot;key:4&quot; 4) &quot;key:14&quot; 5) &quot;key:16&quot; 6) &quot;key:17&quot; 7) &quot;key:15&quot; 8) &quot;key:10&quot; 9) &quot;key:3&quot; 10) &quot;key:7&quot; 11) &quot;key:1&quot;redis 127.0.0.1:6379&gt; scan 171) &quot;0&quot;2) 1) &quot;key:5&quot; 2) &quot;key:18&quot; 3) &quot;key:0&quot; 4) &quot;key:2&quot; 5) &quot;key:19&quot; 6) &quot;key:13&quot; 7) &quot;key:6&quot; 8) &quot;key:9&quot; 9) &quot;key:11&quot; 在上面的示例中，第一次调用使用 0 作为游标来开始一次新的迭代。第二次调用时使用上一次调用返回的游标，即命令回复的第一个元素值，即17。从上面的示例可以看到，SCAN 命令返回值是两个值的数组：第一个值是下一次调用中将要使用的新游标，第二个值是包含返回元素的数组。 由于在第二次调用中返回的游标为 0，因此服务器向调用者发送信号，告知迭代已完成，并且遍历完数据集。从游标值 0 开始迭代，然后调用 SCAN 直到返回的游标再次为 0，表示一个完整迭代。 3. 保证SCAN 命令，以及其他增量迭代命令，在整个完整迭代过程中可以为用户提供一系列的保证： 在完整迭代开始直到完整迭代结束期间内的所有元素都会被遍历返回；这意味着，如果某个给定元素在开始迭代时位于数据集内，并且在终止迭代时仍然存在，那么 SCAN 会在某次迭代时返回给用户。 在完整迭代开始直到完整迭代结束期间内不存在的元素永远都不会被返回；因此，如果某个元素在迭代开始之前就被删除，并且在后续的迭代过程中从未添加回数据集中，那么 SCAN 永远都不会返回该元素 。 但是，由于 SCAN 只有很少的关联状态（仅有游标），因此具有以下缺点： 同一个元素可能会被返回多次。重复元素的问题需要我们自己的应用程序处理， 例如，可以考虑将迭代返回的元素用于幂等操作(可以重复执行多次操作)上。 如果一个元素是在迭代过程中被添加到数据集的，又或者是在迭代过程中从数据集中被删除的，那么这个元素可能会被返回，也可能不会。 4. 每次执行返回数量SCAN 系列的函数不能保证每次调用返回的元素数量会在给定范围内。每次调用可能会返回 0 个元素，但只要返回的游标不为 0，客户端就认为迭代没有结束(即使返回了 0 个元素也不能表示迭代的结束)。返回的元素数量会符合一定的规则： 在迭代大型数据集时，SCAN 最多可能会返回几十个元素。 在迭代小的数据集并且内部为编码数据结构时(小的 Set、Hashe 以及 Sorted Set)，单次调用就可以返回数据集的所有元素。 但是，用户可以使用 COUNT 参数来调整每次调用返回的元素的数量级。 5. COUNT参数虽然 SCAN 不能保证每次迭代返回的元素数量，但是可以使用 COUNT 参数根据经验进行调整。基本上，COUNT 参数的作用就是让用户告知迭代命令，在每次迭代中应该从数据集里返回多少元素。虽然 COUNT 参数只是迭代命令实现上的一种提示（hint），但是在大多数情况下，这种提示是能满足我们的预期： COUNT 默认值为 10。 在迭代一个足够大、由哈希表实现的数据库、Set、Hash 或者 Sorted Set 时，如果用户没有使用 MATCH 参数，那么每次调用返回 COUNT 个元素，或者比 COUNT 稍多的元素。 在迭代一个编码为 IntSet (一个只由整数值构成的小数据集) 或 Hash 的 Set 以及编码为 ZipList (由不同值构成的小的 Hash 或者 Set) 的 Sorted Set 时，通常会无视 COUNT 参数指定的值，并在第一次调用时就将数据集包含的所有元素都返回给用户。 没有必要每次迭代都要使用相同的 COUNT 值。用户可以在每次迭代中按自己的需要随意改变 COUNT 值，只要记得将上次迭代返回的游标用到下次迭代里面就可以了。 6. MATCH参数我们也可以通过匹配一个 Glob 风格的模式来迭代元素，类似于 KEYS 命令。我们只需要在 SCAN 命令后面追加 MATCH &lt;pattern&gt; 参数即可实现。 以下是一个使用 MATCH 参数进行迭代的示例：redis 127.0.0.1:6379&gt; sadd myset 1 2 3 foo foobar feelsgood(integer) 6redis 127.0.0.1:6379&gt; sscan myset 0 match f*1) &quot;0&quot;2) 1) &quot;foo&quot; 2) &quot;feelsgood&quot; 3) &quot;foobar&quot;redis 127.0.0.1:6379&gt; 我们需要注意的是 MATCH 过滤器是在从数据集中检索出元素之后，在将数据返回给客户端之前应用的。这意味着，如果模式匹配到数据集中很少的元素，则 SCAN 命令在很多次迭代中可能不返回元素。一个例子如下所示：redis 127.0.0.1:6379&gt; scan 0 MATCH *11*1) &quot;288&quot;2) 1) &quot;key:911&quot;redis 127.0.0.1:6379&gt; scan 288 MATCH *11*1) &quot;224&quot;2) (empty list or set)redis 127.0.0.1:6379&gt; scan 224 MATCH *11*1) &quot;80&quot;2) (empty list or set)redis 127.0.0.1:6379&gt; scan 80 MATCH *11*1) &quot;176&quot;2) (empty list or set)redis 127.0.0.1:6379&gt; scan 176 MATCH *11* COUNT 10001) &quot;0&quot;2) 1) &quot;key:611&quot; 2) &quot;key:711&quot; 3) &quot;key:118&quot; 4) &quot;key:117&quot; 5) &quot;key:311&quot; 6) &quot;key:112&quot; 7) &quot;key:111&quot; 8) &quot;key:110&quot; 9) &quot;key:113&quot; 10) &quot;key:211&quot; 11) &quot;key:411&quot; 12) &quot;key:115&quot; 13) &quot;key:116&quot; 14) &quot;key:114&quot; 15) &quot;key:119&quot; 16) &quot;key:811&quot; 17) &quot;key:511&quot; 18) &quot;key:11&quot;redis 127.0.0.1:6379&gt; 如上述所述，大多数调用没有返回元素，而最后一次调用使用 COUNT 为1000，强制命令对该迭代进行更多扫描，从而使得命令返回的元素也变多了。 7. TYPE参数从 6.0 版开始，我们可以使用此参数要求 SCAN 命令仅返回与给定类型匹配的对象，从而允许我们遍历数据库以查找特定类型的键。SCAN 可以使用 TYPE 参数，但 HSCAN 或 ZSCAN 等不可用。 type 参数与 TYPE 命令返回的字符串名称相同。需要我们注意的是某些 Redis 类型（例如GeoHashes、HyperLogLogs、Bitmap 以及 Bitfields 等）其内部是使用其他 Redis 类型（例如 String 或 Zset）来实现的，因此 SCAN 命令无法将其与相同类型的其他键区分开。例如，ZSET 和 GEOHASH：redis 127.0.0.1:6379&gt; GEOADD geokey 0 0 value(integer) 1redis 127.0.0.1:6379&gt; ZADD zkey 1000 value(integer) 1redis 127.0.0.1:6379&gt; TYPE geokeyzsetredis 127.0.0.1:6379&gt; TYPE zkeyzsetredis 127.0.0.1:6379&gt; SCAN 0 TYPE zset1) &quot;0&quot;2) 1) &quot;geokey&quot; 2) &quot;zkey&quot; 重要的是，TYPE 过滤器是在从数据库中检索元素之后应用的，因此该参数不会降低服务器完成完整迭代所需的负载，对于稀有类型，我们可能不会收到任何元素。 8. 多次并行迭代不同客户端可能在同一时间迭代同一数据集，客户端每次执行迭代都需要传入一个游标，并在迭代结束之后获得一个新的游标，而这个游标就包含了迭代的所有状态，因此，服务器无须为迭代记录任何状态。 9. 在中间终止迭代由于服务器端不会记录状态，迭代的所有状态都保存在游标中，因此调用方可以自由地中途终止迭代，不用向服务器发送通知。An infinite number of iterations can be started and never terminated without any issue. 10. 使用错误的游标调用SCAN使用错误的，负数的，超出范围的游标或其他无效的游标来调用 SCAN，会导致未定义的行为，但绝不会导致崩溃。未定义的是指 SCAN 将不再确保返回元素的保证。 唯一有效的游标是： 开始迭代时的游标值为0。 上一次调用 SCAN 返回的游标，以便继续迭代。 11. 终止保证只有在保证迭代的数据集大小始终保持在给定的最大上限内时(大小恒定)，才能保证 SCAN 算法能终止；否则，对一直增长的数据集进行迭代可能会导致 SCAN 永远不会终止迭代(死循环)。 这很容易直观地看出：如果数据集不断增长，为了访问所有可能出现的元素，将需要做越来越多的工作，而能否结束一个迭代取决于对 SCAN 的调用次数、COUNT 参数值以及数据集的增长速度。 12. 返回值SCAN，SSCAN，HSCAN 以及 ZSCAN 命令都返回一个包含两个元素的回复，第一个元素表示游标的无符号64位整数，第二个元素是迭代出的元素数组： SCAN 元素数组是键的列表。 SSCAN 元素数组是 Set 成员的列表。 HSCAN 元素数组包含两个元素，即字段和值，对应 Hash 的每个返回元素。 ZSCAN 元素数组包含两个元素，即一个成员及其关联的分数，对应 Sorted Set 中的每个返回元素。 欢迎关注我的公众号和博客： 原文：SCAN]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 如何使用HyperLogLog]]></title>
    <url>%2Fhow-to-use-hyperloglog-in-redis.html</url>
    <content type="text"><![CDATA[1. 概述Redis 在 2.8.9 版本添加了 HyperLogLog 数据结构，用来做基数统计，其优点是在输入元素的数量非常大时，计算基数所需的空间比较小并且一般比较恒定。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存越多的集合形成鲜明对比。但是，因为 HyperLogLog 只会根据输入元素来计算基数，并不会储存输入元素本身，所以 HyperLogLog 不能像集合那样能返回输入的各个元素。 2. 什么是基数?比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。基数估计就是在误差可接受的范围内，快速计算基数。 3. 命令HyperLogLog 目前只支持3个命令，PFADD、PFCOUNT、PFMERGE。我们先来逐一介绍一下。 3.1 PFADD 最早可用版本：2.8.9。时间复杂度：O(1)。 PFADD 命令可以将元素(可以指定多个元素)添加到 HyperLogLog 数据结构中并存储在第一个参数 key 指定的键中。如果命令执行之后，基数估计发生变化就返回1，否则返回0。如果指定的 key 不存在，那么就创建一个空的 HyperLogLog 数据结构(即，指定字符串长度以及编码的 Redis String)。也可以调用不指定元素参数而只指定键的命令。如果键存在，不执行任何操作并返回0；如果键不存在，则会创建一个新的 HyperLogLog 数据结并且返回1。 (1) 语法格式:PFADD key element [element ...] (2) 返回值:整型，如果至少有个元素被添加返回 1， 否则返回 0。 (3) Example:127.0.0.1:6379&gt; PFADD hll a b c d e f g(integer) 1127.0.0.1:6379&gt; pfcount hll(integer) 7 3.2 PFCOUNT 最早可用版本：2.8.9。时间复杂度：O(1)，对于多个比较大的key的时间复杂度是O(N)。 PFCOUNT 命令返回指定 HyperLogLog 的基数估算值。对于单个键，该命令返回的是指定键的基数估算值，如果键不存在，则返回0。对于多个键，返回的是多个 HyperLogLog 并集的基数估算值，通过将多个 HyperLogLog 合并为一个临时的 HyperLogLog 计算基数估算值。可以使用 HyperLogLog 只使用很少且恒定的内存来计算集合的不同元素个数。每个 HyperLogLog 只用 12K 加上键本身的几个字节。 (1) 语法格式:PFCOUNT key [key ...] (2) 返回值:整数，返回指定 HyperLogLog 的基数估算值，如果多个 HyperLogLog 则返回并集的基数估算值。 (3) Example:127.0.0.1:6379&gt; PFADD hll foo bar zap(integer) 1127.0.0.1:6379&gt; PFADD hll zap zap zap(integer) 0127.0.0.1:6379&gt; PFADD hll foo bar(integer) 0127.0.0.1:6379&gt; PFCOUNT hll(integer) 3127.0.0.1:6379&gt; PFADD some-other-hll 1 2 3(integer) 1127.0.0.1:6379&gt; PFCOUNT some-other-hll(integer) 3127.0.0.1:6379&gt; PFCOUNT hll some-other-hll(integer) 6 (4) 限制: HyperLogLog 返回的结果并不精确，错误率大概在 0.81% 左右。 该命令会修改 HyperLogLog，会使用8个字节来存储上一次计算的基数。所以，从技术角度来讲，PFCOUNT 是一个写命令。 (5) 性能问题 即使理论上处理一个密集型 HyperLogLog 需要花费较长时间，但是当只指定一个键时，PFCOUNT 命令仍然具有很高的性能。这是因为 PFCOUNT 会缓存上一次计算的基数，并且这个基数并不会一直变动，因为 PFADD 命令大多数情况下不会更新寄存器。所以才可以达到每秒上百次请求的效果。 当使用 PFCOUNT 命令处理多个键时，会对 HyperLogLog 进行合并操作，这一步非常耗时，更重要的是通过计算出来的并集的基数是不能缓存的。因此当使用多个键时，PFCOUNT 可能需要花费一些时间(毫秒数量级)，因此不应过多使用。 我们应该记住，该命令的单键和多键执行语义上是不同的并且具有不同的性能。 3.3 PFMERGE 最早可用版本：2.8.9。时间复杂度：O(N)，N是要合并的HyperLogLog的数量。 PFMERGE 命令将多个 HyperLogLog 合并为一个 HyperLogLog。合并后的 HyperLogLog 的基数估算值是通过对所有给定 HyperLogLog 进行并集计算得出的。计算完的结果保存到指定的键中。 语法格式:PFMERGE destkey sourcekey [sourcekey ...] 返回值:返回 OK。 Example:127.0.0.1:6379&gt; PFADD hll1 foo bar zap a(integer) 1127.0.0.1:6379&gt; PFADD hll2 a b c foo(integer) 1127.0.0.1:6379&gt; PFMERGE hll3 hll1 hll2OK127.0.0.1:6379&gt; PFCOUNT hll3(integer) 6 欢迎关注我的公众号和博客：]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Lateral View]]></title>
    <url>%2Fhive-lateral-view.html</url>
    <content type="text"><![CDATA[1. 语法lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (&apos;,&apos; columnAlias)*fromClause: FROM baseTable (lateralView)* 2. 描述Lateral View 一般与用户自定义表生成函数(split、explode等UDTF)一起使用，它能够将一行数据拆成多行数据，并在此基础上对拆分后的数据进行聚合。 在 Hive 0.6.0 之前，Lateral View 不支持谓词下推优化。在 Hive 0.5.0 以及更早版本中，如果你使用 WHERE 子句，可能不会被编译。解决方法是在你查询之前添加 set hive.optimize.ppd = false 。这个问题在 Hive 0.6.0 版本得到修复。从 Hive 0.12.0 中，可以省略列别名。 3. 单个Lateral View语句假设我们有一张表 pageAds，它有两列数据，第一列 page_id（网页名称），第二列 adid_list（网页上显示的广告数组）： 名称 类型 page_id STRING adid_list Array 表中有两行实例数据： page_id adid_list contact_page [3, 4, 5] front_page [1, 2, 3] 假设我们要统计各个广告在所有网页中展现的次数。 (1) 拆分广告ID，如下所示：SELECT page_id, ad_idFROM pageAds LATERAL VIEW explode(adid_list) ad_table AS ad_id; Lateral View 与 explode()函数结合使用可以将 adid_list 转换为单独的行。 执行结果如下： page_id ad_id front_page 1 front_page 2 front_page 3 contact_page 3 contact_page 4 contact_page 5 (2) 然后，计算各个广告的展示次数，进行聚合的统计： SELECT ad_id, count(1)FROM pageAds LATERAL VIEW explode(adid_list) ad_table AS ad_idGROUP BY ad_id; 执行结果如下： adid count(1) 1 1 2 1 3 2 4 1 5 1 4. 多个Lateral View语句FROM 子句可以有多个 LATERAL VIEW 子句。后面的 LATERAL VIEWS 子句可以引用出现在 LATERAL VIEWS 左侧表的任何列。 以下面的表为例： Array pageid_list Array adid_list [1, 2, 3] [“a”, “b”, “c”] [3, 4] [“c”, “d”] 例如，如下查询：SELECT page_id, ad_idFROM pageAdsLATERAL VIEW explode(pageid_list) page_table AS page_idLATERAL VIEW explode(adid_list) ad_table AS ad_id; LATERAL VIEW 子句会按照它们出现的顺序执行。下面我们分部执行以下。 (1) 执行单个 Lateral View 查询：SELECT page_id, adid_listFROM pageAdsLATERAL VIEW explode(pageid_list) page_table AS page_id; 执行结果如下所示： page_id adid_list 1 [‘a’, ‘b’, ‘c’] 2 [‘a’, ‘b’, ‘c’] 3 [‘d’, ‘e’, ‘f’] 4 [‘d’, ‘e’, ‘f’] (2) 再加上一个 Lateral View 语句，如下所示：SELECT page_id, ad_idFROM pageAdsLATERAL VIEW explode(pageid_list) page_table AS page_idLATERAL VIEW explode(adid_list) ad_table AS ad_id; 执行结果如下所示： page_id ad_id 1 a 1 b 1 c 2 a 2 b 2 c 3 d 3 e 3 f 4 d 4 e 4 f 5. Outer Lateral Views 在 Hive 0.12.0 版本后引入。 当 LATERAL VIEW 不会生成行时，用户可以指定可选的 OUTER 关键字来生成对应的行。当使用 EXPLODE 函数，拆分的列为空时，就会发生这种情况。在这种情况下，源数据行不会出现在结果中。如果想让源数据行继续出现在结果中，可以使用 OUTER 关键字，并且 UDTF 的空列使用 NULL 值代替。 例如，如下数据表： page_id adid_list bottom_page [] front_page [1, 2, 3] 例如，如下 LATERAL VIEW 查询：SELECT page_id, ad_idFROM tmp_page_adsLATERAL VIEW EXPLODE(adid_list) ad_table AS ad_id; 执行结果如下所示，bottom_page 对应数据行不会出现在结果中： page_id ad_id front_page 1 front_page 2 front_page 3 例如，使用 OUTER 关键词查询：SELECT page_id, ad_idFROM tmp_page_adsLATERAL VIEW OUTER EXPLODE(adid_list) ad_table AS ad_id; 执行结果如下所示，bottom_page 对应数据会出现在结果中： page_id ad_id front_page 1 front_page 2 front_page 3 bottom_page NULL 欢迎关注我的公众号和博客： 原文：LanguageManual LateralView]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 命名空间 Namespace]]></title>
    <url>%2Fhbase-namespace-commands-examples.html</url>
    <content type="text"><![CDATA[1. 简介命名空间是表的逻辑分组，类似于关系数据库系统中的数据库。这种抽象为多租户相关功能奠定了基础： 配额管理(HBASE-8410）：限制一个命名空间可以使用的资源(Region或者Table等)。 命名空间安全管理(HBASE-9206)：为多租户提供另一级别的安全管理。 RegionServer组(HBASE-6721)：一个命名空间或一张表，可以被固定到一组 RegionServer 上，从而保证了数据隔离性。 2. 命名空间管理可以创建，删除或修改命名空间。命名空间成员是在表创建期间通过指定完全限定表名来确定：&lt;table namespace&gt;:&lt;table qualifier&gt; 有如下常用的命名空间命令： create_namespace describe_namespace list_namespace alter_namespace list_namespace_tables drop_namespace 2.1 创建命名空间可以使用 create_namespace 命令创建命名空间：hbase&gt; create_namespace &apos;ns1&apos; 也可以通过可选的配置字典来创建命名空间：hbase&gt; create_namespace &apos;ns1&apos;, &#123;&apos;PROPERTY_NAME&apos;=&gt;&apos;PROPERTY_VALUE&apos;&#125; 2.2 查看命名空间可以使用 describe_namespace 命令查看命名空间：hbase(main):017:0&gt; describe_namespace &apos;ns1&apos;DESCRIPTION&#123;NAME =&gt; &apos;ns1&apos;&#125;Took 0.0033 seconds=&gt; 1 2.3 查看所有命名空间可以使用 list_namespace 命令列出所有可用的名称空间：hbase(main):019:0&gt; list_namespaceNAMESPACEdefaulthbasens13 row(s)hbase(main):022:0&gt; list_namespace &apos;ns*&apos;NAMESPACEns11 row(s) 支持正则表达式 2.4 修改命名空间可以使用 alter_namespace 命令修改已经创建的命名空间。 (1) 添加或者修改命名空间属性：alter_namespace &apos;ns1&apos;, &#123;METHOD =&gt; &apos;set&apos;, &apos;PROPERTY_NAME&apos; =&gt; &apos;PROPERTY_VALUE&apos;&#125; (2) 删除命名空间属性：alter_namespace &apos;ns1&apos;, &#123;METHOD =&gt; &apos;unset&apos;, NAME=&gt;&apos;PROPERTY_NAME&apos;&#125; 2.5 在命名空间中创建表创建命名空间后，我们可以在该命名空间上创建表。就像任何其他 RDBMS Scheme 一样，我们必须在命名空间名称后附加表名称。如果不指定命名空间，默认在 default 命名空间下创建表。 如下示例在 ns1 命名空间中创建表 test：hbase(main):008:0&gt; create &apos;ns1:test&apos;, &apos;f1&apos;, &apos;f2&apos;Created table ns1:testTook 0.9023 seconds=&gt; Hbase::Table - ns1:test 2.6 查看给定命名空间所有可用的表可以使用 list_namespace_tables 命令列出给定命名空间下所有可用的表：hbase(main):011:0&gt; list_namespace_tables &apos;ns1&apos;TABLEtest1 row(s)Took 0.0078 seconds=&gt; [&quot;test&quot;] 2.7 删除命名空间可以使用 drop_namespace 命令删除表中存在的命名空间。我们只能删除空的命名空间。如果删除包含表的命名空间，必须先把该命名空间下创建的表删除。 如下所示删除包含表的命名空间：hbase(main):013:0&gt; drop_namespace 'ns1'ERROR: org.apache.hadoop.hbase.constraint.ConstraintException: Only empty namespaces can be removed. Namespace ns1 has 1 tables at org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.prepareDelete(DeleteNamespaceProcedure.java:217) at org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.executeFromState(DeleteNamespaceProcedure.java:78) at org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.executeFromState(DeleteNamespaceProcedure.java:45) at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:189) at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:965) at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1723) at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1462) at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$1200(ProcedureExecutor.java:78) at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:2039) 如下所示删除一个空的命名空间：hbase(main):014:0&gt; create_namespace &apos;ns2&apos;Took 0.2518 secondshbase(main):015:0&gt; drop_namespace &apos;ns2&apos;Took 0.2303 second 3. 内置命名空间HBase 中有两个内置的特殊命名空间： hbase：系统命名空间，包含 HBase 内部表。 default：如果在创建表时没有显式指定命名空间，默认会在此命名空间创建表。 欢迎关注我的公众号和博客： 参考:Hbase Namespace Commands and Examples]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL查询并不总是以SELECT开始]]></title>
    <url>%2Fsql-queries-dont-start-with-select.html</url>
    <content type="text"><![CDATA[很多 SQL 查询确实以 SELECT 开始(本文仅涉及 SELECT 查询，而不涉及 INSERT 或其他内容)。不过，我在网上搜索 ‘是否可以对窗口函数返回的结果进行过滤’ 这个问题，或者说可以在 WHERE、HAVING 或其他中过滤窗口函数的结果吗？最终我得出的结论是：窗口函数必须在 WHERE 和 GROUP BY 发生之后才能运行，所以答案是我们这样做。于是又引出了另一个问题：SQL 查询的执行顺序是什么样的？ 直觉上这个问题应该很好回答，毕竟我自己已经至少写了 10000 个 SQL 查询了，其中还有一些是很复杂。但事实是，我仍然很难准确地说出它的执行顺序是什么样的。 1. SQL查询按此顺序发生我研究了一下，执行顺序如下所示。SELECT 并不是第一个执行的，而是第五个。 执行顺序如下： FROM/JOIN 以及所有 ON 表达式 WHERE GROUP BY HAVING SELECT (包括窗口函数) ORDER BY LIMIT 2. 这张图可帮助我们回答以下问题这张图与 SQL 查询语义相关，让我们可以推理出给定查询返回的内容，并回答如下问题： 可以在 GRROUP BY 之后使用 WHERE 吗？（不行，WHERE 是在 GROUP BY 之后使用！） 可以对窗口函数返回的结果进行过滤吗？（不行，窗口函数发生在 SELECT 语句中，而 SELECT 发生在 WHERE 和 GROUP BY 之后） 可以对 GROUP BY 里的东西进行 ORDER BY 吗？（可以，ORDER BY 基本在最后执行，所以可以对任何东西进行 ORDER BY） LIMIT 发生在什么时候？（发生在最后！） 实际上，数据库引擎并不一定按照这个顺序执行查询，因为为了使查询运行更快，实现了一系列优化。所以： 当我们只想了解哪些查询是合法的以及如何推理给定查询的返回结果时，可以参考上图。 当我们在推断查询性能或者包含索引的任何东西时，上图就不适用了。 3. 混合因素：列别名Twitter上的有人指出，许多 SQL 可以使用如下语法实现：SELECT CONCAT(first_name, ' ', last_name) AS full_name, count(*)FROM tableGROUP BY full_name 上面的查询看起来 GROUP BY 发生在 SELECT 之后，因为 GROUP BY 引用了 SELECT 中的一个别名。实际上并不需要让 GROUP BY 发生在 SELECT 之后，因为数据库引擎可以将查询重写为：SELECT CONCAT(first_name, ' ', last_name) AS full_name, count(*)FROM tableGROUP BY CONCAT(first_name, ' ', last_name) 这样 GROUP BY 仍然会先执行。 我们的数据库引擎也会进行一系列的检查，以确保在运行查询之前，我们在 SELECT 和 GROUP BY 中输入的内容是合法的，因此在生成执行计划之前必须从整体上检查一下查询。 4. 查询可能不会按上述顺序运行实际上，数据库引擎并不一定会按照 JOIN、WHERE、GROUP BY 的顺序来执行查询，因为它们会进行一系列优化，只要重新排序不会改变查询的结果，它们就会对命令进行重新排序以使查询运行得更快。 下面这个简单的示例说明了为什么需要以不同的顺序运行查询以使其快速运行：SELECT *FROM ownersLEFT JOIN catsON owners.id = cats.ownerWHERE cats.name = 'mr darcy' 按照上图执行顺序我们知道：FROM / LEFT JOIN / ON 会先执行，然后是 WHERE， 最后是 SELECT。 如果只需要查找名为’mr darcy’的猫，那就没必要对两张表的所有行进行左连接，先对猫名为 ‘mr darcy’ 执行过滤会更快。在这种情况下，先执行过滤不会改变查询结果！ 在实践中，数据库引擎还会有很多其他优化措施，这些优化措施可能会使它们以不同的顺序执行查询，因为我不是这方面的专家，所以在这不展开介绍。 欢迎关注我的公众号和博客： 原文:SQL queries don’t start with SELECT]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解HBase架构]]></title>
    <url>%2Fin-depth-look-hbase-architecture.html</url>
    <content type="text"><![CDATA[在这篇博客文章中，我们主要深入看一下H Base 的体系结构以及在 NoSQL 数据存储解决方案主要优势。 1. HBase架构组件从物理上来说 HBase 由主从模式架构的三种服务组成： RegionServer：负责为读写提供数据。访问数据时，客户端可直接与 RegionServer 进行通信。 HBase Master：也称之为 HMaster，负责 Region 的分配，DDL（创建、删除表）操作等。 Zookeeper：作为 HDFS 一部分的，负责维护活跃集群的状态。 Hadoop DataNode 负责存储 RegionServer 管理的数据。所有 HBase 的数据都存储在 HDFS 文件中。RegionServer 和 HDFS DataNode 往往会部署在一起，这样 RegionServer 就能够实现数据本地化（即数据放在离需要尽可能近的地方）。在数据写入 HBase 时满足数据本地性，但是随着 Region 的迁移(由于故障恢复或者负载均衡等原因)可能不再满足本地性了(数据还在原先的 RegionServer 上，只是 Region 交给新的 RegionServer 管理)，等到完成数据压缩才能恢复本地性。 NameNode 维护所有构成文件的物理数据块的元数据信息。 1.1 RegionServer与RegionHBase 表根据 RowKey 的开始和结束范围水平拆分为多个 Region。每个 Region 都包含了 StartKey 和 EndKey 之间的所有行。每个 Region 都会分配到集群的一个节点上，即 RegionServer，由它们为读写提供数。RegionServer 大约可以管理 1000 多个 Region。 1.2 HMasterRegion 的分配，DDL（创建，删除表）操作均由 HMaster 负责处理。 HMaster具体负责： 协调 RegionServer：(1)在启动时分配 Region、在故障恢复或者负载均衡时重新分配 Region。(2)监视集群中的所有 RegionServer 实例（侦听来自 Zookeeper 的通知）。 管理员功能：创建，删除，更新表的接口。 1.3 ZooKeeperHBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务状态。Zookeeper 维护哪些服务处于活跃状态并且是可用的，并提供服务故障通知。Zookeeper 使用一致性协议来保证分布式状态的一致性。请注意，需要有三到五台机器来保证一致性协议。 2. 组件如何协同工作Zookeeper 用于协调分布式系统成员的共享状态信息。RegionServer 与 Active HMaster 通过会话与 Zookeeper 进行连接。Zookeeper 通过心跳为活跃会话维护一个临时节点。 每个 RegionServer 都会创建一个临时节点。HMaster 监视这些节点以发现可用的 RegionServer，并且还监视这些节点是否出现故障。Zookeeper 使用第一个发现的 HMaster，通创建一个临时节点来确保只有它处于 Active 状态。Active HMaster 将心跳发送到 Zookeeper，非 Active HMaster 则侦听 Active HMaster 的故障通知。 如果 RegionServer 或 Active HMaster 无法发送心跳，则会导致会话过期，并会删除相应的临时节点。Active HMaster 侦听 RegionServer，并恢复发生故障的 RegionServer。非 Active HMaster 侦听 Active HMaster 是否出现故障，如果 Active HMaster 发生故障，那么一个非 Active HMaster 会变为 Active 状态。 3. HBase首次读写HBase 中有一个特殊的目录表(META表)，保存了集群中所有 Region 的位置。META 表的位置存储在 Zookeeper 中。 如下是客户端第一次读写时发生的情况： 客户端从 ZooKeeper 中获取负责管理 META 表的 RegionServer。 客户端查询 META 服务来获取我们要访问的 RowKey 所对应的 RegionServer。客户端会将该信息与 META 表位置进行缓存。 客户端查询 RowKey 所在的 RegionServer 并从中获取行。 为了以后的读请求，客户端会缓存检索的 META 表位置以及之前读取的 RowKey。在后面，我们一般不再需要查询 META 表，除非由于 Region 迁移导致缓存失效，然后会重新查询并更新缓存。 4. HBase Meta表 元数据表 META 表是一个 HBase 表，保存了系统中所有的 Region。META 表类似一棵 B 树。META 表结构如下所示： Key：Region 开始键，Region ID Value：RegionServer 5. RegionServer组成RegionServer 在 HDFS 数据节点上运行，并包含如下组件： WAL：预写日志是分布式文件系统上的一个文件。用于存储还没持久化存储的新数据，并在出现故障时可以进行恢复。 BlockCache：读缓存，将经常读取的数据存储在内存中。内存不足时删除最近最少使用的数据。 MemStore：写缓存，存储还没写入磁盘的新数据。在写入磁盘之前先对其进行排序。每个 Region 的每个列族都有一个 MemStore。 HFile：将行以有序的 KeyValue 形式存储在磁盘上。 5.1 HBase写入步骤当客户端发出 Put 请求时，第一步是将数据写入预写日志 WAL 中： 新内容将追加到 WAL 文件(存储在磁盘上)末尾。 WAL 用于恢复服务器崩溃时还没持久化的数据。 第二步是将数据写入 WAL 后，将其存储在 MemoryStore 中(写缓存)。然后将 Put 请求的确认返回给客户端。 5.2 MemStoreMemStore 将更新以有序的 KeyValue 形式存储在内存中(与存储在 HFile 中相同)。每个列族只有一个 MemStore。 5.3 Region Flush当 MemStore 累积足够多的数据时，整个有序集都会被写入到一个新的 HFile。随着时间的流逝，每个 MemStore 会有多个 HFile，因为存储在 MemStore 中的 KeyValue 会不断地刷写到磁盘上。HFile 是存储实际的单元值或 KeyValue 实例的地方。 请注意，这也是为什么 HBase 中的列族数量受到限制的一个原因。每个列族都有一个 MemStore。当 MemStore 满之后就会刷写到磁盘。同时还会保存最后写入的序列号，以便系统知道到目前为止所持久化的内容。 最大序列号存储为每个 HFile 中的一个 meta 字段，以反映持久化在何处结束以及在何处继续。当 Region 启动时，会读取序列号，并将最大的序列号用作新编辑内容的序列号。 5.4 HFile数据以有序的 key/values 形式存储在 HFile 中。当 MemStore 累积足够多的数据时，就会将整个有序 KeyValue 集顺序写入到一个新的 HFile 中。顺序写入的方式会非常快，因为它避免了移动磁盘驱动器磁头。 5.4.1 HFile索引HFile 包含多层索引，从而使 HBase 无需读取整个文件即可查找数据。多级索引类似一个 B+ 树： 键值对以升序存储 Rowkey 对应索引指向 64KB 大小的数据块 每个数据块都有自己的叶子索引 每个数据块的最后一个键放在中间索引中 根索引指向中间索引 三种索引类型：(1) Root Index：根索引 (2) Intermediate Index：中间索引 (3) Leaf Index：叶子索引 Trailer 指向 meta 数据块，并将数据写入到持久化文件的末尾。Trailer 还包含诸如布隆过滤器和时间范围之类的信息。布隆过滤器可以帮助我们跳过不包含在特定行键的文件。时间范围信息可以帮助我们跳过不在读取的时间范围内的文件。 刚才我们讨论的索引，在 HFile 被打开时会被载入内存，这样数据查询只要一次磁盘查询。 6. 读取合并我们已经看到，对应于一行的 KeyValue 单元可以存储在多个位置，已经持久化的行单元位于 HFiles 中，最近更新的单元位于 MemStore 中，而最近读取的单元位于 BlockCache 中。因此，当我们读取一行时，系统如何获取对应的单元返回？读取操作需要通过以下步骤合并来 BlockCache、MemStore 以及 HFiles 中的键值： 首先，扫描程序在 BlockCache(读缓存) 中查找行单元。最近读取过的键值存储在这里，并且当内存不足时需要删除最近最少使用的数据。 接下来，扫描程序在 MemStore(写缓存) 中查找，这里包含最近的写入。 如果扫描程序在 MemStore 和 BlockCache 中没有找到所有行单元，那么 HBase 将使用 BlockCache 索引和布隆过滤器将 HFiles 加载到内存中，这里可能包含目标行单元。 如前所述，每个 MemStore 可能有多个 HFile，这意味着对于读操作而言，可能必须查找多个文件，这可能会影响性能。这称为读取放大。 7. HBase压缩7.1 Minor压缩HBase 会自动选择一些较小的 HFile，将它们重写合并为一些较大的 HFile。 此过程称为 Minor 压缩。这样通过将比较多且较小的文件重写为比较少但较大的文件可以减少存储文件的数量。 7.2 Major压缩Major 压缩会将一个 Region 中的所有 HFile 合并重写为每个列族一个 HFile，在此过程中会删除已删除或已过期的单元。这样可以提高读取性能，但是由于 Major 压缩会重写所有文件，因此这个过程可能会发生大量磁盘 I/O 和网络流量。这称为写放大。 Major 压缩可以调整为自动运行。由于写放大，通常需要在周末或晚上进行 Major 压缩。Major 压缩还可以使由于服务器故障或负载均衡而变成远程文件重新回到 RegionServer 数据本地性。 8. Region拆分让我们快速了解一下 Region： 一个表可以水平拆分为一个或多个 Region。Region 在开始键和结束键之间包含连续的，有序的行 每个 Region 默认大小为1GB 表的 Region 由 RegionServer 提供给客户端 RegionServer 大约可以管理 1,000个 Region（可能属于同一表或不同表） 最初，每个表只有一个 Region。当 Region 过大时，会分为两个子 Region。两个子 Region（代表原始 Region 的一半）可以在同一 RegionServer 上并行打开，拆分时会报告给 HMaster。出于负载均衡的原因，HMaster 可能会将新 Region 迁移到其他服务器。 8.1 读取负载均衡拆分最初发生在同一个 RegionServer 上，但是出于负载均衡的考虑，HMaster 可能会将新 Region 迁移至其他 RegionServer。这会导致新的 RegionServer 从远程 HDFS 节点上访问数据，需要等到 Major 压缩时才将数据文件移动到新的 RegionServer 的本地节点上。HBase 数据在写入时是在本地节点的，但是在迁移 Region 时(用于负载均衡或故障恢复)，会丢失数据本地性。 Region 迁移只是逻辑上的迁移，数据还在原先的 RegionServer 上，只是 Region 交给新的 RegionServer 管理。 9. HDFS数据备份所有读写请求都来自/发往主节点。HDFS 会备份 WAL 和 HFile 数据块。HFile 数据块备份会自动进行。HBase 依赖 HDFS 来保证存储文件的数据安全。当数据写入 HDFS 时，一个副本写入本地，然后将其备份到辅助节点，而第三个副本被写入第三节点。 WAL 文件和 HFiles 被持久化到磁盘上并被备份，那么 HBase 如何恢复在 MemStore 中更新但未持久化到 HFiles 中的数据？答案请参见下一部分。 10. 故障恢复当 RegionServer 发生故障时，崩溃的 Region 会不可用，直到执行检测和恢复步骤时才可以使用。当失去 RegionServer 心跳信号时，Zookeeper 认定为节点发生故障。然后，HMaster 将被告知 RegionServer 发生故障。 当 HMaster 检测到 RegionServer 崩溃时，HMaster 将发生崩溃的 RegionServer 中的 Region 重新分配给 Active RegionServer。为了恢复崩溃的 RegionServer 中的 MemStore 内容(还未刷写到磁盘)。HMaster 将属于崩溃 RegionServer 的 WAL 拆分为不同的文件，并将这些文件存储在新 RegionServer 的数据节点中。然后每个 RegionServer 回放各自拿到的拆分的 WAL，以重建该 MemStore。 11. 数据恢复WAL 文件包含一系列编辑，其中每一个编辑都表示一个 Put 或 Delete 操作。编辑是按时间顺序写入的，因此，持久化时将内容追加到存储在磁盘上的 WAL 文件的末尾。 如果数据仍在内存中但未持久化保存到 HFile 时发生故障，该怎么办？重放 WAL。通过读取 WAL，将包含的编辑内容写入到当前的 MemStore 并对其进行排序来完成 WAL 的重放。最后，刷写 MemStore 以将更改写入 HFile。 欢迎关注我的公众号和博客： 原文:An In-Depth Look at the HBase Architecture]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase RowKey 设计]]></title>
    <url>%2Fintroduction-to-hbase-rowkey-design.html</url>
    <content type="text"><![CDATA[1. RowKey作用1.1 RowKey对查询的影响HBase中 RowKey 用来唯一标识一行记录。在 HBase 中检索数据有以下三种方式： 通过 get 方式，指定 RowKey 获取唯一一条记录。 通过 scan 方式，设置 startRow 和 endRow 参数进行范围匹配。 全表扫描，即直接扫描整张表中所有行记录。 如果我们 RowKey 设计为 uid+event_type+biz_type，那么这种设计可以很好的支持如下场景:uid=10457 AND event_type=click AND biz_type=searchuid=10457 AND event_type=clickuid=10457uid=10? 很难以支持如下场景：uid=10457 AND biz_type=searchevent_type=click AND biz_type=searchevent_type=clickbiz_type=search 从上面的例子中可以看出，在进行查询的时候，根据 RowKey 从前向后匹配，所以我们在设计 RowKey 的时候选择好字段之后，还应该结合我们的实际的高频的查询场景来组合选择的字段，越高频的查询字段排列越靠左。 1.2 RowKey对Region划分影响HBase 表的数据是按照 RowKey 来分散到不同 Region，不合理的 RowKey 设计会导致热点问题。热点问题是大量的 Client 直接访问集群的一个或极少数个节点，而集群中的其他节点却处于相对空闲状态。 在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有 StartRowKey 和 EndRowKey，这表示 Region 存储的 RowKey 的范围，HBase 表的数据时按照 RowKey 来分散到不同的 Region 中。不合理的 RowKey 设计会导致热点问题。 热点问题是大量的 Client 直接访问集群的一个或极少数个节点，而集群中的其他节点却处于相对空闲状态。 2. RowKey设计原则 唯一原则：RowKey对应关系型数据库的唯一键，必须保证 RowKey 的唯一性。若向 HBase 同一张表插入相同 RowKey 的数据，则原先存在的数据会被新的数据覆盖。 散列原则：设计的 RowKey 应均匀的分布在各个 Region 上。避免递增，否则读写负载都会集中在某个热点 Region，降低性能，甚至引起 RegionServer 过载。 长度原则：长度适中，一般从几十字节到几百字节，建议使用定义 RowKey。 3. RowKey设计技巧3.1 热点问题HBase 中的行是以 RowKey 的字典序排序的，这种设计优化了 Scan 操作，可以将相关的行以及会被一起读取的行存储在临近位置。 但是，糟糕的 RowKey 设计是引起热点的常见原因。热点发生在大量的客户端流量直接访问集群的一个或极少数节点。客户端流量可以是读，写，或者其他操作。大量流量会使负责该 Region 的单台机器不堪重负，引起性能下降甚至是 Region 不可用。这也会对同一个 RegionServer 的其他 Region 产生不利影响，因为主机无法满足请求的负载。设计良好的数据访问模式可以充分，均衡的利用集群。 为了避免写入时出现热点，设计 RowKey 时尽量避免不同行在同一个 Region，但从更大的角度看，数据应该被写入集群中的多个 Region，而不是一次写入一个 Region。下面介绍一些避免热点的常用技术，以及它们的一些优点和缺点。 3.1.1 加盐这里的加盐不是密码学中的加盐，而是指给 RowKey 添加随机前缀，以使得它和之前排序不同。分配的前缀个数应该和你想使数据分散到的 Region 个数一致。如果你有一些热点 RowKey 反复出现在其他分布均匀的 RowKey 中，加盐是很有用的。但其弊端也显而易见，会增加读取的成本。现在读操作需要把扫描命令分散到所有 Region 上来查找相应的行，因为它们不再存储在一起。如果需要使用 GET 请求再次获取行数据，我们需要知道添加的随机前缀是什么，所以需要我们在插入时保存原始 RowKey 与随机前缀的映射关系。 下面的例子表明加盐可以将写入负载分散到多个 RegionServer 上，同时也表明了对读取的负面影响。假设我们有如下 RowKey，表中的每一个 Region 对应字母表中的一个字母。前缀 a 的 RowKey 对应一个 Region，前缀 b 的 RowKey 对应另一个 Region。在表中，所有以 f 开头的 RowKey 都在同一个 Region，如下所示：foo0001foo0002foo0003foo0004 现在，假设我们想将上面这些 RowKey 分配到 4 个不同的 Region。我们可以用 4 种不同的盐：a、b、c、d。在这个情况下，每一个字母前缀都对应不同的 Region。加盐之后，RowKey 变成如下所示：a-foo0003b-foo0001c-foo0004d-foo0002 由于我们现在可以写入四个不同的 Region，因此理论上我们现在的写入吞吐量是之前写入相同 Region 时的四倍。现在，我们再增加一行，会随机分配 a、b、c、d 中的一个作为前缀，并以一个现有行作为尾部结束：a-foo0003b-foo0001c-foo0003c-foo0004d-foo0002 由于分配是随机的，因此如果我们想要以字典序取回数据，我们需要做更多的工作。加盐增加了写入吞吐量，但会增加读取的成本。 3.1.2 哈希除了加盐，你也可以使用哈希。哈希会使同一行始终有相同的前缀加盐，使用确定性哈希可以使客户端重新构造完整的 RowKey，并使用 Get 操作正常检索该行。哈希的原理是计算 RowKey 的哈希值，然后取哈希值的部分字符串和原来的 RowKey 进行拼接或者完全替代。这里说的哈希包含 MD5、sha1、sha256或sha512等算法。 对于上面的例子，我们可以使用哈希来代替加盐，这样会使得 RowKey 始终有可预测前缀(对于每一个 RowKey 都有确定性的前缀)。通过哈希我们可以知道前缀进而检索该行数据。我们还可以做一些优化，例如使某些键始终位于同一 Region。比如我们有如下的 RowKey：foo0001foo0002foo0003foo0004 我们使用 md5 算法计算这些 RowKey 的哈希值，然后取前 6 位和原来的 RowKey 拼接得到新的 RowKey：95f18c-foo00016ccc20-foo0002b61d00-foo00031a7475-foo0004 3.1.3 翻转RowKey如果初步设计出的 RowKey 在数据分布上不均匀，但 RowKey 尾部的数据却呈现出了良好的随机性，此时可以考虑将 RowKey 翻转，或者直接将尾部部分放到 RowKey 的前面，这样就可以把最频繁发生变化的部分放在前面。翻转可以有效的使 RowKey 随机分布，但是牺牲了 RowKey 的有序性特性。 翻转是避免热点问题的常用的方法，用户Id一般是关系型数据库的自增主键，通常会将用户Id翻转后在末尾加0补齐。类似的，如果我们使用时间戳作为 RowKey 的一部分，可以使用 Long.MAX_VALUE - 时间戳 进行替换。 3.2 单调递增问题在汤姆·怀特（Tom White）的《 Hadoop：权威指南》的 HBase 一章中，有一个优化注意事项：所有客户端一段时间内一致写入某一个 Region，然后再接着一起写入下一个 Region，依此类推。使用单调递增的 RowKey（例如，使用时间戳），就会发生这种情况。可以参阅 IKai Lan 的漫画 monotonically increasing values are bad，该漫画解释了为什么在像 BigTable 这样的数据存储中使用单调递增的 RowKey 会出现问题。可以通过将输入记录随机化来缓解单调递增键在单个 Region 上堆积所带来的压力，最好避免使用时间戳或序列（例如1、2、3）作为 RowKey。 如果确实需要将时间序列数据上传到 HBase，可以学习 OpenTSDB 是怎么做的。具体细节可以参阅 scheme。OpenTSDB 中的 RowKey 格式为 [metric_type] [event_timestamp]，乍一看这似乎违反了不使用时间戳作为 RowKey 的原则。但是，不同之处在于时间戳不在 RowKey 的关键位置，而这个设计假设存在数十个或数百个（或更多）不同的度量标准类型。 3.3 尽量减小行和列的大小在 HBase 中，RowKey、列名、时间戳总是跟值一起发送。如果 RowKey 和列名比较大，尤其是与单元格值大小相比差异不大时，可能会遇到一些问题。Marc Limotte 在 HBASE-3551 描述了这种情况（推荐看一下！）。 3.3.1 列族尽量使列族名尽可能的短，最好是一个字符（例如，d 表示数据/默认值）。 3.3.2 属性虽然详细的属性名称（例如，myVeryImportantAttribute）有更好的可读性，但是最好在 HBase 中存储较短的属性名称（例如 via）。 3.3.3 RowKey长度RowKey 尽可能的短，但仍可用于必需的数据访问（例如，Get 与 Scan）。如果 RowKey 短的对于数据访问没有用处，那么还不如使用一个长的 RowKey。设计 RowKey 时需要权衡取舍。 3.3.4 字节模式长整型8个字节，我们可以使用这八个字节存储一个最大为18,446,744,073,709,551,615的无符号整数。如果将此数字存储为字符串（假定每个字符一个字节），则需要将近3倍的字节：// long//long l = 1234567890L;byte[] lb = Bytes.toBytes(l);System.out.println("long bytes length: " + lb.length); // returns 8String s = String.valueOf(l);byte[] sb = Bytes.toBytes(s);System.out.println("long as string length: " + sb.length); // returns 10// hash//MessageDigest md = MessageDigest.getInstance("MD5");byte[] digest = md.digest(Bytes.toBytes(s));System.out.println("md5 digest bytes length: " + digest.length); // returns 16String sDigest = new String(digest);byte[] sbDigest = Bytes.toBytes(sDigest);System.out.println("md5 digest as string length: " + sbDigest.length); // returns 26 不幸的是，用二进制类型会使你的数据在代码之外很难以阅读。例如，下面是我们在shell 中增加一个值后看到的：hbase(main):001:0&gt; incr 't', 'r', 'f:q', 1COUNTER VALUE = 1hbase(main):002:0&gt; get 't', 'r'COLUMN CELL f:q timestamp=1369163040570, value=\x00\x00\x00\x00\x00\x00\x00\x011 row(s) in 0.0310 seconds Shell 会尽力打印字符串，但是这种情况下，它只能打印16进制。这也会在你的 RowKey 中发生。如果你知道存的是什么那当然没什么，但是如果任意数据存放在具体的值中，那将难以阅读。这也需要权衡。 4. RowKey Example假设我们正在收集以下数据元素： 主机名(Hostname) 时间戳(Timestamp) 日志事件(Log event) 值(Value) 我们可以将它们存储在名为 LOG_DATA 的 HBase 表中，但是 RowKey 是什么呢？从这些属性中看，RowKey 将是主机名、时间戳以及日志事件的某种组合，但是具体是什么？ 4.1 时间戳在RowKey的主要位置RowKey [timestamp][hostname][log-event] 这种设计出现了我们上面说的 RowKey 单调递增问题。 有一个经常提到的’分桶’时间戳方法，通过对时间戳取模来实现。如果面向时间的 Scan 很重要，那么这可能是一种有用的方法。必须注意分桶的数量，因为这需要相同数量的 Scan 才能返回结果。long bucket = timestamp % numBuckets; 现在 RowKey 设计如下：[bucket][timestamp][hostname][log-event] 如上所述，要选择特定时间范围的数据，需要在每个分桶上执行一次 Scan。例如，100个分桶，我们需要执行100次 Scan 才能获取特定时间戳的数据，因此需要做一些权衡取舍。 4.2 主机名在RowKey的主要位置如果需要读写大量主机名，那么 RowKey [hostname][log-event][timestamp] 是一个不错的备选方案。 4.3 时间戳还是反向时间戳？如果我们经常访问最新事件，那么将时间戳存储为反向时间戳（例如，Long.MAX_VALUE – timestamp），我们就能通过对 [hostname][log-event] 进行 Scan 操作获取最新的事件。 时间戳还是反向时间戳都没有错，具体取决于我们的需求。 4.4 可变长度还是固定长度的RowKey？我们都知道 RowKey 存储在 HBase 的每一列上。如果主机名是 a 并且事件类型是 e1，那么 RowKey 会非常小。但是，如果主机名是 myserver1.mycompany.com，事件类型是 com.package1.subpackage2.subsubpackage3.ImportantService，那怎么办？ 在 RowKey 中使用某些替换是一个不错的方法。我们至少有两种方法：哈希和数字。上面主机名在 RowKey 的主要位置的示例如下所示。 (1) 哈希 使用哈希的复合 RowKey： [MD5 hash of hostname] = 16 bytes [MD5 hash of event-type] = 16 bytes [timestamp] = 8 bytes (2) 使用数字的复合 RowKey：对于这种方法，除了LOG_DATA外，还需要另一个查询表，称为LOG_TYPES。 LOG_TYPES的行键为： [type] 表明是主机名还是日志事件 [bytes] 主机名或事件类型的原始字节 此 RowKey 的列可以是带有指定数字的长整数，可以通过使用 HBase 计数器获得该数字，因此生成的复合 RowKey 为： [hostname 对应的长整形] = 8 bytes [event-type 对应的长整形] = 8 bytes [timestamp] = 8 bytes 无论是哈希还是数字替换方法，主机名和事件类型的原始值可以存储为列值中。 欢迎关注我的公众号和博客： 参考：RowKey Design]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop vs MPP]]></title>
    <url>%2Fhadoop-vs-mpp.html</url>
    <content type="text"><![CDATA[最近我听到了很多关于此话题的讨论。同样，这也是一个大数据领域经验不足的客户非常喜欢提问的问题。实际上，我不喜欢这个含糊不清的词语，但是通常客户会找到我们使用它们，因此我不得不使用。 如果回头看5年前(原文发表于2015年)，那时候大多数公司都不会选择 Hadoop，尤其是对于那些要求稳定和成熟平台的企业。因此那时选型非常简单：当你分析的数据库大小达到5-7TB时，我们只需要启动一个 MPP 迁移项目，迁移到一种成熟的企业 MPP 解决方案即可。那时没人听说过非结构化数据，如果我们要分析日志，需要使用 Perl/Python/Java/C++ 对其进行分析并加载到分析 DBMS 中即可。没有人听说过高速数据，简单的使用传统的 OLTP RDBMS 进行频繁的更新，然后将它们分块以插入到分析 DWH 中即可。 但是随着时间的流转，大数据开始火热起来，在大众媒体和社交网络中开始流行。这是大数据的 Google 趋势图： 人们开始讨论 ‘3V’ 以及处理这些海量数据的方法。Hadoop 已从专利技术发展成为用于数据处理的顶级工具，越来越多的公司投入到 Hadoop 中、给 Hadoop 供应商进行投资，或让自己成为 Hadoop 供应商。随着 Hadoop 越来越流行，MPP 数据库开始受到冷落。我们可以以 Teradata 股票为例，在过去三年中，它们一直在下跌，其主要原因是新的参与者瓜分了他们的市场，而这个参与者就是 Hadoop。 3V:Volume(规模性)、Varity(多样性)、Velocity(高速性) 因此，关于’我应该选择 MPP 解决方案还是基于 Hadoop 的解决方案？’的问题变得非常流行。许多供应商都将 Hadoop 定位为替代传统数据仓库，这意味着可以替代 MPP 解决方案。 那么什么是 MPP？MPP 表示大规模并行处理，网格的所有独立节点都参与协调计算，这就是网格计算的方法。MPP DBMS 是基于此方法构建的数据库管理系统。在这些系统中，我们所关注的每个查询被分解为由 MPP 网格节点并行执行的一组协调处理，从而以比传统 SMP RDBMS 系统更快的速度运行计算。该体系结构为我们提供的另一个优势是可扩展性，因为我们可以通过在网格中添加新节点来轻松扩展网格。为了能够处理大量数据，这些数据通常按每个节点仅处理其本地数据的方式在节点之间拆分（分片）。这进一步加快了数据的处理速度，因为如果这种设计使用共享存储将会更复杂，成本更高，可扩展性更低，网络利用率更高，并行性更低。这就是为什么大多数 MPP DBMS 解决方案都是不共享的(shared-nothing)，并且不能在DAS存储或共享小型服务器组的一组存储机架上工作的原因。Teradata，Greenplum，Vertica，Netezza 以及其他类似解决方案都采用了这种方法。它们都具有专门为MPP解决方案开发的复杂成熟的SQL优化器。所有这些都可以通过内置语言和围绕这些解决方案的工具集进行扩展，无论是地理空间分析还是数据挖掘的全文搜索，这些工具集几乎都可以满足任何客户的需求。 Hadoop 不是一项单独的技术，而是一个生态系统，它有其自己的优点和缺点。最大的优点是可扩展性，出现了许多新组件（例如，Spark），并且它们与 Hadoop 的核心技术保持集成。缺点就是我们自己构建不同技术的平台是一项艰巨的工作，自己手动搭建成本比较高，大多数公司都在运行由 Cloudera 或 Hortonworks 提供的平台。 Hadoop 存储技术基于完全不同的方法。不再是基于某种主键来分片数据，而是将数据分为固定大小（可配置）的块，分布在不同节点之间。这些数据块以及整个文件系统（HDFS）都只是可读的。简单来说，将一个小的只有100行的表加载到 MPP 中，引擎会根据表的主键将数据分片，这样在一个足够大的集群中，每个节点仅存储一行记录的可能性会非常大。相反，在 HDFS 中整个小表都会被写入一个块中，在 DataNode 的文件系统上被表示为一个文件。 接下来，集群资源如何管理？与 MPP 设计相比，Hadoop 资源管理器（YARN）为我们提供了更细粒度的资源管理，MapReduce 作业不需要并行运行所有计算任务。它还具有一系列不错的功能，例如可扩展性持等。但是实际上，它比 MPP 资源管理器要慢，有时在并发性管理方面也不那么好。 接下来是 Hadoop 的 SQL 接口。在这里，我们有各种各样的工具：它可能是运行在 MR/Tez/Spark 上的 Hive，也可能是 SparkSQL，也可能是 Impala、HAWQ 或 IBM BigSQL。我们的选择非常多，很容易不知道如何选择。 第一个选择是 Hive，它是将 SQL 查询转换为 MR/Tez/Spark 作业并在集群上执行的一个引擎。所有作业均基于相同的 MapReduce 概念构建，并为我们提供了良好的集群利用率以及与其他 Hadoop 栈的良好集成。但是缺点也很大，执行查询的延迟大，性能差尤其是对于表联接时。下面这张图片涵盖了过时的 MR1 设计，但在我们的背景下并不重要： 诸如 Impala 和 HAWQ 之类的解决方案则不同，它们是 Hadoop 之上的 MPP 执行引擎，可处理 HDFS 中存储的数据。与其他 MPP 引擎一样，它们可以为我们提供更低的延迟以及更少的查询处理时间，但代价是可扩展性以及稳定性较低。 SparkSQL 介于 MapReduce 和 MPP-over-Hadoop 两者之间，试图吸收两者的优点，但也有其自身的缺点。与 MR 相似，它将 Job 分解为一组单独计划的任务，以提供更好的稳定性。与 MPP 一样，尝试在执行阶段之间流式传输数据以加快处理速度。但是它也结合了这些解决方案的缺点，速度不如 MPP，稳定和可扩展性不如 MapReduce。 下面详细看一下 MPP 与 Hadoop 的对比： MPP Hadoop 平台开放性 专有，也有例外 完全开源 硬件 许多解决方案有特有设备，我们无法在自己的集群上部署软件。所有解决方案都需要特定的企业级硬件。 任何硬件都可以使用，供应商提供了一些配置准则。大多数建议是将便宜的商品硬件与DAS结合使用 扩展性(节点) 平均数十个节点，最大100-200 平均100个节点，最大数千个 扩展性(用户) 平均数十TB，最大PB 平均几百TB，最大数十PB 查询延迟 10-20毫秒 10-20秒 查询平均运行时间 5-7秒 10-15分钟 查询最大运行时间 1-2小时 1-2周 查询优化 复杂的企业查询优化器引擎 没有优化器或优化器功能比较局限 查询调试与分析 有查询执行计划、查询执行统计信息以及解释性错误消息 OOM问题和Java堆 dump 分析、集群GC暂停组件，每个任务的单独日志 技术价格 每个节点数十至数十万美元 免费或每个节点高达数千美元 访问友好性 简单友好的SQL接口和简单可解释的数据库内函数 SQL并不完全符合ANSI，用户应注意执行逻辑，底层数据布局。函数通常需要用Java编写，编译并放在集群中 目标用户 业务分析师 Java开发人员和经验丰富的DBA 目标系统 通用DWH和分析系统 专用数据处理引擎 最小建议大小 任意 GB 最大并发 数十到数百个查询 最多10-20个作业 技术可扩展性 仅使用供应商提供的工具 与介绍的任何开源工具（Spark，Samza，Tachyon等）兼容 解决方案实施复杂度 中等 高 有了所有这些信息，我们就可以得出结论，为什么 Hadoop 不能完全替代传统企业数据仓库，而可以用作分布式处理大量数据并从数据中获得重要信息的引擎。Facebook 安装了300PB 规模的 Hadoop，但他们仍使用小型 50TB Vertica 集群，LinkedIn 拥有庞大的 Hadoop 集群，仍使用 Aster Data 集群。 欢迎关注我的公众号和博客： 原文:Hadoop vs MPP]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache HBase 1.0:一个新时代的开始]]></title>
    <url>%2Fstart-of-a-new-era-apache-hBase-1.0.html</url>
    <content type="text"><![CDATA[在学习 HBase API 时了解到 1.0 版本是一个里程碑式的版本， API 也发生了很多变化。在此我们回顾一下这个重要的版本的发布。 Apache HBase 社区在2015年2月份发布了 Apache HBase 1.0.0 版本。Apache HBase 在花费了将近七年的时间后取得了里程碑式的发展，这次发布提供了一些令人兴奋的特性以及并未牺牲稳定性的新API。 在此文章中，我们介绍一下 Apache HBase 项目的过去，现在以及将来。 1. 版本在列举这个版本的详细特性之前，我们先回顾一下历史版本以及版本号演变。HBase 2007 年始于 Apache Hadoop 的一个子项目，并随同 Hadoop 一起发布。三年后，HBase 成为了一个独立的顶级 Apache 项目。由于 HBase 依赖于 HDFS，所以社区让 HBase 与 Hadoop 的主版本号保持一致。例如，HBase 0.19.x 版本与 Hadoop 0.19.x 版本一起使用，等等。 但是，HBase 社区希望一个 HBase 版本可以与多个 Hadoop 版本一起使用，不仅仅能与其所匹配的主版本号一起使用。因此，一个新的命名方案就诞生了，新版本将从接近 1.0 版本的 0.90 主版本开始，就像上面时间线中展示的一样。我们也运用了一种’偶数-奇数’版本的约定，即奇数版本是开发者预览版本，偶数版本是可以在生产环境中使用的稳定版本。稳定版本系列包括0.90、0.92、0.94、0.96和0.98（详见HBase版本）。 在 0.98 版本之后，我们把主干版本命名为 0.99-SNAPSHOT，但是官方已经用完了所有的数字！欠考虑了，2014年 HBase 社区一致认为这个项目已经足够成熟稳和定，可以发布 1.0.0 版本了。在三个 0.99.x 开发者预览版本以及六个 Apache HBase 1.0.0 备选版本之后，HBase 1.0.0 开始发布了。看上面的图表，显示了每个版本的发布时间、支持的生命周期以及开发者预览版本（例如0.99-&gt;1.0.0），如果有的话。 2. HBase 1.0.0，开启了一个新时代1.0.0版本有三个目标 为将来的1.x系列版本奠定稳定基础 提供稳定运行的 HBase 集群及客户端 让版本和兼容性方面更加明确 包括之前的 0.99.x 系列版本，1.0.0 版本解决了超过1500个 JIRA 跟踪的问题。其中一些主要的修改如下所示。 2.1 API 整理和改变HBase 的客户端 API 已经发展多年了。为了简化语义、支持并提供可扩展性以及在将来更容易使用，我们重构了 1.0 版本之前的API。为此，1.0.0 版本引进了新的 API，并且废弃了一些常用的客户端 API（HTableInterface, HTable 以及 HBaseAdmin）。 建议更新我们的应用程序使用新风格的 API，因为在以后的 2.x 发行版本中会被删除这些废弃 API。更多资料，可以访问这个链接：http://www.slideshare.net/xefyr/apache-hbase-10-release 和 http://s.apache.org/hbase-1.0-api。 所有的客户端 API 都标识有 InterfaceAudience.Public 类注解，表明是 HBase 官方的客户端 API。接下来，对于以注解声明为客户端公开的类，所有的 1.x 版本将会对其 API 兼容。 2.2 使用时间轴一致的Region副本提高可读性作为第一阶段的一部分，这个版本包含了一个实验功能’使用时间轴一致的Region副本提高可读性’(Read availability using timeline consistent region replicas)。也就是说，一个 Region 可以以只读模式在多个 Region Server 上存在。Region 的其中一个副本作为主副本，可以写入数据，其它副本只能共享相同的数据。由于时间轴一致性保证的高可用，读请求可以发送到 Region 的任意一个副本上。查看 JIRA HBASE-10070 了解更多信息。 2.3 在线配置修改及合并0.89-fb分支的一些功能Apache HBase 项目中的 0.89-fb 分支是 Facebook 用于发布修改的分支。JIRA HBASE-12147合入了其中的补丁，可以在不用重启 Region Server 的情况下重新加载 HBase 服务器的一些配置。 除此之外，还有几百项性能优化（优化WAL管道、使用disruptor、多WAL、更多的使用off-heap、bug修复以及还有很多没展示的好东西。详细介绍请查看官方的发布日志。发布日志和白皮书也包含了二进制、源代码和协议的兼容性说明、所支持的 Hadoop 和 Java 版本，从0.94、0.96和0.98版本升级的说明以及其它重要细节。 HBase-1.0.0 版本也是 HBase 使用’语义版本’(semantic versioning）的开始。简单的说，将来的 HBase 版本会使用显示兼容语义的 MAJOR.MINOR.PATCH 版本号。HBase 白皮书包含了所有兼容性方面的内容以及不同版本之间的区别。 3. 未来我们已经将 HBase 1.0.0 版本标记为 HBase 的下一个稳定版本，这就意味着所有新用户都应该开始使用这个版本。然而我们明白，作为一个数据库，切换到更新的版本可能会花费一些时间。我们将会继续维护 0.98.x 版本，直到社区用户已经做好结束老版本的准备。预计 1.0.x、1.1.0、1.2.0 等发行版会从相应的分支进行发布，而 2.0.0 版本以及其他主版本也将会如期到达。 欢迎关注我的公众号和博客： 原文:Start of a new era: Apache HBase 1.0]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase Schema 设计]]></title>
    <url>%2Fintroduction-to-hbase-schema-design.html</url>
    <content type="text"><![CDATA[HBase 与传统关系数据库（例如MySQL，PostgreSQL，Oracle等）在架构的设计以及为应用程序提供的功能方面有很大的不同。HBase 权衡了其中一些功能，以实现更好的可扩展性以及更灵活的模式。与关系数据库相比，HBase 表的设计有很大的不同。下面将通过解释数据模型向您介绍 HBase 表设计的基础知识，并通过一个例子深入探讨 HBase 表的设计。 1. HBase数据模型HBase 数据模型与我们在关系数据库中使用或了解的数据模型有很大不同。如 BigTable 原始论文所述，它是一个稀疏，分布式，持久的多维有序 Map，由行键，列以及时间戳进行索引。我们可能会听到人们将其称为键值存储，面向列族的数据库，有时甚至是存储版本化 Map 的数据库，这些描述都是正确的。HBase 数据模型的最简单描述是表，由行和列组成。这与关系数据库中比较相像，但也就是这点与 RDBMS 数据模型相似。实际上，甚至行和列的概念也略有不同。首先，我们定义一些概念，供后面使用： 表(Table)：HBase 以表的形式组织数据。表名必须由可以在文件系统路径中可以使用的字符组成。 行(Row)：通过行键进行唯一标识。行键没有数据类型，以字节数组来存储。 列族(Column Family)：行中数据按列族分组。列族还影响数据在 HBase 中的物理存储，必须预先定义列族并且不能随便对其进行修改。表中每一行都具有相同的列族，但列族中不一定都有相同列。 列限定符(Column Qualifier)：列族中的数据通过列限定符（或简称为列）进行寻址查询。列限定符不需要预先制定，不同行的列限定符不必保持一致。与行键一样，列限定符也没有数据类型，以字节数组来存储。 单元(Cell)：行键，列族和列限定符唯一标识一个单元。存储在单元中的数据称为该单元的值，同样也没有数据类型，以字节数组来存储。 时间戳：单元中的值会进行版本化控制。版本由版本号进行标识，默认情况下，版本号是写入单元的时间戳。如果在写入时未指定时间戳，则使用当前时间戳。如果读取时未指定时间戳，则返回最新时间戳的单元值。每个列族的单元值版本数量由 HBse 分别维护，默认保留三个版本数据。 HBase 中的表如下图所示: 上表由两个列族（Personal 和 Office）组成。每个列族都有两列，Personal 列族的两列为 Name、ResidencePhone，Office 列族的两列为 Phone、Address。 HBase 用于数据处理的API包含三种主要方法：Get，Put和Scan。Get 和 Put 方法针对特定行，并且需要提供行键。Scan 方法作用在一定范围的行上。该范围可以由开始行键和终止行键定义，如果没有指定开始行键和终止行键，则遍历整个表。 你也可以把 HBase 看成一个多维度的 Map 模型去理解它的数据模型。上表第一行表示为多维 Map 如下所示: 一个行键映射一个列族数组，列族数组中的每个列族又映射一个列限定符数组，列限定符数组中的每一个列限定符又映射到一个时间戳数组，每个时间戳映射到不同版本的值，即单元本身。如果我们要查询行键映射的条目，则可以从所有列中获取数据。如果我们要查询指定列族映射的条目，则可以从该列族下所有列中获取数据。如果我们要查询指定列限定符映射的条目，则可以获取所有时间戳以及相关的值。默认情况下仅返回最新版本的数据，我们可以在查询中请求多个版本的数据。可以认为行键等价于关系数据库表中的主键。在表创建后，我们不能选择其他列将作为行键。换句话说，在将数据放入表之后，我们不能选择 Personal 列族中的 Name 列作为行键。 我们也可以将 HBase 视为键值存储（如下图所示），可以理解行键，列族，列限定符，时间戳的组合作为键，存储在单元中的实际数据为值。稍后，当我们深入了解底层存储的细节时，我们会发现，如果要从给定的行中读取特定单元数据时，HBase 会去读取一个数据块，里面除了有要查询的单元数据，可能同时也会获取到其它单元数据： 如果 HBase 表作为键值存储来看，主键可以只是行键，或者是行键，列族，列限定符，时间戳的组合，具体取决于我们要寻址的单元。如果我们对一行中的所有单元都感兴趣，则主键是行键。如果我们只关注指定单元，则需要将对应的列族和列限定符作为主键的一部分。 2. HBase表设计基础正如上面强调的那样，HBase 数据模型与关系数据库系统完全不同。因此，设计 HBase 表的方法与关系数据库系统的方法不同。在设计 HBase 表时需要考虑以下问题： 行键的结构是什么样，应该包含什么信息。 表应该有多少列族。 列族中应该存储什么样的数据。 每个列族应该有多少列。 列名是什么，尽管无需在创建表时定义列名，但是在写入或读取数据时需要知道它们。 单元中应该存储什么样的数据。 每个单元中存储多少个时间版本。 HBase 表设计的最重要的是定义行键结构。定义行键结构，重要的是预先定义访问模式（读和写）。除此之外，还需要考虑 HBase 表的一些特性： 仅对行键进行索引。 表是根据行键存储的。表中的行根据行键的字典序来进行排序，表中每一块区域的划分都是基于开始行键以及终止行键来决定的。 HBase 表中的所有内容都以字节数组存储，没有数据类型。 仅保证行级别的原子性。跨行不会保证原子性，这意味着不存在多行事务。 列族必须在创建表时预先定义。 列限定符是动态的，可以在表创建之后写入数据时定义。列限定符以字节数组的形式存储，因此我们甚至可以将真实数据存储其中。 学习这些概念的一种最好方法是通过示例来演示。我们以 Twitter 上用户相关注为例进行说明。关注者与关注的本质上是一个图，存储在专门的图数据库可以更有效地使用此类数据集。但是，这个特殊的用例为在 HBase 表中建模提供了一个很好的示例。 在对表进行建模之前第一步是定义应用程序的访问模式。在 Twitter 用户关注下访问模式可以定义如下。 读取模式： 用户关注了谁？ 用户A是否关注了用户B？ 谁关注了用户A？ 写模式： 用户关注一个新用户。 用户取消关注了某个人。 2.1 方案一下面我们开始考虑表的，并探讨其优缺点。如下图所示的表设计，该表每一行代表着某个用户以及他所关注的所有用户，行键是关注者的用户ID，列名为关注用户序号，单元值为关注用户Id： 带有数据的表设计如下图所示： 在这种表结构的设计下，第一个问题’用户关注了谁’很好解决，但对于第二个问题’用户A是否关注了用户B’这个问题在列很多(关注的用户很多)的时候，需要遍历所有列去找到用户B，这样的代价会比较大。并且当添加新的关注用户时，因为不知道给这个新用户分配什么样的列序号，因此需要遍历列族中的所有列找出最后一个列，并将最后一列的序号+1给新的关注用户作为列序号，这样的代价会很大。一种可能的解决方案是保留一个计数器，记录当前列序号，如下图所示： 表中的数据跟之前一样，只是添加了一个计数器，用于记录用户所关注的用户数量。根据上图表的设计，将新关注用户添加到关注用户列表中所需的步骤如下： 第一步获取当前计数器表示的列序号(count:4)。 第二步更新列序号值，加1(count:5)。 第三步添加一个新条目。 第四步将新数据(5:Lui,count:5)写回HBase。 如你所看到的，保持计数器会让客户端代码变的很复杂。每次往A的关注用户列表中增加一个用户，必须先从 HBase 表里读出计数，增加一个用户，更新计数器。这个过程看起来有点像关系型数据库里的事务。 2.2 方案二上面的设计在使用计数器后有所改进，但还是不能解决所有问题。取消关注用户仍然很棘手，我们必须遍历所有列以找出我们需要删除的列。最大的问题是，因为 HBase 不会对跨行或跨RPC调用进行事务保证，在添加关注用户时我们必须在客户端代码中实现某种事务逻辑。 读取计数器以及更新计数器需要有事务的支持，这样会让客户端变的比较复杂。解决这个问题的唯一办法是去掉计数器。 我们之前提到的一个特性是列限定符是动态的，并且像单元一样以字节数组存储。这样一来，我们便可以将任意数据放入列限定符中，基于这个特性我们再改进表的设计。如下图所示，在这种设计中，不再需要计数器，列限定符使用被关注的用户名称，而不在是他们在关注用户列表中的位置。在这种设计下添加关注用户变得不那么复杂(直接添加，不需要计数器获取列序号)。取消关注也得到了简化(直接找到对应列，不需要遍历)： 现在，表中使用用户名作为列限定符，单元值可以是任意内容，因为单元不能是空的，需要我们存储点东西，所以输入数字1。 这种设计几乎解决了所有问题。在读取访问模式中，只剩下第三个问题’谁关注了用户A？’。在当前设计中，由于仅对行键进行索引，因此我们需要进行全表扫描才能知道谁关注了用户A。这就告诉我们，关注的用户也应该以某种方式进行索引。 2.3 方案三有两种方法可以解决这个问题。第一种方法是新建一张表，里面保存用户以及所有关注他的用户。第二种方法是在同一张表中使用不同的行键信息，存储用户以及所有关注他的用户的信息，并能从行键上区分是关注还是被关注，例如，行键为 A_following 的这行保存着用户A关注的所有用户，而行键为 A_followed 的这行保存着所有关注用户A的用户。这样我们的第三个问题只需要查询行键为 A_followed 就能知道谁关注了用户A。 当前表结构还可以进一步的优化。如下图所示的表： 方案一和方案二都是使用的宽表形式。也就是说一行包括很多列。同样的信息可以使用高表形式存储。每行代表一个’关注与被关注’关系。行键里使用了+串联了两个值，你也可以使用你喜欢的任意字符。 在此设计中，有两点需要注意：行键现在由关注用户和被关注用户组成，同时列族的名字被设计成只有一个字母f。列族名称这样的设计可以通过减少从 HBase 读取/写入的数据来减少I/O负载（磁盘和网络），因为列族名称也是返回给客户端的 KeyValue 对象的一部分。 保存了一些样例数据的表如下图所示： 按高表而不是宽表进行设计。把用户名放进列限定符可以节省为了得到用户名到用户表中查询的时间。其负面影响就是，如果用户在用户表里更新他们的名字，你不得不在本表的所有单元里更新用户名字。 表的这种新设计在回答读模式第二个问题’用户A是否关注了用户B？’时会比以前方案快，基于行键使用 Get 操作得到一行也就得到答案了，不用再像早期表设计中那样遍历该行的所有列。获取关注的所有用户从 Get 操作变成简短的 Scan。取消关注变为简单的删除操作。 高表并不总是表设计的最好选择，为了获取高表的性能好处，会在某些操作上放弃了原子性原则。在前面方案中我们可以在一行上用单个 Put 操作更新任何用户的关注列表。Put 运算在行级别是原子不可分的。在这个方案里，我们放弃了这样做的能力 注意，在表中不同的行键可能其长度也不一样。由于每次对表的调用要传输的数据都是不一样的，因此这对性能也会由影响。解决此问题的方法是对行键进行散列。为了在表中有相同长度的行键，我们可以对不同用户ID进行散列并将其拼接在一起。如下图所示我们使用 MD5 对用户Id以及其所关注的用户Id进行散列并拼接 md5(follower)md5(followed)。这样我们就有固定长度的行键，每个用户ID为16个字节。如果我们要要查询某个用户，我们可以计算对应的散列值来查询表： 使用MD5作为行键的一部分可以得到固定长度和更好的分布。 3. 总结本文介绍了HBase模式设计的基础知识。首先介绍了数据模型，然后讨论了设计 HBase 表时要考虑的一些因素。下面是HBase一些关键特性的总结： 行键是 HBase 表设计中最重要的一环，决定了应用程序如何与 HBase 表进行交互，还会影响从 HBase 中读取的性能。 HBase 表很灵活，我们可以以字节数组的形式存储任何内容。 将具有相似访问模式的所有内容存储在同一列族中。 仅对行键进行索引。 高表使操作更快，更简单，但是失去了原子性。宽表，其中每一行都有很多列，允许行级别的原子性。 HBase并不支持事务，所有操作尽量在一次API请求中完成。 哈希可以使固定长度的键有更好的分布，但会失去字符串暗含的有序性。 列限定符可用于存储数据，就像单元一样。 列限定符的长度会影响存储空间，因为可以将数据放入其中。长度也会影响访问数据时的磁盘和网络I/O代价。 列族名称的长度会影响通过网络发送到客户端的数据大小（在KeyValue对象中）。 欢迎关注我的公众号和博客： 原文：Introduction to HBase Schema Design]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解HBase与BigTable]]></title>
    <url>%2Funderstanding-hbase-and-bigtable.html</url>
    <content type="text"><![CDATA[在学习HBase（Google BigTable 的开源实现）的时候，我们面临的最为困难的地方就是需要你重构你的思路来理解 BigTable 的概念。 非常不幸的是，在 BigTable 和 HBase 中都出现了 table 和 base 这两个概念，这很容易让我们与RDBMS（关系型数据库管理系统）产生联想。 本文旨在从概念的角度描述这些分布式数据存储系统。阅读完这篇文章后，我们应该能够更明智的做出决定，即何时使用 HBase 以及何时使用’传统’数据库。 1. 术语幸运的是，Google 的 BigTable Paper 清楚地说明了 BigTable 的真正含义。这是’数据模型’部分的第一句话：Bigtable 是一个稀疏的，分布式的，持久化的多维有序 Map。 A Bigtable is a sparse, distributed, persistent multidimensional sorted map. 论文继续解释到：Map 由行键、列以及时间戳进行索引，在 Map 中的每个值都是无解释的字节数组。 The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes. 在 Hadoop wiki的 HBase Architecture 页面中指出：HBase 使用的数据模型与 Bigtable 非常相似。用户在标记表中存储数据行，数据行中有一个有序的key和任意数量的列。这张表的存储是稀疏的，所以如果用户喜欢的话，甚至可以在同一张表的每行中疯狂的存储差异巨大的列。 HBase uses a data model very similar to that of Bigtable. Users store data rows in labelled tables. A data row has a sortable key and an arbitrary number of columns. The table is stored sparsely, so that rows in the same table can have crazily-varying columns, if the user likes. 上面提到的这些概念似乎很神秘，但其实如果我们把它分解一下就会很好理解。下面我们就按照顺序讨论一下几个主题：Map、持久化、分布式、有序、多维和稀疏。 与其尝试直接描绘一个完整的系统，不如逐渐建立起一个零散的思想框架，以简化它… 2. MapHBase/BigTable 的核心是 Map。根据我们不同编程语言背景，我们可能更熟悉编程语言关联的术语：数组（PHP），字典（Python），哈希（Ruby）或对象（JavaScript）。从维基百科文章来看，Map 是’由一组键和一组值组成的抽象数据类型，其中每个键都与一个值相关联’。 使用 JavaScript 对象表示，这是一个简单的 Map 示例，其中所有值都只是字符串：&#123; "zzzzz" : "woot", "xyz" : "hello", "aaaab" : "world", "1" : "x", "aaaaa" : "y"&#125; 3. 持久化持久化仅表示我们创建或访问的程序运行完成后，我们保留在这个特殊 Map 中的数据会’持久化’。概念上与其他类型的持久化存储（例如文件系统上的文件）没有什么不同。 4. 分布式HBase 和 BigTable 建立在分布式文件系统上，因此底层文件存储分布在不同的计算机上。HBase 使用的是 Hadoop 的分布式文件系统（HDFS）或 Amazon 的简单存储服务（S3），而 BigTable 使用的是 Google 文件系统（GFS）。 数据以一种类似于 RAID 系统的方式在多个参与节点中进行复制。在这里，我们并不在乎使用哪种分布式文件系统来实现。重要的是我们需要知道它是分布式的，它提供了一层保护，以防止集群中的某个节点发生故障。 5. 有序与大多数 Map 实现不同，在 HBase/BigTable 中，键/值对严格按照字母顺序排序。也就是说，键 aaaaa 的行应紧邻键 aaaab 的行，并距离键 zzzzz 的行非常远。排序后的版本如下所示：&#123; "1" : "x", "aaaaa" : "y", "aaaab" : "world", "xyz" : "hello", "zzzzz" : "woot"&#125; ​由于这些系统常常非常巨大而且是分布式的，有序功能是非常重要的。相似的行（例如键）紧密相邻，这样当你必须对表进行扫描时，你最感兴趣的条目之间彼此相邻。 行键的设计非常重要。例如，我们有一个表，行键为域名。我们最好以域名的倒序形式作为行键（使用 com.jimbojw.www 而不是 www.jimbojw.com），这样相关子域名的行就会位于父域名行的附近。这样域名 mail.jimbojw.com 的行会紧邻 www.jimbojw.com 的行，而不会是 mail.xyz.com。 需要注意的是，术语’sorted’在 HBase/BigTable 中并不意味着值是有序的。除了行键之外，没有其他任何自动索引。 6. 多维到现在为止，我们还没有提到 column 的任何概念，而是将 table 视为概念上的常规 Hash/Map。column 这个词也跟 table 和base 的概念一样，承载了太多的 RDBMS 的情感在内。我们可以把它理解为一个多维 Map，即 Map 中嵌套 Map。在 JSON 示例中增加一维：&#123; "1" : &#123; "A" : "x", "B" : "z" &#125;, "aaaaa" : &#123; "A" : "y", "B" : "w" &#125;, "aaaab" : &#123; "A" : "world", "B" : "ocean" &#125;, "xyz" : &#123; "A" : "hello", "B" : "there" &#125;, "zzzzz" : &#123; "A" : "woot", "B" : "1337" &#125;&#125; 在上面的示例中，我们会注意到，每个键都指向具有两个键的 Map：A和 B。从这里开始，我们将顶级键/Map对称为行(Row)。同样，在 BigTable/HBase 命名中，A和 B 映射称为列族。表的列族是在创建表时指定的，以后很难或无法修改。添加新的列族代价可能也很昂贵，因此最好预先指定所有需要的列族。 幸运的是，列族可以具有任意数量的列，用限定符(Qualifier)或标签(Label)列表示。下面是我们的 JSON 示例的子集，这次是添加列限定符维度：&#123; // ... "aaaaa" : &#123; "A" : &#123; "foo" : "y", "bar" : "d" &#125;, "B" : &#123; "" : "w" &#125; &#125;, "aaaab" : &#123; "A" : &#123; "foo" : "world", "bar" : "domination" &#125;, "B" : &#123; "" : "ocean" &#125; &#125;, // ...&#125; 在上面两行中，A 列族有两列：foo 和 bar，而 B 列族只有一列，其限定符为空字符串。 向 HBase/BigTable 查询数据时，我们必须以 &lt;family&gt;:&lt;qualifier&gt; 的形式提供完整的列名。因此，上例中的三列为：A:foo，A:bar 和B:。 尽管列族是静态的，但列不是。考虑以下扩展行：&#123; // ... "zzzzz" : &#123; "A" : &#123; "catch_phrase" : "woot", &#125; &#125;&#125; 在这个示例下，zzzzz 行只有一列 A:catch_phrase。由于每一行都可以有任意数量的不同列，因此没有内置的方法来查询所有行中所有列。要获取该信息，我们必须进行全表扫描。但是，我们可以查询所有列族，因为它们是不变的。 HBase/BigTable 中最后一个维度是时间。我们可以使用整数时间戳（自纪元以来的秒数）或我们选择自定义整数来对数据进行版本控制。客户端可以在插入数据时指定时间戳。 使用任意整数时间戳示例：&#123; // ... "aaaaa" : &#123; "A" : &#123; "foo" : &#123; 15 : "y", 4 : "m" &#125;, "bar" : &#123; 15 : "d", &#125; &#125;, "B" : &#123; "" : &#123; 6 : "w" 3 : "o" 1 : "w" &#125; &#125; &#125;, // ...&#125; 每个列族在保存给定单元格的版本数量方面都有其自己的规则（一个单元格通过其行键/列对来标识）。在大多数情况下，应用程序只是简单地查询给定单元格的数据，无需指定时间戳。在这种常见情况下，HBase/BigTable 将返回最新版本（时间戳最高的版本）的数据。如果应用程序查询给定时间戳版本的数据，HBase 将返回时间戳小于或等于我们提供的时间戳的单元格数据。 例如，查询 aaaaa/A:foo (行/列)单元格数据将返回 y，而查询 aaaaa/A:foo/10 (行/列/时间戳)单元格数据将返回 m。查询 aaaaa/A:foo/2 (行/列/时间戳)单元格数据将返回空。 7. 稀疏最后一个关键字是稀疏。如前所述，给定的行在每个列族中可以有任意数量的列，或者根本没有列。稀疏的另一种类型是基于行的间隙，这仅意味着键之间可能存在间隙。 欢迎关注我的公众号和博客： 原文：Understanding HBase and BigTable]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OLAP vs OLTP]]></title>
    <url>%2Folap-vs-oltp.html</url>
    <content type="text"><![CDATA[1. OLTP定义OLTP 是 Online Transaction Processing 的简称，是一个联机事务处理系统，主要目标是数据处理而不是数据分析。OLTP 系统的主要关注点是记录事务当前的更新，插入以及删除操作。OLTP 的查询比较简短，因此需要比较少的处理时间以及比较少的空间。 OLTP 数据库需要经常更新。OLTP 中的事务可能会在中间过程中失败，这可能会影响数据完整性。因此，必须特别注意数据完整性。OLTP 数据库具有规范化表（3NF）。OLTP 系统成为 OLAP 的数据源。 OLTP 的示例如下： 网上银行业务 网上机票预订 发送短信 将书添加到购物车 2. OLAP定义OLAP 是 Online Analytical Processing system 的简称，是一个联机分析处理系统，主要目标是数据分析而不是数据处理。OLAP 数据库存储的是 OLTP 输入的历史数据。可以允许用户查看不同纬度的数据。使用 OLAP，我们可以从大型数据库中提取信息并进行分析来做决策。 OLAP 还允许用户执行复杂的查询以提取多维数据。在 OLAP 中，即使事务在中间过程中失败，也不会损害数据完整性，因为用户使用 OLAP 系统只是从大型数据库中检索数据进行分析。用户只需再次查询提取数据进行分析即可。 OLAP 中的事务很长，因此需要花费更多的时间以及更大的空间。与 OLTP 相比，OLAP 中的事务较少。甚至 OLAP 数据库中的表也可能不规范。 OLAP 的示例如下： 公司可能会将9月的手机销售与10月的销售进行比较，然后将这些结果与可能存储在另一个位置不同数据库的数据进行比较。 亚马逊分析其客户的购买情况，以提供个性化的主页，其中包含其客户可能感兴趣的产品。 3. 对比 比较基准 OLTP OLAP 目标 数据处理 数据分析 基本 在线交易系统，管理数据库修改 在线数据检索和数据分析系统 操作 主要操作是插入，更新和删除 提取多维数据以进行分析，并做出决策 原始数据 在线交易数据 不同的 OLTP 数据库成为 OLAP 的数据源 事务 短暂但频繁 较长但较少 处理时间 处理时间相对较短 事务的处理时间相对较长 查询 简单查询 复杂查询 规范化 规范化（3NF） 未规范化 完整性 必须维护数据完整性约束 不会经常修改，数据完整性不受影响 4. 结论OLTP 是一个在线数据修改系统，而 OLAP 是一个在线历史多维数据检索系统，该系统检索数据进行分析以帮助做出决策。使用哪种取决于用户需求，两者都可以用于不同的目的。 参考:Difference Between OLTP and OLAP]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 伪分布式模式安装与启动]]></title>
    <url>%2Fhbase-pseudo-distributed-setup-and-start.html</url>
    <content type="text"><![CDATA[1. 环境相关安装 HBase 之前默认我们已经完成了 Hadoop、ZooKeeper 安装，如果还没有安装可以参考如下博文： Hadoop 安装与启动 ZooKeeper 伪集群模式安装与启动 集群配置： JDK: 1.8.0 Hadoop：2.7.7 ZooKeeper：3.4.12 HBase：2.1.6 1.1 Java下表总结了在各种 Java 版本上部署 HBase 的建议。对号符号标示测试的基准以及愿意帮助诊断和解决您可能遇到的问题。类似地，叹号和叉号符号通常表示您在遇到问题时，需要更改 Java 环境。 HBase建议下游用户使用来自OpenJDK项目或供应商的标记为长期支持（LTS）的JDK版本。 截至2018年3月，这意味着Java 8是唯一适用的版本。 我们必须在集群的每个节点上设置 JAVA_HOME。hbase-env.sh 提供了一种方便的机制来执行此操作。 1.2 Hadoop下表总结了每个 HBase 版本支持的 Hadoop 版本。基于 HBase 版本，我们需要选择最合适的 Hadoop 版本。我们可以使用 Apache Hadoop 或供应商的 Hadoop 发行版。该表中未出现的较旧版本不在支持，并且可能缺少必要的功能，而较新版本未经测试，但可以试用： 对号符号标示经过测试并且功能齐全。 叉号符号已知功能不完整或存在CVE，因此我们在较新的次要版本中放弃了对它的支持。 叹号符号未经测试，可能/可能不起作用。 推荐使用 Hadoop2.x。Hadoop 2.x 速度更快，并且具有短路读取（请参阅利用本地数据）等功能，这将有助于改善 HBase 随机读取配置文件。Hadoop 2.x 还包括重要的 bug 修复，这些改进将改善我们 HBase 整体体验。HBase 不支持与 Hadoop 的早期版本一起运行。 由于 HBase 依赖 Hadoop，它配套发布了一个 Hadoop jar 文件在它的 lib 目录下。该套装 jar 仅用于独立模式。在分布式模式下，Hadoop 版本必须和 HBase 下的版本一致。用你运行的分布式 Hadoop 版本 jar 文件替换 HBase lib 目录下的 Hadoop jar 文件，以避免版本不匹配问题。确认替换了集群中所有 HBase 下的 jar 文件。Hadoop 版本不匹配问题有不同表现，但看起来都像挂掉了。 1.2.1 dfs.datanode.max.transfer.threads一个 Hadoop HDFS Datanode 有一个同时处理文件的上限。进行任何加载之前，请确保已配置 Hadoop 的 conf/hdfs-site.xml 文件，并将 dfs.datanode.max.transfer.threads 值至少设置为以下值：&lt;property&gt; &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;&lt;/property&gt; 完成上述配置后，请务必重新启动HDFS。如果没有进行配置，则会导致如下奇怪异常。例如：10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry... 另请参阅 casestudies.max.transfer.threads，并请注意，此属性以前称为 dfs.datanode.max.xcievers（例如，Hadoop HDFS: Deceived by Xciever）。 1.3 ZooKeeper使用 ZooKeeper 3.4.x 以上版本。ZooKeeper 也可以不用自己安装，使用内置的 ZooKeeper。 2. 下载解压HBase选择一个 Apache 下载镜像，下载 HBase Releases。下载后缀为 .tar.gz 的文件，例如，hbase-2.1.6-bin.tar.gz。解压缩到我们的工作目录 ~/opt:tar -zxvf hbase-2.1.6-bin.tar.gz -C ~/opt/ 为了便于以后的升级，我们需要创建一个软连接:ln -s hbase-2.1.6 hbase 修改 /etc/profile 配置环境变量：export HBASE_HOME=/Users/smartsi/opt/hbaseexport PATH=$&#123;HBASE_HOME&#125;/bin:$PATH 3. 运行模式HBase 有两种运行模式：独立运行和分布式集群运行。开箱即用，HBase 以独立模式运行。无论使用哪种模式，都需要通过编辑 HBase conf 目录中的文件来配置 HBase。至少，我们必须编辑 conf/hbase-env.sh 以告知 HBase 使用的 Java。在此文件中，我们需要设置 HBase 环境变量，例如 JVM 的堆大小已经其他选项等。将 JAVA_HOME 设置为指向 Java 安装的根目录。 3.1 独立运行模式这是默认部署模式。在独立模式下，HBase 不使用 HDFS，而是使用本地文件系统。所有 HBase 守护程序和本地 ZooKeeper 都运作在一个 JVM 中。ZooKeeper 监听一个端口，这样客户端就可以连接 HBase 了。 我们经常使用的独立运行模式是，不是持久化到本地文件系统，而是持久化到 HDFS 实例。要配置此独立模式，需要编辑 hbase-site.xml 文件配置 hbase.rootdir 以指向 HDFS 实例中的目录，然后将 hbase.cluster.distributed 设置为 false。例如：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.2 伪分布式集群模式可以将分布式模式细分为 伪分布式模式，所有守护程序都在单个节点上运行。 全分布式模式，完全分布在集群中的所有节点上。 伪分布式模式可以针对本地文件系统运行，也可以针对 Hadoop 分布式文件系统（HDFS）实例运行。全分布式模式只能在 HDFS 上运行。 伪分布式模式意味着 HBase 仍完全在单个节点上运行，但是每个 HBase 守护程序（HMaster，HRegionServer和ZooKeeper）作为单独的进程运行：在独立模式下，所有守护程序都在一个 JVM 进程/实例中运行。 4. 配置在启动 HBase 之前，必须设置 JAVA_HOME 环境变量。为了使此操作更容易，HBase 允许我们在 conf/hbase-env.sh 文件中进行设置。我们必须找到机器上 Java 安装的位置，如果我们不知道其位置，可以使用 whereis java 命令寻找。找到位置后，编辑 conf/hbase-env.sh 文件，并取消注释以 #export JAVA_HOME = 开头的行，然后将其设置为 Java 安装路径。 配置 hbase-env.sh：# JAVA安装路径export JAVA_HOME=$&#123;JAVA_HOME&#125;# HBase的日志路径，默认为logs目录，可以不用配置export HBASE_LOG_DIR=$&#123;HBASE_HOME&#125;/logs# HBase的pids目录export HBASE_PID_DIR=$&#123;HBASE_HOME&#125;/pids# 是否使用外部zk，true表示使用自带zkexport HBASE_MANAGES_ZK=false 编辑 conf/hbase-site.xml，这是主要的 HBase 配置文件。首先，添加以下属性，该属性指示 HBase 以分布式模式运行，每个守护程序一个 JVM 实例:&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 接下来，使用 hdfs:////URI 语法将 hbase.rootdir 从本地文件系统更改为 HDFS 实例的地址。在此示例中，HDFS 在本地主机上的 9000 端口上运行。请确保删除 hbase.unsafe.stream.capability.enforce 的条目或将其设置为 true:&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;&lt;/property&gt; 我们无需在 HDFS 中创建 hbase 目录。HBase 会为我们完成此工作。如果我们自己创建了目录，那么 HBase 将尝试进行迁移，这并不是我们想要的。 hbase.rootdir 里面的 HDFS 地址是要跟 Hadoop 的 core-site.xml 里面的 fs.defaultFS 的 HDFS 的 IP 地址或者域名、端口必须一致 实际上，我们应该仔细考虑我们的 ZooKeeper 配置。此配置将指导 HBase 在集群的每个节点上启动和管理 ZooKeeper 实例。我们需要在本地文件系统上指定 ZooKeeper 写入数据的目录，ZooKeeper的 zoo.conf 中的配置（dataDir所设定的位置）：&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/Users/smartsi/opt/zookeeper/data&lt;/value&gt;&lt;/property&gt; 在这里我们使用外部 zookeeper，如果使用外部 ZK，hbase-env.sh 中属性 HBASE_MANAGES_ZK 必须设置为false，ZK数量必须为奇数，多个可用逗号分隔：&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt; 服务器在重启时都要删除 /tmp 目录下的内容，因此我们应该将数据存储在其他位置。以下配置会将 HBase 的数据存储在名为 smartsi 的用户的主目录中的 opt/hbase/tmp 目录下:&lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/Users/smartsi/opt/hbase/tmp&lt;/value&gt;&lt;/property&gt; 配置hbase-site.xml：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/Users/smartsi/opt/hbase/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/Users/smartsi/opt/zookeeper/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. 运行确保首先运行 HDFS。通过在 HADOOP_HOME 目录中运行 bin/start-hdfs.sh 来启动和停止 Hadoop HDFS 守护程序。我们可以通过将文件上传到 Hadoop 文件系统中以及获取上传内容来确保 HDFS 正确启动。HBase 通常不使用 MapReduce 或 YARN 守护程序。这些可以不需要启动。 如果我们使用的是自己的 ZooKeeper，需要启动并确认其正常运行，否则 HBase 将在启动过程中为我们启动内置的 ZooKeeper。 使用以下命令启动 HBase：bin/start-hbase.sh 在 HBASE_HOME目录下运行以上命令。 使用上述命令启动 HBase。如果系统配置正确，那么 jps 命令应显示正在运行的 HMaster 和 HRegionServer 进程。现在，我们应该有一个正在运行的 HBase 实例。HBase 日志可在 logs 子目录中找到。 默认情况下，Web UI 部署在 Master 上的 16010 端口上（HBase RegionServers 默认在 16020 端口上侦听，并在 16030 端口上建立一个信息性HTTP服务器）。如果 Master 在默认端口上的名为 master.example.org 的主机上运行，可以访问 http://master.example.org:16010 以查看Web界面。 使用以下命令停止 HBase：bin/stop-hbase.sh 停止可能需要一些时间才能完成。如果集群由许多机器组成，那么可能需要更长的时间。如果我们正在运行分布式操作，请确保等到 HBase 完全关闭后再停止 Hadoop 守护程序。 6. 验证我们可以使用 jps 命令来验证我们是否有名为 HMaster、HRegionServer 的正在运行的进程:smartsi:hadoop smartsi$ jps47876 HRegionServer47790 HMaster... 我们可以通过浏览器访问 http://localhost:16010 来查看HBase Web UI。 另外我们还可以检查 HDFS 中的 HBase 目录。如果一切正常，HBase 将在 HDFS 中创建其目录。在上面的配置中，它存储在 HDFS 上的 /hbase/ 目录中。我们可以在 Hadoop 的 bin/ 目录中使用 hadoop fs 命令列出该目录:smartsi:hadoop smartsi$ hadoop fs -ls /hbaseFound 12 itemsdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/.hbckdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/.tmpdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/MasterProcWALsdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/WALsdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/archivedrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/corruptdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/data-rw-r--r-- 1 smartsi supergroup 42 2019-10-07 17:35 /hbase/hbase.id-rw-r--r-- 1 smartsi supergroup 7 2019-10-07 17:35 /hbase/hbase.versiondrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/mobdirdrwxr-xr-x - smartsi supergroup 0 2019-10-07 17:35 /hbase/oldWALsdrwx--x--x - smartsi supergroup 0 2019-10-07 17:35 /hbase/staging 7. 使用使用 hbase shell 命令连接到正在运行的 HBase 实例，该命令位于 HBase 安装目录的 bin/ 目录中。在此示例中，省略了启动 HBase Shell 时打印的一些用法和版本信息。HBase Shell 提示符以&gt;字符结束：smartsi:hbase smartsi$ ./bin/hbase shellhbase(main):001:0&gt; 使用 create 命令创建一个新表。我们必须指定表名称和列族名称：hbase(main):001:0&gt; create &apos;test&apos;, &apos;cf&apos;Created table testTook 1.2144 seconds=&gt; Hbase::Table - testhbase(main):002:0&gt; 使用 list 命令确认表是否存在：hbase(main):002:0&gt; list &apos;test&apos;TABLEtest1 row(s)Took 0.0378 seconds=&gt; [&quot;test&quot;] 使用 put 命令将数据放入表中：hbase(main):005:0&gt; put &apos;test&apos;, &apos;row1&apos;, &apos;cf:a&apos;, &apos;value1&apos;Took 0.1687 seconds 从 HBase 获取数据的一种方法是扫描。使用 scan 命令扫描表中的数据：hbase(main):008:0&gt; scan &apos;test&apos;ROW COLUMN+CELL row1 column=cf:a, timestamp=1570442045109, value=value11 row(s)Took 0.0038 seconds 欢迎关注我的公众号和博客： 参考: Apache HBase ™ Reference Guide]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper ACL权限控制机制]]></title>
    <url>%2Fzookeeper-acl-access-permission-control-mechanism.html</url>
    <content type="text"><![CDATA[ZooKeeper 的 ACL 权限控制和 Unix/Linux 操作系统的ACL有一些区别，我们可以从三个方面来理解 ACL 机制，分别是：权限模式(Scheme)、授权对象(ID)和权限(Permission)，通常使用 scheme:id:perm 来标识一个有效的ACL信息。 需要注意的是，ACL仅与指定 ZNode 有关，不适用于子节点。例如，如果 /app 节点仅可由 ip:172.16.16.1 读取，而 /app/status 设置为 world 模式，那么任何人都可以读取 /app/status。不跟我们想象一样，ACL不是递归的。 1. 权限权限(perm)就是指那些通过权限检查后可以被允许执行的操作。在 ZooKeeper 中，所有对数据的操作权限分为以下五大类： 权限 ACL简写 描述 CREATE C 子节点的创建权限，允许授权对象在该数据节点下创建子节点。 DELETE D 子节点的删除权限，允许授权对象删除该数据节点的子节点。 READ R 数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等。 WRITE W 数据节点的更新权限，允许授权对象对该数据节点进行更新操作。 ADMIN A 数据节点的管理权限，允许授权对象对该数据节点进行ACL相关的设置操作。 2. 授权对象授权对象(id)指的是权限赋予的用户或一个指定实体，例如IP地址或是机器等。在不同的权限模式下，授权对象是不同的，下表中列出了各个权限模式和授权对象之间的对应关系。 权限模式 授权对象Id IP 通常是一个IP地址或是IP段，例如 ‘192.168.0.110”或“192.168.0.1/24’ Digest 自定义，通常是 ‘username:BASE64(SHA-1(username:password))’，例如 ‘user2:lo/iTtNMP+gEZlpUNaCqLYO3i5U=’ World 只有一个ID：’anyone’ Auth 该模式不关注授权对象，但必须有 Super 与Digest模式一致 3. ACL管理权限相关命令: 命令 使用方式 描述 getAcl getAcl 读取 ACL 权限 setAcl setAcl 设置 ACL 权限 addauth addauth 添加认证用户 3.1 设置ACL通过 zkCli 脚本登录 ZooKeeper 服务器后，可以通过两种方式进行 ACL 的设置。一种是在数据节点创建的同时进行 ACL 权限的设置，命名格式如下：create [-s] [-e] path data acl 具体使用如下所示:[zk: 127.0.0.1:2181(CONNECTED) 1] create -e /test/auth-node &apos;auth scheme node&apos; digest:user1:XDkd2dsEuhc9ImU3q8pa8UOdtpI=:cwrdaCreated /test/auth-node 另一种方式则是使用 setAcl 命名单独对已经存在的数据节点进行 ACL 设置：setAcl path acl 具体使用如下所示:[zk: 127.0.0.1:2181(CONNECTED) 2] setAcl /test/auth-node auth:id:crdwacZxid = 0x202dctime = Sun Sep 22 20:31:31 CST 2019mZxid = 0x202dmtime = Sun Sep 22 20:31:31 CST 2019pZxid = 0x202dcversion = 0dataVersion = 0aclVersion = 1ephemeralOwner = 0x100009088560218dataLength = 16numChildren = 0 3.2 获取ACL适用如下方式获取ACL信息：getAcl &lt;path&gt; 具体使用如下所示：[zk: 127.0.0.1:2181(CONNECTED) 3] getAcl /test/auth-node&apos;digest,&apos;user1:XDkd2dsEuhc9ImU3q8pa8UOdtpI=: cdrwa 4. 权限模式权限模式用来确定权限验证中的校验策略。在 ZooKeeper 中，开发人员使用最多的就是以下五种权限模式。 4.1 World模式World 是一种最开放的权限控制模式，从其名字中也可以看出，事实上这种权限控制方式几乎没有任何作用，数据节点的访问权限对所有用户开放，即所有用户都可以在不进行任何权限校验的情况下操作 ZooKeeper 上的数据。另外，World 模式也可以看作是一种特殊的 Digest 模式，它只有一个权限标识，即 world:anyone。World 模式只有一个授权对象(anyone)，表示世界上任意用户。 我们使用如下命令来设置任何人都可以访问的节点：setAcl /&lt;node-name&gt; world:anyone:crdwa 通过正确执行上述操作，我们可以得到如下所示输出： world模式创建节点的默认模式。 4.2 IP模式IP 模式表示有相同 IP 地址的任何用户，通过 IP 地址粒度来进行权限控制，例如配置了ip:192.168.0.110，即表示权限控制都是针对这个IP地址的。同时，IP 模式也支持按照网段的方式进行配置，例如ip:192.168.0.1/24表示针对192.168.0.*这个IP段进行权限控制。以下是使用 IP 模式的 setAcl 的语法：setAcl /&lt;node-name&gt; ip:&lt;IPv4-address&gt;:&lt;permission-set&gt; 使用上面的语法，下面是使用 127.0.0.1 IP地址的示例：setAcl /test/ip-node ip:127.0.0.1:crdwa 通过正确执行上述操作，我们可以得到如下所示输出： 4.3 Digest模式Digest 是最常用的权限控制模式，也更符合我们对于权限控制的认识，其类似于 username:password 形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。 当我们通过 username:password 形式配置了权限标识。这里的密码是密文，ZooKeeper 会对其先后进行两次编码处理，分别是SHA-1算法加密和BASE64编码，在 SHELL 中可以通过以下命令计算：echo -n &lt;user&gt;:&lt;password&gt; | openssl dgst -binary -sha1 | openssl base64 先来算一个密文密码：smartsi:SmartSi smartsi$ echo -n user-1:password-1 | openssl dgst -binary -sha1 | openssl base641g4T1B5w+se9ntA6Ckp90uPaJ30= 以下是使用 Digest 模式的 setAcl 的语法：setAcl /&lt;node-name&gt; digest:&lt;user-name&gt;:&lt;password&gt;:&lt;permission&gt; 使用上面的语法，下面是使用 user-1:password-1 示例：setAcl /test/digest-node-1 digest:user-1:1g4T1B5w+se9ntA6Ckp90uPaJ30=:crdwa 1g4T1B5w+se9ntA6Ckp90uPaJ30= 为 password-1 对应的密文。 通过正确执行上述操作，我们可以得到如下所示输出： 注意的是虽然可以使用明文密码 user-1:password-1 设置 ACL(实际上 ZooKeeper 认为 password-1 是两次编码处理的密文)，但是获取内容时会没有权限： 4.4 Auth模式Auth 是一种特殊模式，不会对任何授权对象ID进行授权，而是对所有已经添加认证的用户进行授权。持久化 ACL 信息时，ZooKeeper 服务器会忽略 scheme:id:perm 中提供的任何授权对象表达式。但是仍必须在 ACL 中提供表达式，可以是一个空串’’，或者其他任意字符串，因为 ACL 必须与 scheme:id:permission 格式匹配:[zk: 127.0.0.1:2181(CONNECTED) 9] setAcl /test/auth-node auth:crdwaauth:crdwa does not have the form scheme:id:perm 如果没有添加身份认证的用户，那么使用 Auth 模式设置 ACL 会报错。如下所示在我没有提供任何认证用户的情况下：[zk: 127.0.0.1:2181(CONNECTED) 3] setAcl /test/auth-node auth::crdwaAcl is not valid : /test/auth-node 使用此模式的正确用法如下：setAcl /&lt;node-name&gt; auth:&lt;id&gt;:&lt;permission&gt; 使用此模式我们可以给多个授权用户使用不同的用户名和密码访问 ZNode。假设我们有3个用户： user-1:password-1 user-2:password-2 user-3:password-3 要注意的一件事是，必须先使用 addauth 命令，然后才能继续使用 Auth 模式设置 ACL。如果我们在执行 addauth 命令之前设置ACL，就会像上述所示抛出异常：Acl is not valid : /test/auth-node 正确的方法是先执行 addauth 命令，然后再执行 setAcl 命令。以下是 addauth 的命令执行语法：addauth digest &lt;username&gt;:&lt;password&gt; 通过添加认证用户并相应地设置ACL，可以确保正确设置ACL： 在其他窗口使用对其用户名和密码组合重复上述步骤如下所示： 5. Super模式Super模式，顾名思义就是超级用户的意思，为管理员所使用，这也是一种特殊的 Digest 模式。在 Super 模式下，超级用户可以对任意 ZooKeeper 上的数据节点进行任何操作，不会被任何节点的 ACL 所限制。 根据ACL权限控制的原理，一旦对一个数据节点设置了 ACL 权限控制，那么其他没有被授权的 ZooKeeper 客户端将无法访问该数据节点，这的确很好的保证了 ZooKeeper 的数据安全。但同时，ACL 权限控制也给 ZooKeeper 的运维人员带来了一个困扰：如果一个持久数据节点包含了 ACL 权限控制，而其创建者客户端已经退出或已不再使用，那么这些数据节点该如何清理呢？这个时候，就需要在 ACL 的 Super 模式下，使用超级管理员权限来进行处理了。要使用超级管理员权限，首先需要在 ZooKeeper 服务器上开启 Super 模式，方法是在 ZooKeeper 服务器启动的时候，添加如下系统属性：-Dzookeeper.DigestAuthenticationProvider.superDigest=super:zUZ0bpqYS7FucDXsnUgxOWTto1s= 其中，super 代表了一个超级管理员的用户名；zUZ0bpqYS7FucDXsnUgxOWTto1s= 是由 ZooKeeper 的系统管理员自主配置密码的两次编码处理后的密文，此例中使用的是 super:root 的编码。 打开 ZooKeeper 目录下的 /bin/zkServer.sh 服务器脚本，找到如下一行：nohup $JAVA &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; 默认只有以上两个配置项，我们需要把上述配置项加到服务器脚本中：nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.DigestAuthenticationProvider.superDigest=admin:0sxEug2Dpm/NpzMPieOlFREd9Ao=&quot; \ -cp &quot;$CLASSPATH&quot; $JVMFLAGS $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp; 完成对 ZooKeeper 服务器的 Super 模式的开启后，重新启动服务器后就可以在应用程序中使用了，下面是一个使用超级管理员权限操作 ZooKeeper 数据节点的示例程序： 从上面输出结果中，我们可以看出，由于 super:root 是一个超级管理员，因此能够对一个受权限控制的数据节点 /test/auth-node 随意进行操作。但是 user-3:password-3 这个普通用户，就无法通过权限验证了。 参考: Apache ZooKeeper – Setting ACL in ZooKeeper Client]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 伪集群模式安装与启动]]></title>
    <url>%2Fzookeeper-standalone-setup-and-run.html</url>
    <content type="text"><![CDATA[1. 安装要在你的计算机上安装 ZooKeeper 框架，请访问该链接并下载最新版本的ZooKeeper。到目前为止，最新稳定版本的 ZooKeeper是3.4.12(ZooKeeper-3.4.12.tar.gz)。 使用以下命令提取tar文件：cd ~/opt/$ tar -zxf zookeeper-3.4.12.tar.gz 创建软连接，便于升级：$ sudo ln -s zookeeper-3.4.12/ zookeeper 创建数据目录：$ cd zookeeper$ mkdir data 2. 配置如果我们手上只有一台机器，那么我们可以作为单机模式进行部署，资源明显有点浪费。如果我们按照集群模式部署的话，那么就需要借助硬件上的虚拟化技术，把一台物理机转换成几台虚拟机，不过这样的操作成本太高。幸运的是，和其他所有分布式系统一样，ZooKeeper 也允许我们在一台机器上完成一个伪集群的搭建。 伪集群就是说集群所有的机器都在一台机器上，但是还是以集群的特性对外提供服务。这种模式和集群模式非常类似，只是把 zoo.cfg 做一些修改:tickTime = 2000dataDir = /Users/smartsi/opt/zookeeper/dataclientPort = 2181initLimit = 10syncLimit = 5server.1=127.0.0.1:2888:3888 server.2=127.0.0.1:2889:3889server.3=127.0.0.1:2890:3890 在 zoo.cfg 配置中，每一行的机器配置都是同一个IP地址，但是后面的端口号配置已经不一样了。因为在同一台机器上启动多个进程，就必须绑定不同的端口。 说明： 参数 默认值 描述 initLimit 10 对于从节点最初连接到主节点时的超时时间，单位为tick值的倍数。 syncLimit 5 对于主节点与从节点进行同步操作时的超时时间，单位为tick值的倍数。 dataDir /tmp/zookeeper 用于配置内存数据库保存的模糊快照的目录。文件信息都存放在data目录下。 clientPort 2181 表示客户端所连接的服务器监听的端口号，默认是2181。即zookeeper对外提供访问的端口号。 server.A=B:C:D 其中A是一个数字，表示这是第几号服务器。B是这台服务器的IP地址。C表示这台服务器与集群中的Leader服务器交换信息的端口。D表示的是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于B都是一样，所以不同的zookeeper实例通信端口号不能一样，所以要给他们分配不同的端口号。 第一台机器的配置如下：tickTime = 2000dataDir = /Users/smartsi/opt/zookeeper/zk1/dataclientPort = 2181initLimit = 10syncLimit = 5server.1=127.0.0.1:2888:3888 server.2=127.0.0.1:2889:3889server.3=127.0.0.1:2890:3890 第二台机器的配置如下：tickTime = 2000dataDir = /Users/smartsi/opt/zookeeper/zk2/dataclientPort = 2182initLimit = 10syncLimit = 5server.1=127.0.0.1:2888:3888 server.2=127.0.0.1:2889:3889server.3=127.0.0.1:2890:3890 第三台机器的配置如下：tickTime = 2000dataDir = /Users/smartsi/opt/zookeeper/zk3/dataclientPort = 2183initLimit = 10syncLimit = 5server.1=127.0.0.1:2888:3888 server.2=127.0.0.1:2889:3889server.3=127.0.0.1:2890:3890 3. 创建myid文件分别在对应 data 目录下新建 myid 文件，并进行修改： 在第一台机器对应的文件中输入1 在第一台机器对应的文件中输入2 在第一台机器对应的文件中输入3 需要确保每台服务器的 myid 文件中数字不同，并且和自己所在机器的 zoo.cfg 中 server.id=host:port:port 的id值一样。另外，id的范围是1～255。 4. 配置环境变量修改 /etc/profile 配置环境变量：# ZOOKEEPERexport ZOOKEEPER_HOME=/Users/smartsi/opt/zookeeperexport PATH=$&#123;ZOOKEEPER_HOME&#125;/bin:$PATH 运行命令 source /etc/profile 使环境变量生效。 5. 启动ZooKeeper分别启动三台 ZooKeeper 服务器，在根目录下执行如下命令：bin/zkServer.sh start conf/zoo1.cfgbin/zkServer.sh start conf/zoo2.cfgbin/zkServer.sh start conf/zoo3.cfg 启动过程中会输出如下信息：smartsi:zookeeper smartsi$ bin/zkServer.sh start conf/zoo1.cfgZooKeeper JMX enabled by defaultUsing config: conf/zoo1.cfgStarting zookeeper ... STARTED 当对三台服务器启动后，我们用 zkServer.sh status 命令来查看启动状态：smartsi:zookeeper smartsi$ zkServer.sh status conf/zoo3.cfgZooKeeper JMX enabled by defaultUsing config: conf/zoo3.cfgMode: followersmartsi:zookeeper smartsi$ zkServer.sh status conf/zoo1.cfgZooKeeper JMX enabled by defaultUsing config: conf/zoo1.cfgMode: followersmartsi:zookeeper smartsi$ zkServer.sh status conf/zoo2.cfgZooKeeper JMX enabled by defaultUsing config: conf/zoo2.cfgMode: leader 三台服务器会选择一台做为leader，另两台为follower。 6. 服务器验证启动完成后，可以使用如下命令来检查服务器启动是否正常：smartsi:zookeeper smartsi$ telnet 127.0.0.1 2181Trying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.statZookeeper version: 3.4.12-e5259e437540f349646870ea94dc2658c4e44b3b, built on 03/27/2018 03:55 GMTClients: /127.0.0.1:51068[0](queued=0,recved=1,sent=0)Latency min/avg/max: 0/0/0Received: 465Sent: 2Connections: 1Outstanding: 0Zxid: 0x0Mode: followerNode count: 4Connection closed by foreign host. 上面就是通过 telnet 方式，使用 stat 命令进行服务器启动的验证，如果出现和上面类似的输出信息，就说明服务器已经正常启动了。 集群模式和单机模式下输出的服务器验证信息基本一致，只有Mode属性不一样，在集群模式中，Mode 显示的是follower，或者 leader。在单机模式中，Mode 显示的是 standalone。 7. 连接到ZooKeeper使用如下命令即可连接到其中一台ZooKeeper服务器：smartsi:bin smartsi$ zkCli.sh -server 127.0.0.1:2181Connecting to 127.0.0.1:2181Welcome to ZooKeeper!JLine support is enabledWATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 127.0.0.1:2181(CONNECTED) 0] 其他自动实现同步，客户端只需要和一台保持连接即可。 成功连接后，系统会输出 ZooKeeper 的相关配置信息和相关环境，并在屏幕上输出 Welcome to ZooKeeper! 等信息。 欢迎关注我的公众号和博客：]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解CAP定理]]></title>
    <url>%2Fan-illustrated-proof-of-the-cap-theorem.html</url>
    <content type="text"><![CDATA[CAP 定理是分布式系统中的一个重要的基本定理，指出任何分布式系统最多只能具有以下三个属性中的其中两个： Consistency(一致性) Availability(可用性) Partition tolerance(分区容错) 1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有上述三个指标。它们的第一个字母分别是 C、A、P。Eric Brewer 说，这三个指标不可能同时做到，因此这个结论就叫做 CAP 定理。 1. 什么是CAP定理CAP定理指出任何分布式系统不可能同时保持一致，可用性以及分区容错性。听起来很简单，但是保持一致性意味着什么？保持可用性呢？保持分区容忍呢？分布式系统到底意味着什么？ 在这篇文章中，我们将介绍一个简单的分布式系统，并说明如果系统保持一致性、可用性以及分区容错性意味着什么。 2. 分布式系统让我们来考虑一个非常简单的分布式系统。我们的分布式系统由两台服务器G1和G2组成；这两台服务器都追踪同一个变量v，变量v的初始值为v0；G1和G2之间可以相互通信，同样也可以与外部的客户端通信；我们的分布式系统的架构如下图所示： 客户端可以向任何服务器发送读写请求。当服务器接收到请求之后，将根据请求执行一些计算，然后把请求结果返回给客户端。一个写请求过程如下所示： 下面是一个读请求过程： 现在我们已经建立好我们的分布式系统，下面我们一起探讨一下分布式系统的一致性、可用性以及分区容错性的含义。 3. 一致性在 Gilbert 和 Lynch 论文中一致性的描述为：在写操作完成之后开始的任何读操作必须返回写操作的值，或者更后续写操作的结果值。 any read operation that begins after a write operation completes must return that value, or the result of a later write operation. 这就意味着在一个一致性的分布式系统中，客户端向任何服务器发起一个写请求，将一个值写入服务器，那么之后向任何服务器（不一定是修改值的服务器）发起的读请求，都必须读取到这个值（或者更新的值）。 下面是一个非一致性的分布式系统的例子: 上图中客户端向G1服务器发起写请求，将变量v的值从v0更新为v1，并得到G1服务器的确认响应。但当向G2服务器读取变量v的值时，读取到的却是旧的值v0，与期待的v1不一致。 下面是一个一致性的分布式系统的例子: 在这个系统中，G1在给客户端发送确认之前，会先把v的新值复制给G2，这样，当客户端从G2读取v的值时就能读取到最新的值v1。 4. 可用性在 Gilbert 和 Lynch 论文中可用性描述为：系统中非故障节点收到的每个请求都必须返回响应。 every request received by a non-failing node in the system must result in a response. 在一个可用性的分布式系统中，如果我们的客户端向服务器发送请求并且服务器没有崩溃，那么服务器最终必须返回响应给客户端。不允许服务器忽略客户端的请求。 5. 分区容错性在 Gilbert 和 Lynch 论文中分区容错性描述为：从一个节点发送到另一节点的过程中网络允许任意消息的丢失。 the network will be allowed to lose arbitrarily many messages sent from one node to another. 这就意味着服务器G1和G2之间互相发送的任意消息都可能丢失。如果所有的消息都丢失了，那么我们的系统就变成了如下所示： 与第一个图相比，两个节点之间缺少了联系。 为了满足分区容错性，我们的系统需要能在网络分区情况下也能正常的工作。 Network Partition: 网络分区（网络分裂） 6. 证明现在我们已经了解了一致性、可用性和分区容错性的含义，现在我们来证明一个系统不可能同时满足这三个属性。 假设存在一个同时满足这三个属性的系统，我们要做的第一件就是让系统发生网络分区，就像下图的情况一样： 接下来，我们有一个客户端向G1发起写请求，将v的值更新为v1。因为系统是可用的，所以G1必须给客户端发送响应，但是由于网络分区，G1无法将其数据复制到G2。 接着，客户端向G2发起一个读请求，因为系统是可用的，所以G2必须给客户端返回响应。又由于网络分区的，G2无法从G1更新v的值，所以G2返回给客户端的是旧的值v0。 客户端已经将G1上v的值修改为v1，但是从G2上读取到的值仍然是v0，这违背了一致性。 我们假设存在一个满足一致性、可用性、分区容错性的分布式系统，但是在出现网络分区等情况下，系统会表现出不一致的行为，因此证明不存在这样一个同时满足一致性、可用性、分区容错性的系统。 7. A还是C如果保证G2的一致性，那么G1必须在写操作时，锁定G2的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2不能读写，所以不能提供可用性；如果保证G2的可用性，那么势必不能锁定G2，所以一致性不成立。 综上所述，G2无法同时做到一致性和可用性。系统设计时只能选择一个。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。 所以，对于一个分布式系统来说，P是一个基本要求，CAP三者中，只能根据系统要求在A和C两者之间做权衡。 欢迎关注我的公众号和博客： 英译对照: Consistency：一致性 Availability：可用性 Partition tolerance：分区容错 原文:An Illustrated Proof of the CAP Theorem]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper可视化监控ZKUI]]></title>
    <url>%2Fzkui-zookeeper-ui-dashboard.html</url>
    <content type="text"><![CDATA[1. 简介ZKUI 提供了一个图形化管理界面，可以针对 ZooKeepr 的节点值进行 CRUD 操作，同时也提供了安全认证。 2. 要求由于 ZKUI 是基于 Java 开发的，所以需要安装 JDK。要求使用 Java 7 以上版本。 3. 安装因为 ZKUI 需要手工进行编译、构建打包，所以还需要下载安装 Maven。从代码库下载源码进行编译:# 克隆代码smartsi:software smartsi$ git clone git@github.com:DeemOpen/zkui.gitsmartsi:software cd zkui/# 编译、构建、打包smartsi:zkui smartsi$ mvn clean install 编译成功后会生成如下文件:smartsi:zkui smartsi$ lltotal 96drwxr-xr-x 15 smartsi staff 480 9 2 14:33 ./drwxr-xr-x 3 smartsi staff 96 9 2 11:44 ../drwxr-xr-x 13 smartsi staff 416 9 2 11:45 .git/-rw-r--r-- 1 smartsi staff 25 9 2 11:45 .gitignore-rw-r--r-- 1 smartsi staff 11358 9 2 11:45 LICENSE-2.0.txt-rw-r--r-- 1 smartsi staff 416 9 2 11:45 Makefile-rw-r--r-- 1 smartsi staff 6216 9 2 11:45 README.md-rw-r--r-- 1 smartsi staff 2357 9 2 11:45 config.cfgdrwxr-xr-x 5 smartsi staff 160 9 2 11:45 docker/drwxr-xr-x 8 smartsi staff 256 9 2 11:45 images/-rw-r--r-- 1 smartsi staff 1746 9 2 11:45 nbactions.xml-rw-r--r-- 1 smartsi staff 5294 9 2 11:45 pom.xml-rw-r--r-- 1 smartsi staff 43 9 2 11:45 run.shdrwxr-xr-x 4 smartsi staff 128 9 2 11:45 src/drwxr-xr-x 10 smartsi staff 320 9 2 11:46 target/ 将 zkui 下的 config.cfg 和 target 下的 zkui-2.0-SNAPSHOT-jar-with-dependencies.jar 复制到我们的工作目录下 ~/opt/zkui:smartsi:opt cp ../software/zkui/target/zkui-2.0-SNAPSHOT-jar-with-dependencies.jar zkui/smartsi:opt cp ../software/zkui/config.cfg zkui/ 4. 配置修改 config.cfg 配置:# 指定端口serverPort=9090# ZooKeeper 实例zkServer=localhost:2181,localhost:2181# 生产环境设置为 prod、开发环境设置为 dev。设置为 dev 每次会清除历史记录env=prod# MySQL 数据库配置jdbcClass=com.mysql.jdbc.DriverjdbcUrl=jdbc:mysql://localhost:3306/zkuijdbcUser=rootjdbcPwd=zxcvbnm1# 设置登录用户及其权限userSet = &#123;&quot;users&quot;: [&#123; &quot;username&quot;:&quot;admin&quot; , &quot;password&quot;:&quot;admin&quot;,&quot;role&quot;: &quot;ADMIN&quot; &#125;,&#123; &quot;username&quot;:&quot;test&quot; , &quot;password&quot;:&quot;123&quot;,&quot;role&quot;: &quot;USER&quot; &#125;]&#125; 上面配置了需要连接的 ZooKeeper 集群的IP地址和端口。多个 zk 实例以逗号进行分割。例如：server1:2181, server2:2181。第一台服务器应始终是领导者。ZKUI 默认的用户名与密码是 admin/manager ，上面配置中我们修改为 admin/admin，同时修改用户 appconfig 的账号密码为 test/123。在这我们使用 MySQL 存储历史记录，必须注释掉 h2 数据库配置。 5. 运行现在使用如下命令启动 ZKUI:nohup java -jar zkui-2.0-SNAPSHOT-jar-with-dependencies.jar ＆ 现在我们可以通过Web界面(http://localhost:9090)访问 ZKUI: 欢迎关注我的公众号和博客： 参考: zkui - Zookeeper UI Dashboard]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 安装与启动]]></title>
    <url>%2Fkafka-setup-and-run.html</url>
    <content type="text"><![CDATA[1. 下载代码下载 2.3.0 版本并解压缩:tar -zxvf kafka_2.12-2.3.0.tgz -C . 创建软连接便于升级:ln -s kafka_2.12-2.3.0/ kafka 配置环境变量:export KAFKA_HOME=/Users/smartsi/opt/kafkaexport PATH=$&#123;KAFKA_HOME&#125;/bin:$PATH 2. 安装ZooKeeperKafka 依赖 ZooKeeper，如果你还没有 ZooKeeper 服务器，你需要先启动一个 ZooKeeper 服务器。可以先参考ZooKeeper 安装与启动来安装 ZooKeeper。ZooKeeper 配置如下:tickTime=2000initLimit=10syncLimit=5dataDir=/Users/smartsi/opt/zookeeper/dataclientPort=2181server.1=localhost:2888:3888 你也可以通过与 kafka 打包在一起的便捷脚本来快速简单地创建一个单节点 ZooKeeper 实例:&gt; bin/zookeeper-server-start.sh config/zookeeper.properties[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)... 3. 配置Kafka第一个 broker 配置 server-9092.properties 如下:broker.id=0listeners=PLAINTEXT://127.0.0.1:9092log.dirs=/Users/smartsi/opt/kafka/logs/log1-9092zookeeper.connect=localhost:2181/kafka-2.3.0zookeeper.connection.timeout.ms=6000 运行起来至少要配置四项。上面的前四项。 第二个 broker 配置 server-9093.properties 如下:broker.id=1listeners=PLAINTEXT://127.0.0.1:9093log.dirs=/Users/smartsi/opt/kafka/logs/log-9093zookeeper.connect=localhost:2181/kafka-2.3.0zookeeper.connection.timeout.ms=6000 第三个 broker 配置 server-9094.properties 如下:broker.id=2listeners=PLAINTEXT://127.0.0.1:9094log.dirs=/Users/smartsi/opt/kafka/logs/log-9094zookeeper.connect=localhost:2181/kafka-2.3.0zookeeper.connection.timeout.ms=6000 我们必须重写端口和日志目录，因为我们在同一台机器上运行这些，我们不希望所有都在同一个端口注册，或者覆盖彼此的数据。所以用端口号9092、9093、9094分别代表三个 broker。 下面具体解释一下我们的配置项: (1) Broker相关:broker.id=0 broker 的 Id。每一个 broker 在集群中的唯一标示，要求是正数。每个 broker 都不相同。 (2) Socket服务设置:listeners=PLAINTEXT://127.0.0.1:9092 Socket服务器监听的地址，如果没有设置，则监听 java.net.InetAddress.getCanonicalHostName() 返回的地址。 (3) ZooKeeper相关:zookeeper.connect=localhost:2181/kafka-2.3.0zookeeper.connection.timeout.ms=6000 zookeeper.connect 是一个逗号分隔的 host:port 键值对，每个对应一个 zk 服务器。例如 127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002。你还可以将可选的客户端命名空间 Chroot 字符串追加到 URL 上以指定所有 kafka 的 Znode 的根目录。另外这个 kafka-2.3.0 这个节点需要你提前建立。让 Kafka 把他需要的数据结构都建立在这个节点下，否则会建立在根节点 / 节点下。 (3) 日志相关:log.dirs=/Users/smartsi/opt/kafka/logs/log-9092 Kafka存储Log的目录。 4. 启动Kafka服务器有两种方式可以启动 Kafka 服务器:# 第一种方式（推荐）bin/kafka-server-start.sh -daemon config/server.properties# 第二种方式nohup bin/kafka-server-start.sh config/server.properties &amp; 我们以第一种方式启动 Kafka 服务器:bin/kafka-server-start.sh -daemon config/server-9092.propertiesbin/kafka-server-start.sh -daemon config/server-9093.propertiesbin/kafka-server-start.sh -daemon config/server-9094.properties 查看进程和端口:smartsi:kafka smartsi$ jps8914 DataNode42802 Jps9252 NodeManager41253 Kafka41541 Kafka42790 Kafka41670 ZooKeeperMain167319164 ResourceManager1997 我们现在看一下 Kafka 在 ZooKeeper 上创建的节点:[zk: 127.0.0.1:2181(CONNECTED) 23] ls /kafka-2.3.0[cluster, controller_epoch, controller, brokers, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config] 看一下我们在ZooKeeper上注册的两个 broker:[zk: 127.0.0.1:2181(CONNECTED) 3] ls /kafka-2.3.0/brokers/ids[0, 1, 2][zk: 127.0.0.1:2181(CONNECTED) 4] get /kafka-2.3.0/brokers/ids/0&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://127.0.0.1:9092&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;127.0.0.1&quot;,&quot;timestamp&quot;:&quot;1567390121522&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;cZxid = 0x92ctime = Mon Sep 02 10:08:41 CST 2019mZxid = 0x92mtime = Mon Sep 02 10:08:41 CST 2019pZxid = 0x92cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x100009088560012dataLength = 188numChildren = 0[zk: 127.0.0.1:2181(CONNECTED) 5] get /kafka-2.3.0/brokers/ids/1&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://127.0.0.1:9093&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;127.0.0.1&quot;,&quot;timestamp&quot;:&quot;1567390128813&quot;,&quot;port&quot;:9093,&quot;version&quot;:4&#125;cZxid = 0xa7ctime = Mon Sep 02 10:08:48 CST 2019mZxid = 0xa7mtime = Mon Sep 02 10:08:48 CST 2019pZxid = 0xa7cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x100009088560014dataLength = 188numChildren = 0[zk: 127.0.0.1:2181(CONNECTED) 6] get /kafka-2.3.0/brokers/ids/2&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://127.0.0.1:9094&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;127.0.0.1&quot;,&quot;timestamp&quot;:&quot;1567390749151&quot;,&quot;port&quot;:9094,&quot;version&quot;:4&#125;cZxid = 0xbdctime = Mon Sep 02 10:19:09 CST 2019mZxid = 0xbdmtime = Mon Sep 02 10:19:09 CST 2019pZxid = 0xbdcversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x100009088560018dataLength = 188numChildren = 0 5. 测试Kafka5.1 创建Topic让我们创建一个名为 test 的 Topic，它有一个分区和一个副本：bin/kafka-topics.sh --create --zookeeper localhost:2181/kafka-2.3.0 --replication-factor 1 --partitions 1 --topic test 现在我们可以运行 list 命令来查看这个 Topic:smartsi:kafka smartsi$ bin/kafka-topics.sh --list --zookeeper localhost:2181/kafka-2.3.0test 或者，你也可将代理配置为：在发布的topic不存在时，自动创建topic，而不是手动创建。 5.2 启动生产者Kafka 自带一个命令行客户端，它从文件或标准输入中获取输入，并将其作为消息发送到 Kafka 集群。默认情况下，每行将作为单独的消息发送。 运行 Producer (生产者)，然后在控制台输入一些消息以发送到服务器:smartsi:kafka smartsi$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt;this is my first message&gt;this is my second message 5.3 启动消费者Kafka 还有一个命令行 Consumer（消费者），将消息转储到标准输出:smartsi:kafka smartsi$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningthis is my first messagethis is my second message 如果你将上述命令在不同的终端中运行，那么现在就可以将消息输入到生产者终端中，并将它们在消费终端中显示出来。 欢迎关注我的公众号和博客： 原文:Quickstart]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 如何使用Watcher]]></title>
    <url>%2Fhow-to-use-watcher-in-zookeeper.html</url>
    <content type="text"><![CDATA[1. 工作流程ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，当服务端的一些指定事件触发了这个 Watcher，那么就向指定客户端（注册了对应 Watcher 监听的客户端）发送一个事件通知来实现分布式的通知功能。整个 Watcher 注册与通知过程如下图所示: 从上图可以看出 ZooKeeper 的 Watcher 机制主要由客户端线程、客户端 WatchManager 以及 ZooKeeper 服务器三部分组成。在具体流程上，客户端在向 ZooKeeper 服务器注册 Watcher 的同时(步骤一)，会将 Watcher 对象存储在客户端的 WatchManager 上(步骤二)。当 ZooKeeper 服务器触发了 Watcher 事件后，会向客户端发送通知(步骤三)。客户端线程从 WatchManager 取出对应的 Watcher 对象来执行回调逻辑(步骤四)。 2. Watcher接口如果要想使用 Watcher 机制，我们需要实现 Watcher 接口类，实现其中的 process() 方法:public void process(WatchedEvent event); 在 ZooKeeper 中，接口类 Watcher 用于表示一个标准的事件处理器，其定义了事件通知相关的逻辑，包含 KeeperState 和 EventType 两个枚举类，分别代表了通知状态和事件类型。 2.1 Watcher类型ZooKeeper 中有两个重要的 Watcher，一个是数据监视点，另一个是子节点监视点：public static enum WatcherType &#123; // 子节点监视点 Children(1), // 数据监视点 Data(2), Any(3); ...&#125; getData() 和 exists() 可以设置数据监视点。getChildren() 可以设置子节点监视点。我们也可以根据方法返回的数据类型来判断设置的监视点类型。getData() 和 exists() 返回有关节点的数据信息，而 getChildren() 返回子节点列表。因此，可以轻松通过返回的数据类型判断监视点类型。 创建、删除或者设置一个 ZNode 节点的数据都会触发其数据监视点。子节点监视点只有在 ZNode 的子节点创建或者删除时才会被触发。setData() 会触发正在设置的 ZNode 节点的数据监视点。create() 会同时触发正在创建的 ZNode 节点的数据监视点以及父 ZNode 节点的子节点监视点。delete() 会同时触发正要删除的 Znode 节点的数据监视点、子节点监视点，以及父 ZNode 节点的子节点监视点。 2.2 通知状态与事件类型ZooKeeper 通知状态:@Publicpublic static enum KeeperState &#123; @Deprecated Unknown(-1), Disconnected(0), @Deprecated NoSyncConnected(1), SyncConnected(3), AuthFailed(4), ConnectedReadOnly(5), SaslAuthenticated(6), Expired(-112), Closed(7); ...&#125; ZooKeeper 事件类型:@Publicpublic static enum EventType &#123; NodeCreated(1), NodeDeleted(2), NodeDataChanged(3), NodeChildrenChanged(4), None(-1), DataWatchRemoved(5), ChildWatchRemoved(6); ...&#125; 前三个事件类型都只涉及单个ZNode节点，而第四个事件类型涉及监视的 ZNode 节点的子节点。 同一个事件类型在不同的通知状态中代表的含义有所不同，下表列举了常见的通知状态和事件类型: 通知状态 状态说明 事件类型 设置方法 触发条件 Unknown(-1) 从3.1.0版本开始被废弃 Disconnected(0) 客户端和服务器处于断开连接状态 None(-1) 客户端与ZooKeeper服务器断开连接 NoSyncConnected(1) 从3.1.0版本开始被废弃 SyncConnected(3) 客户端和服务器处于连接状态 None(-1) 客户端与服务器成功建立会话 SyncConnected(3) 客户端和服务器处于连接状态 NodeCreated(1) 通过exists调用设置 Watcher监听的对应数据节点被创建，通过create调用触发 SyncConnected(3) 客户端和服务器处于连接状态 NodeDeleted(2) 通过exists或者getData调用设置 Watcher监听的对应数据节点被删除，通过delete调用触发 SyncConnected(3) 客户端和服务器处于连接状态 NodeDataChanged(3) 通过exists或者getData调用设置 Watcher监听的对应数据节点的数据内容发生变更，通过setData调用触发 SyncConnected(3) 客户端和服务器处于连接状态 NodeChildrenChanged(4) 通过getChildren调用设置 Watcher监听的对应数据节点的子节点列表发生变更，通过create、delete调用触发 AuthFailed(4) 权限验证失败状态，通常同时也会收到AuthFailedException异常 None(-1) 通常有两种情况：(1)使用错误的scheme进行权限检查。(2)SASL权限检查失败。 Expired(-112) 此时客户端会话失效，通常同时也会收到SessionExpiredException异常 None(-1) 会话超时 上表中列举了 ZooKeeper 中最常见的几个通知状态和事件类型。对于 NodeDataChanged 事件类型，此处所说的变更包括节点的数据内容和数据的版本号 dataVersion 的变更。因此即使使用相同的数据内容来更新，也会触发这个事件通知，因为对于 ZooKeeper 来说，无论数据内容是否变更，一旦有客户端调用了数据更新的接口，且更新成功，就会更新 dataVersion 值。 NodeChildrenChanged 事件会在数据节点的子节点列表发生变更的时候被触发，这里说的子节点列表变化特指子节点个数和组合情况的变更，即新增子节点或删除子节点，而子节点内容的变化是不会触发这个事件的。 对于AuthFailed这个事件，需要注意的地方是，它的触发条件并不是简简单单因为当前客户端会话没有权限，而是授权失败。 2.3 回调方法ProcessProcess 方法是 Watcher 接口中的一个回调方法，当 ZooKeeper 向客户端发送一个 Watcher 事件通知时，客户单就会对相应的 process 方法进行回调，从而实现对事件的处理。process方法的定义如下：abstract public void process(WatchedEvent event); 这个回调方法的定义非常简单，我们重点看下方法的参数定义：WatchedEvent。WatchedEvent 包含了每一个事件的三个基本属性：通知状态（keeperState）、事件类型（eventType）和节点路径（path）。ZooKeeper 使用 WatchedEvent 对象来封装服务端事件并传递给 Watcher，从而方便回调方法 process 对服务端事件进行处理。 Example:public void process(WatchedEvent event) &#123; Event.KeeperState state = event.getState(); String path = event.getPath(); // 连接状态 if (state == Event.KeeperState.SyncConnected) &#123; System.out.println("客户端与ZooKeeper服务器处于连接状态"); connectedSignal.countDown(); if(event.getType() == Event.EventType.None &amp;&amp; null == event.getPath()) &#123; System.out.println("监控状态变化"); &#125; else if(event.getType() == Event.EventType.NodeCreated) &#123; System.out.println("监控到节点[" + path + "]被创建"); &#125; else if(event.getType() == Event.EventType.NodeDataChanged) &#123; System.out.println("监控到节点[" + path + "]的数据内容发生变化"); &#125; else if(event.getType() == Event.EventType.NodeDeleted) &#123; System.out.println("监控到节点[" + path + "]被删除"); &#125; &#125; // 断开连接状态 else if (state == Event.KeeperState.Disconnected)&#123; System.out.println("客户端与ZooKeeper服务器处于断开连接状态"); &#125; // 会话超时 else if (state == Event.KeeperState.Expired)&#123; System.out.println("客户端与ZooKeeper服务器会话超时"); &#125;&#125; 3. 注册Watcher我们知道创建一个 ZooKeeper 客户端对象实例时，可以向构造方法中传入一个默认的Watcher：public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher); 这个 Watcher 将作为整个 ZooKeeper 会话期间的默认 Watcher，会一直被保存在客户端 ZKWatchManager 的 defaultWatcher 中。ZooKeeper 的API中所有读操作: getData()、getChildren() 以及 exists() 都可以选择在读取的 ZNode 节点上注册 Watcher。对于 ZooKeeper 节点的事件通知，我们可以使用默认的 Watcher，也可以单独实现一个 Watcher。例如，getData调用有两种方式注册 Watcher：public byte[] getData(String path, boolean watch, Stat stat)public byte[] getData(final String path, Watcher watcher, Stat stat) 在这两个接口上都可以进行 Watcher 的注册，第一个接口通过一个 boolean 参数来标识是否使用上文提到的默认 Watcher 来进行注册，具体的注册逻辑和第二个接口是一致的。 4. Watcher特性4.1 一次性无论是服务端还是客户端，一旦一个 Watcher 被触发，ZooKeeper 都将其从相应的存储中移除。因此，开发人员在 Watcher 的使用上要记住的一点是需要反复注册。例如，如果客户端执行 getData(&quot;/znode1&quot;，true)，后面对 /znode1 的更改或删除，客户端都会获得 /znode1 的监控事件通知。如果 /znode1 再次更改，如果客户端没有执行新一次设置新监视点的读取，是不会发送监视事件通知的。 这样的设计有效地减轻了服务端的压力。试想，如果注册一个 Watcher 之后一直有效，那么，针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能的影响都非常大。 4.2 客户端串行执行客户端Watcher回调的过程是一个串行同步的过程，这为我们保证了顺序，同时，需要开发人员注意的一点是，千万不要因为一个Watcher的处理逻辑影响了整个客户端的Watcher回调。 4.3 轻量WatchedEvent 是 ZooKeeper 整个 Watcher 通知机制的最小通知单元，这个数据结构中只包含三部分内容：通知状态、事件类型和节点路径。也就是说，Watcher 通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。例如针对 NodeDataChanged 事件，ZooKeeper 的 Watcher 只会通知客户端指定数据节点的数据内容发生了变更，而对于原始数据以及变更后的新数据都无法从这个事件中直接获取到，而是需要客户端主要重新去获取数据——这也是 ZooKeeper 的 Watcher 机制的一个非常重要的特性。 另外，客户端向服务端注册 Watcher 的时候，并不会把客户端真实的 Watcher 对象传递给服务端，仅仅只是在客户端请求中使用 boolean 类型属性进行了标记，同时服务端也仅仅只是保存了当前连接的 ServerCnxn 对象。如此轻量的Watcher机制设计，在网络开销和服务端内存开销上都是非常廉价的。 欢迎关注我的公众号和博客： 英译对照: Watch: 监视点 参考： ZooKeeper分布式过程协同技术详解 从Paxos到ZooKeeper分布式一致性原理与实践]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中排除SELECT查询列]]></title>
    <url>%2Fexclude-columns-from-select-query-in-hive.html</url>
    <content type="text"><![CDATA[1. 简介在 Hive 表中可能存在很多列，也有可能就存在几列。如果我们想要表中所有列，毫无疑问我们可以使用 SELECT *。但在某些情况下，我们可能拥有 100 多列，并且我们只不需要其中几列。在这种情况下，之前都是手动的添加 SELECT 查询中的所有列名。由于列数很多，比较啰嗦。因此，我们希望能在 Hive 中从 SELECT 查询中排除某些列。 2. 方案我们可以使用正则表达式来排除某些列。如果要使用正则表达式，需要将属性 hive.support.quoted.identifiers 设置为 none。 下面是我们的样本数据。此表中一共有100多列，如下图所示(只展示了8列): 如果我们不想要 event_ts 这一列。我们会使用如下查询来排除这一列:SELECT `(event_ts)?+.+` FROM &lt;table&gt;; 上面语句等价于:SELECT user_id, event_tm, os, os_version, app_version, ..., prov, cityFROM &lt;table&gt;; 如果我们不想要 event_ts 和 event_tm 两列。我们会使用如下查询来排除这两列:SELECT `(event_ts|event_tm)?+.+` FROM &lt;table&gt;; 如果我们要排除多列，使用 | 分割。 欢迎关注我的公众号和博客：]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper日志配置]]></title>
    <url>%2Fhow-to-config-log-in-zookeeper.html</url>
    <content type="text"><![CDATA[1. 简介ZooKeeper 使用 SLF4J 作为日志的抽象层，默认使用 Log4J 来做实际的日志工作。使用两层日志抽象看起来似乎是多余的。这里简要的说明如何来配置 Log4J，虽然 Log4J 非常灵活且功能强大，但是也有一些复杂，这里只是简要的介绍一下基本的用法。 Log4J 的配置文件为 log4j.properties，系统会从 classpath 中加载这个文件。如果没有找到 log4j.properties 文件，会输出如下警告信息:log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 如果看到上述日志，那么后续所有的日志消息会被丢弃。通常 log4j.properties 文件会保存在 classpath 中的 conf 目录下。 2. 配置文件2.1 组成部分来看看 ZooKeeper 中 log4j.properties 的主要组成部分:zookeeper.root.logger=INFO, CONSOLEzookeeper.console.threshold=INFO zookeeper.log.dir=. zookeeper.log.file=zookeeper.log zookeeper.log.threshold=DEBUG zookeeper.tracelog.dir=. zookeeper.tracelog.file=zookeeper_trace.loglog4j.rootLogger=$&#123;zookeeper.root.logger&#125;log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppenderlog4j.appender.CONSOLE.Threshold=$&#123;zookeeper.console.threshold&#125;log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] -...log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppenderlog4j.appender.ROLLINGFILE.Threshold=$&#123;zookeeper.log.threshold&#125;log4j.appender.ROLLINGFILE.File=$&#123;zookeeper.log.dir&#125;/$&#123;zookeeper.log.file&#125; log4j.appender.ROLLINGFILE.MaxFileSize=10MB log4j.appender.ROLLINGFILE.MaxBackupIndex=10 log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayoutlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - ... 2.2 详解zookeeper.root.logger=INFO, CONSOLEzookeeper.console.threshold=INFO zookeeper.log.dir=. zookeeper.log.file=zookeeper.log zookeeper.log.threshold=DEBUG zookeeper.tracelog.dir=. zookeeper.tracelog.file=zookeeper_trace.log 上面配置中，所有配置项均以 zookeeper. 开头，配置了该文件的默认值，这些配置项实际是系统属性配置，可以通过 java 命令行指定 -D 参数来覆盖 JVM 的配置。第一行的日志配置中，默认配置了日志消息的级别为 INFO，即所有低于 INFO 级别的日志消息都会被丢弃，使用的 appender 为 CONSOLE。你可以指定多个 appender，例如，如果你想将日志信息同时输出到 CONSOLE 和 ROLLINGFILE 时，那么可以配置 zookeeper.root.logger 为 INFO, CONSOLE, ROLLINGFILE。 log4j.rootLogger=$&#123;zookeeper.root.logger&#125; rootLogger 指定了处理所有日志消息的级别 INFO 以及日志处理器 CONSOLE。 log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender 该行配置 CONSOLE appender 实现类 ConsoleAppender。 log4j.appender.CONSOLE.Threshold=$&#123;zookeeper.console.threshold&#125; 在 appender 的定义中也可以过滤日志消息。该行配置了这个 appender 会忽略所有低于 INFO 级别的消息，因为 zookeeper.root.logger 中定义了全局阈值为 INFO。 log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayoutlog4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - appender 使用的布局类在输出前对输出日志进行格式化操作。我们通过布局模式定义了输出日志消息外还定义了输出日志的级别、日期、线程信息和调用位置等信息。 log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender RollingFileAppender 实现了滚动日志文件的输出，而不是不断的输出到一个单独的文件或者控制台。除非 ROLLINGFILE 被 rootLogger 引用，否则该 appender 会被忽略。 log4j.appender.ROLLINGFILE.Threshold=$&#123;zookeeper.log.threshold&#125; 定义 ROLLINGFILE 的输出级别为 DEBUG，因为 rootLogger 过滤了所有低于 INFO 级别的日志，所以，你如果想看 DEBUG 消息，就必须将 zookeeper.root.logger 从 INFO 改成 DEBUG。 log4j.appender.ROLLINGFILE.File=$&#123;zookeeper.log.dir&#125;/$&#123;zookeeper.log.file&#125;log4j.appender.ROLLINGFILE.MaxFileSize=10MB log4j.appender.ROLLINGFILE.MaxBackupIndex=10 log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayoutlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] -... 上面配置设置了滚动输出日志路径以及文件最大大小。此外还使用布局类在日志输出前进行格式化操作。我们通过布局模式定义了输出日志消息外还定义了输出日志的级别、日期、线程信息和调用位置等信息。 日志记录功能会影响到进程的性能，尤其是在开启 DEBUG 级别时。同时 DEBUG 日志会提供大量有价值的信息，可以帮助我们诊断问题。 3. 修改日志输出路径当执行 zkServer.sh 时，会在执行命令的文件夹下会产生 zookeeper.out 日志文件来记录 ZooKeeper 的运行日志。这种方式会让日志文件不便于查找，对输出路径和大小不能进行控制，所以需要修改日志输出方式。 3.1 修改zkEnv.sh修改 $ZOOKEEPER_HOME/bin 目录下的 zkEnv.sh 文件，ZOO_LOG_DIR 指定日志输出的目录，ZOO_LOG4J_PROP，指定日志输出级别 INFO,ROLLINGFILE:# 以下是原配置if [ "x$&#123;ZOO_LOG_DIR&#125;" = "x" ]then ZOO_LOG_DIR="."fiif [ "x$&#123;ZOO_LOG4J_PROP&#125;" = "x" ]then ZOO_LOG4J_PROP="INFO,CONSOLE"fi# 以下是修改后配置if [ "x$&#123;ZOO_LOG_DIR&#125;" = "x" ]then ZOO_LOG_DIR="/Users/smartsi/opt/zookeeper/logs"fiif [ "x$&#123;ZOO_LOG4J_PROP&#125;" = "x" ]then ZOO_LOG4J_PROP="INFO,ROLLINGFILE"fi ZOO_LOG_DIR=&quot;${ZOOKEEPER_HOME}/logs&quot;，日志会输出都 /Users/xxx/opt/zookeeper/bin/~/opt/zookeeper/logs 目录下。需要设置为 /Users/smartsi/opt/zookeeper/logs 目录，才会正确输出到 /Users/xxx/opt/zookeeper/logs 目录下。 3.2 修改log4j.properties修改 $ZOOKEEPER_HOME/conf/log4j.properties 文件 zookeeper.root.logger 的值与前一个文件的 ZOO_LOG4J_PROP 保持一致，在这日志配置是以日志大小进行滚动:# 以下是原配置zookeeper.root.logger=INFO, CONSOLElog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender# 以下是修改后配置zookeeper.root.logger=INFO, ROLLINGFILElog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender 上述两个文件修改后，重新启动服务，ZooKeeper 会将日志文件保存到 ${ZOOKEEPER_HOME}/logs 目录下，文件名为 log4j.properties 文件中配置的 zookeeper.log。 欢迎关注我的公众号和博客： 来源:]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper Java API]]></title>
    <url>%2Fjava-api-in-zookeeper.html</url>
    <content type="text"><![CDATA[ZooKeeper API 的核心部分是 ZooKeeper 类。在构造函数中提供一些参数来连接 ZooKeeper，并提供如下方法: connect − 连接 ZooKeeper 服务器。 create − 创建一个 ZNode 节点。 exists − 检查指定的节点是否存在。 getData − 从指定的节点获取数据。 setData − 为指定的节点设置数据。 getChildren − 获取指定节点的所有子节点。 delete − 删除指定节点以及子节点。 close − 关闭连接。 ZooKeeper 大部分 API 都提供了同步和异步方法。同步方法一般会有返回值，并且会抛出相应的异常。异步方法没有返回值，也不会抛出异常。此外异步方法参数在同步方法参数的基础上，会增加 cb 和 ctx 两个参数。 1. 开发环境在工程的 pom.xml 文件中加入下面的依赖：&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.5.5&lt;/version&gt;&lt;/dependency&gt; 截止目前最新版本为 3.5.5 版本。 2. 连接服务器构造 ZooKeeper 类对象的过程就是与 ZooKeeper 服务器建立连接的过程。 2.1 语法ZooKeeper 通过构造函数连接服务器。构造函数如下所示:ZooKeeper(String connectionString, int sessionTimeout, Watcher watcher) throws IOException ZooKeeper 构造函数一共有三个参数： connectionString: 第一个参数是 ZooKeeper 服务器地址（可以指定端口，默认端口号为2181）。 sessionTimeout: 第二个参数是以毫秒为单位的会话超时时间。表示 ZooKeeper 等待客户端通信的最长时间，之后会声明会话结束。例如，我们设置为5000，即5s，这就是说如果 ZooKeeper 与客户端有5s的时间无法进行通信，ZooKeeper 就会终止客户端的会话。ZooKeeper 会话一般设置超时时间为5-10s。 watcher: 第三个参数是 Watcher 对象实例。Watcher 对象接收来自于 ZooKeeper 的回调，以获得各种事件的通知。这个对象需要我们自己创建，因为 Watcher 定义为接口，所以需要我们自己实现一个类，然后初始化这个类的实例并传入 ZooKeeper 的构造函数中。客户端使用 Watcher 接口来监控与 ZooKeeper 之间会话的健康情况。与 ZooKeeper 服务器之间建立或者断开连接时会产生事件。 2.2 Example下面会创建一个 ZooKeeperConnection 类并实现一个 connect 方法。connect 方法创建一个 ZooKeeper 对象，连接到 ZooKeeper 集群，然后返回该对象。CountDownLatch 阻塞主进程，直到客户端连接到 ZooKeeper 集群。 ZooKeeper 通过 Watcher 回调返回连接状态。一旦客户端与 ZooKeeper 集群连接，Watcher 回调函数会被调用，Watcher 回调调用 CountDownLatch的 countDown 方法释放锁。package com.zookeeper.example;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;import java.util.concurrent.CountDownLatch;// 连接ZooKeeper服务器public class ZooKeeperConnection &#123; // ZooKeeper实例 private ZooKeeper zoo; private final CountDownLatch connectedSignal = new CountDownLatch(1); // 连接服务器 public ZooKeeper connect(String host) throws IOException,InterruptedException &#123; zoo = new ZooKeeper(host,5000, new Watcher() &#123; public void process(WatchedEvent event) &#123; Event.KeeperState state = event.getState(); // 连接成功 if (state == Event.KeeperState.SyncConnected) &#123; System.out.println("与ZooKeeper服务器连接成功"); connectedSignal.countDown(); &#125; // 断开连接 else if (state == Event.KeeperState.Disconnected)&#123; System.out.println("与ZooKeeper服务器断开连接"); &#125; &#125; &#125;); connectedSignal.await(); return zoo; &#125; // 断开与服务器的连接 public void close() throws InterruptedException &#123; zoo.close(); &#125;&#125; 3. 创建Znode节点3.1 语法ZooKeeper 类提供了 create 方法，用于在 ZooKeeper 集群中创建新的 Znode。create 方法如下所示: // 同步方式String create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode) throws KeeperException, InterruptedException// 异步方式void create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode, StringCallback cb, Object ctx) 同步 create() 方法一共有四个参数: path: 第一个参数是要创建的节点路径。 data: 第二个参数是存储在指定创建节点中的数据。参数类型是字节数组。 acl: 第三个参数是创建节点的访问权限列表。ZooKeeper API提供了一个静态接口 ZooDefs.Ids 来获取一些基本的 acl 列表。 createMode: 第四个参数是创建节点的类型，可以是临时节点，也可以是顺序节点。CreateMode 是一个枚举，它有如下取值: PERSISTENT、PERSISTENT_SEQUENTIAL、EPHEMERAL、EPHEMERAL_SEQUENTIAL、CONTAINER、PERSISTENT_WITH_TTL 以及 PERSISTENT_SEQUENTIAL_WITH_TTL。 除了同步 create 方法中的四个参数以外，异步模式的 create 方法还增加了 cb 和 ctx 两个参数。 3.2 Example同步方法会返回创建节点的路径，并且会抛出相应的异常。异步方法没有返回值，也不会抛出异常。StringCallback 接口中的 processResult 方法会在节点创建好之后被调用，它有四个参数: resultCode: 第一个参数为创建节点的结果码，成功创建节点时，resultCode 的值为0。 path: 第二个参数是创建节点的路径。 ctx: 第三个参数是上下文名称，当一个 StringCallback 对象作为多个 create 方法的参数时，这个参数就很有用了。 name: 第四个参数是创建节点的名字，其实与path参数相同。 package com.zookeeper.example;import org.apache.zookeeper.*;import java.io.IOException;// 创建Znode节点public class ZKCreate &#123; // 创建ZooKeeper实例 private static ZooKeeper zk; // 创建ZooKeeperConnection实例 private static ZooKeeperConnection conn; // 同步方式创建Znode节点 public static void createNodeSync(String path, byte[] data) throws KeeperException,InterruptedException &#123; String nodePath = zk.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println("NodePath: " + nodePath); &#125; // 异步方式创建Znode节点 public static void createNodeAsync(String path, byte[] data) &#123; zk.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, new AsyncCallback.StringCallback() &#123; public void processResult(int resultCode, String path, Object ctx, String name) &#123; System.out.println("ResultCode: " + resultCode); System.out.println("Path: " + path); System.out.println("Ctx: " + ctx); System.out.println("Name: " + name); &#125; &#125;, "create_node_async" ); &#125; public static void main(String[] args) throws IOException, InterruptedException, KeeperException &#123; // 路径 String path1 = "/demo/node1"; String path2 = "/demo/node2"; // 数据 byte[] data1 = "zookeeper node1".getBytes(); byte[] data2 = "zookeeper node2".getBytes(); // 建立连接 conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); // 同步方式创建节点 createNodeSync(path1, data1); // 异步方式创建节点 createNodeAsync(path2, data2); // 断开连接 conn.close(); &#125;&#125; 编译并执行应用程序后，将在 ZooKeeper 集群中创建具有指定数据的 Znode。你可以使用 zkCli.sh进行检查:[zk: 127.0.0.1:2181(CONNECTED) 10] ls /demo[node2, node1] 4. 判断Znode是否存在4.1 语法ZooKeeper 类提供了检查 Znode 是否存在的 exists 方法。如果指定的 Znode 存在，则返回 Znode 的元数据。exists 方法如下所示:// 同步方式Stat exists(String path, Watcher watcher) throws KeeperException, InterruptedExceptionStat exists(String path, boolean watch) throws KeeperException, InterruptedException// 异步方式void exists(String path, Watcher watcher, StatCallback cb, Object ctx)void exists(String path, boolean watch, StatCallback cb, Object ctx) 同步 exists 方法一共有两个参数: path: 第一个参数是 Znode 路径。 watch或者watcher: 第二个参数是一个布尔类型的 watch或者是一个自定义的 Watcher 对象。同步模式和异步模式分别有两个方法，第一个方法传递一个新的 Watcher 对象（我们需要自定义实现）。 第二个方法使用默认 Watcher，如果开启 Watcher 只需要将第二个参数设置为 true（不需要我们自己实现）。用来监控节点数据变化以及节点的创建和删除。 除了同步 exists 方法中的两个参数以外，异步模式的 exists 方法还增加了 cb 和 ctx 两个参数。 4.2 Exampleexists 方法的 watch 参数比较特别，如果将其指定为 true，那么代表你对该节点的创建、删除以及数据内容变更都感兴趣，所以会响应三种事件类型。package com.zookeeper.example;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;import java.io.IOException;// 判断指定路径节点是否存在public class ZKExists &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // 同步方式判断指定路径的节点是否存在 public static Stat existSync(String path) throws KeeperException,InterruptedException &#123; return zk.exists(path, true); &#125; // 异步方式判断指定路径的节点是否存在 public static void existAsync(String path) &#123; zk.exists(path, true, new AsyncCallback.StatCallback() &#123; public void processResult(int resultCode, String path, Object ctx, Stat stat) &#123; System.out.println("ResultCode:" + resultCode); System.out.println("Path: " + path); System.out.println("Ctx: " + ctx); System.out.println("Stat: " + stat); &#125; &#125;, "exist_async"); &#125; public static void main(String[] args) throws InterruptedException, KeeperException, IOException &#123; String path1 = "/demo/node1"; String path2 = "/demo/node2"; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); // 同步方式判断指定路径的节点是否存在 Stat stat = existSync(path1); if(stat != null) &#123; System.out.println("Node exists and the node version is " + stat.getVersion()); &#125; else &#123; System.out.println("Node does not exists"); &#125; // 同步方式判断指定路径的节点是否存在 existAsync(path2); // 断开连接 zk.close(); &#125;&#125; 5. 获取节点数据5.1 语法ZooKeeper 类提供了 getData 方法来获取指定 Znode 中的数据以及状态。getData 方法的如下所示:// 同步方式byte[] getData(String path, Watcher watcher, Stat stat) throws KeeperException, InterruptedExceptionbyte[] getData(String path, boolean watch, Stat stat) throws KeeperException, InterruptedException// 异步方式void getData(String path, Watcher watcher, DataCallback cb, Object ctx)void getData(String path, boolean watch, DataCallback cb, Object ctx) 同步 getData 方法返回值就是节点中存储的数据值，它有三个参数: path: 第一个参数是节点的路径，用于表示要获取哪个节点中的数据。 watch或者watcher: 第二个参数是一个布尔类型的 watch或者是一个自定义的 Watcher 对象。用来监控节点数据变化以及节点是否被删除。 stat: 第三个参数用于存储节点的状态信息。 除了同步 getData 方法中的三个参数以外，异步模式的 getData 方法还增加了 cb 和 ctx 两个参数。 5.2 Example在调用 getData 方法前，会先构造一个空的 Stat 类型对象作为参数传给 getData 方法，当 getData 方法调用返回后，节点的状态信息会被填充到 stat 对象中。 值得注意的是，zooKeeper 设置的监听只生效一次，如果在接收到事件后还想继续对该节点的数据内容改变进行监听，需要在事件处理逻辑中重新调用 getData 方法并将 watch 参数设置为 true。package com.zookeeper.example;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;import java.io.IOException;// 获取指定节点中的数据public class ZKGetData &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // 同步方式获取数据 public static void getDataSync(String path) throws KeeperException, InterruptedException &#123; Stat stat = new Stat(); byte[] data = zk.getData(path, true, stat); System.out.println("Data: " + new String(data)); System.out.println("Stat: " + stat); &#125; // 异步方式获取数据 public static void getDataAsync(String path)&#123; zk.getData(path, true, new AsyncCallback.DataCallback()&#123; public void processResult(int resultCode, String path, Object ctx, byte[] data, Stat stat) &#123; System.out.println("ResultCode: " + resultCode); System.out.println("Path: " + path); System.out.println("Ctx: " + ctx); System.out.println("Data: " + new String(data)); System.out.println("Stat: " + stat); &#125; &#125;, "get_data_async"); &#125; public static void main(String[] args) throws InterruptedException, KeeperException, IOException &#123; String path1 = "/demo/node1"; String path2 = "/demo/node2"; // 连接服务器 conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); // 同步方式获取数据 getDataSync(path1); // 异步方式获取数据 getDataAsync(path2); // 断开连接 conn.close(); &#125;&#125; 6. 修改节点的数据内容6.1 语法ZooKeeper 类提供了 setData 方法来修改指定 Znode中的数据。setData 方法如下所示:// 同步方法Stat setData(String path, byte[] data, int version) throws KeeperException, InterruptedException// 异步方法void setData(String path, byte[] data, int version, StatCallback cb, Object ctx) 同步 setData 方法有三个参数: path: 第一个参数是节点的路径。 data: 第二个参数是修改的数据值。 version: 最后一个参数当前 Znode 版本。每当数据发生变化时，ZooKeeper 都会更新 Znode 的版本号。 除了同步 setData 方法中的三个参数以外，异步模式的 setData 方法还增加了 cb 和 ctx 两个参数。 6.2 Example在调用 setData 方法修改节点数据内容时，只有当 version 参数的值与节点状态信息中的 dataVersion 值相等时，数据才能修改，否则会抛出 BadVersion 异常。主要是为了防止丢失数据的更新，在 ZooKeeper 提供的API中，所有的写操作都必有 version 参数。package com.zookeeper.example;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;import java.io.IOException;// 修改节点的数据内容public class ZKSetData &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // 同步方式修改节点数据内容 public static void setDataSync(String path, byte[] data) throws KeeperException,InterruptedException &#123; zk.setData(path, data, zk.exists(path,true).getVersion()); &#125; // 异步方式修改节点数据内容 public static void setDataAsync(String path, byte[] data) throws KeeperException, InterruptedException &#123; zk.setData(path, data, zk.exists(path,true).getVersion(), new AsyncCallback.StatCallback() &#123; public void processResult(int resultCode, String path, Object ctx, Stat stat) &#123; System.out.println("ResultCode: " + resultCode); System.out.println("Path: " + path); System.out.println("Ctx: " + ctx); System.out.println("Stat: " + stat); &#125; &#125;, "set_data_async"); &#125; public static void main(String[] args) throws InterruptedException, KeeperException, IOException &#123; String path1 = "/demo/node1"; String path2 = "/demo/node2"; byte[] data1 = "zookeeper node1 update success".getBytes(); byte[] data2 = "zookeeper node2 update success".getBytes(); // 连接服务器 conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); // 同步方式修改节点数据内容 setDataSync(path1, data1); // 异步方式修改节点数据内容 setDataAsync(path2, data2); // 断开连接 conn.close(); &#125;&#125; 7. 获取节点的子节点列表7.1 语法ZooKeeper 类提供 getChildren 方法来获取指定 Znode 下的所有子节点。getChildren 方法如下所示:// 同步方式List&lt;String&gt; getChildren(String path, Watcher watcher) throws KeeperException, InterruptedExceptionList&lt;String&gt; getChildren(String path, boolean watch) throws KeeperException, InterruptedException// 异步方式void getChildren(String path, Watcher watcher, ChildrenCallback cb, Object ctx)void getChildren(String path, boolean watch, Children2Callback cb, Object ctx) 同步 getChildren 方法有两个参数: path: 第一个参数是节点路径。 watch或者watcher: 第二个参数是一个布尔类型的 watch或者是一个自定义的 Watcher 对象。用来监控节点数据变化以及节点是否被删除。用于监控节点的删除以及子节点的创建与删除操作。 除了同步 getChildren 方法中的三个参数以外，异步模式的 getChildren 方法还增加了 cb 和 ctx 两个参数。 7.2 Examplepackage com.zookeeper.example;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;import java.io.IOException;import java.util.List;// 获取节点的子节点列表public class ZKGetChildren &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // 同步方式获取节点的子节点列表 public static void getChildrenSync(String path) throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zk.getChildren(path, true); for(int i = 0; i &lt; children.size(); i++) &#123; System.out.println("Child: " + children.get(i)); &#125; &#125; // 异步方式获取节点的子节点列表 public static void getChildrenAsync(String path) &#123; zk.getChildren(path, true, new AsyncCallback.Children2Callback() &#123; public void processResult(int resultCode, String path, Object ctx, List&lt;String&gt; children, Stat stat) &#123; System.out.println("ResultCode: " + resultCode); System.out.println("Path: " + path); System.out.println("Ctx: " + ctx); System.out.println("Stat: " + stat); for(String child : children) &#123; System.out.println("Child: " + child); &#125; &#125; &#125;, "get_children_async"); &#125; public static void main(String[] args) throws InterruptedException, KeeperException, IOException &#123; String path = "/demo"; // 连接服务器 conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); // 同步方式获取节点的子节点列表 getChildrenSync(path); // 异步方式获取节点的子节点列表 getChildrenAsync(path); // 断开连接 conn.close(); &#125;&#125; 8. 删除节点8.1 语法ZooKeeper 类提供了 delete 方法来删除指定的 Znode。 delete 方法如下所示:// 同步方式void delete(String path, int version) throws InterruptedException, KeeperException// 异步方式void delete(String path, int version, VoidCallback cb, Object ctx) 同步 delete 方法有两个参数: path: 第一个参数是节点路径。 version: 最后一个参数是当前 Znode 版本。每当数据发生变化时，ZooKeeper 都会更新 Znode 的版本号。 除了同步 delete 方法中的两个参数以外，异步模式的 delete 方法还增加了 cb 和 ctx 两个参数。 8.2 Examplepackage com.zookeeper.example;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;// 删除节点public class ZKDelete &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // 同步方式删除节点 public static void deleteSync(String path) throws KeeperException,InterruptedException &#123; zk.delete(path, zk.exists(path,true).getVersion()); &#125; // 异步方式删除节点 private static void deleteAsync(String path) throws KeeperException, InterruptedException &#123; zk.delete(path, zk.exists(path,true).getVersion(), new AsyncCallback.VoidCallback() &#123; public void processResult(int resultCode, String path, Object ctx) &#123; System.out.println("ResultCode: " + resultCode); System.out.println("Path: " + path); System.out.println("Ctx: " + ctx); &#125; &#125;, "delete_async"); &#125; public static void main(String[] args) throws InterruptedException, KeeperException, IOException &#123; String path1 = "/test/node1"; String path2 = "/test/node2"; // 连接服务器 conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); // 同步方式删除节点 deleteSync(path1); // 异步方式删除节点 deleteAsync(path2); // 断开连接 conn.close(); &#125;&#125; 欢迎关注我的公众号和博客： 参考： zookeeper java api介绍 Zookeeper - API]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenTSDB 底层 HBase 的 Rowkey 是如何设计的]]></title>
    <url>%2Fhow-hbase-rowkey-is-designed-of-opentsdb.html</url>
    <content type="text"><![CDATA[OpenTSDB 是基于 HBase 的可扩展、开源时间序列数据库(Time Series Database)，可以用于存储监控数据、物联网传感器、金融K线等带有时间的数据。它的特点是能够提供最高毫秒级精度的时间序列数据存储，能够长久保存原始数据并且不失精度。它拥有很强的数据写入能力，支持大并发的数据写入，并且拥有可无限水平扩展的存储容量。目前，阿里云 HBase 产品是直接支持 OpenTSDB 组件的。 OpenTSDB 拥有如此的强大的读写和近乎无限的存储能力源自于基于 HBase 的架构设计，我们甚至可以说 OpenTSDB 就是 HBase 的一个应用。熟悉 HBase 的同学肯定知道，要看 HBase 的表设计的好不好，关键是看其 Rowkey 设计的好不好，HBase 的 Rowkey 设计会考虑到实际的查询场景。所以读到这里，大家肯定知道这篇文章是要讲什么内容的。 1. OpenTSDB 基本概念在介绍 OpenTSDB 系统如何设计 Rowkey 之前，我们先来了解 OpenTSDB 的一些基本概念。（因为本文侧重于介绍 HBase 的 Rowkey 设计，所以关于 OpenTSDB 的其他一些知识本文并不会涉及，如果你对这部分知识感兴趣，请自行去网上搜索相关文章。） 我们往 OpenTSDB 里面写入一条时序数据，至少包含以下几个数据： 指标名字：这个就是我们监控的指标，比如 sys.cpu.user； 时间戳：监控数据产生的时间； 值：Long 或者 Double 类型的数据，这个是监控指标在某个时间的具体值； 标签：包括标签名字（tagk）和标签值（tagv），比如 tagk1=tagv1，主要用于描述数据属性，每条时序数据必须包含一组和多组的标签数据。目前 OpenTSDB 最多支持8组标签。 所以如果我们使用终端往 OpenTSDB 写入时序数据，格式如下：put &lt;metric&gt; &lt;timestamp&gt; &lt;value&gt; &lt;tagk1=tagv1[ tagk2=tagv2 ...tagkN=tagvN]&gt; 比如put sys.cpu.user 1541946115 42.5 host=iteblog cpu=0 2. OpenTSDB 的 Rowkey 设计上面我们已经简单了解了 OpenTSDB 每条时序数据所包含的要素。基于这些时序数据，OpenTSDB 为我们提供的查询功能其实很简单：指定指标名称和时间范围，并且给定一个或多个标签名称和标签的值作为过滤条件，以此查询符合条件的数据。 2.1 Rowkey 设计版本一OpenTSDB 为我们提供的查询业务场景已经有了，我们可以很快设计出 HBase 的 Rowkey：metric + timestamp + tagk1 + tagv1 + tagk2 + tagv2 + ... + tagkn + tagvn 注意，实际存储的时候 + 并不会写入到磁盘，这里只是为了说明方便，人为加了这个符号。比如如果我们往 OpenTSDB 插入下面的数据:put sys.cpu.user 1541946115 42.5 host=iteblog cpu=0 那么按照上面的思路 Rowkey 应该为:sys.cpu.user+1541946115+host+iteblog+cpu+0 那如果这个指标有很多监控数据，其存储在 HBase 的 key-value 如下： 2.2 Rowkey 设计版本二上面表格记录着指标名 sys.cpu.user 标签为 host=iteblog cpu=0 和 标签为 host=iteblog cpu=1 每隔十秒的监控数据。有些同学可能已经看出来了，如果我们按照这样的方式去设计 HBase 表的 Rowkey，虽然可以满足我们的查询需求，但是这种存储数据的方式导致 Key 大量的重复存储，这样会导致数据的急剧增加，所以 OpenTSDB 并没有这样存储的。在 OpenTSDB 里面，会对每个指标名、标签以及标签值进行编码，每个指标的编码都不一样；同理，每个标签的编码也不一样，但是标签和指标名称可以编码一样，不同类型之间的编码互不影响。所以编码后的数据如下：sys.cpu.user =&gt; \x00\x00\x01host =&gt; \x00\x00\x01iteblog =&gt; \x00\x00\x01cpu =&gt; \x00\x00\x020 =&gt; \x00\x00\x021 =&gt; \x00\x00\x03 在上面，OpenTSDB 默认使用三个字节来编码指标名称，三个字节编码标签名称以及标签值。经过这样的编码之后，OpenTSDB 的 Rowkey 就变成了下面的形式：sys.cpu.user+1541946115+host+iteblog+cpu+0 变成\x00\x00\x01+1541946115+\x00\x00\x01+\x00\x00\x01+\x00\x00\x02+\x00\x00\x02 所以上表的数据就变成下面的了： 这样我们可以节省一些存储空间（不要看这张表好像比上面的表要长了，这里其实是用十六进制表示的，每个\x00占用一个字节，整个指标名称默认只占用三个字节，如果用字符串表示是不止三个字节的）。 2.3 Rowkey 设计版本三但是细心的同学肯定发现了，上表中同一个指标每隔十秒发送一条监控数据，但是每条监控数据就只是当前指标的监控值，如上表的42.5、39.1、41.4、40.0。而每次发送的数据都在 HBase 里面存储一行，这样会导致重复存储大量相同的指标名、标签名、标签值等数据。我们仔细观察可以发现，Rowkey 组成中同一个指标的监控数据除了的时间不一样，其他都是一样的！基于这个特点，OpenTSDB 对 Rowkey 进行了进一步的优化，思想为：将 Rowkey 中时间戳由原来的秒级别或毫秒级别统一转换成小时级别的，多余的秒数据或者毫秒数据作为 HBase 的列名称。可能大家没有理解这句话的含义，下面我们来具体介绍这个实现。 1541946115 时间戳转换成时间为 2018-11-11 22:21:55，其对应的整点小时为 2018-11-11 22:00:00，这个转换成时间戳是 1541944800。1541946115 相对于 1541944800 多余出来的秒数为 1315，在 HBase 里面，1315 就作为当前指标对应值的列名。经过这样的优化之后，同一小时的监控数据都放在一行的不同列里面，所以上面的表格就变成下面的了： 注意： 第三张表中为了展示方便，我将 000001+1541944800+000001+000001+000002+000003 简写为 001+1541944800+001+001+002+003； 上面几张表中的 Rowkey 部分我这里都是使用时间戳的形式显示的，只是为了查看方便，在实际存储中时间戳其实是以二进制形式存储的，比如 1541944800 的十六进制表示为 5BE835E0；所以上面表格中 Rowkey 为 001+1541944800+001+001+002+003 在 HBase 实际存储为（十六进制表示） 0000015BE835E0000001000001000002000003； 第三张表中的列名称在实际存储中除了包含相对于 Rowkey 的秒数或者毫秒数，其实还包含了当前列值的数据类型，数据长度等标识。 如果说用一张图表示上面的过程，可以如下所示: 如果想通过例子进一步了解 Rowkey 到底是如何组织以及列名称是如何组成的，可以进一步阅读通过例子剖析 OpenTSDB 的 Rowkey 及列名设计。 欢迎关注我的公众号和博客： 原文:OpenTSDB 底层 HBase 的 Rowkey 是如何设计的]]></content>
      <categories>
        <category>OpenTSDB</category>
      </categories>
      <tags>
        <tag>OpenTSDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper CLI]]></title>
    <url>%2Fhow-to-use-cki-in-zookeeper.html</url>
    <content type="text"><![CDATA[ZooKeeper 命令行界面（CLI）用于与 ZooKeeper 集合进行交互以进行开发。它有助于调试和解决不同的选项。要执行 ZooKeeper CLI 操作，首先打开 ZooKeeper 服务器（bin/zkServer.sh start），然后打开 ZooKeeper 客户端（bin/zkCli.sh）。客户端启动后，你就可以执行以下操作： 创建ZNode 获取数据 监视ZNode的变化 设置数据 创建ZNode的子节点 列出ZNode的子节点 检查状态 移除/删除ZNode 现在让我们用一个例子逐个了解上面的命令。 1. 创建ZNode用给定的路径创建一个 znode。用 flag 参数标示创建的这个 znode 是临时的，持久的还是顺序的。默认情况下，所有 znode 都是持久的。当会话过期或客户端断开连接时，临时节点（flag：-e）会被自动删除。 顺序节点保证 znode 路径必须是唯一的。 ZooKeeper 集合向 znode 路径填充10位序列号。例如，znode 路径 /myapp 将转换为 /myapp0000000001，下一个序列号为 /myapp0000000002。如果没有指定 flag，znode 被认为是持久的。 1.1 持久ZNode语法：create /path /data 示例：create /first_znode &quot;my zookeeper first znode&quot; 输出：[zk: 192.34.15.82:2181(CONNECTED) 1] create /first_znode &quot;my zookeeper first znode&quot; Created /first_znode 1.2 顺序ZNode要创建顺序节点，请添加 flag：-s，如下所示。 语法：create -s /path /data 示例：create -s /first_znode &quot;my zookeeper second znode&quot; 输出：[zk: 192.34.15.82:2181(CONNECTED) 2] create -s /first_znode &quot;my zookeeper second znode&quot;Created /first_znode0000000001 1.3 临时ZNode要创建临时节点，请添加 flag：-e，如下所示。 语法：create -e /path /data 示例：create /second_znode &quot;my zookeeper tmp znode&quot; 输出：[zk: 192.34.15.82:2181(CONNECTED) 3] create /second_znode &quot;my zookeeper tmp znode&quot;Created /second_znode 当客户端断开连接时，临时节点会被删除。你可以通过退出 ZooKeeper CLI，然后重新打开 CLI 来尝试。 2. 获取数据返回 znode 的相关数据以及指定 znode 的元数据。你会获得一些信息，例如，上次修改数据的时间，修改位置以及数据的相关信息。CLI 还用于分配监视器以显示数据相关的通知。 语法：get /path 示例：get /first_znode 输出：[zk: 192.34.15.82:2181(CONNECTED) 5] get /first_znodemy zookeeper first znodecZxid = 0x100000003ctime = Tue Aug 14 12:52:36 CST 2018mZxid = 0x100000003mtime = Tue Aug 14 12:52:36 CST 2018pZxid = 0x100000003cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 24numChildren = 0 要访问顺序节点，必须输入 znode 的完整路径。 示例：get /first_znode0000000001 输出：[zk: 10.86.218.105:2181(CONNECTED) 6] get /first_znode0000000001my zookeeper second znodecZxid = 0x100000004ctime = Tue Aug 14 12:56:15 CST 2018mZxid = 0x100000004mtime = Tue Aug 14 12:56:15 CST 2018pZxid = 0x100000004cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 25numChildren = 0 3. Watch（监视）当指定的 znode 或子 znode 发生变化时，监视器会显示通知。你只能在 get 命令中设置 watch。 语法：get /path [watch] 1 示例：get /first_znode 1 输出：[zk: 10.86.218.105:2181(CONNECTED) 7] get /first_znode 1my zookeeper first znodecZxid = 0x100000003ctime = Tue Aug 14 12:52:36 CST 2018mZxid = 0x100000003mtime = Tue Aug 14 12:52:36 CST 2018pZxid = 0x100000003cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 24numChildren = 0 输出类似于普通的 get 命令，但它会在后台等待节点改变。 4. 设置数据设置指定 znode 的数据。完成此操作后，你可以使用 get CLI 命令检查数据。 语法：set /path /data 示例：set /second_znode &quot;update my zookeeper second znode&quot; 输出：[zk: 10.86.218.105:2181(CONNECTED) 10] set /second_znode &quot;update my zookeeper second znode&quot;cZxid = 0x100000007ctime = Tue Aug 14 13:05:08 CST 2018mZxid = 0x100000008mtime = Tue Aug 14 13:05:44 CST 2018pZxid = 0x100000007cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 32numChildren = 0 如果你在 get 命令中设置了 watch 选项（如上一个命令），则输出将类似如下所示：[zk: 10.86.218.105:2181(CONNECTED) 13] set /second_znode 1WATCHER::cZxid = 0x100000007WatchedEvent state:SyncConnected type:NodeDataChanged path:/second_znodectime = Tue Aug 14 13:05:08 CST 2018mZxid = 0x100000009mtime = Tue Aug 14 13:06:54 CST 2018pZxid = 0x100000007cversion = 0dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0 5. 创建子节点创建子节点类似于创建新的 znode。唯一的区别是，子 znode 的路径必须包含父路径。 语法：create /parent/path/subnode/path /data 示例：create /first_znode/first_child_znode first_children 输出：[zk: 10.86.218.105:2181(CONNECTED) 14] create /first_znode/first_child_znode first_children Created /first_znode/first_child_znode 6. 列出子项该命令用于列出和展示子 znode。 语法：ls /path 示例：ls /first_znode 输出：[zk: 10.86.218.105:2181(CONNECTED) 15] ls /first_znode[first_child_znode] 7. 检查状态状态描述了指定的 znode 的元数据。包含了时间戳，版本号，ACL，数据长度和子 znode 等详细信息。 语法：stat /path 示例：stat /first_znode 输出：[zk: 10.86.218.105:2181(CONNECTED) 16] stat /first_znodecZxid = 0x100000003ctime = Tue Aug 14 12:52:36 CST 2018mZxid = 0x100000003mtime = Tue Aug 14 12:52:36 CST 2018pZxid = 0x10000000acversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 24numChildren = 1 8. 删除Znode删除指定的 znode 以及递归删除其所有子节点。这只有在 znode 可用时发生。 语法：rmr /path 示例：rmr /third_znode 输出：[zk: 10.86.218.105:2181(CONNECTED) 18] create /third_znode &quot;my zookeeper third znode&quot;Created /third_znode[zk: 10.86.218.105:2181(CONNECTED) 19] rmr /third_znode[zk: 10.86.218.105:2181(CONNECTED) 20] get /third_znodeNode does not exist: /third_znode 删除(delete/path)命令类似于 remove 命令，但它只适用于没有子节点的 znode。 欢迎关注我的公众号和博客： 原文：ZooKeeper CLI]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Grafana安装与启动]]></title>
    <url>%2Fhow-to-install-and-run-grafana.html</url>
    <content type="text"><![CDATA[1. Homebrew安装1.1 下载与安装安装最新稳定版：brew updatebrew install grafana 如果要升级 Grafana，可以使用如下命令重新安装：brew updatebrew reinstall grafana 你也可以从 Git 安装最新非稳定版 Grafana：brew install --HEAD grafana/grafana/grafana 如果我们已从 HEAD 分支安装过，需要升级 Grafana：brew reinstall --HEAD grafana/grafana/grafana 1.2 启动如果要使用 homebrew 服务启动 Grafana，首先要确保安装了 homebrew/services：brew tap homebrew/services 使用如下命令启动Grafana：brew services start grafana 1.3 配置配置文件位于 /usr/local/etc/grafana/grafana.ini。详细配置请参考:如何配置Grafana 1.4 日志日志文件位于 /usr/local/var/log/grafana/grafana.log。 1.5 插件如果你想手动安装一个插件，需要将它放在这里：/usr/local/var/lib/grafana/plugins。 1.6 数据库默认数据库 Sqlite 文件位于 /usr/local/var/lib/grafana。 2. 二进制方式安装2.1 下载与安装下载最新的.tar.gz文件并解压缩：tar -zxvf grafana-6.2.5.darwin-amd64.tar.gz -C . 这将会解压到以我们下载的版本命名的文件夹中。此文件夹包含运行 Grafana 所需的所有文件，但不包含 init 脚本或安装脚本：smartsi:grafana smartsi$ lltotal 56drwxr-xr-x 11 smartsi staff 352 8 2 11:29 ./drwxrwxrwx 24 smartsi staff 768 8 2 11:31 ../-rw-r--r-- 1 smartsi staff 11343 6 26 02:12 LICENSE-rw-r--r-- 1 smartsi staff 108 6 26 02:12 NOTICE.md-rw-r--r-- 1 smartsi staff 5743 6 26 02:12 README.md-rw-r--r-- 1 smartsi staff 5 6 26 02:23 VERSIONdrwxr-xr-x 6 smartsi staff 192 6 26 02:23 bin/drwxr-xr-x 6 smartsi staff 192 8 3 20:14 conf/drwxr-xr-x 13 smartsi staff 416 6 26 02:23 public/drwxr-xr-x 22 smartsi staff 704 6 26 02:23 scripts/drwxr-xr-x 3 smartsi staff 96 6 26 02:23 tools/ 2.2 配置要配置 Grafana，需要将名为 custom.ini 的配置文件添加到 conf 文件夹，这将会覆盖 conf/defaults.ini 中定义的任何配置。详细配置请参考:如何配置Grafana 下面是我们以 MySQL 作为 Grafana 的数据库的一些配置(custom.ini)：[paths]data = $&#123;GRAFANA_HOME&#125;/datatemp_data_lifetime = 24hlogs = $&#123;GRAFANA_HOME&#125;/logsplugins = $&#123;GRAFANA_HOME&#125;/pluginsprovisioning = conf/provisioning[database]type = mysqlhost = 127.0.0.1:3306name = grafanauser = rootpassword = zxcvbnm1url = mysql://root:zxcvbnm1@localhost:3306/grafana 2.3 启动通过执行 ./bin/grafana-server web 来启动 Grafana。smartsi:grafana smartsi$ ./bin/grafana-server webINFO[08-03|22:39:24] Starting Grafana logger=server version=6.2.5 commit=6082d19 branch=HEAD compiled=2019-06-26T01:56:19+0800INFO[08-03|22:39:24] Config loaded from logger=settings file=/Users/smartsi/opt/grafana/conf/defaults.iniINFO[08-03|22:39:24] Config loaded from logger=settings file=/Users/smartsi/opt/grafana/conf/custom.iniINFO[08-03|22:39:24] Path Home logger=settings path=/Users/smartsi/opt/grafanaINFO[08-03|22:39:24] Path Data logger=settings path=/Users/smartsi/opt/grafana/dataINFO[08-03|22:39:24] Path Logs logger=settings path=/Users/smartsi/opt/grafana/logsINFO[08-03|22:39:24] Path Plugins logger=settings path=/Users/smartsi/opt/grafana/pluginsINFO[08-03|22:39:24] Path Provisioning logger=settings path=/Users/smartsi/opt/grafana/conf/provisioningINFO[08-03|22:39:24] App mode production logger=settingsINFO[08-03|22:39:24] Initializing SqlStore logger=server... 3. 登录要运行 Grafana，请打开浏览器并转到 http://localhost:3000/。如果你尚未配置其他端口，则 Grafana 侦听的默认 Http 端口为 3000。 默认登录账号与密码为: admin/ admin 欢迎关注我的公众号和博客： 原文:Installing on Mac 我的博客即将同步至腾讯云+社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=30370wau6tesw]]></content>
      <categories>
        <category>Grafana</category>
      </categories>
      <tags>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm UI REST API]]></title>
    <url>%2Fstorm-ui-rest-api.html</url>
    <content type="text"><![CDATA[1. 简介Storm UI 守护进程提供了 REST API, 允许我们与 Storm 集群进行交互, 其中包括查看指标数据，配置信息以及启动或停止拓扑的管理操作。REST API 结果以 JSON 形式返回。 2. 用法REST API 是 Storm UI 守护进程（由 storm ui 启动）的一部分，因此与 Storm UI 守护进程在同一主机和端口上运行。通过 ui.port 参数来配置端口，默认为 8080。 UI 守护进程通常与 Nimbus 守护进程在同一主机上运行。 API 基本 URL 形式如下:http://&lt;ui-host&gt;:&lt;ui-port&gt;/api/v1/... 我们可以使用 curl 等工具来操作 REST API:# 请求集群配置# 注意: 假设 ui.port 配置的为默认值 8080$ curl http://&lt;ui-host&gt;:8080/api/v1/cluster/configuration 3. API介绍3.1 GET 操作3.1.1 /api/v1/cluster/configuration使用如下 API 返回集群配置：.../api/v1/cluster/configuration 返回结果如下:&#123; "dev.zookeeper.path": "/tmp/dev-storm-zookeeper", "topology.tick.tuple.freq.secs": null, "topology.builtin.metrics.bucket.size.secs": 60, "topology.fall.back.on.java.serialization": true, "topology.max.error.report.per.interval": 5, "zmq.linger.millis": 5000, "topology.skip.missing.kryo.registrations": false, "storm.messaging.netty.client_worker_threads": 1, "ui.childopts": "-Xmx768m", "storm.zookeeper.session.timeout": 20000, "nimbus.reassign": true, "topology.trident.batch.emit.interval.millis": 500, "storm.messaging.netty.flush.check.interval.ms": 10, "nimbus.monitor.freq.secs": 10, "logviewer.childopts": "-Xmx128m", "java.library.path":"/usr/local/lib:/opt/local/lib:/usr/lib", "topology.executor.send.buffer.size": 1024, ...&#125; 3.1.2 /api/v1/cluster/summary使用如下 API 返回集群摘要信息，例如 nimbus 正常运行时间或 Supervisors 数量:.../api/v1/cluster/summary 返回字段如下: Field Value Description stormVersion String Storm version supervisors Integer Number of supervisors running topologies Integer Number of topologies running slotsTotal Integer Total number of available worker slots slotsUsed Integer Number of worker slots used slotsFree Integer Number of worker slots available executorsTotal Integer Total number of executors tasksTotal Integer Total tasks schedulerDisplayResource Boolean Whether to display scheduler resource information totalMem Double The total amount of memory in the cluster in MB totalCpu Double The total amount of CPU in the cluster availMem Double The amount of available memory in the cluster in MB availCpu Double The amount of available cpu in the cluster memAssignedPercentUtil Double The percent utilization of assigned memory resources in cluster cpuAssignedPercentUtil Double The percent utilization of assigned CPU resources in cluster 返回结果如下:&#123; "stormVersion": "0.9.2-incubating-SNAPSHOT", "supervisors": 1, "slotsTotal": 4, "slotsUsed": 3, "slotsFree": 1, "executorsTotal": 28, "tasksTotal": 28, "schedulerDisplayResource": true, "totalMem": 4096.0, "totalCpu": 400.0, "availMem": 1024.0, "availCPU": 250.0, "memAssignedPercentUtil": 75.0, "cpuAssignedPercentUtil": 37.5&#125; 3.1.3 /api/v1/supervisor/summary使用如下 API 返回所有 Supervisor 的摘要信息:.../api/v1/supervisor/summary 返回字段如下: Field Value Description id String Supervisor’s id host String Supervisor’s host name uptime String Shows how long the supervisor is running uptimeSeconds Integer Shows how long the supervisor is running in seconds slotsTotal Integer Total number of available worker slots for this supervisor slotsUsed Integer Number of worker slots used on this supervisor schedulerDisplayResource Boolean Whether to display scheduler resource information totalMem Double Total memory capacity on this supervisor totalCpu Double Total CPU capacity on this supervisor usedMem Double Used memory capacity on this supervisor usedCpu Double Used CPU capacity on this supervisor 返回结果如下:&#123; "supervisors": [ &#123; "id": "0b879808-2a26-442b-8f7d-23101e0c3696", "host": "10.11.1.7", "uptime": "5m 58s", "uptimeSeconds": 358, "slotsTotal": 4, "slotsUsed": 3, "totalMem": 3000, "totalCpu": 400, "usedMem": 1280, "usedCPU": 160, ... &#125; ], "schedulerDisplayResource": false, "logviewerPort": 8000&#125; 3.1.4 /api/v1/nimbus/summary使用如下 API 返回所有 Nimbus 主机的摘要信息:.../api/v1/nimbus/summary 返回字段如下: Field Value Description host String Nimbus’ host name port int Nimbus’ port number status String Possible values are Leader, Not a Leader, Dead nimbusUpTime String Shows since how long the nimbus has been running nimbusUpTimeSeconds String Shows since how long the nimbus has been running in seconds nimbusLogLink String Logviewer url to view the nimbus.log version String Version of storm this nimbus host is running 返回结果如下:&#123; "nimbuses": [ &#123; "host": "192.168.202.1", "port": 6627, "nimbusLogLink": "http:\/\/192.168.202.1:8000\/log?file=nimbus.log", "status": Leader, "version": "0.10.0-SNAPSHOT", "nimbusUpTime": "3m 33s", "nimbusUpTimeSeconds": "213" &#125; ]&#125; 3.1.5 /api/v1/history/summary使用如下 API 返回当前用户提交运行过的拓扑ID列表:.../api/v1/history/summary 返回字段如下: Field Value Description topo-history List List of Topologies’ IDs 返回结果如下:&#123; "topo-history": [ "Lark-video-recommend-topology-prod-4-1563888740", "Lark-video-recommend-topology-prod-1-1560338080", "Lark-video-recommend-topology-prod-2-1562916038", "Lark-video-recommend-topology-prod-3-1563883103" ]&#125; 3.1.6 /api/v1/supervisor使用如下 API 查询特定ID的 Supervisor 或某一台主机上运行的所有 Supervisor 的摘要信息：.../api/v1/supervisor Example:# 根据Host查询Supervisorhttp://&lt;ui-host&gt;:&lt;ui-port&gt;/api/v1/supervisor?host=xxx# 根据Id查询Supervisorhttp://&lt;ui-host&gt;:&lt;ui-port&gt;/api/v1/supervisor?id=3d175f35-e427-4ede-be4a-0bccec80ea36 请求参数: Parameter Value Description id String. Supervisor id If specified, respond with the supervisor and worker stats with id. Note that when id is specified, the host argument is ignored. host String. Host name If specified, respond with all supervisors and worker stats in the host (normally just one) sys String. Values 1 or 0. Default value 0 Controls including sys stats part of the response 返回字段如下: Field Value Description supervisors Array Array of supervisor summaries workers Array Array of worker summaries schedulerDisplayResource Boolean Whether to display scheduler resource information 根据如下字段指定 Supervisor: Field Value Description id String Supervisor’s id host String Supervisor’s host name uptime String Shows how long the supervisor is running uptimeSeconds Integer Shows how long the supervisor is running in seconds slotsTotal Integer Total number of worker slots for this supervisor slotsUsed Integer Number of worker slots used on this supervisor schedulerDisplayResource Boolean Whether to display scheduler resource information totalMem Double Total memory capacity on this supervisor totalCpu Double Total CPU capacity on this supervisor usedMem Double Used memory capacity on this supervisor usedCpu Double Used CPU capacity on this supervisor 根据如下字段指定 worker: Field Value Description supervisorId String Supervisor’s id host String Worker’s host name port Integer Worker’s port topologyId String Topology Id topologyName String Topology Name executorsTotal Integer Number of executors used by the topology in this worker assignedMemOnHeap Double Assigned On-Heap Memory by Scheduler (MB) assignedMemOffHeap Double Assigned Off-Heap Memory by Scheduler (MB) assignedCpu Number Assigned CPU by Scheduler (%) componentNumTasks Dictionary Components -&gt; # of executing tasks uptime String Shows how long the worker is running uptimeSeconds Integer Shows how long the worker is running in seconds workerLogLink String Link to worker log viewer page 返回结果如下:&#123; "supervisors": [ &#123; "totalMem": 4096.0, "host":"192.168.10.237", "id":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e", "uptime":"7m 8s", "totalCpu":400.0, "usedCpu":495.0, "usedMem":3432.0, "slotsUsed":2, "version":"0.10.1", "slotsTotal":4, "uptimeSeconds":428 &#125;], "schedulerDisplayResource":true, "workers":[&#123; "topologyName":"ras", "topologyId":"ras-4-1460229987", "host":"192.168.10.237", "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e", "assignedMemOnHeap":704.0, "uptime":"2m 47s", "uptimeSeconds":167, "port":6707, "workerLogLink":"http:\/\/host:8000\/log?file=ras-4-1460229987%2F6707%2Fworker.log", "componentNumTasks": &#123; "word":5 &#125;, "executorsTotal":8, "assignedCpu":130.0, "assignedMemOffHeap":80.0 &#125;, &#123; "topologyName":"ras", "topologyId":"ras-4-1460229987", "host":"192.168.10.237", "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e", "assignedMemOnHeap":904.0, "uptime":"2m 53s", "port":6706, "workerLogLink":"http:\/\/host:8000\/log?file=ras-4-1460229987%2F6706%2Fworker.log", "componentNumTasks":&#123; "exclaim2":2, "exclaim1":3, "word":5 &#125;, "executorsTotal":10, "uptimeSeconds":173, "assignedCpu":165.0, "assignedMemOffHeap":80.0 &#125;]&#125; 3.1.7 /api/v1/topology/summary使用如下 API 返回所有拓扑的信息：.../api/v1/topology/summary 返回字段如下： Field Value Description id String Topology Id name String Topology Name status String Topology Status uptime String Shows how long the topology is running uptimeSeconds Integer Shows how long the topology is running in seconds tasksTotal Integer Total number of tasks for this topology workersTotal Integer Number of workers used for this topology executorsTotal Integer Number of executors used for this topology replicationCount Integer Number of nimbus hosts on which this topology code is replicated requestedMemOnHeap Double Requested On-Heap Memory by User (MB) requestedMemOffHeap Double Requested Off-Heap Memory by User (MB) requestedTotalMem Double Requested Total Memory by User (MB) requestedCpu Double Requested CPU by User (%) assignedMemOnHeap Double Assigned On-Heap Memory by Scheduler (MB) assignedMemOffHeap Double Assigned Off-Heap Memory by Scheduler (MB) assignedTotalMem Double Assigned Total Memory by Scheduler (MB) assignedCpu Double Assigned CPU by Scheduler (%) schedulerDisplayResource Boolean Whether to display scheduler resource information 返回结果如下：&#123; "topologies": [ &#123; "id": "WordCount3-1-1402960825", "name": "WordCount3", "status": "ACTIVE", "uptime": "6m 5s", "uptimeSeconds": 365, "tasksTotal": 28, "workersTotal": 3, "executorsTotal": 28, "replicationCount": 1, "requestedMemOnHeap": 640, "requestedMemOffHeap": 128, "requestedTotalMem": 768, "requestedCpu": 80, "assignedMemOnHeap": 640, "assignedMemOffHeap": 128, "assignedTotalMem": 768, "assignedCpu": 80 &#125; ], "schedulerDisplayResource": true&#125; 3.1.8 /api/v1/topology-workers/\使用如下 API 返回 Id 指定拓扑的 Worker 信息：.../api/v1/topology-workers/&lt;id&gt; 返回字段如下： Field Value Description hostPortList List Workers’ information for a topology name Integer Logviewer Port 返回结果如下：&#123; "hostPortList": [ &#123; "host": "192.168.202.2", "port": 6701 &#125;, &#123; "host": "192.168.202.2", "port": 6702 &#125;, &#123; "host": "192.168.202.3", "port": 6700 &#125; ], "logviewerPort": 8000&#125; 3.1.9 /api/v1/topology/\使用如下 API 返回 Id 指定拓扑信息与统计指标：.../api/v1/topology/&lt;id&gt; 请求参数字段如下： Parameter Value Description id String (required) Topology Id window String. Default value :all-time Window duration for metrics in seconds sys String. Values 1 or 0. Default value 0 Controls including sys stats part of the response 返回字段如下： Field Value Description id String Topology Id name String Topology Name uptime String How long the topology has been running uptimeSeconds Integer How long the topology has been running in seconds status String Current status of the topology, e.g. “ACTIVE” tasksTotal Integer Total number of tasks for this topology workersTotal Integer Number of workers used for this topology executorsTotal Integer Number of executors used for this topology msgTimeout Integer Number of seconds a tuple has before the spout considers it failed windowHint String window param value in “hh mm ss” format. Default value is “All Time” schedulerDisplayResource Boolean Whether to display scheduler resource information replicationCount Integer Number of nimbus hosts on which this topology code is replicated debug Boolean If debug is enabled for the topology samplingPct Double Controls downsampling of events before they are sent to event log (percentage) assignedMemOnHeap Double Assigned On-Heap Memory by Scheduler (MB) assignedMemOffHeap Double Assigned Off-Heap Memory by Scheduler (MB) assignedTotalMem Double Assigned Off-Heap + On-Heap Memory by Scheduler(MB) assignedCpu Double Assigned CPU by Scheduler(%) requestedMemOnHeap Double Requested On-Heap Memory by User (MB) requestedMemOffHeap Double Requested Off-Heap Memory by User (MB) requestedCpu Double Requested CPU by User (%) topologyStats Array Array of all the topology related stats per time window topologyStats.windowPretty String Duration passed in HH:MM:SS format topologyStats.window String User requested time window for metrics topologyStats.emitted Long Number of messages emitted in given window topologyStats.trasferred Long Number messages transferred in given window topologyStats.completeLatency String (double value returned in String format) Total latency for processing the message topologyStats.acked Long Number of messages acked in given window topologyStats.failed Long Number of messages failed in given window workers Array Array of workers in topology workers.supervisorId String Supervisor’s id workers.host String Worker’s host name workers.port Integer Worker’s port workers.topologyId String Topology Id workers.topologyName String Topology Name workers.executorsTotal Integer Number of executors used by the topology in this worker workers.assignedMemOnHeap Double Assigned On-Heap Memory by Scheduler (MB) workers.assignedMemOffHeap Double Assigned Off-Heap Memory by Scheduler (MB) workers.assignedCpu Number Assigned CPU by Scheduler (%) workers.componentNumTasks Dictionary Components -&gt; # of executing tasks workers.uptime String Shows how long the worker is running workers.uptimeSeconds Integer Shows how long the worker is running in seconds workers.workerLogLink String Link to worker log viewer page spouts Array Array of all the spout components in the topology spouts.spoutId String Spout id spouts.executors Integer Number of executors for the spout spouts.emitted Long Number of messages emitted in given window spouts.completeLatency String (double value returned in String format) Total latency for processing the message spouts.transferred Long Total number of messages transferred in given window spouts.tasks Integer Total number of tasks for the spout spouts.lastError String Shows the last error happened in a spout spouts.errorLapsedSecs Integer Number of seconds elapsed since that last error happened in a spout spouts.errorWorkerLogLink String Link to the worker log that reported the exception spouts.acked Long Number of messages acked spouts.failed Long Number of messages failed spouts.requestedMemOnHeap Double Requested On-Heap Memory by User (MB) spouts.requestedMemOffHeap Double Requested Off-Heap Memory by User (MB) spouts.requestedCpu Double Requested CPU by User (%) bolts Array Array of bolt components in the topology bolts.boltId String Bolt id bolts.capacity String (double value returned in String format) This value indicates number of messages executed * average execute latency / time window bolts.processLatency String (double value returned in String format) Average time of the bolt to ack a message after it was received bolts.executeLatency String (double value returned in String format) Average time to run the execute method of the bolt bolts.executors Integer Number of executor tasks in the bolt component bolts.tasks Integer Number of instances of bolt bolts.acked Long Number of tuples acked by the bolt bolts.failed Long Number of tuples failed by the bolt bolts.lastError String Shows the last error occurred in the bolt bolts.errorLapsedSecs Integer Number of seconds elapsed since that last error happened in a bolt bolts.errorWorkerLogLink String Link to the worker log that reported the exception bolts.emitted Long Number of tuples emitted bolts.requestedMemOnHeap Double Requested On-Heap Memory by User (MB) bolts.requestedMemOffHeap Double Requested Off-Heap Memory by User (MB) bolts.requestedCpu Double Requested CPU by User (%) Example:1. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-14029608252. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825?sys=13. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825?window=600 返回结果如下:&#123; "name": "WordCount3", "id": "WordCount3-1-1402960825", "workersTotal": 3, "window": "600", "status": "ACTIVE", "tasksTotal": 28, "executorsTotal": 28, "uptime": "29m 19s", "uptimeSeconds": 1759, "msgTimeout": 30, "windowHint": "10m 0s", "schedulerDisplayResource": true, "workers": [ &#123; "topologyName": "WordCount3", "topologyId": "WordCount3-1-1402960825", "host": "my-host", "supervisorId": "9124ca9a-42e8-481e-9bf3-a041d9595430", "assignedMemOnHeap": 1452.0, "uptime": "27m 26s", "port": 6702, "workerLogLink": "logs", "componentNumTasks": &#123; "spout": 2, "count": 3, "split": 10 &#125;, "executorsTotal": 15, "uptimeSeconds": 1646, "assignedCpu": 260.0, "assignedMemOffHeap": 160.0 &#125; ]"topologyStats": [ &#123; "windowPretty": "10m 0s", "window": "600", "emitted": 397960, "transferred": 213380, "completeLatency": "0.000", "acked": 213460, "failed": 0 &#125;, &#123; "windowPretty": "3h 0m 0s", "window": "10800", "emitted": 1190260, "transferred": 638260, "completeLatency": "0.000", "acked": 638280, "failed": 0 &#125;, &#123; "windowPretty": "1d 0h 0m 0s", "window": "86400", "emitted": 1190260, "transferred": 638260, "completeLatency": "0.000", "acked": 638280, "failed": 0 &#125;, &#123; "windowPretty": "All time", "window": ":all-time", "emitted": 1190260, "transferred": 638260, "completeLatency": "0.000", "acked": 638280, "failed": 0 &#125; ], "workers": [ &#123; "topologyName": "WordCount3", "topologyId": "WordCount3-1-1402960825", "host": "192.168.10.237", "supervisorId": "bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e-169.254.129.212", "uptime": "2m 47s", "uptimeSeconds": 167, "port": 6707, "workerLogLink": "http:\/\/192.168.10.237:8000\/log?file=WordCount3-1-1402960825%2F6707%2Fworker.log", "componentNumTasks": &#123; "spout": 5 &#125;, "executorsTotal": 8, "assignedMemOnHeap": 704.0, "assignedCpu": 130.0, "assignedMemOffHeap": 80.0 &#125; ], "spouts": [ &#123; "executors": 5, "emitted": 28880, "completeLatency": "0.000", "transferred": 28880, "acked": 0, "spoutId": "spout", "tasks": 5, "lastError": "", "errorLapsedSecs": null, "failed": 0 &#125; ], "bolts": [ &#123; "executors": 12, "emitted": 184580, "transferred": 0, "acked": 184640, "executeLatency": "0.048", "tasks": 12, "executed": 184620, "processLatency": "0.043", "boltId": "count", "lastError": "", "errorLapsedSecs": null, "capacity": "0.003", "failed": 0 &#125;, &#123; "executors": 8, "emitted": 184500, "transferred": 184500, "acked": 28820, "executeLatency": "0.024", "tasks": 8, "executed": 28780, "processLatency": "2.112", "boltId": "split", "lastError": "", "errorLapsedSecs": null, "capacity": "0.000", "failed": 0 &#125; ], "configuration": &#123; "storm.id": "WordCount3-1-1402960825", "dev.zookeeper.path": "/tmp/dev-storm-zookeeper", "topology.tick.tuple.freq.secs": null, "topology.builtin.metrics.bucket.size.secs": 60, "topology.fall.back.on.java.serialization": true, "topology.max.error.report.per.interval": 5, "zmq.linger.millis": 5000, "topology.skip.missing.kryo.registrations": false, "storm.messaging.netty.client_worker_threads": 1, "ui.childopts": "-Xmx768m", "storm.zookeeper.session.timeout": 20000, "nimbus.reassign": true, "topology.trident.batch.emit.interval.millis": 500, "storm.messaging.netty.flush.check.interval.ms": 10, "nimbus.monitor.freq.secs": 10, "logviewer.childopts": "-Xmx128m", "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib", "topology.executor.send.buffer.size": 1024, "storm.local.dir": "storm-local", "storm.messaging.netty.buffer_size": 5242880, "supervisor.worker.start.timeout.secs": 120, "topology.enable.message.timeouts": true, "nimbus.cleanup.inbox.freq.secs": 600, "nimbus.inbox.jar.expiration.secs": 3600, "drpc.worker.threads": 64, "topology.worker.shared.thread.pool.size": 4, "nimbus.seeds": [ "hw10843.local" ], "storm.messaging.netty.min_wait_ms": 100, "storm.zookeeper.port": 2181, "transactional.zookeeper.port": null, "topology.executor.receive.buffer.size": 1024, "transactional.zookeeper.servers": null, "storm.zookeeper.root": "/storm", "storm.zookeeper.retry.intervalceiling.millis": 30000, "supervisor.enable": true, "storm.messaging.netty.server_worker_threads": 1 &#125;, "replicationCount": 1&#125; 3.1.10 /api/v1/topology/\/metrics使用如下 API 返回 Id 指定拓扑每个组件的详细度量指标：.../api/v1/topology/&lt;id&gt;/metrics 请求参数如下： Parameter Value Description id String (required) Topology Id window String. Default value :all-time window duration for metrics in seconds sys String. Values 1 or 0. Default value 0 Controls including sys stats part of the response 返回字段如下： Field Value Description window String. Default value “:all-time” window duration for metrics in seconds windowHint String window param value in “hh mm ss” format. Default value is “All Time” spouts Array Array of all the spout components in the topology spouts.id String Spout id spouts.emitted Array Array of all the output streams this spout emits messages spouts.emitted.stream_id String Stream id for this stream spouts.emitted.value Long Number of messages emitted in given window spouts.transferred Array Array of all the output streams this spout transfers messages spouts.transferred.stream_id String Stream id for this stream spouts.transferred.value Long Number messages transferred in given window spouts.acked Array Array of all the output streams this spout receives ack of messages spouts.acked.stream_id String Stream id for this stream spouts.acked.value Long Number of messages acked in given window spouts.failed Array Array of all the output streams this spout receives fail of messages spouts.failed.stream_id String Stream id for this stream spouts.failed.value Long Number of messages failed in given window spouts.complete_ms_avg Array Array of all the output streams this spout receives ack of messages spouts.complete_ms_avg.stream_id String Stream id for this stream spouts.complete_ms_avg.value String (double value returned in String format) Total latency for processing the message bolts Array Array of all the bolt components in the topology bolts.id String Bolt id bolts.emitted Array Array of all the output streams this bolt emits messages bolts.emitted.stream_id String Stream id for this stream bolts.emitted.value Long Number of messages emitted in given window bolts.transferred Array Array of all the output streams this bolt transfers messages bolts.transferred.stream_id String Stream id for this stream bolts.transferred.value Long Number messages transferred in given window bolts.acked Array Array of all the input streams this bolt acknowledges of messages bolts.acked.component_id String Component id for this stream bolts.acked.stream_id String Stream id for this stream bolts.acked.value Long Number of messages acked in given window bolts.failed Array Array of all the input streams this bolt receives fail of messages bolts.failed.component_id String Component id for this stream bolts.failed.stream_id String Stream id for this stream bolts.failed.value Long Number of messages failed in given window bolts.process_ms_avg Array Array of all the input streams this spout acks messages bolts.process_ms_avg.component_id String Component id for this stream bolts.process_ms_avg.stream_id String Stream id for this stream bolts.process_ms_avg.value String (double value returned in String format) Average time of the bolt to ack a message after it was received bolts.executed Array Array of all the input streams this bolt executes messages bolts.executed.component_id String Component id for this stream bolts.executed.stream_id String Stream id for this stream bolts.executed.value Long Number of messages executed in given window bolts.executed_ms_avg Array Array of all the output streams this spout receives ack of messages bolts.executed_ms_avg.component_id String Component id for this stream bolts.executed_ms_avg.stream_id String Stream id for this stream bolts.executed_ms_avg.value String (double value returned in String format) Average time to run the execute method of the bolt Example：1. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825/metrics2. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825/metrics?sys=13. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825/metrics?window=600 返回结果如下：&#123; "window": ":all-time", "window-hint": "All time", "spouts": [ &#123; "id": "spout", "emitted": [ &#123; "stream_id": "__metrics", "value": 20 &#125;, &#123; "stream_id": "default", "value": 17350280 &#125;, &#123; "stream_id": "__ack_init", "value": 17328160 &#125;, &#123; "stream_id": "__system", "value": 20 &#125; ], "transferred": [ &#123; "stream_id": "__metrics", "value": 20 &#125;, &#123; "stream_id": "default", "value": 17350280 &#125;, &#123; "stream_id": "__ack_init", "value": 17328160 &#125;, &#123; "stream_id": "__system", "value": 0 &#125; ], "acked": [ &#123; "stream_id": "default", "value": 17339180 &#125; ], "failed": [ ], "complete_ms_avg": [ &#123; "stream_id": "default", "value": "920.497" &#125; ] &#125; ], "bolts": [ &#123; "id": "count", "emitted": [ &#123; "stream_id": "__metrics", "value": 120 &#125;, &#123; "stream_id": "default", "value": 190748180 &#125;, &#123; "stream_id": "__ack_ack", "value": 190718100 &#125;, &#123; "stream_id": "__system", "value": 20 &#125; ], "transferred": [ &#123; "stream_id": "__metrics", "value": 120 &#125;, &#123; "stream_id": "default", "value": 0 &#125;, &#123; "stream_id": "__ack_ack", "value": 190718100 &#125;, &#123; "stream_id": "__system", "value": 0 &#125; ], "acked": [ &#123; "component_id": "split", "stream_id": "default", "value": 190733160 &#125; ], "failed": [ ], "process_ms_avg": [ &#123; "component_id": "split", "stream_id": "default", "value": "0.004" &#125; ], "executed": [ &#123; "component_id": "split", "stream_id": "default", "value": 190733140 &#125; ], "executed_ms_avg": [ &#123; "component_id": "split", "stream_id": "default", "value": "0.005" &#125; ] &#125;, &#123; "id": "split", "emitted": [ &#123; "stream_id": "__metrics", "value": 60 &#125;, &#123; "stream_id": "default", "value": 190754740 &#125;, &#123; "stream_id": "__ack_ack", "value": 17317580 &#125;, &#123; "stream_id": "__system", "value": 20 &#125; ], "transferred": [ &#123; "stream_id": "__metrics", "value": 60 &#125;, &#123; "stream_id": "default", "value": 190754740 &#125;, &#123; "stream_id": "__ack_ack", "value": 17317580 &#125;, &#123; "stream_id": "__system", "value": 0 &#125; ], "acked": [ &#123; "component_id": "spout", "stream_id": "default", "value": 17339180 &#125; ], "failed": [ ], "process_ms_avg": [ &#123; "component_id": "spout", "stream_id": "default", "value": "0.051" &#125; ], "executed": [ &#123; "component_id": "spout", "stream_id": "default", "value": 17339240 &#125; ], "executed_ms_avg": [ &#123; "component_id": "spout", "stream_id": "default", "value": "0.052" &#125; ] &#125; ]&#125; 3.1.11 /api/v1/topology/\/component/\使用如下 API 返回 Id 指定拓扑特定组件的详细度量指标以及Executor信息：.../api/v1/topology/&lt;id&gt;/component/&lt;component&gt; 组件Id 请求参数如下： Parameter Value Description id String (required) Topology Id component String (required) Component Id window String. Default value :all-time window duration for metrics in seconds sys String. Values 1 or 0. Default value 0 controls including sys stats part of the response 返回字段如下： Field Value Description user String Topology owner id String Component id encodedId String URL encoded component id name String Topology name executors Integer Number of executor tasks in the component tasks Integer Number of instances of component requestedMemOnHeap Double Requested On-Heap Memory by User (MB) requestedMemOffHeap Double Requested Off-Heap Memory by User (MB) requestedCpu Double Requested CPU by User (%) schedulerDisplayResource Boolean Whether to display scheduler resource information topologyId String Topology id topologyStatus String Topology status encodedTopologyId String URL encoded topology id window String. Default value “All Time” window duration for metrics in seconds componentType String component type: SPOUT or BOLT windowHint String window param value in “hh mm ss” format. Default value is “All Time” debug Boolean If debug is enabled for the component samplingPct Double Controls downsampling of events before they are sent to event log (percentage) eventLogLink String URL viewer link to event log (debug mode) profilingAndDebuggingCapable Boolean true if there is support for Profiling and Debugging Actions profileActionEnabled Boolean true if worker profiling (Java Flight Recorder) is enabled profilerActive Array Array of currently active Profiler Actions componentErrors Array of Errors List of component errors componentErrors.errorTime Long Timestamp when the exception occurred (Prior to 0.11.0, this field was named ‘time’.) componentErrors.errorHost String host name for the error componentErrors.errorPort String port for the error componentErrors.error String Shows the error happened in a component componentErrors.errorLapsedSecs Integer Number of seconds elapsed since the error happened in a component componentErrors.errorWorkerLogLink String Link to the worker log that reported the exception spoutSummary Array (only for spouts) Array of component stats, one element per window. spoutSummary.windowPretty String Duration passed in HH:MM:SS format spoutSummary.window String window duration for metrics in seconds spoutSummary.emitted Long Number of messages emitted in given window spoutSummary.completeLatency String (double value returned in String format) Total latency for processing the message spoutSummary.transferred Long Total number of messages transferred in given window spoutSummary.acked Long Number of messages acked spoutSummary.failed Long Number of messages failed boltStats Array (only for bolts) Array of component stats, one element per window. boltStats.windowPretty String Duration passed in HH:MM:SS format boltStats.window String window duration for metrics in seconds boltStats.transferred Long Total number of messages transferred in given window boltStats.processLatency String (double value returned in String format) Average time of the bolt to ack a message after it was received boltStats.acked Long Number of messages acked boltStats.failed Long Number of messages failed inputStats Array (only for bolts) Array of input stats inputStats.component String Component id inputStats.encodedComponentId String URL encoded component id inputStats.executeLatency Long The average time a tuple spends in the execute method inputStats.processLatency Long The average time it takes to ack a tuple after it is first received inputStats.executed Long The number of incoming tuples processed inputStats.acked Long Number of messages acked inputStats.failed Long Number of messages failed inputStats.stream String The name of the tuple stream given in the topology, or “default” if none specified outputStats Array Array of output stats outputStats.transferred Long Number of tuples emitted that sent to one ore more bolts outputStats.emitted Long Number of tuples emitted outputStats.stream String The name of the tuple stream given in the topology, or “default” if none specified Examples: 1. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825/component/spout2. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825/component/spout?sys=13. http://&lt;ui.host&gt;:&lt;ui.port&gt;/api/v1/topology/WordCount3-1-1402960825/component/spout?window=600 返回结果如下：&#123; "name": "WordCount3", "id": "spout", "componentType": "spout", "windowHint": "10m 0s", "executors": 5, "componentErrors": [ &#123; "errorTime": 1406006074000, "errorHost": "10.11.1.70", "errorPort": 6701, "errorWorkerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log", "errorLapsedSecs": 16, "error": "java.lang.RuntimeException: java.lang.StringIndexOutOfBoundsException: Some Error\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128)\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)\n\tat org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)\n\tat backtype...more.." &#125; ], "topologyId": "WordCount3-1-1402960825", "tasks": 5, "window": "600", "profilerActive": [ &#123; "host": "10.11.1.70", "port": "6701", "dumplink": "http:\/\/10.11.1.70:8000\/dumps\/ex-1-1452718803\/10.11.1.70%3A6701", "timestamp": "576328" &#125; ], "profilingAndDebuggingCapable": true, "profileActionEnabled": true, "spoutSummary": [ &#123; "windowPretty": "10m 0s", "window": "600", "emitted": 28500, "transferred": 28460, "completeLatency": "0.000", "acked": 0, "failed": 0 &#125;, &#123; "windowPretty": "3h 0m 0s", "window": "10800", "emitted": 127640, "transferred": 127440, "completeLatency": "0.000", "acked": 0, "failed": 0 &#125;, &#123; "windowPretty": "1d 0h 0m 0s", "window": "86400", "emitted": 127640, "transferred": 127440, "completeLatency": "0.000", "acked": 0, "failed": 0 &#125;, &#123; "windowPretty": "All time", "window": ":all-time", "emitted": 127640, "transferred": 127440, "completeLatency": "0.000", "acked": 0, "failed": 0 &#125; ], "outputStats": [ &#123; "stream": "__metrics", "emitted": 40, "transferred": 0, "completeLatency": "0", "acked": 0, "failed": 0 &#125;, &#123; "stream": "default", "emitted": 28460, "transferred": 28460, "completeLatency": "0", "acked": 0, "failed": 0 &#125; ], "executorStats": [ &#123; "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log", "emitted": 5720, "port": 6701, "completeLatency": "0.000", "transferred": 5720, "host": "10.11.1.7", "acked": 0, "uptime": "43m 4s", "uptimeSeconds": 2584, "id": "[24-24]", "failed": 0 &#125;, &#123; "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6703.log", "emitted": 5700, "port": 6703, "completeLatency": "0.000", "transferred": 5700, "host": "10.11.1.7", "acked": 0, "uptime": "42m 57s", "uptimeSeconds": 2577, "id": "[25-25]", "failed": 0 &#125;, &#123; "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6702.log", "emitted": 5700, "port": 6702, "completeLatency": "0.000", "transferred": 5680, "host": "10.11.1.7", "acked": 0, "uptime": "42m 57s", "uptimeSeconds": 2577, "id": "[26-26]", "failed": 0 &#125;, &#123; "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log", "emitted": 5700, "port": 6701, "completeLatency": "0.000", "transferred": 5680, "host": "10.11.1.7", "acked": 0, "uptime": "43m 4s", "uptimeSeconds": 2584, "id": "[27-27]", "failed": 0 &#125;, &#123; "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6703.log", "emitted": 5680, "port": 6703, "completeLatency": "0.000", "transferred": 5680, "host": "10.11.1.7", "acked": 0, "uptime": "42m 57s", "uptimeSeconds": 2577, "id": "[28-28]", "failed": 0 &#125; ]&#125; 3.2 分析和调试GET操作3.2.1 /api/v1/topology/\/profiling/start/\/\使用如下 API 请求在 Worker 上启动的分析器（带有超时时间）。 返回状态以及 worker 上分析器组件的链接：.../api/v1/topology/&lt;id&gt;/profiling/start/&lt;host-port&gt;/&lt;timeout&gt; 请求字段如下： Parameter Value Description id String (required) Topology Id host-port String (required) Worker Id timeout String (required) Time out for profiler to stop in minutes 返回字段如下: Field Value Description id String Worker id status String Response Status timeout String Requested timeout dumplink String Link to logviewer URL for worker profiler documents. Examples: 1. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/start/10.11.1.7:6701/102. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/start/10.11.1.7:6701/53. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/start/10.11.1.7:6701/20 返回结果如下:&#123; "status": "ok", "id": "10.11.1.7:6701", "timeout": "10", "dumplink": "http:\/\/10.11.1.7:8000\/dumps\/wordcount-1-1446614150\/10.11.1.7%3A6701"&#125; 3.2.2 /api/v1/topology/\/profiling/dumpprofile/\使用如下 API 请求在 Worker 上 dump 分析器记录。返回请求的状态和 Worker ID:.../api/v1/topology/&lt;id&gt;/profiling/dumpprofile/&lt;host-port&gt; 请求字段如下： Parameter Value Description id String (required) Topology Id host-port String (required) Worker Id 返回字段如下: Field Value Description id String Worker id status String Response Status Examples:1. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/dumpprofile/10.11.1.7:6701 返回结果如下:&#123; "status": "ok", "id": "10.11.1.7:6701",&#125; 3.2.3 /api/v1/topology/\/profiling/stop/\使用如下 API 请求停止在 Worker 上的分析器。返回状态以及请求的 Worker Id:/api/v1/topology/&lt;id&gt;/profiling/stop/&lt;host-port&gt; 请求字段如下: Parameter Value Description id String (required) Topology Id host-port String (required) Worker Id 返回字段如下: Field Value Description id String Worker id status String Response Status Examples: 1. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/stop/10.11.1.7:6701 返回结果:&#123; "status": "ok", "id": "10.11.1.7:6701",&#125; 3.2.4 /api/v1/topology/\/profiling/dumpjstack/\使用如下 API 请求在 Worker 上 dump jstack。返回请求的状态和 Worker Id:.../api/v1/topology/&lt;id&gt;/profiling/dumpjstack/&lt;host-port&gt; 请求字段如下： Parameter Value Description id String (required) Topology Id host-port String (required) Worker Id 返回字段如下: Field Value Description id String Worker id status String Response Status Examples:1. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/dumpjstack/10.11.1.7:6701 返回结果如下:&#123; "status": "ok", "id": "10.11.1.7:6701",&#125; 3.2.5 /api/v1/topology/\/profiling/dumpheap/\使用如下 API 请求在 Worker 上 dump heap。返回请求的状态和 Worker Id:.../api/v1/topology/&lt;id&gt;/profiling/dumpheap/&lt;host-port&gt; 请求字段如下： Parameter Value Description id String (required) Topology Id host-port String (required) Worker Id 返回字段如下: Field Value Description id String Worker id status String Response Status Examples:1. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1446614150/profiling/dumpheap/10.11.1.7:6701 返回结果如下:&#123; "status": "ok", "id": "10.11.1.7:6701",&#125; 3.3 POST 操作3.3.1 /api/v1/topology/\/activate使用如下 API 激活指定拓扑：.../api/v1/topology/&lt;id&gt;/activate 请求参数如下： Parameter Value Description id String (required) Topology Id 返回结果如下：&#123; "topologyOperation": "activate", "topologyId": "wordcount-1-1420308665", "status": "success"&#125; 3.3.2 /api/v1/topology/\/deactivate使用如下 API 停用指定拓扑：.../api/v1/topology/&lt;id&gt;/deactivate 请求参数如下： Parameter Value Description id String (required) Topology Id 返回结果如下：&#123; "topologyOperation": "deactivate", "topologyId": "wordcount-1-1420308665", "status": "success"&#125; 3.3.3 /api/v1/topology/\/rebalance/\使用如下 API 重平衡指定拓扑：.../api/v1/topology/&lt;id&gt;/rebalance/&lt;wait-time&gt; 请求参数如下： Parameter Value Description id String (required) Topology Id wait-time String (required) Wait time before rebalance happens rebalanceOptions Json (optional) topology rebalance options 平衡可选参数如下：&#123; "rebalanceOptions": &#123; "numWorkers": 2, "executors": &#123; "spout": 4, "count": 10 &#125; &#125;, "callback": "foo"&#125; Example：curl -i -b ~/cookiejar.txt -c ~/cookiejar.txt -X POST -H &quot;Content-Type: application/json&quot;-d &apos;&#123;&quot;rebalanceOptions&quot;: &#123;&quot;numWorkers&quot;: 2, &quot;executors&quot;: &#123; &quot;spout&quot; : &quot;5&quot;, &quot;split&quot;: 7, &quot;count&quot;: 5 &#125;&#125;, &quot;callback&quot;:&quot;foo&quot;&#125;&apos;http://localhost:8080/api/v1/topology/wordcount-1-1420308665/rebalance/0 返回结果如下：&#123; "topologyOperation": "rebalance", "topologyId": "wordcount-1-1420308665", "status": "success"&#125; 3.3.4 /api/v1/topology/\/kill/\使用如下 API 杀死指定拓扑：.../api/v1/topology/&lt;id&gt;/kill/&lt;wait-time&gt; 请求参数如下： Parameter Value Description id String (required) Topology Id wait-time String (required) Wait time before rebalance happens 返回结果如下：&#123; "topologyOperation": "kill", "topologyId": "wordcount-1-1420308665", "status": "succe"&#125; 英译对照: host: 主机 Storm版本: 2.0.0 欢迎关注我的公众号和博客： 原文:Storm UI REST API]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
        <tag>Storm 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Roaring Bitmap更好的位图压缩算法]]></title>
    <url>%2Fbetter-bitmap-performance-with-roaring-bitmaps.html</url>
    <content type="text"><![CDATA[1. 概述Bitsets（也称为Bitmaps）通常用作快速数据结构。不幸的是，他们可能会占用太多内存。为了降低内存的使用，我们经常会使用压缩的位图。 Roaring Bitmaps 是一种压缩的位图，要优于常规的压缩位图，例如 WAH，EWAH 或者 Concise。在某些情况下，可以比它们快几百倍，并且通常提供更好的压缩。 Roaring Bitmaps 已经被很多重要系统使用： Apache Lucene Apache Druid Apache Spark Apache CarbonData LinkedIn Pinot Apache Kylin 几乎所有流行的编程语言（Java，C，C ++，Go，C＃，Rust，Python ……）都提供了 Roaring Bitmaps。 2. 主要思想我们以存放 Integer 值的 Bitmap 来举例，RBM 把一个 32 位的 Integer 划分为高 16 位和低 16 位，通过高 16 位找到该数据存储在哪个桶中（高 16 位可以划分 2^16 个桶），把剩余的低 16 位放入该桶对应的 Container 中。 每个桶都有对应的 Container，不同的 Container 存储方式不同。依据不同的场景，主要有 2 种不同的 Container，分别是 Array Container 和 Bitmap Container。Array Container 存放稀疏的数据，Bitmap Container 存放稠密的数据。若一个 Container 里面的元素数量小于 4096，使用 Array Container 来存储。当 Array Container 超过最大容量 4096 时，会转换为 Bitmap Container。 3. Array ContainerArray Container 是 Roaring Bitmap 初始化默认的 Container。Array Container 适合存放稀疏的数据，其内部数据结构是一个有序的 Short 数组。数组初始容量为 4，数组最大容量为 4096，所以 Array Container 是动态变化的，当容量不够时，需要扩容，并且当超过最大容量 4096 时，就会转换为 Bitmap Container。由于数组是有序的，存储和查询时都可以通过二分查找快速定位其在数组中的位置。 后面会讲解为什么超过最大容量 4096 时变更 Container 类型。 下面我们具体看一下数据如何被存储的，例如，0x00020032（十进制131122）放入一个 RBM 的过程如下图所示： 0x00020032 的前 16 位是 0002，找到对应的桶 0x0002。在桶对应的 Container 中存储低 16 位，因为 Container 元素个数不足 4096，因此是一个 Array Container。低 16 位为 0032（十进制为50）, 在 Array Container 中二分查找找到相应的位置插入即可（如上图50的位置）。 相较于原始的 Bitmap 需要占用 16K (131122/8/1024) 内存来存储这个数，而这种存储实际只占用了4B（桶中占 2 B，Container中占 2 B，不考虑数组的初始容量）。 4. Bitmap Container第二种 Container 是 Bitmap Container。它的数据结构是一个 Long 数组，数组容量恒定为 1024，和上文的 Array Container 不同，Array Container 是一个动态扩容的数组。Bitmap Container 不用像 Array Container 那样需要二分查找定位位置，而是可以直接通过下标直接寻址。 由于每个 Bitmap Container 需要处理低 16 位数据，也就是需要使用 Bitmap 来存储需要 8192 B（2^16/8）, 而一个 Long 值占 8 个 B，所以数组大小为 1024。因此一个 Bitmap Container 固定占用内存 8 KB。 下面我们具体看一下数据如何被存储的，例如，0xFFFF3ACB（十进制4294916811）放入一个 RBM 的过程如下图所示： 0xFFFF3ACB 的前 16 位是 FFFF，找到对应的桶 0xFFFF。在桶对应的 Container 中存储低 16 位，因为 Container 中元素个数已经超过 4096，因此是一个 Bitmap Container。低 16 位为 3ACB（十进制为15051）, 因此在 Bitmap Container 中通过下标直接寻址找到相应的位置，将其置为 1 即可（如上图15051的位置）。 可以看到元素个数达到 4096 之前，Array Container 占用的空间比 Bitmap Container 的少，当 Array Container 中元素到 4096 个时，正好等于 Bitmap Container 所占用的 8 KB。当元素个数超过了 4096 时，Array Container 所占用的空间还是继续线性增长，而 Bitmap Container 的内存空间并不会增长，始终还是占用 8 KB，与数据量无关。所以当 Array Container 超过最大容量 4096 会转换为 Bitmap Container。 欢迎关注我的公众号和博客： 参考: 不深入而浅出 Roaring Bitmaps 的基本原理]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis使用Pipeline加速查询速度]]></title>
    <url>%2Fusing-pipelining-to-speedup-redis-queries.html</url>
    <content type="text"><![CDATA[1. RTTRedis 是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下 Redis 客户端执行一条命令分为如下四个过程： 发送命令 命令排队 命令执行 返回结果 客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。服务端处理命令，并将结果返回给客户端。客户端和服务端通过网络进行连接。这个连接可以很快，也可能很慢。无论网络如何延迟，数据包总是能从客户端到达服务端，服务端返回数据给客户端。 这个时间被称为 RTT (Round Trip Time)，例如上面过程的发送命令和返回结果两个过程。当客户端需要连续执行多次请求时很容易看到这是如何影响性能的（例如，添加多个元素到同一个列表中）。例如，如果 RTT 时间是250毫秒（网络连接很慢的情况下），即使服务端每秒能处理100k的请求量，那我们每秒最多也只能处理4个请求。如果使用的是本地环回接口，RTT 就短得多，但如如果需要连续执行多次写入，这也是一笔很大的开销。 下面我们看一下执行 N 次命令的模型: 2. Pipeline我们可以使用 Pipeline 改善这种情况。Pipeline 并不是一种新的技术或机制，很多技术上都使用过。RTT 在不同网络环境下会不同，例如同机房和同机房会比较快，跨机房跨地区会比较慢。Redis 很早就支持 Pipeline 技术，因此无论你运行的是什么版本，你都可以使用 Pipeline 操作 Redis。 Pipeline 能将一组 Redis 命令进行组装，通过一次 RTT 传输给 Redis，再将这组 Redis 命令按照顺序执行并将结果返回给客户端。上图没有使用 Pipeline 执行了 N 条命令，整个过程需要 N 次 RTT。下图为使用 Pipeline 执行 N 条命令，整个过程仅需要 1 次 RTT： Redis 提供了批量操作命令(例如 mget，mset等)，有效的节约了RTT。但大部分命令是不支持批量操作的。 3. Java PipelineJedis 也提供了对 Pipeline 特性的支持。我们可以借助 Pipeline 来模拟批量删除，虽然不会像 mget 和 mset 那样是一个原子命令，但是在绝大数情况下可以使用：public void mdel(List&lt;String&gt; keys)&#123; Jedis jedis = new Jedis("127.0.0.1"); // 创建Pipeline对象 Pipeline pipeline = jedis.pipelined(); for (String key : keys)&#123; // 组装命令 pipeline.del(key); &#125; // 执行命令 pipeline.sync();&#125; 4. 性能测试下表给出了不同网络环境下非 Pipeline 和 Pipeline 执行 10000 次 set 操作的效果： 网络 延迟 非Pipeline Pipeline 本机 0.17ms 573ms 134ms 内网服务器 0.41ms 1610ms 240ms 异地机房 7ms 78499ms 1104ms 因测试环境不同可能会得到不同的测试数据，本测试 Pipeline 每次携带 100 条命令。 我们可以从上表中得出如下结论: Pipeline 执行速度一般比逐条执行要快。 客户端和服务端的网络延时越大，Pipeline 的效果越明显。 5. 批量命令与Pipeline对比下面我们看一下批量命令与 Pipeline 的区别: 原生批量命令是原子的，Pipeline 是非原子的。 原生批量命令是一个命令对应多个 key，Pipeline 支持多个命令。 原生批量命令是 Redis 服务端支持实现的，而 Pipeline 需要服务端和客户端的共同实现。 6. 注意点使用 Pipeline 发送命令时，每次 Pipeline 组装的命令个数不能没有节制，否则一次组装的命令数据量过大，一方面会增加客户端的等待时间，另一方面会造成一定的网络阻塞，可以将一次包含大量命令的 Pipeline 拆分成多个较小的 Pipeline 来完成。 欢迎关注我的公众号和博客： 参考: Using pipelining to speedup Redis queries Redis开发与运维]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka如何实现每秒上百万的高并发写入]]></title>
    <url>%2Fhow-kafka-achieves-millions-of-concurrent-writes-per-second.html</url>
    <content type="text"><![CDATA[Kafka是高吞吐低延迟的高并发、高性能的消息中间件，在大数据领域有极为广泛的运用。配置良好的Kafka集群甚至可以做到每秒几十万、上百万的超高并发写入。 那么Kafka到底是如何做到这么高的吞吐量和性能的呢？这篇文章我们来一点一点说一下。 1. 页缓存技术 + 磁盘顺序写首先Kafka每次接收到数据都会往磁盘上去写，如下图所示： 那么在这里我们不禁有一个疑问了，如果把数据基于磁盘来存储，频繁的往磁盘文件里写数据，这个性能会不会很差？大家肯定都觉得磁盘写性能是极差的。没错，要是真的跟上面那个图那么简单的话，那确实这个性能是比较差的。 但是实际上Kafka在这里有极为优秀和出色的设计，就是为了保证数据写入性能，首先Kafka是基于操作系统的页缓存来实现文件写入的。操作系统本身有一层缓存，叫做page cache，是在内存里的缓存，我们也可以称之为os cache，意思就是操作系统自己管理的缓存。你在写入磁盘文件的时候，可以直接写入这个os cache里，也就是仅仅写入内存中，接下来由操作系统自己决定什么时候把os cache里的数据真的刷入磁盘文件中。 仅仅这一个步骤，就可以将磁盘文件写性能提升很多了，因为其实这里相当于是在写内存，不是在写磁盘，大家看下图： 接着另外一个就是kafka写数据的时候，非常关键的一点，他是以磁盘顺序写的方式来写的。也就是说，仅仅将数据追加到文件的末尾，不是在文件的随机位置来修改数据。普通的机械磁盘如果你要是随机写的话，确实性能极差，也就是随便找到文件的某个位置来写数据。但是如果你是追加文件末尾按照顺序的方式来写数据的话，那么这种磁盘顺序写的性能基本上可以跟写内存的性能本身也是差不多的。 所以大家就知道了，上面那个图里，Kafka在写数据的时候，一方面基于了os层面的page cache来写数据，所以性能很高，本质就是在写内存罢了。另外一个，它是采用磁盘顺序写的方式，所以即使数据刷入磁盘的时候，性能也是极高的，也跟写内存是差不多的。基于上面两点，kafka就实现了写入数据的超高性能。 那么大家想想，假如说kafka写入一条数据要耗费1毫秒的时间，那么是不是每秒就是可以写入1000条数据？但是假如kafka的性能极高，写入一条数据仅仅耗费0.01毫秒呢？那么每秒是不是就可以写入10万条数？所以要保证每秒写入几万甚至几十万条数据的核心点，就是尽最大可能提升每条数据写入的性能，这样就可以在单位时间内写入更多的数据量，提升吞吐量。 2. 零拷贝技术说完了写入这块，再来谈谈消费这块。大家应该都知道，从Kafka里我们经常要消费数据，那么消费的时候实际上就是要从kafka的磁盘文件里读取某条数据然后发送给下游的消费者，如下图所示： 那么这里如果频繁的从磁盘读数据然后发给消费者，性能瓶颈在哪里呢？假设要是kafka什么优化都不做，就是很简单的从磁盘读数据发送给下游的消费者，那么大概过程如下所示： 先看看要读的数据在不在os cache里，如果不在的话就从磁盘文件里读取数据后放入os cache。 接着从操作系统的os cache里拷贝数据到应用程序进程的缓存里，再从应用程序进程的缓存里拷贝数据到操作系统层面的Socket缓存里，最后从Socket缓存里提取数据后发送到网卡，最后发送出去给下游消费。 整个过程，如下图所示： 大家看上图，很明显可以看到有两次没必要的拷贝吧！一次是从操作系统的cache里拷贝到应用进程的缓存里，接着又从应用程序缓存里拷贝回操作系统的Socket缓存里。而且为了进行这两次拷贝，中间还发生了好几次上下文切换，一会儿是应用程序在执行，一会儿上下文切换到操作系统来执行。所以这种方式来读取数据是比较消耗性能的。 Kafka为了解决这个问题，在读数据的时候是引入零拷贝技术。也就是说，直接让操作系统的cache中的数据发送到网卡后传输给下游的消费者，中间跳过了两次拷贝数据的步骤，Socket缓存中仅仅会拷贝一个描述符过去，不会拷贝数据到Socket缓存。 大家看下图，体会一下这个精妙的过程： 通过零拷贝技术，就不需要把os cache里的数据拷贝到应用缓存，再从应用缓存拷贝到Socket缓存了，两次拷贝都省略了，所以叫做零拷贝。对Socket缓存仅仅就是拷贝数据的描述符过去，然后数据就直接从os cache中发送到网卡上去了，这个过程大大的提升了数据消费时读取文件数据的性能。而且大家会注意到，在从磁盘读数据的时候，会先看看os cache内存中是否有，如果有的话，其实读数据都是直接读内存的。 如果kafka集群经过良好的调优，大家会发现大量的数据都是直接写入os cache中，然后读数据的时候也是从os cache中读。相当于是Kafka完全基于内存提供数据的写和读了，所以这个整体性能会极其的高。 说个题外话，下回有机会给大家说一下Elasticsearch的架构原理，其实ES底层也是大量基于os cache实现了海量数据的高性能检索的，跟Kafka原理类似。 3. 最后的总结通过这篇文章对kafka底层的页缓存技术的使用，磁盘顺序写的思路，以及零拷贝技术的运用，大家应该就明白Kafka每台机器在底层对数据进行写和读的时候采取的是什么样的思路，为什么他的性能可以那么高，做到每秒几十万的吞吐量。 这种设计思想对我们平时自己设计中间件的架构，或者是出去面试的时候，都有很大的帮助。 作者：中华石杉来源：石杉的架构笔记订阅号（ID：shishan100） 欢迎关注我的公众号和博客： 原文:Kafka如何实现每秒上百万的高并发写入？]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[案例-马蜂窝实时计算平台演进之路]]></title>
    <url>%2Fevolution-of-real-time-computing-platform-in-mafengwo.html</url>
    <content type="text"><![CDATA[MES 是马蜂窝统一实时计算平台，为各条业务线提供稳定、高效的实时数据计算和查询服务。在整体设计方面，MES 借鉴了 Lambda 架构的思想。本篇文章，我们将从四个方面了解 MES： 关于 Lambda 架构 MES 架构和原理 MES 优化历程 近期规划 1. 关于 Lambda 架构Lambda 架构是由 Storm 作者 NathanMarz 根据自己在 Twitter 的分布式数据处理系统经验，提出的一个实时大数据处理框架，具有高容错、低延时和可扩展等特性。 Lambda 架构核心的思想主要可以归纳成两点： 数据从上游 MQ 消息中间件过来后分为 2 路，一路离线批处理, 一路实时处理并有各自的 View 以供查询。 Query 时，对数据做 Function, 结合 Batch View 和 Realtime View，得到最终结果。 具体来说，Lambda 架构将大数据系统架构为多个层次：批处理层（Batch layer）、实时处理层（Speed Layer）、服务层（Serving Layer）。 我们结合一张经典的 Lambda 架构图分别来看： 1.1 批处理层（Batch Layer）批处理层承担的任务是对从上游进来的所有被系统认为不可变的原始数据。类比目前的数据平台架构来看, 即离线的那几张保存原始数据的主表。这 3 张主表是所有完整的数据并且是不可变的，基于这几张主表，数据经过 Batch 、ETL，产生供批处理查询的 Batch View。 1.2 加速层（Speed Layer）批处理层虽然可以很好地处理离线数据，但它不能很好满足对于时间粒度的需求。对于需要不断实时生成和实时查询处理的数据，通常会放在加速层来进行实时处理和转化。 加速层与批处理层最大的区别在于，加速层只处理最近的数据，而批处理层处理所有数据。另外在数据的读取方面，为了满足最小延迟，加速层不会在同一数据读取所有新数据，而是在收到新数据时更新 Realtime View，所以我们说，在加速层进行的是一种增量的计算。 1.3 服务层（Serving Layer）服务层用于响应用户的查询请求，合并 Batch View 和 Realtime View 中的结果数据集到最终的数据集，并向外对用户通过统一接口，提供实时+离线的数据统计服务。 基于 Lambda 的数据平台架构, 可以按照分层集成众多的大数据组件。在对 MES 的架构设计中，我们借鉴了 Lambda 架构的思想来实现更快、更准、鲁棒性更好的特性。 2. 马蜂窝实时计算平台MES为了保证 MES 实时计算平台的性能，我们结合马蜂窝的实际业务场景，主要围绕低延迟，高吞吐、容灾能力和 Exacty Once 的流式语义这四点，来进行架构设计和技术选型。 2.1 整体架构设计对照 Lambda 架构，我们选用 Kafka 作为消息中间件，批处理层选择 Hive、Presto，加速层也就是实时处理层选择 Spark、Flink 等。 数据从 Kafka 出来后走两条线，一条是 Spark Streaming，支持秒级别的实时数据，计算结果会入库到 Redis 里。第二天凌晨，Redis 中前一天的所有数据 Batch 到 HBase 中。另外一条是 Flink+Druid，用来处理分钟级和小时级的数据。上面提供一层 Restful API / Thrift API 封装，供 MES 页面或其他业务通过接口的方式来获取数据。 如果实时数据出了问题，我们会通过 HDFS 中的离线主表进行重算，也是有两条路径： 一是为用户服务的 MES 重算系统，用户可以自助化选取重算规则，提交重算任务。这个任务会被提交到 PrestoSQL 集群，计算结果最终落地到 HBase 里，重算后 MES 的历史数据就会和离线数据算出来的数据保持一致； 另外一条线是 Spark 全量重算，由数据平台的小伙伴内部使用，解决的是基于所有事件组、所有规则的全天数据重算。Spark 会读取配置规则，重算所有前一天的数据后入库到 HBase，保持实时数据和离线数据的一致性； 监控系统是 Grafana，它开放了通用接口给 Python、Java 等语言来上报相关信息，只要按照接口上报要想关注的指标并进行简单配置，就可以查询结果，比如 MES 的延迟时间、一些 Restful 接口的调用量等, 如果出现不正常的情况将通过邮件告警； 最右边是贯穿始终的 MES 规则，我们可以抽象地把它看作是实时的配置流。 2.2 MES 实时计算引擎2.2.1 技术选型结合马蜂窝的业务需求，我们对三大主流实时计算引擎 Storm、Spark Streaming、Flink 进行了选型对比。 2.2.1.1 StormStorm 是第一代流式计算引擎，实现了一个数据流 (Data Flow) 的模型。我们可以把它想象成一个发射点，一条一条产生数据，形成的数据流分布式地在集群上按照 Bolt 的计算逻辑进行转换，完成计算、过滤等操作，在下游实现聚合。 Storm 的优势是实时性好，可以达到毫秒级。但是它的吞吐量欠佳，并且只能为消息提供「至少一次」的处理机制, 这意味着可以保证每条消息都能被处理，但也可能发生重复。 2.2.1.2 Spark StreamingSpark Streaming 不像 Storm 那样一次一个地处理数据流，而是在处理前按时间间隔预先将其切分为一段一段，进行「微批次」处理作业。这样一来解决了吞吐量的问题，但它的实时性就没有 Storm 那么高，不过也可以达到秒级处理。 在流式语义方面，由于 Spark Streaming 容错机制基于 RDD，依靠 CheckPoint，出错之后会从该位置重新计算，不会导致重复计算。当然我们也可以自己来管理 offset，保证 Exactly Once (只算一次的语义) 的处理。 2.2.1.3 FlinkFlink 是新一代流式计算引擎，国内的阿里就是 Flink 的重度使用和贡献者。Flink 是原生的流处理系统，把所有的数据都看成是流，认为批处理是流处理中的一种特殊情况。数据基于 Flink Stream Source 流入，中间经过 Operator，从 Sink 流出。 为了解决流处理的容错问题，Flink 巧妙地运用了分布式快照的设计与可部分重发的数据源实现容错。用户可自定义对整个 Job 进行快照的时间间隔。当任务失败时，Flink 会将整个 Job 恢复到最近一次快照，并从数据源重发快照之后的数据。Flink 同时保证了实时性和吞吐量，流式语义也做得非常好，能够保证 Exactly Once。 在此之外，组件技术选型的时候在满足自己业务现状的同时, 还需要从以前几个方面考虑: 开源组件是否能覆盖需求 开源组件的扩展性和二次开发的难度 开源组件 API 是否稳定 开源组件是否有应用于生产环境的案例，比如多少公司应用于生产环境 开源组件社区是否活跃，比如可以看 github，issues，jiras 这些活跃程度 开源组件 License 限定问题 开源组件之间的耦合问题 2.2.2 设计下图描述了 MES 实时计算引擎处理数据的过程： 数据从 Kafka 源源不断地过来形成数据流，用户通过 UI 配置的一些规则形成实时配置流，数据流和配置流进入到实时计算引擎 Spark Streaming 后进行聚合计算。计算出的实时数据写入到 Redis，历史数据入库到 HBase。UI 目前通过 Restful API 来获取实时和历史数据。 2.2.3 演进关于 MES 实时计算的引擎，我们主要经历了两次演进。 2.2.3.1 第一代：Spark Streaming + Redis + HBase在设计第一代 MES 时，我们希望可以支持秒级的计算，并且精确计算每一个用户。所以在当时的现状下，我们综合考虑选择了 Spark Streaming。这个方案计算出来的 UV 是比较精确的。但它有自己的局限性： 首先，这一套架构用到的几个组件其实对资源都比较依赖, 而且 SparkStreaming 对那种时不时的流量高峰的数据处理不是非常友好。数据先在 Spark Streaming 算好然后再入 Redis，最后再入库到 Hbase，数据链路比较长，不好维护。 另外，第一代 MES 只支持自助配置规则，有了规则才会实时计算。所以对于比较自由的 OLAP 交叉分析不友好。而且如果由于集群不稳定等原因导致的任务失败数据少算, 那么不管是用户自助提交 Presto 还是利用 Spark 批处理全量重算，都是一个消耗集群资源的过程。由于批处理重算需要一定的时间来完成对历史数据的修复，这对一些需要数据准确并及时提供的用户不是非常友好。 我们考虑，在数据量大的情况下，我们是不是可以适当牺牲 UV 精准度的计算，来保障整个系统的性能和稳定性。所以就引入了 Flink + Druid。 2.2.3.2 第二代：引入 Flink + Druid刚才我们已经简单了解过 Flink，再来说下 Druid。Druid 是一个大数据实时查询和分析的高容错、高性能的开源分布式系统，用来快速处理大规模的数据，它能够实现对大量数据的快速查询和分析，不足是存在一个 2% 的误差。但事实上，在数据量非常大的情况下，2% 的误差是可以接受的。后面我们又通过 Yahoo 提供的 Data Sketch，实现 UV 计算的精确调控，可以实现在 800w 之下的数据量，UV 都是准确的。最终的计算结果通过 restful 接口提供给 MES 获取数据并展现。 Flink + Druid 部分主要是用来处理数据量大、维度多，且不需要精确到秒级的业务数据，比如 Page logdata、mobile page、以及 Server Push。在最近 15 天的数据是可以精确到分钟级别查询的，对于历史数据，粒度越精确，持久化到 Druid 里面的数据量就越大。 在离线批量导入部分，目前 Druid 支持小时级以及 T+1 天级的数据校正。因为如果在 Flink +Tranquility 实时摄取数据这一阶段 task 有异常的话，会导致实时数据到 Druid 有丢失的情况出现。因此根据 Lambda 架构的思想，我们可以用小时级或者天级离线数据对丢失的数据进行重算补全。 对比一下两代计算引擎，Flink + Druid 的引入很好地解决了因为大量数据的 UV 计算带来的压力： 2.3 MES 优化历程为了更好地满足业务需求，提升整个系统的性能，我们也在不断对 MES 进行优化，主要包括实时计算集群、计算引擎、查询接口和监控方面。这里主要和大家分享两点。 2.3.1 实时计算集群优化 Spark，Druid，Flink 集群框架版本升级及相关参数优化； Redis，Hbase 节点扩容和参数优化; 集群网络，Yarn，Mesos 等资源管理框架调整和优化 2.3.2 实时计算引擎优化(1) 数据结构和计算逻辑对于 Spark 来讲，Prefer 原生数据类型以及数组结构，对于指针类型以及嵌套的结构处理起来性能不是非常友好。因此要注意这一点，妥善优化自己的数据结构。 计算逻辑的部分也要考虑好。比如写 Redis 的时候是事先规划好要存入 Redis 中的数据结构来利用 Akka 并发每条来写入，还是在 Streaming 中算好一批结果最后来一次性写入 Redis，这 2 种方式在性能上还是有很大区别的。 (2) 参数优化 序列化方式首先是 Kyro 的方式，其次才是 Java，序列化的方式不同对网络的传输以及处理起来的性能是有影响的。 Spark 推测执行机制。根据我们集群目前的现状，有各种各样的任务同时在跑，如果遇到集群资源使用高峰期，那么一个 Spark 任务落在比较慢的节点上就会拖累整个 Job 的执行进度。开启推测执行之后，系统会把进程慢的任务主动杀死，然后重新产生一个相同的任务分配到资源充沛的节点上去快速完成它。 数据本地化。分布式计算有一个经典的理念是：移动数据不如移动计算。比如说我把一个任务分成很多并行的任务，有可能获得的任务刚好需要处理的数据就在处理的节点上，也有可能不是。所以这里有一个本地化等待时间的参数可以控制数据本地化的处理等级并对性能产生很大影响。 另外还用一些关于并行度控制、JVM GC 方面的调优就比较细节了，如果大家感兴趣可以留言给我们交流。 下面是优化前后结果对比的一个简单展示： 3. 未来规划马蜂窝实时计算平台的发展还需要不断探索，未来我们主要会在以下几个方面重点推进： 实时计算任务统一资源管理和任务调度 支持复杂的实时 SQL OLAP 计算 实时数据血缘关系及监控预警 复杂实时 CEP 规则系统 作者：董良，马蜂窝大数据平台研发技术专家。2017年加入马蜂窝，现负责马蜂窝实时计算平台和数据中台服务。2008年毕业于西安邮电大学，曾在Talend、神州专车等公司工作，先后从事数据集成中间件，数据仓库，实时计算平台等方向的研发工作。 欢迎关注我的公众号和博客： 原文:马蜂窝实时计算平台演进之路]]></content>
      <categories>
        <category>案例</category>
      </categories>
      <tags>
        <tag>案例</tag>
        <tag>Flink</tag>
        <tag>HBase</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop无法访问50070端口]]></title>
    <url>%2Ftrouble-shooting-cannot-access-port-50070-in-hadoop.html</url>
    <content type="text"><![CDATA[最近在搭建 Hadoop（3.1.2）环境时遇到了一件比较奇葩的问题。Hadoop 配置文件正常，各个守护进程正常启动，但是启动后无法在浏览器中访问 50070 端口，但是又可以访问 8088 端口: 参考网上的各中解决方案，有的方案没有解决我的问题，最终发现如下方案可以解决我的问题。有可能是 NameNode 初始化默认端口失效，于是决定手动修改配置文件设置默认端口:&lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;localhost:50070&lt;/value&gt;&lt;/property&gt; 然后重新格式化 NameNode，启动 Hadoop。最终访问正常: 重新格式化 NameNode 之前需要清空 dfs 下的 name 和 data 文件夹以解决 DataNode 无法启动的问题。具体配置路径在 hdfs-site.xml 中配置:&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/Users/xxx/tmp/hadoop/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/Users/xxx/tmp/hadoop/dfs/data&lt;/value&gt;&lt;/property&gt; 欢迎关注我的公众号和博客：]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>TroubleShooting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[案例-ClickHouse在头条的技术演进]]></title>
    <url>%2Ftechnology-evolution-of-clickHouse-in-toutiao.html</url>
    <content type="text"><![CDATA[1. ClickHouse 简介ClickHouse 是由号称“俄罗斯 Google”的 Yandex 开发而来，在 2016 年开源，在计算引擎里算是一个后起之秀，在内存数据库领域号称是最快的。大家从网上也能够看到，它有几倍于 GreenPlum 等引擎的性能优势。 如果大家研究过它的源码，会发现其实它采用的技术并不新。ClickHouse 是一个列导向数据库，是原生的向量化执行引擎。它在大数据领域没有走 Hadoop 生态，而是采用 Local attached storage 作为存储，这样整个 IO 可能就没有 Hadoop 那一套的局限。它的系统在生产环境中可以应用到比较大的规模，因为它的线性扩展能力和可靠性保障能够原生支持 shard + replication 这种解决方案。它还提供了一些 SQL 直接接口，有比较丰富的原生 client。另外就是它比较快。 大家选择 ClickHouse 的首要原因是它比较快，但其实它的技术没有什么新的地方，为什么会快？我认为主要有三个方面的因素： 它的数据剪枝能力比较强，分区剪枝在执行层，而存储格式用局部数据表示，就可以更细粒度地做一些数据的剪枝。它的引擎在实际使用中应用了一种现在比较流行的 LSM 方式，可以做到数据的局部有序。 它对整个资源的垂直整合能力做得比较好，并发 MPP + SMP 这种执行方式可以很充分地利用机器的集成资源。它的实现又做了很多性能相关的优化，它的一个简单的汇聚操作有很多不同的版本，会根据不同 Key 的组合方式有不同的实现。对于高级的计算指令，数据解压时，它也有少量使用。 我当时选择它的一个原因，ClickHouse 是一套完全由 C++ 模板 Code 写出来的实现，代码还是比较优雅的。 2. 字节跳动如何使用 ClickHouse头条做技术选型的时候为什么会选用 ClickHouse？这可能跟我们的应用场景有关，下面简单介绍一下 ClickHouse 在头条的使用场景。 头条内部第一个使用 ClickHouse 的是用户行为分析系统。该系统在使用 ClickHouse 之前，engine 层已经有两个迭代。他们尝试过 Spark 全内存方案还有一些其他的方案，都存在很多问题。主要因为产品需要比较强的交互能力，页面拖拽的方式能够给分析师展示不同的指标，查询模式比较多变，并且有一些查询的 DSL 描述，也不好用现成的 SQL 去表示，这就需要 engine 有比较好的定制能力。 行为分析系统的表可以打成一个大的宽表形式，join 的形式相对少一点。系统的数据量比较大，因为产品要支持头条所有 APP 的用户行为分析，包含头条全量和抖音全量数据，用户的上报日志分析，面临不少技术挑战。大家做了一些调研之后，在用 ClickHouse 做一些简单的 POC 工作，我就拿着 ClickHouse 按需求开始定制了。 综合来看，从 ClickHouse 的性能、功能和产品质量来说，效果还不错，因为开发 ClickHouse 的公司使用的场景实际上跟头条用户分析是比较类似的，因此有一定的借鉴意义。 目前头条 ClickHouse 集群的规模大概有几千个节点，最大的集群规模可能有 1200 个节点，这是一个单集群的最大集群节点数。数据总量大概是几十个 PB，日增数据 100TB，落地到 ClickHouse，日增数据总量大概是它的 3 倍，原始数据也就 300T 左右，大多数查询的响应时间是在几秒钟。从交互式的用户体验来说，一般希望把所有的响应控制在 30 秒之内返回，ClickHouse 基本上能够满足大部分要求。覆盖的用户场景包括产品分析师做精细化运营，开发人员定位问题，也有少量的广告类客户。 下图是一个 API 的框架图，相当于一个统一的指标出口，也提供服务。围绕着 ClickHouse 集群，它可以支撑不同的数据源，包括离线的数据、实时的消息中间件数据，也有些业务的数据，还有少量高级用户会直接从 Flink 上消费一些 Databus 数据，然后批量写入，之后在它外围提供一个数据 ETL 的 Service，定期把数据迁移到 ClickHouse local storage 上，之后他们在这之上架了一个用户使用分析系统，也有自研的 BI 系统做一些多维分析和数据可视化的工作，也提供 SQL 的网关，做一些统一指标出口之类的工作，上面的用户可能是多样的。 综合来说，我们希望在头条内部把 ClickHouse 打造成为支持数据中台的查询引擎，满足交互式行为的需求分析，能够支持多种数据源，整个数据链路对业务做到透明。在工作过程中，我们也碰到了很多的问题。 3. 问题与解决方案接下来我会详细介绍我们在使用 ClickHouse 的过程中碰到过什么问题，希望对大家有一些借鉴意义。 3.1 数据源到 ClickHouse 服务化我们在做 ClickHouse 服务化的过程中，第一步就是如何把数据落到 ClickHouse 集群中。原生的 ClickHouse 没有 HDFS 访问能力，我们同时还需要保证对用户透明，就可能存在几个问题： 怎么访问离线数据？ ClickHouse 没有事务支持，如果在数据导入过程中发生了 Fail，如何做 Fail over？ ClickHouse 数据就绪速度。我们整个数据就绪的压力很大，上游就绪的时间比较晚，每天早上就会有一些分析师在 ClickHouse 上看指标，整个数据落到 ClickHouse 留给我们的空挡可能不是太长。 我们针对这些问题做了一些改动。第一，从 HAWQ 上移植过来 HDFS client，让 ClickHouse 能够直接访问数据，我们的 ETL 服务实际上维护了一套外部事务的逻辑，然后做数据一致性的保证；为了保证就绪时间，我们充分利用各个节点的计算能力和数据的分布式能力，实际上最终都会在外围服务把数据作一些 Repartition，直接写入各个节点本地表。另外，我们还有一些国际化的场景，像 TikTok、Musical.ly 等，数据就绪和分析师分析的时间是有重叠的，数据写和查询交互的影响还是有一些。我们最近也在尝试把数据构建和查询分离出来，并开发相应的 Feature，但是还没有上线，从 Demo 来看，这条路是行得通的。 3.2 Map 数据类型：动态 Schema我们在做整个框架的过程中发现，有时候产品存在动态 Schema 的需求。我们当时增加了 Map 的数据类型，主要解决的问题是产品支持的 APP 很多，上报的 Model 也是多变的，它跟用户的日志定义有关，有很多用户自定义参数，就相当于动态的 Schema。从数据产品设计的角度来看又需要相对固定的 Schema，二者之间就会存在一定的鸿沟。最终我们是通过 Map 类型来解决的。 实现 Map 的方式比较多，最简单的就是像 LOB 的方式，或者像 Two-implicit column 的方式。当时产品要求访问 Map 单键的速度与普通的 column 速度保持一致，那么比较通用的解决方案不一定能够满足我们的要求。当时做的时候，从数据的特征来看，我们发现虽然叫 Map，但是它的 keys 总量是有限的，因为依赖于用户自定义的参数不会特别多，在一定的时间范围内，Keys 数量会是比较固定的。而 ClickHouse 有一个好处：它的数据在局部是自描述的，Part 之间的数据差异自动能够 Cover 住。 最后我们采用了一个比较简单的展平模型，在我们数据写入过程中，它会做一个局部打平。以下图为例，表格中两行总共只有三个 key，我们就会在存储层展开这三列。这三列的描述是在局部描述的，有值的用值填充，没有值就直接用 N 填充。现在 Map 类型在头条 ClickHouse 集群的各种服务上都在使用，基本能满足大多数的需求。 另外，为了满足访问 key 的高效性，我们在执行层做自动改写，key 的访问会直接改写成对隐私列的访问:select c_map&#123;‘a’&#125; from table 会转换为:select c_map_a from table 这样架构会有一个比较大的问题，它对于 Map 列的全值访问代价比较大，需要从隐式列反构建出全值列:select c_map from table 对于这个问题，我们也没有很好地解决，因为实际上在很多时候我们只关心 key 的访问效率。 另外一个问题，这是 LSM 架构，存在一个数据合并的过程，合并时可能需要重构 Map。我们为了提高合并的速度，做了一些相应的优化，可以做到无序重构。这些做完后，收益还是比较大的。首先，Table 的 schema 能够简化，理论上现在 Table 的定义只需要做几种技术类型的组合就可以；然后 ETL 构建的逻辑不再需要关注用户的隐私列参数，可以简化 ETL 的构建逻辑；最后，对数据的自动化接入帮助也很大。下面是我们优化之后的语法，大家可以看到相对比较简单： Create table t(c1 UInt64, c2 Map(String, UInt8)) ENGINE=MergeTree…. insert into t values(1, {‘abc’:1, ‘bcd’:2}) Select c2{‘abc’} from t 3.3 大数据量和高可用不知道大家在使用 ClickHouse 的过程中有没有一个体会，它的高可用方案在大的数据量下可能会有点问题。主要是 zookeeper 的使用方式可能不是很合理，也就是说它原生的 Replication 方案有太多的信息存在 ZK 上，而为了保证服务，一般会有一个或者几个副本，在头条内部主要是两个副本的方案。 我们当时有一个 400 个节点的集群，还只有半年的数据。突然有一天我们发现服务特别不稳定，ZK 的响应经常超时，table 可能变成只读模式，发现它 znode 的太多。而且 ZK 并不是 Scalable 的框架，按照当时的数据预估，整个服务很快就会不可用了。 我们分析后得出结论，实际上 ClickHouse 把 ZK 当成了三种服务的结合，而不仅把它当作一个 Coordinate service，可能这也是大家使用 ZK 的常用用法。ClickHouse 还会把它当作 Log Service，很多行为日志等数字的信息也会存在 ZK 上；还会作为表的 catalog service，像表的一些 schema 信息也会在 ZK 上做校验，这就会导致 ZK 上接入的数量与数据总量会成线性关系。按照这样的数据增长预估，ClickHouse 可能就根本无法支撑头条抖音的全量需求。 社区肯定也意识到了这个问题，他们提出了一个 mini checksum 方案，但是这并没有彻底解决 znode 与数据量成线性关系的问题。所以我们就基于 MergeTree 存储引擎开发了一套自己的高可用方案。我们的想法很简单，就是把更多 ZK 上的信息卸载下来，ZK 只作为 coordinate Service。只让它做三件简单的事情：行为日志的 Sequence Number 分配、Block ID 的分配和数据的元信息，这样就能保证数据和行为在全局内是唯一的。 关于节点，它维护自身的数据信息和行为日志信息，Log 和数据的信息在一个 shard 内部的副本之间，通过 Gossip 协议进行交互。我们保留了原生的 multi-master 写入特性，这样多个副本都是可以写的，好处就是能够简化数据导入。下图是一个简单的框架图。 以这个图为例，如果往 Replica 1 上写，它会从 ZK 上获得一个 ID，就是 Log ID，然后把这些行为和 Log Push 到集群内部 shard 内部活着的副本上去，然后当其他副本收到这些信息之后，它会主动去 Pull 数据，实现数据的最终一致性。我们现在所有集群加起来 znode 数不超过三百万，服务的高可用基本上得到了保障，压力也不会随着数据增加而增加。 解决了以上几个问题之后，我们还在对 ClickHouse 做持续改进。我们最近也碰到了一些 Log 调度之类的问题，当时我们对 Log 调度并没有做特别的优化，实际上还是用 ClickHouse 的原生调度，在有些集群上可能会碰到一些问题，比如有些表的 Log 调度延迟会比较高一点，我们现在也正在尝试解决。 3.4 String 类型处理效率：Global Dictionary另外，为了满足交互式的需求，在相当长的一段时间我们都在思考怎么提高数据执行的性能。大家在做数仓或者做大数据场景的时候会发现，用户特别喜欢字符串类型，但是你如果做执行引擎执行层，就特别不喜欢处理这类 String 类型的数据，因为它是变长的，存在执行上有较高代价。String 类型的处理效率，跟数字类型的处理效率有 10 倍的差距，所以我们做了一个全局字典压缩的解决方案，目的并不是为了节省存储空间，而是为了提高执行的效率，这是相当重要一个出发点。我们希望把一些常见的算子尽量在压缩域上执行，不需要做数据的解压。 目前我们只做了一个 pure dictionary compression，支持的算子也比较少，比如 predication 支持等值比较或者 in 等类似的比较能够在压缩域上直接执行，这已经能够覆盖我们很多的场景，像 group by 操作也能够在压缩域上做。 说到 Global Dictionary，其实也并不是完全的 Global ，每个节点有自己的 Dictionary，但是在一个集群内部，各个节点之前的字典可能是不一样的。为什么没有做全局在集群内部做一个字典？ 第一，全局字典会把 coordinate 协议搞得特别复杂，我以前做数据库的时候有个项目，采用了集群级别 Global Dictionary，碰到了比较多的挑战。字典压缩只支持了 MergeTree 相关的存储引擎。压缩的行为发生主要有三种操作，像数据的插入或者数据的后台合并，都会触发 compression，还有很多数据的批量 roll in 或 roll out，也会做一些字典的异步构建。 刚才也提到，我们的主要出发点就是想在执行层去做非解压的计算，主要是做 Select query，每一个 Select 来的时候，我们都会在分析阶段做一些合法性的校验，评估其在压缩域上直接执行是否可行，如果满足标准，就会改写语法树。如果压缩的 column 会出现在输出的列表中，会显式地加一个 Decompress Stream 这样可选的算子，然后后续执行就不太需要改动，而是可以直接支持。当 equality 的比较以及 group by 操作直接在压缩上执行，最后整体的收益大概提高 20% 到 30%。 刚才提到，我们的字典不是一个集群水平的，那大家可能会有所疑问，比如对分布式表的 query 怎么在压缩域上做评估？我们稍微做了一些限制，很多时候使用压缩场景的是用户行为分析系统，它是按用户 ID 去做 shard，然后节点之间基本做到没有交互。我们也引入了一个执行模式，稍微在它的现有计算上改了一下，我们叫做完美分布加智能合并的模式。在这个模式下，分布式表的 query 也是能够在字典上做评估。收益也还可以，满足当时设计时候的要求。 3.5 特定场景内存 OOM有时候做一个系统，内存使用的问题也会比较严重。尤其当做数据量大的系统时，经常发生内存受限的问题，或者说 OOM 最后被系统杀掉。ClickHouse 因为有很多数据的加速，比如 Index &amp; mark 文件，信息会在实例启动的时候加载，整个加载过程非常慢，有时候一个集群起来可能得要半个小时。 虽然我们对这个问题做了一些优化，能够做到并行加载，但是也得好几分钟。如果实例被系统 Kill 了之后，对服务还会有影响，我们的系统经常要回答一些用户这样的查询，例如需要查 60 天内用户的转化率或者整个用户的行为路径对应的每天转化率。这种 Block 的操作需要把很多数据从底层捞出来，在时间纬度上进行排序，找出对应的模式。 如果不进行优化，基本上一个 Query 需要使用的内存会超过一百 G，如果稍微并发一下，内存那可能就支撑不了。并且，由于其使用的内存分配器的原因，也很难把内存的实际使用量限制得很准，这就偶尔会发生被系统 Kill 的场景。 我们想从 engine 优化的角度去解决问题，本质上就是 Blocked Aggregator 的操作，它没有感知到底层的数据分布。这个 Feature 有点意思，也是我们从数据分布到执行共同优化的一个尝试，实现相对来说比较粗糙，但是现在线上也已经开始用了。 它的思路是这样的，我们的 Aggregator 执行路径可以由 HINT 来控制，HINT 的生成是由上面的产品生成的，因为产品能够感知数据分布，也能够知道这些指标的语义。HINT 最关键的一个作用是把 Blocked Aggregator 局部做到流水线化，比如计算 60 天的指标，它可以生成一个 read planner 控制底层的 reader，每一批处理的是那一部分数据。上层的指标输出可以把这些信息 aggregate 到对应的地方，做从下向上的执行输出。最上层的 schedule 流输出指标可以把每天的计算结果汇聚起来，然后做一个总体的整理，最终就形成一个输出。 这些优化工作完成以后有了很明显的收益，与默认没有开启的时候相比，系统的内存使用可能会下降 5 倍左右。现在应用场景主要在两个指标的计算上，像漏斗之类的和计算用户行为路径会使用。 3.6 Array 类型处理下面介绍一下我们怎么处理 Array 类型，并将它做得更高效。 Array 类型处理的需求主要来自于 AB 实验的需求。当前我们的系统也会做一些实时 AB 指标的输出，实验 ID 在我们系统中以数组的形式存储。头条内部的 AB 实验也比较多，对于一个单条记录，它可能命中的实验数会有几百上千个。有时候我们需要查询命中实验的用户行为是什么样的，也就是要做一些 Array hasAny 语义的评估。 从 Array 执行来看，因为它的数据比较长，所以说从数据的反序列化代价以及整个 Array 在执行层的 Block 表示来说不是特别高效，要花相当大的精力去做 Array column 的解压。而且会在执行层消耗特别大的内存去表示，如果中间发生了 Filter 的话，要做 Block column 过滤，permutation 会带上 Array，执行起来会比较慢。那我们就需要一个比较好的方式去优化从读取到执行的流程。 做大数据，可能最有效的优化方式就是怎么样做到底层数据的剪枝，数据少是提高数据处理速度的终极法宝。我们提出了现在的剪枝方法，一个是 Part level，一个是 MRK range level。那有没有一种针对于 Array column 的剪枝方式？我们做了下面两个尝试： 首先做了一个双尺度的 Bloom Filter，记录 Array 里面 Key 的运动情况，实现了 Part level 和细粒度的 MRK range level，做完后在一些小的产品上效果还挺好的，但最后真正在大产品上，像抖音、头条全量，我们发现 Fill factor 太高，实际上没太大帮助。之后我们开发了一个 BitMap 索引，基本的想法是把 Array 的表示转化成 Value 和 Bit 的结合。在执行层改写，把 has 的评估直接转换成 get BitMap。 做完之后，我们上线了一两个产品，在一些推荐的场景上使用。这个方案主要问题就是 BitMap 数据膨胀问题稍微严重了一点，最近我们也做了一些优化，可能整体的数据占用是原始数据的 50% 左右，比如 Array 如果是 1G，可能 Bit map 也会有 500M。我们整个集群的副本策略是一个 1：N 的策略，副本存储空间比较有压力，我们现在也没有大范围的使用，但效果是很好的，对于评估基本上也会有一、二十倍的提升效果。 4. 其他问题和改进以上是我今天分享的主要内容，后面的内容相对比较弹性。字节跳动自身的数据源是比较多样的，我们对其他数据源也做了一些特定的优化。比如我们有少量业务会消费 Kafka，而现在的 Kafka engine 没有做主备容错，我们也在上面做了一些高可用的 HaKafka engine 的实现，来做到主备容错以及一些指定分区消费功能，以满足一些特定领域的需求。 另外，我们发现它的 update/delete 功能比较弱，而我们有一部分业务场景想覆盖业务数据库上面的数据，像 MySQL 上也是会有一些增删操作的。我们就基于它 Collapse 的功能做了一些设计，去支持轻量级的 update/delete，目前产品还属于刚起步的阶段，但是从测试结果来看，能够支撑从 MySQL 到 ClickHouse 的迁移，基于 delta 表的方案也是可行的。 我们还做了一些像小文件读取的问题，提供了一个多尺度分区的方案，但由于各种原因没有在线上使用。 说到底，我们的需求还有很多，现在也还有很多工作正在做，比如控制面的开发，简化整体运维，还有 Query cache 以及整个数据指标的正确性还不能达到百分之百的保障，特别是像实时数据流的数据，我们也想做更深层次的优化。我们还希望增强物化视图，也准备提高分布式 Join 能力，因为我们自研 BI 对此还有比较强的需求，未来我们会在这一块做一些投入。 以上就是去年一年我们在 ClickHouse 这块主要做的一些工作。总体来说 ClickHouse 是一个比较短小精干的引擎，也比较容易上手和定制，大家都可以去尝试一下。 欢迎关注我的公众号和博客： 原文:最快开源OLAP引擎！ClickHouse在头条的技术演进]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>案例</tag>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm与Redis集成]]></title>
    <url>%2Fhow-to-use-redis-in-storm.html</url>
    <content type="text"><![CDATA[Storm-redis 使用 Jedis 作为 Redis 客户端。 1. 如何使用添加Maven依赖：&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-redis&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt;&lt;/dependency&gt; 2. 常用BoltStorm-redis 提供了基本的 Bolt 实现：RedisLookupBolt，RedisStoreBolt 以及 RedisFilterBolt。 根据名字我们就可以知道其功能，RedisLookupBolt 从 Redis 中检索指定键的值，RedisStoreBolt 将键/值存储到 Redis 上。RedisFilterBolt 过滤键或字段不在 Redis 上的元组。 一个元组匹配一个键/值对，你可以在 TupleMapper 中定义匹配模式。你还可以从 RedisDataTypeDescription 中选择你需要的数据类型。通过 RedisDataTypeDescription.RedisDataType 来查看支持哪些数据类型。一些数据类型，例如散列，有序集，还需要指定额外的键来将元组转换为元素：public RedisDataTypeDescription(RedisDataType dataType, String additionalKey) &#123; this.dataType = dataType; this.additionalKey = additionalKey; if (dataType == RedisDataType.HASH || dataType == RedisDataType.SORTED_SET || dataType == RedisDataType.GEO) &#123; if (additionalKey == null) &#123; throw new IllegalArgumentException("Hash, Sorted Set and GEO should have additional key"); &#125; &#125;&#125; 这些接口与 RedisLookupMapper，RedisStoreMapper 以及 RedisFilterMapper 组合使用，分别适用于 RedisLookupBolt，RedisStoreBolt 以及 RedisFilterBolt。当你实现 RedisFilterMapper 时，请确保在 declareOutputFields() 中声明与输入流相同的字段，因为 FilterBolt 只是转发存在 Redis 上输入元组。 2.1 RedisLookupBolt实现RedisLookupMapper：class WordCountRedisLookupMapper implements RedisLookupMapper &#123; private RedisDataTypeDescription description; private final String hashKey = "wordCount"; public WordCountRedisLookupMapper() &#123; description = new RedisDataTypeDescription( RedisDataTypeDescription.RedisDataType.HASH, hashKey); &#125; @Override public List&lt;Values&gt; toTuple(ITuple input, Object value) &#123; String member = getKeyFromTuple(input); List&lt;Values&gt; values = Lists.newArrayList(); values.add(new Values(member, value)); return values; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("wordName", "count")); &#125; @Override public RedisDataTypeDescription getDataTypeDescription() &#123; return description; &#125; @Override public String getKeyFromTuple(ITuple tuple) &#123; return tuple.getStringByField("word"); &#125; @Override public String getValueFromTuple(ITuple tuple) &#123; return null; &#125;&#125; 根据如下方式使用：JedisPoolConfig poolConfig = new JedisPoolConfig.Builder() .setHost(host).setPort(port).build();RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper); 2.2 RedisStoreBolt实现RedisStoreMapper：class WordCountStoreMapper implements RedisStoreMapper &#123; private RedisDataTypeDescription description; private final String hashKey = "wordCount"; public WordCountStoreMapper() &#123; description = new RedisDataTypeDescription( RedisDataTypeDescription.RedisDataType.HASH, hashKey); &#125; @Override public RedisDataTypeDescription getDataTypeDescription() &#123; return description; &#125; @Override public String getKeyFromTuple(ITuple tuple) &#123; return tuple.getStringByField("word"); &#125; @Override public String getValueFromTuple(ITuple tuple) &#123; return tuple.getStringByField("count"); &#125;&#125; 根据如下方式使用：JedisPoolConfig poolConfig = new JedisPoolConfig.Builder() .setHost(host).setPort(port).build();RedisStoreMapper storeMapper = new WordCountStoreMapper();RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper); 2.3 RedisFilterBolt实现RedisFilterMapper：class BlacklistWordFilterMapper implements RedisFilterMapper &#123; private RedisDataTypeDescription description; private final String setKey = "blacklist"; public BlacklistWordFilterMapper() &#123; description = new RedisDataTypeDescription( RedisDataTypeDescription.RedisDataType.SET, setKey); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word", "count")); &#125; @Override public RedisDataTypeDescription getDataTypeDescription() &#123; return description; &#125; @Override public String getKeyFromTuple(ITuple tuple) &#123; return tuple.getStringByField("word"); &#125; @Override public String getValueFromTuple(ITuple tuple) &#123; return null; &#125;&#125; 根据如下方式使用：JedisPoolConfig poolConfig = new JedisPoolConfig.Builder() .setHost(host).setPort(port).build();RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper); 3. 自定义Bolt如果你的场景不适合 RedisStoreBolt，RedisLookupBolt 以及 RedisFilterBolt，那么 storm-redis 还提供了 AbstractRedisBolt，你可以自定义自己的业务逻辑。public static class LookupWordTotalCountBolt extends AbstractRedisBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class); private static final Random RANDOM = new Random(); public LookupWordTotalCountBolt(JedisPoolConfig config) &#123; super(config); &#125; public LookupWordTotalCountBolt(JedisClusterConfig config) &#123; super(config); &#125; @Override public void execute(Tuple input) &#123; JedisCommands jedisCommands = null; try &#123; jedisCommands = getInstance(); String wordName = input.getStringByField("word"); String countStr = jedisCommands.get(wordName); if (countStr != null) &#123; int count = Integer.parseInt(countStr); this.collector.emit(new Values(wordName, count)); // print lookup result with low probability if(RANDOM.nextInt(1000) &gt; 995) &#123; LOG.info("Lookup result - word : " + wordName + " / count : " + count); &#125; &#125; else &#123; // skip LOG.warn("Word not found in Redis - word : " + wordName); &#125; &#125; finally &#123; if (jedisCommands != null) &#123; returnInstance(jedisCommands); &#125; this.collector.ack(input); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // wordName, count declarer.declare(new Fields("wordName", "count")); &#125;&#125; 4. Trident State 用法 RedisState 和 RedisMapState，为单机 Redis 模式提供了 Jedis 接口。 RedisClusterState 和 RedisClusterMapState，为 Redis 集群模式提供了 JedisCluster 接口。 RedisState:JedisPoolConfig poolConfig = new JedisPoolConfig.Builder() .setHost(redisHost).setPort(redisPort) .build();RedisStoreMapper storeMapper = new WordCountStoreMapper();RedisLookupMapper lookupMapper = new WordCountLookupMapper();RedisState.Factory factory = new RedisState.Factory(poolConfig);TridentTopology topology = new TridentTopology();Stream stream = topology.newStream("spout1", spout);stream.partitionPersist(factory, fields, new RedisStateUpdater(storeMapper).withExpire(86400000), new Fields());TridentState state = topology.newStaticState(factory);stream = stream.stateQuery(state, new Fields("word"), new RedisStateQuerier(lookupMapper), new Fields("columnName","columnValue")); RedisClusterState:Set&lt;InetSocketAddress&gt; nodes = new HashSet&lt;InetSocketAddress&gt;();for (String hostPort : redisHostPort.split(",")) &#123; String[] host_port = hostPort.split(":"); nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));&#125;JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes).build();RedisStoreMapper storeMapper = new WordCountStoreMapper();RedisLookupMapper lookupMapper = new WordCountLookupMapper();RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);TridentTopology topology = new TridentTopology();Stream stream = topology.newStream("spout1", spout);stream.partitionPersist(factory, fields, new RedisClusterStateUpdater(storeMapper).withExpire(86400000, new Fields());TridentState state = topology.newStaticState(factory);stream = stream.stateQuery(state, new Fields("word"), new RedisClusterStateQuerier(lookupMapper), new Fields("columnName","columnValue")); storm版本:2.0.0-SNAPSHOT 欢迎关注我的公众号和博客： 原文：Storm Redis Integration]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
        <tag>Storm 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm BasicBolt vs RichBolt]]></title>
    <url>%2Fhow-to-choose-basicbolt-and-richbolt-of-storm.html</url>
    <content type="text"><![CDATA[IComponent 是所有组件的接口，例如 IBasicBolt、IRichBolt、IBatchBolt 都继承自 IComponent，为拓扑中所有组件提供共同的方法。BaseComponent 是 Storm 提供的一个比较方便的抽象类，这个抽象类及其子类都或多或少实现了其接口定义的部分方法。IBolt 接口是 IRichBolt 要继承的接口。还有一些以 Base 开头的 Bolt 类，如 BaseBasicBolt，BaseRichBolt 等，在这些类中所实现的方法都为空，或者返回值为 NULL。从下图中，可以从整体上看到这些类的关系图，从而理清这些类之间的关系及结构。 1. IComponent 与 BaseComponentIComponent 继承 Serializable 接口，为拓扑中所有组件提供共同的方法，是所有组件的接口：public interface IComponent extends Serializable &#123; void declareOutputFields(OutputFieldsDeclarer declarer); Map&lt;String, Object&gt; getComponentConfiguration();&#125; 使用如下方法为拓扑中的流声明输出模式，OutputFieldsDeclarer 用于声明输出流ID，输出字段以及每个输出流是否是直接流：void declareOutputFields(OutputFieldsDeclarer declarer); 使用如下方法声明针对当前组件的配置，只能覆盖 topology.* 配置。使用 TopologyBuilder 构建拓扑时，可以进一步覆盖该组件配置：Map&lt;String, Object&gt; getComponentConfiguration(); BaseComponent 是 Storm 提供的一个比较方便的抽象类，这个抽象类及其子类都简单实现了其接口定义的部分方法，所实现的方法返回值为 NULL：public abstract class BaseComponent implements IComponent &#123; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125; &#125; 2. IBoltIBolt 继承了 Serializable 接口，输入元组经过处理后输出相应的元组，可以执行过滤，连接以及聚合等操作。IBolt 可以不必立即处理接收的元组，而是保留元组在内存中以便稍后处理。 public interface IBolt extends Serializable &#123; void prepare(Map stormConf, TopologyContext context, OutputCollector collector); void execute(Tuple input); void cleanup();&#125; IBolt 的生命周期如下：在客户端上创建 IBolt 对象。在 Nimbus 上提交 Topology 后，创建出来的 IBolt 在序列化后被发送到具体执行的 Worker 上。在 Worker 上执行时，先调用 prepare 方法传入当前执行的上下文，然后调用 execute 方法，对元组进行处理。如果要参数化 IBolt 对象，需要通过构造函数来设置参数，并将参数保存在实例变量中（然后将其序列化并传送到跨集群执行的每个任务上）。使用传入的 OutputCollector 的 ack 方法或 fail 方法来反馈处理结果。 当初始化 Worker 上该组件的一个任务时会调用如下方法，并提供执行环境。stormConf 为 Bolt 提供配置，并与集群提供的配置进行合并。context 用来获取有关此任务在拓扑中的位置信息，包括此任务的任务ID和组件ID，输入和输出信息等。collector 用来从 Bolt 向下游发送元组，元组可以在任何时间点发送，不必处理完立即发送。collector 是线程安全的，可以保存在一个实例变量：void prepare(Map stormConf, TopologyContext context, OutputCollector collector); 处理单个输入元组时会调用如下方法。Tuple 对象包含有关它来自哪个组件/流/任务的元数据。可以使用 Tuple.getValue 访问元组的值。IBolt 不必立即处理元组，而是挂起稍后处理。使用 prepare 方法提供的 OutputCollector 来发送元组。要求所有输入元组使用 OutputCollector 的 ack 或 fail 方法进行反馈。否则，Storm 无法确定从 Spout 发送的元组什么时候完成：void execute(Tuple input); 当停掉 Bolt 实例时会调用如下方法，但是不保证一定会调用该方法：void cleanup(); 3. RichBolt VS BasicBoltStorm 提供了两种不同类型的 Bolt，分别是 RichBolt(IRichBolt, BaseRichBolt) 和 BasicBolt(IBasicBolt, BaseBasicBolt)，很多使用 Storm 的人无法分清 BasicBolt 和 RichBolt 之间的区别。我们的建议是尽可能的使用 BasicBolt。 这两个类继承的父类如第一个图所示，它们的共同之处是父类中都有 BaseComponent 和 ICompont。不同之处是 BaseRichBolt 实现有 IBolt 和 IRichBolt 接口，而 BaseBasicBolt 只有 IBasicBolt 接口。其实本质的区别在于 IBolt 和 IBasicBolt 的区别：public interface IBasicBolt extends IComponent &#123; void prepare(Map stormConf, TopologyContext context); void execute(Tuple input, BasicOutputCollector collector); void cleanup();&#125;public interface IBolt extends Serializable &#123; void prepare(Map stormConf, TopologyContext context, OutputCollector collector); void execute(Tuple input); void cleanup();&#125; RichBolt 继承 IBolt 接口，使用 OutputCollector 的如下方法来发送元组:// 向指定数据流发送锚定的元组, 需要向 Acker 发送 ack 确认, 可靠传递List&lt;Integer&gt; emit(String streamId, Tuple anchor, List&lt;Object&gt; tuple);// 向指定数据流发送未锚定的元组, 不需要向 Acker 发送 ack 确认, 是不可靠传递List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple);// 向默认数据流发送锚定的元组, 需要向 Acker 发送 ack 确认, 可靠传递List&lt;Integer&gt; emit(Tuple anchor, List&lt;Object&gt; tuple);// 向默认数据流发送未锚定的元组, 不需要向 Acker 发送 ack 确认, 是不可靠传递List&lt;Integer&gt; emit(List&lt;Object&gt; tuple); BasicBolt 使用 BasicOutputCollector 的如下方法来发送元组:// 向指定数据流发送锚定的元组, 需要向 Acker 发送 ack 确认, 可靠传递List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple);// 向默认数据流发送锚定的元组, 需要向 Acker 发送 ack 确认, 可靠传递List&lt;Integer&gt; emit(List&lt;Object&gt; tuple); 两个 Bolt 都可以实现可靠性消息传递，不过 RichBolt 需要自己做很多周边的事情（例如，建立 Anchor 树，以及手动 ACK/FAIL 通知 Acker），而 BasicBolt 则由 Storm 帮忙实现了很多周边的事情，实现起来方便简单。 4. 实现(不)可靠性消息传递下面我们看一下如何使用上面的 Bolt 来实现(不)可靠性消息传递。 (1) 使用 BaseRichBolt 实现不可靠的Boltpublic class SplitSentence extends BaseRichBolt &#123; private OutputCollector collector; public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; public void execute(Tuple tuple) &#123; String sentence = tuple.getString(0); for(String word: sentence.split(" ")) &#123; collector.emit(new Values(word)); &#125; // 在这即使Ack也是没有用处的 // collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word")); &#125; &#125; 这种方式我们没有手动建立 Anchor 树以及手动 Ack 或者 Fail，所以这是一种不可靠的消息传递方式。 (2) 使用 BaseRichBolt 实现可靠的Boltpublic class SplitSentence extends BaseRichBolt &#123; private OutputCollector collector; public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; public void execute(Tuple tuple) &#123; String sentence = tuple.getString(0); for(String word: sentence.split(" ")) &#123; collector.emit(tuple, new Values(word)); &#125; collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word")); &#125; &#125; 这种方式需要我们手动的建立 Anchor 树以及手动的 Ack 或者 Fail，所以这是一种可靠的消息传递方式。 (3) 使用 BaseBasicBolt 实现可靠的Boltpublic class SplitSentence extends BaseBasicBolt &#123; public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String sentence = tuple.getString(0); for(String word: sentence.split(" ")) &#123; collector.emit(new Values(word)); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word")); &#125; &#125; 这种方式由 Storm 自动帮助我们建立 Anchor 树以及发送 Ack 或者 Fail。这是一种可靠的消息传递方式。我们只需要关心业务逻辑即可。 英译对照: 直接流: direct stream 欢迎关注我的公众号和博客： 参考： Storm 的可靠性保证测试 IBasicBolt vs IRichBolt]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
        <tag>Storm 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Redis Bitmap简单快速实时计算指标]]></title>
    <url>%2Ffast-easy-realtime-metrics-using-redis-bitmaps.html</url>
    <content type="text"><![CDATA[传统上，度量指标一般由批处理作业执行（每小时运行，每天运行等）。Redis 中的 Bitmap 可以允许我们实时计算指标，并且非常节省空间。在1.28亿用户场景中，经典度量指标（如’日活’）在 MacBook Pro上只需不到50毫秒，而且只需要16 MB内存。 1. Bitmap 又可以称之为 Bitset。 Bitmap 或 Bitset 是一个由 0 和 1 构成的数组。在 Bitmap 中每一个 bit 被设置为 0 或 1，数组中的每个位置被称为 offset。AND，OR，XOR等操作符，以及其他位操作都是 Bitmaps 的常用操作。 2. 基数Bitmap 中 1 的个数称之为基数。我们有一种有效算法来计算基数，例如，在 MacBook Pro 上，在包含10亿位填充90％的 Bitmap 上计算基数耗时 21.1 ms。 3. Redis中的BitmapRedis 允许二进制键和二进制值。Bitmap 也是二进制值。将键指定 offset 设置为 0 或 1，setbit（key，offset，value） 操作需要用 O(1) 时间复杂度。 4. 一个简单的例子：每日活跃用户为了统计今天登录的不同用户，我们创建了一个 Bitmap，其中每个用户都由一个 offset 标识。当用户访问页面或执行操作时，会将表示用户ID的 offset 设置为 1。 在这个简单的例子中，每次用户登录时，我们都会执行：redis.setbit(daily_active_users，user_id，1) 这会将 daily_active_users Bitmap 键对应 offset 设置为1。这是一个 O(1) 时间复杂度操作。对此 Bitmap 进行基数统计会统计出今天一共登录了 9 个用户。键是 daily_active_users，值为 1011110100100101。 当然，由于每天活跃用户每天都会在改变，我们需要一种方法每天创建一个新的 Bitmap。我们只需在 Bitmap 键后面追加一个日期即可。例如，如果我们想要计算某天在音乐应用中播放至少1首歌曲的不同用户，我们可以将键名称设置为 play:yyyy-mm-dd。如果我们想要计算每小时播放至少一首歌曲的用户数量，我们可以将键名称设置为 play:yyyy-mm-dd-hh。为了计算每日指标，只要用户播放歌曲，我们就会在 play：yyyy-mm-dd 键中将用户对应的 bit 设置为1。redis.setbit(play:yyyy-mm-dd, user_id, 1) 今天播放歌曲的不同用户是存储以 play:yyyy-mm-dd 为键的值。要计算每周或每月度量指标，我们可以简单地计算一周或一个月中所有每日 Bitmap 的并集，然后计算结果 Bitmap 的总体基数。 你还可以非常轻松地提取更复杂的指标。例如，11月播放歌曲的会员用户为：(play:2011-11-01 ∪ play:2011-11-02 ∪...∪play:2011-11-30) ∩ premium:2011-11 5. 使用1.28亿用户进行性能比较下表显示了针对1.28亿用户在1天，7天和30天计算的比较。通过组合每日 Bitmap 计算7日和30日指标： 周期 耗时 (MS) 每日 50.2 每周 392.0 每月 1624.8 6. 优化在上面的示例中，我们可以通过在 Redis 中缓存计算的每日，每周，每月计数来优化每周和每月计算。 这是一种非常灵活的方法。缓存的另一个好处是它允许快速群组分析，例如使用手机的每周唯一用户 - 手机用户 Bitmap 与每周活跃用户 Bitmap 的交集。或者，如果我们想要滚动计算过去n天内的唯一用户，那么缓存每日唯一用户的计数会使这变得简单 - 只需从缓存中获取前n-1天并将其与实时每日计数结合起来即可，而这只需要50ms。 7. 示例代码下面的Java代码片段指定用户操作和日期来计算唯一用户：import redis.clients.jedis.Jedis;import java.util.BitSet;...Jedis redis = new Jedis("localhost");...public int uniqueCount(String action, String date) &#123; String key = action + ":" + date; BitSet users = BitSet.valueOf(redis.get(key.getBytes())); return users.cardinality();&#125; 下面的代码片段计算指定用户操作和日期列表的唯一用户：import redis.clients.jedis.Jedis;import java.util.BitSet;...Jedis redis = new Jedis("localhost");...public int uniqueCount(String action, String... dates)&#123; BitSet all = new BitSet(); for (String date : dates) &#123; String key = action + ":" + date; BitSet users = BitSet.valueOf(redis.get(key.getBytes())); all.or(users); &#125; return all.cardinality();&#125; 英译对照 基数: Population Count 欢迎关注我的公众号和博客： 原文:REDIS BITMAPS – FAST, EASY, REALTIME METRICS]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[影响Flink有状态函数和算子性能的3个重要因素]]></title>
    <url>%2Fperformance-factors-stateful-functions-operators-flink.html</url>
    <content type="text"><![CDATA[本文重点介绍开发人员在有状态流处理应用中使用 Flink 的 Keyed State 的函数或算子评估性能时应牢记的3个重要因素。 Keyed State 是 Flink 中两种状态中的其中一种，另一种是 Operator State。顾名思义，Keyed State 绑定到键，只适合处理来自 KeyedStream 数据的函数和算子。Operator State 和 Keyed State 之间的区别在于 Operator State 是作用于算子的每个并发实例上（子任务），而 Keyed State 是基于每个键的一个状态分区进行分区或分片。 下面我们讨论3个会影响 Flink Keyed State 性能的因素，在开发有状态流应用程序时应该记住这些因素。 1. 选择状态后端对 Flink 应用程序有状态函数或算子性能影响最大的是我们所选择的状态后端。最明显的因素是每个状态后端以不同的方式处理状态序列化以持久化保存。 例如，在使用 FsStateBackend 或 MemoryStateBackend 时，本地状态在运行时期间作为堆上对象进行维护，因此在访问和更新时开销比较低。仅在创建状态快照以创建 Flink 检查点或保存点时才会发生序列化开销。使用这些状态后端的缺点是状态大小受 JVM 堆大小的限制，并且可能会遇到 OutOfMemory 错误或垃圾回收的长暂停。 相反，诸如 RocksDBStateBackend 之类的核外（out-of-core）状态后端可以通过在磁盘上维护本地状态来允许更大的状态大小。需要权衡的是每个状态的读写都需要序列化/反序列化。 为了解决这个问题，如果你的状态大小很小并且预计不会超过堆大小，那么使用堆上后端将是明智的选择，因为它避免了序列化开销。否则，RocksDBStateBackend 将成为大型状态应用程序的首选。 2. 每个键的状态原语 ValueState / ListState / MapState 另一个重要因素是选择正确的状态原语。Flink 目前支持3个用于 Keyed State 的状态原语：ValueState，ListState和MapState。 Flink 新手可能犯的一个常见错误就是选择状态原语，例如，ValueState&lt;Map&lt;String，Integer &gt;&gt;，map 条目希望能随机访问。在这种情况下，使用 MapState &lt;String，Integer&gt; 肯定会更好，尤其是考虑到核外状态后端（例如 RocksDBStateBackend），在访问时需要序列化/反序列化整个 ValueState 状态，而对于 MapState，只会序列化每个条目。 3. 访问模式继上一节关于状态原语之后，我们已经知道访问状态的应用程序逻辑有助于我们确定使用哪种状态结构。正如开发人员在设计任何类型的应用程序时期望的那样，为应用程序的特定数据访问模式使用不合适的数据结构会对整体性能产生严重影响。 4. 结论开发人员应该考虑上述所有三个因素，因为它们可以在很大程度上影响 Flink 中有状态函数和算子的性能。 英译对照 算子: operator 状态后端: state backend 检查点: checkpoints 保存点: savepoints 状态原语: state primitives 欢迎关注我的公众号和博客： 原文:3 important performance factors for stateful functions and operators in Flink]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink如何实现新的流处理应用第二部分:版本化状态]]></title>
    <url>%2Fhow-apache-flink-enables-new-streaming-applications-part-2.html</url>
    <content type="text"><![CDATA[这是我们关于 Flink 如何实现新的流处理应用系列中的第二篇博文。第一部分介绍了事件时间和乱序处理。 这篇文章是关于版本化应用程序状态，后面是关于会话和高级窗口的文章。 1. 有状态数据流处理流处理可以分为无状态处理和有状态处理。无状态流处理应用仅是接收事件，然后基于接收的单个事件的信息产生某种响应(例如，报警或事件转换)。因此，没有”记忆”或聚合能力。但是在许多场景下还是有用的(例如，过滤，简单的转换)，许多有趣的流处理应用，例如基于时间窗口的聚合，复杂事件处理，多事件的模式匹配，以及事务处理都是有状态的。 早期的流处理系统，如 Apache Storm(使用 core API)不支持状态(Storm Trident，Storm 通过附带的库来支持状态)。Storm 程序可以在 Bolts 上定义 Java 对象来保存状态，与外部数据库和键/值存储系统进行交互，但是出现故障的时候，系统并不能提供状态的正确性保证，可能退回到 At-Least-Once 语义(数据重复)，或 At-Most-Once 语义(数据丢失)。这种缺乏准确性保证，再加上无法处理大数据流(高吞吐量)，使得必须使用像 Lambda 这样的混合解决方案。Flink 代表了新一代的流处理系统，并保证了状态的正确性，使得有状态的应用变得更加容易实现。在 Flink 程序中，你可以使用如下方式定义状态： 使用 Flink 的窗口转换操作，你可以定义基于事件时间或处理时间的时间窗口，计数窗口以及自定义窗口。请参阅这里了解 Flink 窗口的简短介绍。 使用 Checkpoint 接口，你可以注册任何类型的 Java/Scala 对象(例如，HashMap)，以确保在失败后能正确恢复。 使用 key/value 状态接口，你可以使用集群上通过键分区的状态。 状态在哪里存储？首先，所有上述形式的状态都存储在 Flink 可配置的 状态后端中。目前（注:发表此文时为2016年，现在有三种可选的状态后端），Flink 将状态存储在内存中，并将状态备份到文件系统中（例如，HDFS）。我们正在积极努力提供其他的状态后端和备份选项。例如，我们最近贡献了一个基于 RocksDB 的状态后端，而且我们正在开发一个使用 Flink 管理内存的状态后端，如果需要的话，可以从内存溢出到磁盘上。根据我们的经验，流处理应用程序，特别是有状态的流处理应用程序比批处理作业更难操作。批处理作业可以在一晚上运行完，如果结果不符合要求或者作业运行失败，可以重新运行。但是，流式作业 7*24 小时不间断运行，应用程序通常面向用户，因此不能随便地停止和重新运行。Flink 线上用户有必要担心在作业升级(应用程序代码和Flink本身)，出现故障以及应用程序和集群维护的过程中作业的表现情况。 2. 保存点：版本化状态在 Flink 中，我们引入了保存点功能，可以解决上述问题以及未来更多问题。保存点可以从正在运行的 Flink 作业上获取，实质上是在一个时间点上定义可以从外部访问的作业的快照。包含当前正在从数据源读取数据的偏移量，以在这个偏移量处的程序状态。在内部，保存点只是 Flink 普通的定期检查点，以保证在发生故障时的正确性。主要区别是： 保存点可以手动触发。 保存点永不过期，除非用户手动进行处理。 通过命令行使用指定 JobID 获取正在运行作业的保存点，只需运行:flink savepoint JobID 上述会返回存储保存点的路径(默认配置文件系统，例如本地，HDFS，S3等)。要从保存点恢复作业，只需运行如下即可:flink run -s pathToSavePoint jobJar 使用保存点，不必从头开始重新读取事件流以重新填充 Flink 作业的状态，因为你可以随时获取一致性快照并从该检查点恢复。另外，当日志保留期限有限时，定期保存状态是非常有用的，因为日志不能从头开始读取。另一种理解保存点的方式是在定义好的时间点保存应用程序状态的版本，类似于使用 git 等版本控制系统来保存应用程序的版本。最简单的例子是在修改应用程序代码的同时以一定时间间隔获取快照： 更重要的是，你可以从多个保存点分支出来，创建一个应用程序版本树： 这里，时间 t1 和 t2 分别在正在运行的作业 v0 上生成两个保存点，创建版本 v0t1 和 v0t2。他们都可以用来恢复作业。举个例子，利用 t1 时间点的保存点，我们使用修改了的应用程序代码来恢复作业，创建 v1 作业。在时间 t3 和 t4，分别从版本 v0 和 v1 获取更多的保存点。保存点可用于解决流式作业线上各种问题： 应用程序代码升级：假设你在已经运行的应用程序中发现了一个 bug，希望未来的事件能够使用修改错误后的代码来处理。通过获取作业的保存点，使用新的代码从该保存点重新启动，下游应用程序看不到任何差异。 Flink 版本升级：升级 Flink 本身也变得更容易，因为你可以获取正在运行数据流的保存点并使用升级后的 Flink 版本从保存点重新读取它们。 维护和迁移：使用保存点，可以轻松”暂停和恢复”应用程序。这对于集群维护以及将作业迁移到新集群尤其有用。另外，这对开发，测试和调试应用程序也非常有用，因为你不需要读取已经完成的事件流。 假设模拟(复原):很多时候，运行一个可选的应用程序逻辑来模拟过去可控制点的”假设”场景非常有用。 A/B测试：通过从完全相同的保存点并行运行两个不同版本的应用程序代码，可以对A/B测试场景进行建模。 3. 结论通过这篇文章，我们可以看到: 许多有趣的流式应用案例，如时间窗口上的聚合，复杂事件处理或模式匹配，在系统内都需要有状态程序的支持。Flink 对状态的支持使这些类型的应用程序成为可能，并允许 Flink 对状态的正确性(确切地说是一种语义)做出保证。 有状态流处理应用程序会面临许多操作上的问题，例如升级时的表现(应用程序代码和 Flink 本身)，出现故障以及应用程序和集群维护。Flink 对保存点的支持通过允许你对应用程序代码和状态进行版本化来帮助解决这些操作问题。 目前的限制是应用程序的并发度必须与生成保存点的应用程序的并发度相匹配。如何使用保存点，请查看有关保存点如何工作的文档以及如何如何使用命令行使用它们。 英译对照 状态: state 状态后端: state backend 偏移量: offset 原文:How Apache Flink™ Enables New Streaming Applications, Part 2]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4个步骤让Flink应用程序达到生产状态]]></title>
    <url>%2F4-steps-flink-application-production-ready.html</url>
    <content type="text"><![CDATA[这篇文章阐述了 Flink 应用程序达到生产状态所必须的配置步骤。在以下部分中，我们概述了在 Flink 作业达到生产状态之前技术领导、DevOps、工程师们需要仔细考虑的重要配置参数。Flink 为大多数配置选项都提供了开箱即用的默认选项，在许多情况下它们是POC阶段（概念验证）或探索 Flink 不同 API 和抽象的很好的起点。 然而，将 Flink 应用程序投入生产还需要额外的配置，这些配置可以高效地扩展应用程序规模，使其达到生产状态，并能与不同系统要求，Flink 版本，连接器兼容，以保证未来迭代和升级。 下面是我们收集的需要在 Flink 应用上线前检查的一些配置点： 1. 明确定义 Flink 算子的最大并发度Flink 的 KeyedState 是由 key group 进行组织，然后分发到 Flink 算子的各个并发实例上。这是分发的最小原子单元，因此也会影响 Flink 应用程序的可伸缩性。作业的每个算子的 key group 个数只能设置一次，可以手动配置或者直接使用默认配置。默认值粗略地使用 operatorParallelism * 1.5，下限 128，上限 32768 。可以通过 setMaxParallelism(int maxParallelism) 手动地为每个作业或算子设置最大并发度。 上线的任何作业都应该指定最大并发度。但是，一定要仔细考虑这个值的大小。因为一旦设置了最大并发度，就无法在以后更新。一个作业想要改变最大并发度，就只能从全新的状态重新开始。目前还无法在更改最大并发度后，从上一个成功的检查点或保存点恢复。 最大并发度设置后不能修改，修改的话会从全新的状态重新开始，因此需要仔细考虑最大并发度大小。 建议将最大并发度设置的足够大以满足未来应用程序的可扩展性和可用性，同时，又要选一个相对较低的值以避免影响应用程序整体的性能。这是因为 Flink 需要维护某些元数据来进行扩容，很高的最大并发度会增加 Flink 应用程序的整体状态大小。 最大并发度不能太大也不能太小。 Flink文档提供了有关使用检查点如何配置使用大状态的应用程序的其他信息和指导。 2. 为 Flink 算子分配唯一用户ID（UUID）对于有状态的 Flink 应用程序，建议为所有算子分配唯一的用户ID（UUID）。 这是非常有必要的，因为一些内置的 Flink 算子（如windows）是有状态的，而有些算子是无状态的，这就很难知道哪些内置算子是有状态的，哪些是没有状态。 可以使用 uid（String uid） 方法为 Flink 算子分配 UUID。算子 UUID 可以使 Flink 有效地将算子的状态从保存点映射到恰当的算子上，这是保存点在 Flink 应用程序中正常工作的一个基本要素。 3. 充分考虑 Flink 应用程序的状态后端由于 Flink 目前还不支持状态后端之间的互通性，所以开发人员和工程负责人在将应用程序上线前应仔细考虑 Flink 应用程序的状态后端类型。如果从保存点恢复状态，那么保存点必须采用相同的状态后端。 在我们之前的一篇博文中，详细说明了 Flink 目前支持的3种类型的状态后端之间的差异。 对于生产用例，强烈建议使用 RocksDB 状态后端，因为这是目前唯一一种支持大状态和异步操作（如快照）的状态后端，异步操作允许在不阻塞 Flink 操作的情况下进行快照。另一方面，使用 RocksDB 状态后端可能存在性能折衷，因为所有状态访问和检索都需要序列化（和反序列化）来跨越JNI边界，与内存状态后端相比这可能会影响应用程序的吞吐量。 4. 配置 Job Manager 的高可用性（HA）高可用性（HA）配置确保了 Flink 应用程序 JobManager 组件在出现潜在故障时可以自动恢复，从而将停机时间降至最低。JobManager 的主要职责是协调 Flink 部署，例如调度和适当的资源分配。 默认情况下，Flink 为每个 Flink 集群配置一个 JobManager 实例。这会产生单点故障（SPOF）：如果 JobManager 崩溃了，就会无法提交新程序，并且正在运行的程序也会失败。因此，强烈建议为生产用例配置高可用性（HA）。 上述4个步骤遵循社区设置的最佳实践，允许 Flink 应用程序在维护状态的同时任意扩展，处理更大容量的数据流和状态大小，并增加可用性保证。 英译对照： 算子: operator 保存点: savepoint 状态: state 原文:4 steps to get your Flink application ready for production]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有状态流处理:Flink状态后端]]></title>
    <url>%2Fstateful-stream-processing-apache-flink-state-backends.html</url>
    <content type="text"><![CDATA[这篇文章我们将深入探讨有状态流处理，更确切地说是 Flink 中可用的不同状态后端。在以下部分，我们将介绍 Flink 的3个状态后端，它们的局限性以及根据具体案例需求选择最合适的状态后端。 在有状态的流处理中，当开发人员启用了 Flink 中的检查点功能时，状态会持久化存储以防止数据的丢失并确保发生故障时能够完全恢复。为应用程序选择何种状态后端，取决于状态持久化的方式和位置。 Flink 提供了三种可用的状态后端：MemoryStateBackend，FsStateBackend，和RocksDBStateBackend。 1. MemoryStateBackendMemoryStateBackend 是将状态维护在 Java 堆上的一个内部状态后端。键值状态和窗口算子使用哈希表来存储数据值和定时器。当应用程序 checkpoint 时，状态后端会在将状态发给 JobManager 之前对状态进行快照，JobManager 会将状态存储在 Java 堆上。默认情况下，MemoryStateBackend 会配置成支持异步快照。异步快照可以避免阻塞数据流的处理，从而避免反压的发生。 使用 MemoryStateBackend 时的注意点： 默认情况下，每一个状态最大为 5 MB。可以通过 MemoryStateBackend 的构造函数增加最大大小。 状态大小受到 Akka 帧大小的限制，所以无论在配置中怎么配置状态大小，都不能大于 Akka 的帧大小。 状态的总大小不能超过 JobManager 的内存。 什么时候使用 MemoryStateBackend： 本地开发或调试时建议使用 MemoryStateBackend，因为这种场景的状态大小的是有限的。 MemoryStateBackend 非常适合状态比较小的用例和流处理程序。例如一次仅一条记录的函数（Map, FlatMap，或 Filter）或者 Kafka consumer。 2. FsStateBackendFsStateBackend 配置需要文件系统的 URL（类型，地址，路径）等来配置。举个例子，比如可以是： hdfs://namenode:40010/flink/checkpoints s3://flink/checkpoints 当选择 FsStateBackend 时，正在处理的数据会保存在 TaskManager 的内存中。在 checkpoint 时，状态后端会将状态快照写入配置的文件系统目录和文件中，同时会在 JobManager 或者 Zookeeper（在高可用场景下）的内存中存储极少的元数据。 默认情况下，FsStateBackend 会配置提供异步快照，以避免在写状态 checkpoint 时阻塞数据流的处理。该特性可以通过在实例化 FsStateBackend 时将布尔标志设置为 false 来禁用，例如：new FsStateBackend(path, false); 当前的状态仍然会先存在 TaskManager 中，所以状态的大小同样不能超过 TaskManager 的内存。 什么时候使用 FsStateBackend： FsStateBackend 非常适合处理大状态，长窗口，或大键值状态的有状态流处理作业。 FsStateBackend 非常适合高可用方案。 3. RocksDBStateBackendRocksDBStateBackend 的配置同样需要文件系统的 URL（类型，地址，路径）等来配置，如下所示： hdfs://namenode:40010/flink/checkpoints s3://flink/checkpoints RocksDBStateBackend 将正在处理的数据使用 RocksDB 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将增量差异数据存储到配置的文件系统中。该状态后端同时也会在 JobManager 或者 Zookeeper（在高可用场景下）的内存中存储极少的元数据。。RocksDB 默认也是配置成异步快照。 使用 RocksDBStateBackend 时的注意点： RocksDB 的每个 key 和 value 的最大大小为 2^31 字节。这是因为 RocksDB 的 JNI API 是基于 byte[] 的。 我们需要在此强调，对于使用合并操作的有状态流处理应用程序，例如 ListState，随着时间的推移可能会累积超过 2^31 字节大小，这将会导致后续的任何检索的失败。 何时使用 RocksDBStateBackend： RocksDBStateBackend 非常适合处理大状态，长窗口，或大键值状态的有状态流处理作业。 RocksDBStateBackend 非常适合高可用方案。 RocksDBStateBackend 是目前唯一支持有状态流处理应用程序增量检查点的状态后端。 在使用 RocksDB 时，状态大小只受限于磁盘可用空间的大小。这也使得 RocksDBStateBackend 成为管理超大状态的比较好的选择。使用 RocksDB 的权衡点在于所有状态的访问和检索都需要序列化（或反序列化）才能跨越 JNI 边界。与上面提到的堆上后端相比，这可能会影响应用程序的吞吐量。 不同的状态后端可以满足不同开发人员的需求，在开始开发应用程序之前应该仔细考虑和规划后选择。这可确保选择了正确的状态后端以最好地满足应用程序和业务需求。 英译对照： 状态后端：state backend 检查点: checkpointing 定时器: Timers 原文:Stateful Stream Processing: Apache Flink State Backends]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 如何使用ProcessFunction]]></title>
    <url>%2Fhow-to-user-process-function-of-flink.html</url>
    <content type="text"><![CDATA[1. ProcessFunctionProcessFunction 函数是低阶流处理算子，可以访问流应用程序所有（非循环）基本构建块： 事件 (数据流元素) 状态 (容错和一致性) 定时器 (事件时间和处理时间) ProcessFunction 可以被认为是一种提供了对 KeyedState 和定时器访问的 FlatMapFunction。每在输入流中接收到一个事件，就会调用来此函数来处理。 对于容错的状态，ProcessFunction 可以通过 RuntimeContext 访问 KeyedState，类似于其他有状态函数访问 KeyedState。 定时器可以对处理时间和事件时间的变化做一些处理。每次调用 processElement() 都可以获得一个 Context 对象，通过该对象可以访问元素的事件时间戳以及 TimerService。TimerService 可以为尚未发生的事件时间/处理时间实例注册回调。当定时器到达某个时刻时，会调用 onTimer() 方法。在调用期间，所有状态再次限定为定时器创建的键，允许定时器操作 KeyedState。 如果要访问 KeyedState 和定时器，那必须在 KeyedStream 上使用 ProcessFunction。 stream.keyBy(...).process(new MyProcessFunction()) 2. 低阶Join要在两个输入上实现低阶操作，应用程序可以使用 CoProcessFunction。这个函数绑定了两个不同的输入，并为来自两个不同输入的记录分别调用 processElement1() 和 processElement2()。 实现低阶 Join 通常遵循以下模式： 为一个输入（或两个）创建状态对象。 在从输入中收到元素时更新状态。 在从其他输入收到元素时扫描状态对象并生成 Join 结果。 例如，你可能会将客户数据与金融交易数据进行 Join，并将客户数据存储在状态中。如果你比较关心无序事件 Join 的完整性和确定性，那么当客户数据流的 Watermark 已经超过交易时间时，你可以使用定时器来计算和发出交易的 Join。 3. Example在以下示例中，KeyedProcessFunction 为每个键维护一个计数，并且会把一分钟(事件时间)内没有更新的键/值对输出： 计数，键以及最后更新的时间戳会存储在 ValueState 中，ValueState 由 key 隐含定义。 对于每条记录，KeyedProcessFunction 增加计数器并修改最后的时间戳。 该函数还会在一分钟后调用回调（事件时间）。 每次调用回调时，都会检查存储计数的最后修改时间与回调的事件时间时间戳，如果匹配则发送键/计数键值对（即在一分钟内没有更新） 这个简单的例子可以用会话窗口实现。在这里使用 KeyedProcessFunction 只是用来说明它的基本模式。 Java版本：import org.apache.flink.api.common.state.ValueState;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.ProcessFunction;import org.apache.flink.streaming.api.functions.ProcessFunction.Context;import org.apache.flink.streaming.api.functions.ProcessFunction.OnTimerContext;import org.apache.flink.util.Collector;// 数据源DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;// 对KeyedStream应用ProcessFunctionDataStream&lt;Tuple2&lt;String, Long&gt;&gt; result = stream .keyBy(0) .process(new CountWithTimeoutFunction());/** * 存储在state中的数据类型 */public class CountWithTimestamp &#123; public String key; public long count; public long lastModified;&#125;/** * 维护了计数和超时间隔的ProcessFunction实现 */public class CountWithTimeoutFunction extends KeyedProcessFunction&lt;Tuple, Tuple2&lt;String, String&gt;, Tuple2&lt;String, Long&gt;&gt; &#123; /** 这个状态是通过 ProcessFunction 维护*/ private ValueState&lt;CountWithTimestamp&gt; state; @Override public void open(Configuration parameters) throws Exception &#123; state = getRuntimeContext().getState(new ValueStateDescriptor&lt;&gt;("myState", CountWithTimestamp.class)); &#125; @Override public void processElement( Tuple2&lt;String, String&gt; value, Context ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; // 查看当前计数 CountWithTimestamp current = state.value(); if (current == null) &#123; current = new CountWithTimestamp(); current.key = value.f0; &#125; // 更新状态中的计数 current.count++; // 设置状态的时间戳为记录的事件时间时间戳 current.lastModified = ctx.timestamp(); // 状态回写 state.update(current); // 从当前事件时间开始注册一个60s的定时器 ctx.timerService().registerEventTimeTimer(current.lastModified + 60000); &#125; @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; // 得到设置这个定时器的键对应的状态 CountWithTimestamp result = state.value(); // 检查定时器是过时定时器还是最新定时器 if (timestamp == result.lastModified + 60000) &#123; // emit the state on timeout out.collect(new Tuple2&lt;String, Long&gt;(result.key, result.count)); &#125; &#125;&#125; Scala版本:import org.apache.flink.api.common.state.ValueStateimport org.apache.flink.api.common.state.ValueStateDescriptorimport org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.streaming.api.functions.ProcessFunctionimport org.apache.flink.streaming.api.functions.ProcessFunction.Contextimport org.apache.flink.streaming.api.functions.ProcessFunction.OnTimerContextimport org.apache.flink.util.Collector// the source data streamval stream: DataStream[Tuple2[String, String]] = ...// apply the process function onto a keyed streamval result: DataStream[Tuple2[String, Long]] = stream .keyBy(0) .process(new CountWithTimeoutFunction())/** * The data type stored in the state */case class CountWithTimestamp(key: String, count: Long, lastModified: Long)/** * The implementation of the ProcessFunction that maintains the count and timeouts */class CountWithTimeoutFunction extends KeyedProcessFunction[Tuple, (String, String), (String, Long)] &#123; /** The state that is maintained by this process function */ lazy val state: ValueState[CountWithTimestamp] = getRuntimeContext .getState(new ValueStateDescriptor[CountWithTimestamp]("myState", classOf[CountWithTimestamp])) override def processElement( value: (String, String), ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#Context, out: Collector[(String, Long)]): Unit = &#123; // initialize or retrieve/update the state val current: CountWithTimestamp = state.value match &#123; case null =&gt; CountWithTimestamp(value._1, 1, ctx.timestamp) case CountWithTimestamp(key, count, lastModified) =&gt; CountWithTimestamp(key, count + 1, ctx.timestamp) &#125; // write the state back state.update(current) // schedule the next timer 60 seconds from the current event time ctx.timerService.registerEventTimeTimer(current.lastModified + 60000) &#125; override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#OnTimerContext, out: Collector[(String, Long)]): Unit = &#123; state.value match &#123; case CountWithTimestamp(key, count, lastModified) if (timestamp == lastModified + 60000) =&gt; out.collect((key, count)) case _ =&gt; &#125; &#125;&#125; 在 Flink 1.4.0 版本之前，当调用处理时间定时器时，ProcessFunction.onTimer() 方法会将当前处理时间设置为事件时间时间戳。用户可能会注意不到，但是这是有问题的，因为处理时间时间戳是不确定的，不与 Watermark 对齐。此外，如果用户实现的逻辑依赖于这个错误的时间戳，很可能会出现出乎意料的错误。升级到 1.4.0 版本后，使用不正确的事件时间戳的作业会失败，用户必须将作业调整为正确的逻辑。 4. KeyedProcessFunctionKeyedProcessFunction 作为 ProcessFunction 的扩展，可以在 onTimer() 方法中访问定时器的键： Java版本:@Overridepublic void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;OUT&gt; out) throws Exception &#123; K key = ctx.getCurrentKey(); // ...&#125; Scala版本:override def onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT]): Unit = &#123; var key = ctx.getCurrentKey // ...&#125; 5. 定时器TimerService 在内部维护两种类型的定时器（处理时间和事件时间定时器）并排队执行。 TimerService 会删除每个键和时间戳重复的定时器，即每个键在每个时间戳上最多有一个定时器。如果为同一时间戳注册了多个定时器，则只会调用一次 onTimer（） 方法。 Flink同步调用 onTimer() 和 processElement() 方法。因此，用户不必担心状态的并发修改。 5.1 容错定时器具有容错能力，并且与应用程序的状态一起进行快照。如果故障恢复或从保存点启动应用程序，就会恢复定时器。 在故障恢复之前应该触发的处理时间定时器会被立即触发。当应用程序从故障中恢复或从保存点启动时，可能会发生这种情况。 5.2 定时器合并由于 Flink 仅为每个键和时间戳维护一个定时器，因此可以通过降低定时器的频率来进行合并以减少定时器的数量。 对于频率为1秒的定时器（事件时间或处理时间），我们可以将目标时间向下舍入为整秒数。定时器最多提前1秒触发，但不会迟于我们的要求，精确到毫秒。因此，每个键每秒最多有一个定时器。 Java版本:long coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerProcessingTimeTimer(coalescedTime); Scala版本:val coalescedTime = ((ctx.timestamp + timeout) / 1000) * 1000ctx.timerService.registerProcessingTimeTimer(coalescedTime) 由于事件时间定时器仅当 Watermark 到达时才会触发，因此我们可以将当前 Watermark 与下一个 Watermark 的定时器一起调度和合并： Java版本:long coalescedTime = ctx.timerService().currentWatermark() + 1;ctx.timerService().registerEventTimeTimer(coalescedTime); Scala版本:val coalescedTime = ctx.timerService.currentWatermark + 1ctx.timerService.registerEventTimeTimer(coalescedTime) 可以使用一下方式停止一个处理时间定时器：Java版本:long timestampOfTimerToStop = ...ctx.timerService().deleteProcessingTimeTimer(timestampOfTimerToStop); Scala版本:val timestampOfTimerToStop = ...ctx.timerService.deleteProcessingTimeTimer(timestampOfTimerToStop) 可以使用一下方式停止一个事件时间定时器：             Java版本：long timestampOfTimerToStop = ...ctx.timerService().deleteEventTimeTimer(timestampOfTimerToStop); Scala版本：val timestampOfTimerToStop = ...ctx.timerService.deleteEventTimeTimer(timestampOfTimerToStop) 如果没有给指定时间戳注册定时器，那么停止定时器没有任何效果。                                Flink版本:1.8 英译对照: 事件: Events 状态: State 定时器: Timers 欢迎关注我的公众号和博客： 原文:Process Function (Low-level Operations)]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink如何管理Kafka的消费偏移量]]></title>
    <url>%2Fhow-flink-manages-kafka-consumer-offsets.html</url>
    <content type="text"><![CDATA[在这篇文章中我们将结合例子逐步讲解 Flink 是如何与 Kafka 工作来确保将 Kafka Topic 中的消息以 Exactly-Once 语义处理。 检查点(Checkpoint)是一种能使 Flink 从故障恢复的内部机制。检查点是 Flink 应用程序状态的一致性副本，包括了输入的读取位点。如果发生故障，Flink 通过从检查点加载应用程序状态来恢复应用程序，并从恢复的读取位点继续处理，就好像什么事情都没发生一样。你可以把检查点理解为电脑游戏的存档。如果你在游戏中存档之后发生了什么事情，你可以随时读档重来一次。 检查点使 Flink 具有容错能力，并确保在发生故障时也能保证流应用程序的语义。检查点每隔固定的间隔来触发，该间隔可以在应用中配置。 Flink 中的 Kafka 消费者是一个有状态的算子(operator)并且集成了 Flink 的检查点机制，它的状态是所有 Kafka 分区的读取偏移量。当一个检查点被触发时，每一个分区的偏移量都保存到这个检查点中。Flink 的检查点机制保证了所有算子任务的存储状态都是一致的，即它们存储状态都是基于相同的输入数据。当所有的算子任务成功存储了它们的状态，一个检查点才成功完成。因此，当从潜在的系统故障中恢复时，系统提供了 Excatly-Once 的状态更新语义。 下面我们将一步步的介绍 Flink 如何对 Kafka 消费偏移量做检查点的。在本文的例子中，数据存储在 Flink 的 JobMaster 中。值得注意的是，在 POC 或生产用例下，这些数据通常是存储到一个外部文件系统（如HDFS或S3）中。 1. 第一步如下实例，从包含两个分区的 Kafka Topic 中读取数据，每个分区都含有 ‘A’, ‘B’, ‘C’, ‘D’, ‘E’ 5条消息。我们将两个分区的偏移量都设置为0。 2. 第二步第一步，Kafka 消费者开始从分区 0 读取消息。消息 ‘A’ 正在被处理，第一个消费者的偏移量变成了1。 3. 第三步第三步，消息 ‘A’ 到达了 Flink Map Task。两个消费者都开始读取他们下一条消息（分区 0 读取 ‘B’，分区 1 读取 ‘A’）。两个分区各自将偏移量更新为 2 和 1 。同时，Flink 的 JobMaster 决定在 source 触发一个检查点。 4. 第四步接下来，Kafka 消费者为它们的状态创建第一个快照（”offset = 2, 1”），并存储到 Flink 的 JobMaster 中。Source 在消息 ‘B’ 和 ‘A’ 后面发送一个 checkpoint barrier。Checkopint barrier 用于在所有算子任务之间对齐检查点，保证了整个检查点的一致性。消息 ‘A’ 到达了 Flink Map Task，而上面的消费者继续读取下一条消息（消息 ‘C’）。 5. 第五步这一步中，Flink Map Task 接收到两个 Source 的 checkpoint barrier 后（对齐 barrier），那么就会将它自己的状态存储到 JobMaster。同时，消费者会继续从 Kafka 分区中读取更多消息。 6. 第六步一旦 Flink Map Task 完成了状态快照后，会与 Flink JobMaster 进行通信（汇报已经完成了checkpoint）。当一个作业的所有的 Task 都确认完成状态快照后，JobMaster 就成功完成了这个 checkpoint。从此刻开始，这个 checkpoint 就可以用于故障恢复了。值得一提的是，Flink 并不依赖 Kafka 的偏移量从系统故障中恢复。 7. 故障恢复在发生故障时（例如，某个 worker 崩溃），所有的算子任务都会被重启，而他们的状态会被重置到最近一次成功的 checkpoint。如下图所示： Kafka Source 分别从偏移量 2 和 1 重新开始读取消息（因为这是最近一次成功的 checkpoint 中的偏移量）。当作业重启后，我们可以期待正常的系统操作，就好像之前没有发生故障一样。 备注： 偏移量 Offset 算子 operator 分区 partition 消费者 consumer 原文:How Apache Flink manages Kafka consumer offsets]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 轻松理解Watermark]]></title>
    <url>%2Fwatermarks-in-apache-flink-made-easy.html</url>
    <content type="text"><![CDATA[当我们第一次使用 Flink 时，可能会对 Watermark 感到困惑，其实 Watermark 并不复杂。让我们通过一个简单的例子来说明为什么我们需要 Watermark，以及它是如何工作的。 在下文中的例子中，我们有一个带有时间戳的事件流，这些事件并不是按顺序到达的。图中的数字表示事件发生的时间戳。第一个事件在时间 4 到达，后面跟着一个发生更早时间的事件（时间 2），以此类推： 注意这是一个基于事件时间处理的例子，这意味着时间戳反映的是事件发生的时间，而不是事件处理的时间。基于事件时间处理的强大之处在于创建流处理程序无论是处理实时的数据还是重新处理历史的数据，都能保证结果的一致。 关于不同概念时间（例如事件时间，处理时间或摄入时间）的更多信息可以参考Flink1.4 事件时间与处理时间。 现在假设我们正在创建一个排序的数据流。这意味着应用程序处理流中的乱序到达的事件，并生成同样事件但按时间戳（事件时间）排序的新数据流。 1. 理解一数据流中的第一个元素是时间 4，但是我们不能直接将它作为排序后数据流的第一个元素输出。因为数据可能是乱序到达的，可能还有一个更早发生的数据还没有到达。事实上，我们可以预见这个流的一些未来。我们知道新数据流至少要等到时间 2 到达后才输出结果。 进行缓存，就必然有延迟。 2. 理解二如果我们做错了（没有更早的数据了），我们可能会永远等待下去。首先，我们应用程序看到的第一个事件是时间 4，然后是时间 2 。是否会有一个比时间 2 更早的数据到达呢？也许会，也许不会。我们可以一直等下去，但可能会永远等不到时间 1 。 我们不能无限制的等待下去，我们必须果敢地输出时间 2 作为排序后新数据流的第一个结果。 3. 理解三我们需要定义某种策略来决定什么时候不再去停止等待更早数据的到来。 这就是 Watermark 的作用，定义了什么时候不再等待更早的数据。 Flink 中基于事件时间的处理依赖于一种特殊的带时间戳的元素，我们称之为 Watermark，它们由数据源或是 Watermark 生成器插入数据流中。带有时间戳 t 的 Watermark 可以理解为所有时间戳小于等于 t 的事件都（在某种合理的概率上）已经到达了。 我们什么时候应该停止等待，然后输出时间 2 来开启新的数据流？当时间戳大于等于 2 的 Watermark 到达时我们停止等待。 4. 理解四 我们有不同的策略来生成 Watermark。 我们都知道每个事件都会在延迟一段时间后到达，而不同事件的延迟会不一样，所以会有些事件比其他事件延迟更多。一种简单的方法是假设这些延迟不会超过某个最大值延迟时间。Flink 把这种策略称之为有界无序 Watermark（bounded-out-of-orderness）。当然也有很多更复杂的方法来生成 Watermark，但是对于大多数应用来说，固定延迟的方法已经足够了。 如果想要创建一个类似排序的流应用程序，可以使用 Flink 的 ProcessFunction。它提供了对事件时间计时器（即，基于 Watermark 到达触发的回调）的访问，还提供了管理状态的接口（缓存事件直到它们发送到下游）。 原文:Watermarks in Apache Flink Made Easy]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 状态管理和容错机制介绍]]></title>
    <url>%2Fflink-introduction-to-state-management-and-fault-tolerance.html</url>
    <content type="text"><![CDATA[本文主要内容如下： 有状态的流数据处理； Flink中的状态接口； 状态管理和容错机制实现； 1. 有状态的流数据处理1.1 什么是有状态的计算计算任务的结果不仅仅依赖于输入，还依赖于它的当前状态，其实大多数的计算都是有状态的计算。比如wordcount,给一些word,其计算它的count,这是一个很常见的业务场景。count做为输出，在计算的过程中要不断的把输入累加到count上去，那么count就是一个state。 1.2 传统的流计算系统缺少对于程序状态的有效支持 状态数据的存储和访问; 状态数据的备份和恢复; 状态数据的划分和动态扩容; 在传统的批处理中，数据是划分为块分片去完成的，然后每一个Task去处理一个分片。当分片执行完成后，把输出聚合起来就是最终的结果。在这个过程当中，对于state的需求还是比较小的。 对于流计算而言，对State有非常高的要求，因为在流系统中输入是一个无限制的流，会运行很长一段时间，甚至运行几天或者几个月都不会停机。在这个过程当中，就需要将状态数据很好的管理起来。很不幸的是，在传统的流计算系统中，对状态管理支持并不是很完善。比如storm，没有任何程序状态的支持，一种可选的方案是storm+hbase这样的方式去实现，把状态数据存放在Hbase中，计算的时候再次从Hbase读取出来，做更新在写入进去。这样就会有如下几个问题： 流计算系统的任务和Hbase的数据存储有可能不在同一台机器上，导致性能会很差。这样经常会做远端的访问，走网络和存储； 备份和恢复是比较困难，因为Hbase是没有回滚的，要做到 Exactly-Onces 语义很困难。在分布式环境下，如果程序出现故障，只能重启Storm，那么Hbase的数据也就无法回滚到之前的状态。比如广告计费的这种场景，Storm+Hbase是是行不通的，出现的问题是钱可能就会多算，解决以上的办法是Storm+Mysql，通过Mysql的回滚解决一致性的问题。但是架构会变得非常复杂。性能也会很差，要commit确保数据的一致性。 对于storm而言状态数据的划分和动态扩容也是非常难做。一个很严重的问题是所有用户都会在Strom上重复的做这些工作，比如搜索，广告都要在做一遍，由此限制了部门的业务发展。 1.3 Flink丰富的状态访问和高效的容错机制Flink在最早设计的时候就意识到了这个问题，并提供了丰富的状态访问和容错机制。如下图所示： 2. Flink中的状态管理按照数据的划分和扩张方式，Flink中大致分为2类： Keyed States Operator States 2.1 Keyed StatesKeyed States 的使用： Flink也提供了Keyed States多种数据结构类型： Keyed States的动态扩容： 2.2 Operator State2.2.1 Operator States的使用： Operator States的数据结构不像Keyed States丰富，现在只支持List。 2.2.2 Operator States多种扩展方式 Operator States的动态扩展是非常灵活的，现提供了3种扩展，下面分别介绍： ListState:并发度在改变的时候，会将并发上的每个List都取出，然后把这些List合并到一个新的List,然后根据元素的个数在均匀分配给新的Task; UnionListState:相比于ListState更加灵活，把划分的方式交给用户去做，当改变并发的时候，会将原来的List拼接起来。然后不做划分，直接交给用户； BroadcastState:如大表和小表做Join时，小表可以直接广播给大表的分区，在每个并发上的数据都是完全一致的。做的更新也相同，当改变并发的时候，把这些数据COPY到新的Task即可以上是Flink Operator States提供的3种扩展方式，用户可以根据自己的需求做选择。 2.2.3 使用Checkpoint提高程序的可靠性用户可以根据的程序里面的配置将checkpoint打开，给定一个时间间隔后，框架会按照时间间隔给程序的状态进行备份。当发生故障时，Flink会将所有Task的状态一起恢复到Checkpoint的状态。从哪个位置开始重新执行。 Flink也提供了多种正确性的保障，包括： AT LEAST ONCE; Exactly-once; 2.2.4 备份为保存在State中的程序状态数据Flink也提供了一套机制，允许把这些状态放到内存当中。做Checkpoint的时候，由Flink去完成恢复。 2.2.5 从已停止作业的运行状态中恢复当组件升级的时候，需要停止当前作业。这个时候需要从之前停止的作业当中恢复，Flink提供了2种机制恢复作业: Savepoint:是一种特殊的checkpoint，只不过不像checkpoint定期的从系统中去触发的，它是用户通过命令触发，存储格式和checkpoint也是不相同的，会将数据按照一个标准的格式存储，不管配置什么样，Flink都会从这个checkpoint恢复，是用来做版本升级一个非常好的工具； External Checkpoint：对已有checkpoint的一种扩展，就是说做完一次内部的一次Checkpoint后，还会在用户给定的一个目录中，多存储一份checkpoint的数据； 3. 状态管理和容错机制实现下面介绍一下状态管理和容错机制实现方式。 3.1 StateBackendFlink提供了3种不同的StateBackend MemoryStateBackend FsStateBackend RockDBStateBackend 用户可以根据自己的需求选择，如果数据量较小，可以存放到 MemoryStateBackend 和 FsStateBackend 中，如果数据量较大，可以放到 RockDB 中。下面介绍 HeapKeyedStateBackend 和 RockDBKeyedStateBackend。 3.1.1 HeapKeyedStateBackend 3.1.2 RockDBKeyedStateBackend 3.2 Checkpoint3.2.1 Checkpoint的执行流程Checkpoint的执行流程是按照Chandy-Lamport算法实现的: 3.2.2 Checkpoint Barrier的对齐 3.2.3 全量Checkpoint全量Checkpoint会在每个节点做备份数据时，只需要将数据都便利一遍，然后写到外部存储中，这种情况会影响备份性能。在此基础上做了优化。 3.2.4 RockDB的增量CheckpointRockDB的数据会更新到内存，当内存满时，会写入到磁盘中。增量的机制会将新产生的文件COPY持久化中，而之前产生的文件就不需要COPY到持久化中去了。通过这种方式减少COPY的数据量，并提高性能。 原文：Flink状态管理和容错机制介绍]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.7发布中的新功能]]></title>
    <url>%2Fapache-flink-1-7-0-release.html</url>
    <content type="text"><![CDATA[Apache Flink 社区正式宣布 Apache Flink 1.7.0 发布。最新版本包括解决了420多个问题以及令人兴奋的新增功能，我们将在本文进行描述。有关更多的详细信息请查看完整目录。 Flink 1.7.0 版本与 1.xy 版本使用 @Public 注解注释的API兼容。该版本现已上市，我们鼓励大家下载该版本并查看更新的文档。 1. Flink 1.7.0 - 扩展流处理的范围在 Flink 1.7.0，我们更关注实现快速数据处理以及以无缝方式为 Flink 社区构建数据密集型应用程序。我们最新版本包括一些令人兴奋的新功能和改进，例如对 Scala 2.12 的支持，Exactly-Once 语义的 S3 文件接收器，复杂事件处理与流SQL的集成，更多的功能我们在下面解释。 2. 新功能与改进2.1 Flink中的Scala 2.12支持 FLINK-7811 Flink 1.7.0 是第一个完全支持 Scala 2.12 的版本。这可以让用户使用新的 Scala 版本编写 Flink 应用程序以及利用 Scala 2.12 的生态系统。 2.2 状态变化 FLINK-9376 在许多情况下，由于需求的变化，长期运行的 Flink 应用程序会在其生命周期内发生变化。在不丢失当前应用程序进度状态的情况下更改用户状态是应用程序变化的关键要求。Flink 1.7.0 版本中社区添加了状态变化，允许我们灵活地调整长时间运行的应用程序的用户状态模式，同时保持与先前保存点的兼容。通过状态变化，我们可以在状态模式中添加或删除列。当使用 Avro 生成类作为用户状态时，状态模式变化可以开箱即用，这意味着状态模式可以根据 Avro 的规范进行变化。虽然 Avro 类型是 Flink 1.7 中唯一支持模式变化的内置类型，但社区仍在继续致力于在未来的 Flink 版本中进一步扩展对其他类型的支持。 evolution 译为 变化 2.3 Exactly-once语义的S3 StreamingFileSink FLINK-9752 Flink 1.6.0 中引入的 StreamingFileSink 现在已经扩展到 S3 文件系统，并保证 Exactly-once 语义。使用此功能允许所有 S3 用户构建写入 S3 的 Exactly-once 语义端到端管道。 2.4 Streaming SQL中支持MATCH_RECOGNIZE FLINK-6935 这是 Apache Flink 1.7.0 的一个重要补充，它为 Flink SQL 提供了 MATCH_RECOGNIZE 标准的初始支持。此功能融合了复杂事件处理（CEP）和SQL，可以轻松地对数据流进行模式匹配，从而实现一整套新的用例。此功能目前处于测试阶段。 2.5 Streaming SQL中的 Temporal Tables 和 Temporal Joins FLINK-9712 Temporal Tables 是 Apache Flink 中的一个新概念，它为表的更改历史记录提供（参数化）视图，可以返回表在任何时间点的内容。例如，我们可以使用具有历史货币汇率的表。随着时间的推移，表会不断发生变化，并增加更新的汇率。Temporal Table 是一种视图，可以返回汇率在任何时间点的实际状态。通过这样的表，可以使用正确的汇率将不同货币的订单流转换为通用货币。 Temporal Joins 允许 Streaming 数据与不断变化/更新的表的内存和计算效率的连接，使用处理时间或事件时间，同时符合ANSI SQL。 流式 SQL 的其他功能除了上面提到的主要功能外，Flink 的 Table＆SQL API 已经扩展到更多用例。以下内置函数被添加到API：TO_BASE64，LOG2，LTRIM，REPEAT，REPLACE，COSH，SINH，TANH。SQL Client 现在支持在环境文件和 CLI 会话中自定义视图。此外，CLI 中还添加了基本的 SQL 语句自动完成功能。社区添加了一个 Elasticsearch 6 table sink，允许存储动态表的更新结果。 2.6 版本化REST API FLINK-7551 从 Flink 1.7.0 开始，REST API 已经版本化。这保证了 Flink REST API 的稳定性，因此可以在 Flink 中针对稳定的 API开发第三方应用程序。因此，未来的 Flink 升级不需要更改现有的第三方集成。 2.7 Kafka 2.0 Connector FLINK-10598 Apache Flink 1.7.0 继续添加更多的连接器，使其更容易与更多外部系统进行交互。在此版本中，社区添加了 Kafka 2.0 连接器，可以从 Kafka 2.0 读写数据时保证 Exactly-Once 语义。 2.8 本地恢复 FLINK-9635 Apache Flink 1.7.0 通过扩展 Flink 的调度来完成本地恢复功能，以便在恢复时考虑之前的部署位置。如果启用了本地恢复，Flink 将在运行任务的机器上保留一份最新检查点的本地副本。将任务调度到之前的位置，Flink 可以通过从本地磁盘读取检查点状态来最小化恢复状态的网络流量。此功能大大提高了恢复速度。 2.9 删除Flink的传统模式 FLINK-10392 Apache Flink 1.7.0 标志着 Flip-6 工作已经完全完成并且与传统模式达到功能奇偶校验。因此，此版本删除了对传统模式的支持。 欢迎关注我的公众号： 原文: What’s new in the latest Apache Flink 1.7.0 release]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Realease</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink如何实现端到端的Exactly-Once处理语义]]></title>
    <url>%2Fend-to-end-exactly-once-processing-apache-flink-apache-kafka.html</url>
    <content type="text"><![CDATA[这篇文章改编自2017年柏林Flink Forward上Piotr Nowojski的演讲。你可以在Flink Forward Berlin网站上找到幻灯片和演示文稿。 2017年12月发布的Apache Flink 1.4.0为Flink的流处理引入了一个重要特性：TwoPhaseCommitSinkFunction 的新功能（此处为相关的Jira），提取了两阶段提交协议的通用逻辑，使得在Flink和一系列数据源和接收器（包括Apache Kafka 0.11 版本以及更高版本）之间构建端到端的 Exactly-Once 语义的应用程序成为可能。它提供了一个抽象层，用户只需实现几个方法就可以实现端到端的 Exactly-Once 语义。 如果这就是你需要了解的全部内容，可以去这个地方了解如何使用 TwoPhaseCommitSinkFunction。或者你可以直接去看实现 Exactly-Once 语义的Kafka 0.11 producer的文档，这也是在 TwoPhaseCommitSinkFunction 之上实现的。 如果你想了解更多信息，我们将在这篇文章中去深入了解一下新特性以及在 Flink 幕后发生的事情。 纵览全篇，有以下几点： 描述一下 Flink 检查点在Flink应用程序保证 Exactly-Once 语义的作用。 展现 Flink 如何通过两阶段提交协议与数据源（source）和数据接收器（sink）交互，以提供端到端的 Exactly-Once 语义保证。 通过一个简单的示例，了解如何使用 TwoPhaseCommitSinkFunction 实现一个 Exactly-Once 语义的文件接收器。 1. Flink应用程序的Exactly-Once语义当我们说Exactly-Once语义时，我们的意思是每个传入的事件只会影响最终结果一次。即使机器或软件出现故障，也没有重复数据，也没有丢失数据。 Flink 在很久之前就提供了 Exactly-Once 语义。在过去几年中，我们已经深入探讨过 Flink 的检查点，这是 Flink 提供 Exactly-Once 语义的核心。Flink文档提供了这个功能的全面概述。 在继续之前，我们先对检查点机制进行简要概述，这对我们理解检查点是有必要的。Flink 中的检查点是以下内容的一致快照： 应用程序的当前状态 输入流中的位置 Flink 以固定的时间间隔（可配置）生成检查点，然后将检查点写入持久存储系统，例如S3或HDFS。将检查点数据写入持久存储是异步发生的，这意味着 Flink 应用程序在写检查点过程中可以继续处理数据。 如果发生机器或软件故障重新启动后，Flink 应用程序从最近成功完成的检查点恢复。在处理开始之前，Flink 从检查点恢复应用程序状态并回滚到输入流中的正确位置。这意味着 Flink 的计算结果就好像从未发生过故障一样。 在 Flink 1.4.0 之前，Exactly-Once 语义仅局限于 Flink 应用程序内部，不能扩展到 Flink 在数据处理完后发送的大多数外部系统。Flink 应用程序与各种数据输出端进行交互，开发人员需要有能力自己维护组件的上下文来保证 Exactly-Once 语义。 为了提供端到端的 Exactly-Once 语义 - 也就是说，除了 Flink 应用程序之外，这些语义也同样适用于 Flink 写入的外部系统中 - 这些外部系统必须提供提交或回滚的方法，然后通过 Flink 的检查点机制来协调。 在分布式系统中的协调提交和回滚的一种常用方法是两阶段提交协议。下面我们会讨论 Flink 的 TwoPhaseCommitSinkFunction 是如何利用两阶段提交协议提供端到端的 Exactly-Once 语义。 2. Flink的端到端Exactly-Once语义应用程序下面我们将介绍两阶段提交协议以及它如何在一个读取和写入 Kafka 的 Flink 应用程序示例中实现端到端的 Exactly-Once 语义。Kafka 是一个流行的消息中间件系统，经常与 Flink 一起使用。Kafka 在 0.11 版本中添加了对事务的支持。这意味着当你通过 Flink 读写 Kafka时，有必要提供端到端的 Exactly-Once 语义的支持。 Flink 对端到端 Exactly-Once 语义的支持不仅限于 Kafka，可以与任何提供协调机制的数据源/接收器一起使用。例如，来自Dell/EMC的开源流处理存储系统 Pravega 也可以通过 TwoPhaseCommitSinkFunction 提供 Flink 端到端 Exactly-Once 语义。 在我们今天要讨论的 Flink 应用程序示例中，我们有： 从 Kafka 读取数据的数据源（在 Flink 为 KafkaConsumer） 窗口聚合 将数据写回 Kafka 的数据接收器（在 Flink 为 KafkaProducer） 要使数据接收器提供 Exactly-Once 语义保证，必须在一个事务中将所有数据写入 Kafka。提交捆绑了两个检查点之间的所有写入数据。这可确保在发生故障时能回滚所有写入的数据。 但是，在具有多个并发运行的接收器任务的分布式系统中，简单的提交或回滚是远远不够的，因为必须确保所有组件在提交或回滚时一致才能确保一致的结果。Flink 使用两阶段提交协议及预提交阶段来解决这一问题。 检查点的启动表示我们的两阶段提交协议的预提交阶段。当检查点启动时，Flink JobManager 会将检查点 Barrier 注入数据流中（将数据流中的记录分为进入当前检查点的集合与进入下一个检查点的集合）。 Barrier 在算子之间传递。对于每个算子，它会触发算子状态后端生成状态的快照。 数据源存储 Kafka 的偏移量，完成此操作后将检查点 Barrier 传递给下一个算子。 这种方法只适用于算子只有内部状态（Internal state）的情况。内部状态是 Flink 状态可以存储和管理的所有内容 - 例如，第二个算子中的窗口总和。当一个进程只有内部状态时，除了写入到已定义的状态变量之外，不需要在预提交阶段执行任何其他操作。Flink 负责在检查点成功的情况下正确提交这些写入，或者在出现故障时中止这些写入。 但是，当一个进程具有外部状态（External state）时，状态处理会有所不同。外部状态通常以写入外部系统（如Kafka）的形式出现。在这种情况下，为了提供 Exactly-Once 语义保证，外部系统必须支持事务，这样才能和两阶段提交协议集成。 我们示例中的数据接收器具有外部状态，因为它正在向 Kafka 写入数据。在这种情况下，在预提交阶段，除了将其状态写入状态后端之外，数据接收器还必须预先提交其外部事务。 当检查点 Barrier 通过所有算子并且触发的快照回调成功完成时，预提交阶段结束。所有触发的状态快照都被视为该检查点的一部分。检查点是整个应用程序状态的快照，包括预先提交的外部状态。如果发生故障，我们可以回滚到上次成功完成快照的时间点。 下一步是通知所有算子检查点已成功完成。这是两阶段提交协议的提交阶段，JobManager 为应用程序中的每个算子发出检查点完成的回调。 数据源和窗口算子没有外部状态，因此在提交阶段，这些算子不用执行任何操作。但是，数据接收器有外部状态，因此此时应该提交外部事务： 我们总结一下： 一旦所有算子完成预提交，就会发出一个提交。 如果至少有一个预提交失败，那么所有其他的提交也都会中止，并将回滚到上一个成功完成的检查点。 在预提交成功之后，必须保证提交最终成功 - 我们的算子和外部系统都需要保证这点。如果一个提交失败（例如，由于间歇性网络问题），整个 Flink 应用程序将会失败，应用程序将根据用户的重启策略重新启动，并且还会尝试一次提交。这个过程至关重要，因为如果提交最终失败，将会发生数据丢失。 因此，我们要确定所有算子都同意检查点的最终结果：所有算子都同意数据提交或中止提交并回滚。 3. 在Flink中实现两阶段提交算子实现完整的两阶段提交协议可能有点复杂，这就是 Flink 为什么将两阶段提交协议的通用逻辑提取到 TwoPhaseCommitSinkFunction 抽象类中。 下面我们讨论一下如何在一个简单的基于文件的示例上实现 TwoPhaseCommitSinkFunction。我们只需实现四个函数就能为文件接收器提供 Exactly-Once 语义： beginTransaction：在开启事务之前，我们在目标文件系统的临时目录中创建一个临时文件。后面我们在处理数据时将数据写入此文件。 preCommit：在预提交阶段，刷写（flush）文件，然后关闭文件，之后就不能写入到文件了。我们还将为属于下一个检查点的任何后续写入启动新事务。 commit：在提交阶段，我们将预提交的文件原子性地移动到真正的目标目录中。请注意，这会增加输出数据可见性的延迟。 abort：在中止阶段，我们删除临时文件。 我们知道，如果发生故障时，Flink 会将应用程序的状态恢复到最新的成功检查点。有一种极端情况，在成功预提交之后但在提交通知到算子之前发生故障。在这种情况下，Flink 会将我们的算子恢复到已经预提交但尚未提交的状态。 我们必须在检查点状态下保存有关预提交事务的足够信息，以便能够在重新启动后正确中止或提交事务。在我们的示例中，这些信息是临时文件和目标目录的路径。 TwoPhaseCommitSinkFunction 已经将这种情况考虑在内了，当从检查点恢复状态时优先发出一个提交。我们需要以幂等方式实现提交。一般来说，这应该不难。在我们的示例中，我们可以识别出这样的情况：临时文件不在临时目录中，已经移到目标目录中。还有一些其他边缘情况，TwoPhaseCommitSinkFunction 也考虑到了。 4. 总结下面是我们这篇文章的一些要点： Flink 检查点是支持两阶段提交协议并提供端到端的 Exactly-Once 语义的基础。 这个方案的一个优点是: Flink 不像其他一些系统那样，通过网络传输存储（materialize）数据 - 不需要像大多数批处理程序那样将计算的每个阶段写入磁盘。 Flink 新的 TwoPhaseCommitSinkFunction 提取了两阶段提交协议的通用逻辑，并使构建端到端的 Exactly-Once 语义的应用程序（使用 Flink 和支持事务的外部系统）成为可能。 从 Flink 1.4.0 开始，Pravega 和 Kafka 0.11 producer 都提供了 Exactly-Once 语义；在 Kafka 0.11 中首次引入了事务，这使得 Kafka 在 Flink 实现 Exactly-Once producer 成为可能。 Kafka 0.11 producer 是在 TwoPhaseCommitSinkFunction 基础之上实现的，与 At-Least-Once 语义的 Kafka producer 相比，它的开销非常低。 原文：An Overview of End-to-End Exactly-Once Processing in Apache Flink]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm Tick 元组]]></title>
    <url>%2Ftick-tuple-in-storm.html</url>
    <content type="text"><![CDATA[在某些情况下，Bolt 在执行某些操作之前需要将数据缓存几秒钟，例如每隔5秒清理一次缓存或在单个请求中将一批记录插入数据库。 Tick 元组是系统生成的（Storm生成的）元组，我们可以在每个 Bolt 级别配置它们。我们可以在编写 Bolt 时在代码中配置 Tick 元组。 我们需要在 Bolt 中覆盖以下方法以启用 Tick 元组：@Overridepublic Map&lt;String, Object&gt; getComponentConfiguration() &#123; Config conf = new Config(); int tickFrequencyInSeconds = 10; conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, tickFrequencyInSeconds); return conf;&#125; 在上面的代码中，我们将 Tick 元组配置为10秒。现在，Storm 会每10秒钟生成一个 Tick 元组。 接下来创建 isTickTuple 方法来确定我们收到的元组是 Tick 元组还是正常元组：protected static boolean isTickTuple(Tuple tuple) &#123; return tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID) &amp;&amp; tuple.getSourceStreamId().equals(Constants.SYSTEM_TICK_STREAM_ID);&#125; Tick 元组会与你正在处理的其他正常元组混合在一起，所以需要我们判断元组的类型。 最后，在 Bolt 的 execute 方法中添加如下代码来判断元组的类型进行处理：@Overridepublic void execute(Tuple tuple, BasicOutputCollector collector) &#123; try &#123; if (isTickTuple(tuple)) &#123; // do tick tuple &#125; else &#123; // do normal tuple &#125; // do your bolt stuff &#125; catch (Exception e) &#123; LOG.error("Bolt execute error: &#123;&#125;", e); collector.reportError(e); &#125;&#125; 现在你的 Bolt 每10秒就会收到一个 Tick 元组。 如果希望 Topology 中的每个 Bolt 都每隔一段时间做一些操作，那么可以定义一个 Topology 全局的 Tick，同样是设置 Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS的值：Config conf = new Config();conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 10);]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
        <tag>Storm 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm 理解内部消息缓冲机制]]></title>
    <url>%2Funderstanding-storm-internal-message-buffers.html</url>
    <content type="text"><![CDATA[优化 Storm 拓扑性能有助于我们理解 Storm 内部消息队列的配置和使用，在这篇文章中，我将向大家解释并说明在 Storm（0.8或0.9）版本中工作进程以及相关联的 Executor 线程是如何完成内部通信的。 1. Storm工作进程中的内部消息当我说内部消息时，我的意思是在 Storm 工作进程内发生的消息，这只局限在同一个 Storm 节点内发生的通信。Storm 依赖于 LMAX Disruptor 支持的各种消息队列来完成此通信，LMAX Disruptor 是一个高性能的线程间消息通信库。 请注意，同一工作进程中线程间通信与工作进程间通信不同，后者通常发生在不同主机之间，因此需要通过网络传输，Storm 默认使用 ZeroMQ（在Storm 0.9 开始实验性的支持 Netty）进行通信。也就是说，当一个工作进程中的 Task 想要将数据发送到 Storm 集群另一台机器的 Task 时，需要使用 ZeroMQ/Netty 进行传输。所以有如下结论： Storm 工作进程内部通信（同一 Storm 节点上的线程间）：LMAX Disruptor。 工作进程间通信（跨网络的节点到节点）：ZeroMQ或Netty。 拓扑间通信：没有内置于 Storm 中，你必须自己处理这个问题。可以使用消息传递系统，如Kafka/RabbitMQ，数据库等。 如果你不了解 Storm 的工作进程，Executor 线程和 Task 之间的差异，可以参考 理解 Storm 拓扑的并行度。 在我们讨论下一节中的细节之前，让我们从下图开始。 上图说明了 Storm 工作进程内部消息队列的概述。与工作进程相关的队列以红色表示，与工作进程的 Executor 线程相关的队列以绿色表示。为了更好的可读性，上图只显示一个工作进程（通常一个 Storm 节点运行多个工作进程），同时工作进程中只显示一个 Executor 线程（通常每个工作进程通常有多个 Executor 线程）。 2. 内部实现现在我们对 Storm 的工作进程内部消息机制有了一定了解，接下来可以深入讨论细节了。 2.1 工作进程为了管理输入和输出消息，每个工作进程都有一个监听工作进程TCP端口的接收线程（通过 supervisor.slots.ports 配置），接收线程将输入消息发送到工作进程 Executor 线程的输入队列中。工作进程接收线程的缓冲区大小通过 topology.receiver.buffer.size 配置。同样地，每个工作进程都有一个发送线程，负责从工作进程的传输队列中读取消息，并通过网络将消息发送给下游消费者。工作进程传输队列的大小通过 topology.transfer.buffer.size 配置。 每一个工作进程都会有一个接收线程和一个发送线程。接收线程用于将网络中的输入消息发送到 Executor 线程的输入队列中，发送线程用于将传输队列的消息发送给下游消费者。 topology.receiver.buffer.size 是一次批处理的最大消息数，工作进程接收线程从网络读取消息发送到 Executor 的输入队列中。如果将此参数设置得太高可能会导致很多问题，比如心跳线程饥饿，吞吐量急剧下降等。此参数默认为8个元素，值必须为2的幂（此要求间接来自LMAX Disruptor）。 // Example: configuring via Java APIConfig conf = new Config();conf.put(Config.TOPOLOGY_RECEIVER_BUFFER_SIZE, 16); // default is 8 请注意，topology.receiver.buffer.size 与本文中描述的其他缓冲区大小相关参数不同，它不是配置的 LMAX Disruptor 队列的大小，而是配置的一个简单 ArrayList 的大小，用于缓冲输入消息。因为在这种特定情况下，数据结构不需要与其他线程共享，即它专属于工作进程的接收线程。但是因为此缓冲区的内容用于填充 Disruptor 队列（Executor 输入队列），所以它也必须是2的幂。 使用 topology.transfer.buffer.size 配置的输出队列的每个元素实际上都是一个元组列表。不同的 Executor 发送线程批量的将输出的元组从输出队列发送到传输队列中。默认值为1024个元素。//示例：通过Java API进行配置conf.put（Config.TOPOLOGY_TRANSFER_BUFFER_SIZE，32）; //默认为1024 2.2 Executor每个工作进程都有一个或多个 Executor 线程。每个 Executor 线程都有自己的输入队列和输出队列。如上所述，工作进程运行一个专用的接收线程将输入消息发送到工作进程的 Executor 线程的输入队列中。同样地，每个 Executor 都有一个专用的发送线程，将 Executor 的输出消息从其输出队列发送到“父”工作进程的传输队列。Executor 的输入和输出队列的大小分别通过 topology.executor.receive.buffer.size 和 topology.executor.send.buffer.size 配置。 每个 Executor 线程都有一个线程来处理 Spout/Bolt 的用户逻辑（即你的应用程序代码），以及一个发送线程将消息从 Executor 的输出队列发送到工作进程的传输队列。 通过 topology.executor.receive.buffer.size 配置 Executor 输入队列的大小。队列的每个元素都是元组列表。这里，元组追加形成一个批次。默认值为1024个元素，值必须为2的幂（此要求来自LMAX Disruptor）。conf.put（Config.TOPOLOGY_EXECUTOR_RECEIVE_BUFFER_SIZE，16384）; //批处理 默认值是1024 通过 topology.executor.send.buffer.size 配置 Executor 输出队列的大小。队列的每个元素都只包含一个元组。默认值为1024个元素，值必须为2的幂（此要求来自LMAX Disruptor）。conf.put（Config.TOPOLOGY_EXECUTOR_SEND_BUFFER_SIZE，16384）; 3. 配置3.1 如何配置内部消息缓冲上面提到的各种默认值在 conf/defaults.yaml 中定义。你可以在 Storm 集群的 conf/storm.yaml 中全局覆盖这些值。你还可以通过 Storm 的 Java API 中的 backtype.storm.Config 为每个 Storm 拓扑配置这些参数。 3.2 如何配置拓扑并行度Storm 的消息缓冲区的正确配置与拓扑的工作负载模式以及拓扑的已配置并行度密切相关。有关后者的更多详细信息，请参考理解 Storm 拓扑的并行度。 3.3 了解Storm拓扑内部在做什么Storm UI 是检查正在运行的 Storm 拓扑的关键指标的一个很好的工具。例如，它向你展示了 Spout/Bolt 的所谓 capacity。各种指标会帮助你确定本文中描述的与缓冲区相关的配置参数的更改是否对 Storm 拓扑的性能产生正面或负面影响。有关详细信息，请参考运行在Storm集群的多节点上。 除此之外，你还可以生成自己的应用程序指标，并使用 Graphite 等工具进行跟踪。有关详细信息，请参阅我的文章将Storm指标发送到Graphite以及通过RPM和Supervisord安装和运行Graphite。也许值得在 GitHub 上查看 ooyala 的 metrics_storm 项目（我还没有使用它）。 3.4 优化建议可以看看 Nathan Marz（Storm作者）的演讲：Tuning and Productionization of Storm. 开始的时候可以试试如下参数配置，看看是否能够提升 Storm 集群的性能：conf.put(Config.TOPOLOGY_RECEIVER_BUFFER_SIZE, 8);conf.put(Config.TOPOLOGY_TRANSFER_BUFFER_SIZE, 32);conf.put(Config.TOPOLOGY_EXECUTOR_RECEIVE_BUFFER_SIZE, 16384);conf.put(Config.TOPOLOGY_EXECUTOR_SEND_BUFFER_SIZE, 16384); 原文：Understanding the Internal Message Buffers of Storm]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
        <tag>Storm 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Storm 拓扑的并行度]]></title>
    <url>%2Funderstanding-the-parallelism-of-Storm-topology.html</url>
    <content type="text"><![CDATA[1. 什么让拓扑运行Storm 区分以下 3 个主要的实体，用来运行 Storm 集群中拓扑: Worker 进程 Executors 线程 Tasks 这是一个简单的例子, 以说明他们之间的关系 一个 Worker 进程用来执行一个拓扑的子集。属于一个指定拓扑的 Worker 进程, 为该拓扑的一个或多个组件(spouts 或 bolts)运行一个或多个 Executors。一个正在运行的拓扑由多个这样的进程组成, 它们运行在 Storm 集群的多个机器上。 Executor 是一个线程，由 Worker 进程产生。一个 Executor 可以为同一个组件(spout 或 blot)运行一个或多个 Tasks。 Task 执行实际的数据处理 - 在你代码中实现的 spout 或 bolt 在集群上执行尽可能多的 Task。一个组件的 Task 数目在整个拓扑生命周期中总是相同的，但是一个组件的 Executors 数目会随时间变化。这意味着以下条件成立: #threads ≤ #tasks。默认情况下，Tasks 的数目与 Executors 的数目设置成一样，即，Storm 在每个线程上运行一个 Task。 2. 配置拓扑的并行度请注意，在 Storm 的术语中, parallelism 专门用来描述所谓的 parallelism hint，表示一个组件的 Executor 的初始化数量。在这篇文章中, 尽管我们一般使用 parallelism 术语来描述如何配置 Executor 的数目，但同时也可以配置 Worker 进程的数目和 Storm 拓扑的 Tasks 数目。 以下部分概述了各种配置参数以及如何在代码中进行设置。尽管可以有多种方法去设置这些参数，但下面只列出了其中的一些。Storm 目前配置的优先顺序为: defaults.yaml &lt; storm.yaml &lt; 特定拓扑的配置 &lt; 特定内部组件的配置 &lt; 特定外部组件的配置。 2.1 Worker 进程的数量 描述: 在集群的机器上为拓扑创建多少个 Worker 进程。 配置参数: TOPOLOGY_WORKERS。 如何在代码中设置:conf.setNumWorkers(4)。 2.2 Executors的数量 描述: 为每个组件创建多少个 Executors。 配置参数: None (传递 parallelism_hint 参数到 setSpout 或 setBolt)。 如何在代码中设置: TopologyBuilder#setSpout() 或 TopologyBuilder#setBolt()。 参数现在指定了 Bolt 的 Executors 的初始化数量(不是 Tasks)。 2.3 Tasks的数量 描述: 为每个组件创建多少个 Tasks。 配置参数: TOPOLOGY_TASKS。 如何在代码中设置:ComponentConfigurationDeclarer#setNumTasks() 3. 运行拓扑示例Config conf = new Config();// use two worker processesconf.setNumWorkers(2);// set parallelism hint to 2topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2);topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2) .setNumTasks(4) .shuffleGrouping("blue-spout");topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6) .shuffleGrouping("green-bolt");StormSubmitter.submitTopology( "mytopology", conf, topologyBuilder.createTopology()); 下图显示了上述简单拓扑是如何运行的。该拓扑由 3 个组件构成: 一个为 BlueSpout 的 spout 和两个为 GreenBolt 和 YellowBolt 的 bolts。BlueSpout 将其输出发送到 GreenBolt，GreenBolt 将其输出发送到 YellowBolt。 Storm 还提供了额外的配置来设置拓扑的并行度: TOPOLOGY_MAX_TASK_PARALLELISM: 此参数设置单个组件 Executor 数量的上限。通常在测试期间使用它来限制在本地模式下运行拓扑时产生的线程数。你可以通过 Config#setMaxTaskParallelism() 来设置此选项。 4. 如何改变正在运行中的拓扑的并行度Storm 的一个很好的特性是可以增加或减少 Worker 进程 或 Executor 的数量，不需要重新启动集群拓扑。这样的行为称之为 rebalance。 你有两个选项来 rebalance 一个拓扑: 使用 Storm web UI 来 rebalance 指定的拓扑。 使用 CLI 工具 storm rebalance, 如下所示。以下是一个使用 CLI 工具的示例:## 重新配置拓扑 &quot;mytopology&quot; 使用 5 个 Worker 进程## 重新配置spout &quot;blue-spout&quot; 使用 3 个 Executors## 重新配置bolt &quot;yellow-bolt&quot; 使用 10 个 Executors$ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10 版本：1.2.2 原文：Understanding the Parallelism of a Storm Topology]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
        <tag>Storm 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 安装与启动]]></title>
    <url>%2Fhow-install-and-startup-redis.html</url>
    <content type="text"><![CDATA[在安装 Redis 前需要了解 Redis 的版本规则以选择最合适自己的版本，Redis 约定次版本（即第一个小数点后的数字）为偶数的版本是稳定版本(如 2.4版本，2.6版本)，奇数版本是非稳定版本(如2.5版本，2.7版本)，推荐使用稳定版本进行开发和在生产环境中使用． 1. 下载当前最新版本为 3.2.9： 官网下载：https://redis.io/ 中文官网下载：http://www.redis.cn/download.html 2. 安装解压缩文件到指定目录：tar -zxvf redis-3.2.9.tar.gz -C opt/ 进入解压之后的目录，进行编译cd opt/redis-3.2.9/make 看到如下信息，表示可以继续下一步：CC redis-check-rdb.o CC geo.o LINK redis-server INSTALL redis-sentinel CC redis-cli.o LINK redis-cli CC redis-benchmark.o LINK redis-benchmark INSTALL redis-check-rdb CC redis-check-aof.o LINK redis-check-aofHint: It&apos;s a good idea to run &apos;make test&apos; ;)make[1]: Leaving directory &apos;/home/xiaosi/opt/redis-3.2.9/src&apos; 二进制文件编译完成后在src目录下，进入 src 目录之后进行安装操作:xiaosi@yoona:~/opt/redis-3.2.9/src$ sudo make installHint: It&apos;s a good idea to run &apos;make test&apos; ;) INSTALL install INSTALL install INSTALL install INSTALL install INSTALL install 备注: 一般情况下，在Ubuntu系统中，都是需要使用sudo提升权限。 在安装成功之后，可以运行测试，确认 Redis 的功能是否正常：make test 看到如下信息，表示 Redis 已经安装成功：\o/ All tests passed without errors!Cleanup: may take some time... OK 3. 启动安装完 Redis 后的下一步就是启动它，下面将介绍在开发环境和生产环境中运行 Redis 的方法以及正确停止 Redis 的步骤。在这之前，我们先了解 Redis 包含的可执行文件都有哪些，如下表： 文件名你 说明 redis-server Redis服务器 redis-cli Redis命令行客户端 redis-benchmark Redis性能测试工具 redis-check-aof AOF文件修复工具 我们最常使用的两个程序是 redis-server 和 redis-cli，其中 redis-server 是 Redis 的服务器，启动 Redis 即运行它；而 redis-cli 是 Redis 自带的 Redis 命令行客户端． 3.1 启动Redis启动 Redis 有直接启动和通过初始化脚本启动两种方式，分别适用于开发环境和生产环境。 3.1.1 直接启动直接运行 redis-server 即可启动 Redis：xiaosi@yoona:~$ redis-server11657:C 30 May 21:52:39.810 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf11657:M 30 May 21:52:39.813 * Increased maximum number of open files to 10032 (it was originally set to 1024). _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.9 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 11657 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 11657:M 30 May 21:52:39.815 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.11657:M 30 May 21:52:39.815 # Server started, Redis version 3.2.911657:M 30 May 21:52:39.815 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.11657:M 30 May 21:52:39.815 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.11657:M 30 May 21:52:39.815 * The server is now ready to accept connections on port 6379 Redis 服务器默认使用 6379 端口，通过 –port 参数可以自定义端口号：redis-server --port 6390 3.1.2 通过初始化脚本启动Redis在 Linux 系统中可以通过初始化脚本启动 Redis，使得 Redis 能随系统自动运行，在生产环境中推荐使用此方法运行 Redis．在 Redis 源代码目录的 utils 文件夹中有一个名为 redis-init-script 的初始化脚本文件，内容如下：#!/bin/sh## Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.REDISPORT=6379EXEC=/usr/local/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cliPIDFILE=/var/run/redis_$&#123;REDISPORT&#125;.pidCONF=&quot;/etc/redis/$&#123;REDISPORT&#125;.conf&quot;case &quot;$1&quot; in start) if [ -f $PIDFILE ] then echo &quot;$PIDFILE exists, process is already running or crashed&quot; else echo &quot;Starting Redis server...&quot; $EXEC $CONF fi ;; stop) if [ ! -f $PIDFILE ] then echo &quot;$PIDFILE does not exist, process is not running&quot; else PID=$(cat $PIDFILE) echo &quot;Stopping ...&quot; $CLIEXEC -p $REDISPORT shutdown while [ -x /proc/$&#123;PID&#125; ] do echo &quot;Waiting for Redis to shutdown ...&quot; sleep 1 done echo &quot;Redis stopped&quot; fi ;; *) echo &quot;Please use start or stop as first argument&quot; ;;esac 我们需要配置 Redis 的运行方式和持久化文件，日志文件的存储位置等，具体步骤如下： (1) 配置初始化脚本首先将初始化脚本复制到 /etc/init.d 目录下，文件名为 redis_端口号，其中端口号表示要让 Redis 监听的端口号，客户端通过该端口连接 Redis：xiaosi@yoona:~/opt/redis-3.2.9/utils$ sudo cp redis_init_script /etc/init.d/xiaosi@yoona:/etc/init.d$ sudo mv redis_init_script redis_6379 然后修改第六行的 REDISPORT 变量的值为同样的端口号REDISPORT=6379 (2) 建立需要的文件夹 建立如下表列出的目录： 目录名 说明 /etc/redis 存放Redis的配置文件 /var/redis/端口号 存放Redis的持久化文件 xiaosi@yoona:~$ sudo mkdir /etc/redisxiaosi@yoona:~$ sudo mkdir /var/redisxiaosi@yoona:~$ sudo mkdir /var/redis/6379 (3) 修改配置文件 首先将配置文件模板复制到 /etc/redis 目录中，以端口号命名(例如，”6379.conf”)，然后按照下标对其中的部分参数进行编辑：xiaosi@yoona:~/opt/redis-3.2.9$ sudo cp redis.conf /etc/redis/xiaosi@yoona:/etc/redis$ sudo mv redis.conf 6379.conf 参数 值 说明 daemonize yes 使Redis以守护进程模式运行 pidfile /var/run/redis_端口号.pid 设置Redis的PID文件位置 port 端口号 设置Redis监听的端口号 dir /var/redis/端口号 设置持久化文件存放位置 现在就可以使用 /etc/init.d/redis_端口号 start 来启动 Redis，而后需要执行下面的命令使 Redis 随系统自动启动：sudo update-rc.d redis_端口号 defaults 3.2 客户端可以使用内置的客户端命令 redis-cli 进行使用:xiaosi@yoona:~$ redis-cli127.0.0.1:6379&gt; 在上面的提示中，127.0.0.1 是计算机的IP地址，6379 是运行 Redis 服务器的端口。现在键入以下PING命令:127.0.0.1:6379&gt; pingPONG 这表明 Redis 已成功在您的计算机上安装了。 3.3 停止Redis考虑到 Redis 有可能正在将内存中的数据同步到磁盘中，强行终止 Redis 进程可能会导致数据丢失．正确停止 Redis 的方式应该是向 Redis 发送 SHUTDOWN 命令，方法为：redis-cli SHUTDOWN 当 Redis 收到 SHUTDOWN 命令后，会先断开所有客户端连接，然后根据配置执行持久化，最后完成退出。 欢迎关注我的公众号和博客： 来自于＜Redis入门指南＞]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 为Next主题添加评论功能]]></title>
    <url>%2Fadd-a-comment-function-to-the-next-theme-of-hexo.html</url>
    <content type="text"><![CDATA[1. 添加评论功能之前使用的来必力，最近评论功能一直出不来，用户体验比较差，所以重新评估选择了Valine。 1.1 创建账号这个评论系统是基于 LeanCloud 进行的，先去注册一个账号，点这里进行注册。 点这里创建应用，应用名看个人喜好。选择刚刚创建的应用，点击设置按钮，再点击应用Key按钮，你就能看到你的APP ID和APP KEY了： 为了您的数据安全，请填写应用&gt;设置&gt;安全设置中的Web 安全域名，如下图： 1.2 配置修改你主题目录下的_config.yml文件：valine: enable: true appid: # 你的 App ID appkey: # 你的 App Key notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 有什么问题，欢迎留言指正与交流... # comment box placeholder avatar: robohash # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 发布之后，我们看一下效果如何： 在进行评论之后，我们可以从后台看一下我们的评论数据： 这样我们就可以管理我们的评论数据了。 2. 设置评论头像目前默认头像有以下7种默认值可选: 下面我们换一种头像展现，在NexT主题中，进行如下修改，把我们的默认头像修改为一种具有不同颜色、面部等的机器人：avatar: robohash # gravatar style 如果想自定义我们的头像，我们可以在这点击进行创建用户，并上传自己的头像： 经过一定时间的数据全球化同步，头像会根据你设置的 E-mail 进行匹配： 如果想显示我们上传的头像，在进行评论时一定要留下设置的 E-mail 账号，否则会显示默认头像。 3. 邮件提醒进入Leancloud&gt;选择你的评论所存放的应用&gt;设置&gt;邮件模板，按下图设置好用于重置密码的邮件主题&gt;然后保存: 请注意修改链接为你的博客或者网站首页。 &lt;p&gt;Hi, &#123;&#123;username&#125;&#125;&lt;/p&gt;&lt;p&gt;你在 &#123;&#123;appname&#125;&#125; 的评论收到了新的回复，请点击查看：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;你的网址首页链接&quot; style=&quot;display: inline-block; padding: 10px 20px; border-radius: 4px; background-color: #3090e4; color: #fff; text-decoration: none;&quot;&gt;马上查看&lt;/a&gt;&lt;/p&gt; 这还需要修改你主题目录下的_config.yml文件，开启邮件提醒与验证码功能：valine: enable: true appid: # 你的 App ID appkey: # 你的 App Key notify: true # mail notifier , https://github.com/xCss/Valine/wiki verify: true # Verification code placeholder: 有什么问题，欢迎留言指正与交流... # comment box placeholder avatar: robohash # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 注意事项: 发送次数过多，可能会暂时被Leancloud 屏蔽邮件发送功能 由于邮件提醒功能使用的Leancloud的密码重置邮件提醒，只能传递昵称、邮箱两个属性，所以邮件提醒链接无法直达指定文章页。请悉知。 开启邮件提醒会默认开启验证码选项。 该功能目前还在测试阶段，谨慎使用。 目前邮件提醒正处于测试阶段，仅在子级对存在邮件地址的父级发表评论时发送邮件 4. 增强版邮件提醒4.1 快速开始首先需要确保 Valine 的基础功能是正常的，参考 Valine Docs。然后进入 Leancloud 对应的 Valine 应用中。点击 云引擎 -&gt; 设置 填写代码库并保存：https://github.com/panjunwen/Valine-Admin.git。 切换到部署标签页，分支使用 master，点击部署即可： 4.2 配置项此外，你需要设置云引擎的环境变量以提供必要的信息，点击云引擎的设置页，设置如下信息： 下面介绍一下各个参数： 变量 示例 说明 SITE_NAME SmartSi 博客名称 SITE_URL https://smartsi.club 首页地址 SMTP_HOST smtp.qq.com SMTP服务器地址 SMTP_PORT 465 SMTP端口 SMTP_USER xxx@qq.com SMTP登录用户 SMTP_PASS xxx SMTP登录密码，一般为授权码（QQ邮箱为授权码） SENDER_NAME SmartSi 发件人 SENDER_EMAIL xxxxxx@qq.com 发件邮箱 BLOGGER_EMAIL xxxxx@qq.com 博主通知收件地址 MAIL_SUBJECT ${SITE_NAME}上有新回复了 @通知邮件主题模板 MAIL_TEMPLATE 见下文 @通知邮件内容模板 MAIL_SUBJECT_ADMIN ${SITE_NAME}上有新评论了 博主邮件通知主题模板 MAIL_TEMPLATE_ADMIN 见下文 博主邮件通知内容模板 ADMIN_URL https://xxx.leanapp.cn/ Web主机二级域名，用于自动唤醒 以上参数请务必全部设置，方能保证通知邮件正确发送。可以自定义邮件通知模板，也可以直接复制我的默认模板。 默认被@通知邮件内容模板如下：&lt;span style=&quot;font-size:16px;color:#212121&quot;&gt;Hi，$&#123;PARENT_NICK&#125;&lt;/span&gt;&lt;p&gt;$&#123;NICK&#125;在$&#123;SITE_NAME&#125;上@了你，内容如下：&lt;/p&gt;&lt;p&gt;$&#123;COMMENT&#125;&lt;/p&gt;&lt;br&gt;&lt;p&gt;原评论内容为：$&#123;PARENT_COMMENT&#125;&lt;/p&gt;&lt;p&gt; &lt;a href=&quot;$&#123;POST_URL&#125;&quot;&gt;点击前往查看&lt;/a&gt;&lt;br&gt;&lt;p&gt;&lt;a href=&quot;$&#123;SITE_URL&#125;&quot;&gt;$&#123;SITE_NAME&#125;&lt;/a&gt;欢迎你的再度光临&lt;/p&gt; 其中可用变量如下（注，这是邮件模板变量，请勿与云引擎环境变量混淆）：变量|说明—|—SITE_NAME|博客名称SITE_URL|博客首页地址POST_URL|文章地址（完整路径）PARENT_NICK|收件人昵称（被@者，父级评论人）PARENT_COMMENT|父级评论内容NICK|新评论者昵称COMMENT|新评论内容 默认博主通知邮件内容模板如下：&lt;p&gt;$&#123;NICK&#125;在$&#123;SITE_NAME&#125;上给你留下新评论了，内容如下：&lt;/p&gt; $&#123;COMMENT&#125; &lt;br&gt;&lt;p&gt;&lt;a href=&quot;$&#123;POST_URL&#125;&quot;&gt;点击前往查看&lt;/a&gt; 其中，PARENT_NICK 和 PARENT_COMMENT 变量不再可用，其他与@通知一致。 4.3 LeanCloud 休眠策略LeanCloud 为所有用户提供免费的体验实例，体验实例有 每天6小时的强制休眠时间。30 分钟内没有外部请求，则休眠。休眠后如果有新的外部请求实例会马上启动（但激活时发送邮件会失败）。点击查看详情。 我的做法是在 VPS 上添加一个定时任务，每天6:00 ~ 23:00每20分钟访问一次我的LeanCloud网址，防止云引擎进入休眠，保证通知邮件的及时发送。cron定时任务：*/20 6-23 * * * curl https://smartsi.leanapp.cn 4.4 Web评论管理此项目还为 Valine 提供了更方便的评论管理功能，可以在 web 端对评论进行查看与删除操作。配置方式如下。 后台登录需要账号密码，需要在这里设置，只需要填写 email、password、username，这三个字段即可, 使用 email 作为账号登陆即可。（为了安全考虑，此 email 必须为配置中的 SMTP_USER 或 TO_EMAIL， 否则不允许登录）： 上述完成之后，可以点击这，根据上面设置的账号与密码登录评论后台管理： 资料： 头像配置 Valine – 一款极简的评论系统 邮件提醒 增强版邮件通知]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务之两阶段提交(2PC)]]></title>
    <url>%2Ftwo-phase-commit-of-distributed-transaction.html</url>
    <content type="text"><![CDATA[1. 概述在计算机网络以及数据库领域内，二阶段提交（Two-phase Commit）是指，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法。通常，二阶段提交也被称为是一种协议。在分布式系统中，虽然每个节点可以知道自己的操作是成功还是失败，但却无法知道其他节点的操作是成功还是失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一协调所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为： 参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是中止操作。 2. 前提二阶段提交算法的成立基于以下假设： 分布式系统中，存在一个节点作为协调者，其他节点作为参与者。且节点之间可以进行网络通信。 所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏也不会导致日志数据的消失。 所有节点不会永久性损坏，即使损坏后仍然可以恢复。 3. 两阶段提交所谓的两个阶段是指：第一阶段的提交请求阶段(投票阶段)和第二阶段的提交阶段（完成阶段）。 3.1 第一阶段：提交请求阶段可以进一步将提交请求阶段分为以下三个步骤： 协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。 各参与者节点返回协调者节点发起询问的响应。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 有时候，第一阶段也被称作投票阶段，即各参与者投票是否要继续接下来的提交操作。 3.2 第二阶段：提交执行阶段如果协调者从所有参与者节点获得的相应消息都为”同意”，则向所有参与者节点发出”正式提交”的请求，否则发出”回滚操作”的请求。参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。 (1) 当协调者节点从所有参与者节点获得的相应消息都为”同意”时（成功）： 协调者节点向所有参与者节点发出”正式提交”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点收到所有参与者节点反馈的”完成”消息后，完成事务。 (2) 如果任一参与者节点在第一阶段返回的响应消息为”终止”，或者协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时（失败）： 协调者节点向所有参与者节点发出”回滚操作”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点收到所有参与者节点反馈的”回滚完成”消息后，取消事务。 有时候，第二阶段也被称作完成阶段，因为无论结果怎样，协调者都必须在此阶段结束当前事务。 4. 缺点(1) 同步阻塞：两阶段提交的最大缺点就在于它的执行过程中间节点都处于阻塞状态。即节点之间在等待对方的相应消息时，它将什么也做不了。特别是，当一个节点在已经占有了某项资源的情况下，为了等待其他节点的响应消息而陷入阻塞状态时，当第三个节点尝试访问该节点占有的资源时，这个节点也将连带陷入阻塞状态。 (2) 超时机制：协调者节点指示参与者节点进行提交等操作时，如有参与者节点出现了崩溃等情况而导致协调者始终无法获取所有参与者的响应信息，这时协调者将只能依赖协调者自身的超时机制来处理。但往往超时机制处理时，协调者都会指示参与者进行回滚操作。这样的策略显得比较保守。 (3) 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题） (4) 数据不一致：在第二阶段中，当协调者向参与者发送提交请求之后，如果发生了局部网络异常或者在发送提交请求过程中协调者发生了故障，这会导致只有一部分参与者接受到了提交请求。而在这部分参与者接到提交请求之后就会执行提交操作。但是其他未接到提交请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。 参考： 维基百科：二阶段提交 2PC 两阶段提交协议]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 使用Flink进行高吞吐，低延迟和Exactly-Once语义流处理]]></title>
    <url>%2Fhigh-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink.html</url>
    <content type="text"><![CDATA[在本文中，我们将深入探讨Flink新颖的检查点机制是如何工作的，以及它是如何取代旧架构以实现流容错和恢复。我们在各种类型的流处理应用程序上对Flink性能进行测试，并通过在Apache Storm（一种广泛使用的低延迟流处理器）上运行相同的实验来进行对比。 1. 流式架构的演变在流处理中保证高性能同时又要保证容错是比较困难的。在批处理中，当作业失败时，可以容易地重新运行作业的失败部分来重新计算丢失的结果。这在批处理中是可行的，因为文件可以从头到尾重放。但是在流处理中却不能这样处理。数据流是无穷无尽的，没有开始点和结束点。带有缓冲的数据流可以进行重放一小段数据，但从最开始重放数据流是不切实际的（流处理作业可能已经运行了数月）。此外，与仅具有输入和输出的批处理作业相比，流计算是有状态的。这意味着除了输出之外，系统还需要备份和恢复算子状态。由于这个问题比较复杂，因此在开源生态系统中有许多容错方法去尝试解决这个问题。 用于容错机制对整个框架的架构有比较深的影响。很难将不同的容错机制进行插件化来整合到现有框架中。因此，在我们选择一个流处理框架时，容错机制也非常重要。 下面我们去了解一下流处理架构的几种容错方法，从记录确认到微批处理，事务更新和分布式快照。我们将从以下几个维度讨论不同方法的优缺点，最终选出融合不同方法优点适合流处理程序的融合方法： Exactly-once语义保证：故障后有状态算子的状态能正确恢复。 低延迟：延迟越低越好。许多应用程序需要亚秒级延迟。 高吞吐量：随着数据速率的增长，通过管道推送大量数据至关重要。 强大的计算模型：框架应该提供一种编程模型，该模型不会对用户进行限制并保证应用程序在没有故障的情况下容错机制的低开销。 流量控制：处理速度慢的算子产生的背压应该由系统和数据源自然吸收，以避免因消费缓慢而导致崩溃或性能降低。 上面我们忽略了一个共同特征，即失败后的快速恢复，不是因为它不重要，而是因为（1）所有介绍的系统都能够基于完全并行进行恢复，以及（2）在有状态的应用程序中，状态恢复的瓶颈通常在于存储而不是计算框架。 2. 记录确认机制(Apache Storm)虽然流处理已经在金融等行业中广泛使用多年，但最近流处理才成为大数据基础设施的一部分。开源框架的可用性一直在推动着流处理的发展。开源中第一个广泛使用的大规模流处理框架可能是Apache Storm。Storm使用上游备份和记录确认机制来保证在失败后重新处理消息。请注意，Storm不保证状态一致性，任何可变状态的处理都需要委托给用户处理（Storm的Trident API可以确保状态一致性，将在下一节中介绍）。 记录确认机制的工作方式如下：算子(Operator)处理的每条记录都会向前一个算子发回一个已经处理过的确认。拓扑的 Source 节点会保留它产生的所有元组的一个备份。直到 Source 中记录收到其所产生的到Sink的所有派生记录的确认之后，就可以删除上游备份的备份。当发生故障时，如果没有收到所有的确认，Source 记录就会重新发送。这种机制可以保证不会丢失数据，但很有可能导致重复处理记录（我们称之为At-Least-Once语义）。Storm 使用一种巧妙的机制来实现这种容错方式，每个数据源记录只需要几个字节的存储空间就可以跟踪确认。Twitter Heron 保持与 Storm 相同的确认机制，但提高了记录重放的效率（从而提高了恢复时间和整体吞吐量）。 纯记录确认体系结构，无论其性能如何，都无法提供Exactly-once语义保证，这给应用程序开发人员带来了删除重复数据的负担。对于某些应用程序而言，这可能是可以接受的，但对于其他应用程可能并不能接受。Storm的机制的其他问题还有吞吐量低和流量控制的问题，在出现背压的情况下，记录确认机制会导致上游节点错误地认为数据处理出现了故障(实际上仅仅是由于出现背压导致记录来不及处理，而无法发送确认)。这导致了基于微批处理的流式架构的发展。 3. 微批处理(Apache Storm Trident, Apache Spark Streaming)Storm和先前的流处理系统不能满足一些对大规模应用程序至关重要的需求，特别是高吞吐量，快速并行恢复以及托管状态的Exactly-once语义。 容错流式架构的下一个发展阶段是微批处理或离散化流。这个想法非常简单：为了解决连续计算模型（处理和缓冲记录）所带来的记录级别同步的复杂性和开销，连续计算分解为一系列小的原子性的批处理作业（称为微批次）。每个微批次可能会成功或失败，如果发生故障，重新计算最近的微批次即可。 微批处理可以应用到现有引擎（有能力进行数据流计算）之上。例如，可以在批处理引擎（例如，Spark）之上应用微批处理以提供流功能（这是Spark Streaming背后的基本机制），也可以应用于流引擎之上（例如，Storm）提供 Exactly-once 语义保证和状态恢复（这是Storm Trident背后的基本机制）。在 Spark Streaming 中，每个微批次计算都是一个 Spark 作业，而在 Trident 中，每个微批次中的所有记录都会被合并为一个大型记录。 基于微批处理的系统可以实现上面列出的多个的要求（Exactly-once语义保证，高吞吐量），但也有不足之处： 编程模型：例如，Spark Streaming 为了实现上述目标将编程模型从流式更改为微批处理。这意味着用户不能再以任意时间而只能在检查点间隔的倍数上窗口化数据，并且模型不支持许多应用程序所需的基于计数或会话的窗口。这些都是应用程序开发人员关注的问题。具有可以改变状态的持续计算的纯流模型为用户提供了更大的灵活性。 流量控制：使用基于时间划分批次的微批次架构仍然具有背压的问题。如果微批处理在下游操作中（例如，由于计算密集型算子处理不过来或向外部存储数据比较缓慢）比在划分批次的算子（通常是源）中花费更长时间，则微批次将花费比配置更长的时间（译者注：下游算子处理速度跟不上划分批次算子的速度）。这导致越来越多的批次排队，或者导致微批量增加。 延迟：微批处理显然将作业的延迟限制为微批处理的延迟。虽然亚秒级的批处理延迟对于简单应用程序是可以接受的，但是具有多个网络Shuffle的应用程序很容易将延迟时间延长到数秒。 微批处理模型的最大局限可能是它连接了两个不应连接的概念：应用程序定义的窗口大小和系统内部恢复间隔。假设一个程序（下面示例是Flink代码）每5秒聚合一次记录：dataStream .map(transformRecords) .groupBy(&quot;sessionId&quot;) .window(Time.of(5, TimeUnit.SECONDS)) .sum(&quot;price&quot;) 这些应用非常适合微批处理模型。系统累积5秒的数据，对它们求和，并在流上进行一些转换后进行聚合计算。下游应用程序可以直接消费上述5秒聚合后的结果，例如在仪表板上显示。但是，现在假设背压开始起作用（例如，由于计算密集型的 transformRecords 函数），或者 devops 团队决定通过将时间间隔增加到10秒来控制作业的吞吐量。然后，微批次大小变的不可控制（在出现背压情况下），或者直接变为10秒（第二种情况）。这意味着下游应用程序（例如，包含最近5秒统计的 Web 仪表板）读取的聚合结果是错误的，下游应用程序需要自己处理此问题。 微批处理可以实现高吞吐量和Exactly-Once语义保证，但是当前的实现是以抛弃低延迟，流量控制和纯流式编程模型为代价实现上述目标的。显而易见的问题是，是否有两全其美的办法：保持连续计算模型的所有优势，同时还能保证Exactly-Once语义并提供高吞吐量。后面讨论的后流式架构实现了这种组合，并将微批处理作为流式处理的基本模型。 通常，微批处理被认为是一次处理一条记录的替代方法。这是一种错误的认识：连续算子不需要一次只处理一条记录。实际上，所有精心设计的流处理系统（包括下面讨论的Flink和Google Dataflow）在通过网络传输之前都会缓冲许多记录，同时又具备连续的处理能力。 4. 事务更新(Google Cloud Dataflow)在保留连续算子模型（低延迟，背压容错，可变状态等）的优势的同时又保证Exactly-Once处理语义的一种强大而又优雅的方法是原子性地记录需要处理的数据并更新到状态中。失败后，可以从日志中重新恢复状态以及需要处理的记录。 例如，在Google Cloud Dataflow中实现了此概念。系统将计算抽象为一次部署并长期运行的连续算子的DAG。在Dataflow中，shuffle是流式传输的，中间结果不需要物化（译者注：数据的计算结果放在内存中）。这为低延迟提供了一种自然的流量控制机制，因为中间过程的缓冲可以缓解背压，直到反压到数据源（基于Pull模式的数据源，例如Kafka消费者可以处理这个问题）。该模型还提供了一个优雅的流编程模型，可以提供更丰富的窗口而不是简单的基于时间的窗口以及可以更新到长期可变的状态中。值得注意的是，流编程模型包含微批处理模型。 例如，下面Google Cloud Dataflow程序（请参阅此处）会创建一个会话窗口，如果某个key的事件没有在10分钟内到达，则会触发该会话窗口。在10分钟后到达的数据将会启动一个新窗口。 PCollection&lt;String&gt; items = ...;PCollection&lt;String&gt; session_windowed_items = items.apply( Window.&lt;String&gt;into(Sessions.withGapDuration(Duration.standardMinutes(10)))) 这在流处理模型中很容易实现，但在微批处理模型中却很难实现，因为窗口不对应固定的微批量大小。 这种架构的容错工作原理如下。通过算子的每个中间记录与更新的状态以及后续产生的记录一起创建一个提交记录，该记录以原子性的方式追加到事务日志或插入到数据库中。在失败的情况下，重放部分数据库日志来恢复计算状态，以及重放丢失的记录。 Apache Samza遵循类似的方法，但只能提供At-Least-Once语义保证，因为它使用Apache Kafka作为后台存储。Kafka（现在）不提供事务编写器，因此对状态和后续产生的流记录的更新不能作为原子事务一起提交。 事务更新体系结构具有许多优点。事实上，它实现了我们在本文开头提出的所有需求。该体系结构的基础是能够频繁地写入具有高吞吐量的分布式容错存储系统中。分布式快照（在下一节中进行了解释）将拓扑的状态作为一个整体进行快照，从而减少了对分布式存储的写入量和频率。 5. 分布式快照(Apache Flink)提供 Exactly-Once 语义保证的问题实际上可以归结为确定当前流式计算所处的状态（包括正在处理中记录以及算子状态），然后生成该状态的一致性快照，并将快照存储在持久存储中。如果可以经常执行上述操作，那么从故障中恢复意味着仅从持久存储中恢复最新快照，并将流数据源（例如，Apache Kafka）回退到生成快照的时间点再次’重放’。Flink的分布式快照算法可以参阅本文; 在下文中，我们会给出一个简短的总结。 Flink的分布式快照算法基于Chandy和Lamport在1985年设计的一种算法，用于生成分布式系统当前状态的一致性快照（详细介绍请参阅此处），不会丢失信息且不会记录重复项。Flink使用的是Chandy Lamport算法的一个变种，定期生成正在运行的流拓扑的状态快照，并将这些快照存储到持久存储中（例如，存储到HDFS或内存中文件系统）。检查点的存储频率是可配置的。 这有点类似于微批处理方法，两个检查点之间的所有计算都作为一个原子整体，要么全部成功，要么全部失败。然而，只有这一点的相似之处。Chandy Lamport算法的一个重要特点是我们不必在流处理中按下’暂停’按钮（译者注：等待检查点完成之后）来调度下一个微批次。相反，常规数据处理一直运行，数据到达就会处理，而检查点发生在后台。引用原始论文：全局状态检测算法会被设计在基础计算上：它们必须同时运行，但不能改变基础计算。 因此，这种架构融合了连续算子模型（低延迟，流量控制和真正的流编程模型），高吞吐量，Chandy-Lamport算法提供的的Exactly-Once语义保证的优点。除了备份有状态计算的状态（其他容错机制也需要这样做）之外，这种容错机制几乎没有其他开销。对于小状态（例如，计数或其他统计），备份开销通常可以忽略不计，而对于大状态，检查点间隔会在吞吐量和恢复时间之间进行权衡。 最重要的是，该架构将应用程序开发与流量控制和吞吐量控制分开。更改快照间隔对流作业的结果完全没有影响，因此下游应用程序可以放心地依赖于接收到的正确结果。 Flink的检查点机制基于流经算子和渠道的 ‘barrier’（认为是Chandy Lamport算法中的一种’标记’）来实现。Flink的检查点的描述改编自Flink文档。 ‘Barrier’ 在 Source 节点中被注入到普通流数据中（例如，如果使用Apache Kafka作为源，’barrier’ 与偏移量对齐），并且作为数据流的一部分与数据流一起流过DAG。’barrier’ 将记录分为两组：当前快照的一部分（’barrier’ 表示检查点的开始），以及属于下一个快照的那些组。 ‘Barrier’ 流向下游并在通过算子时触发状态快照。算子首先将所有流入的流分区的 ‘barrier’ 对齐（如果算子具有多个输入），并会缓存较快的分区数据（上游来源较快的流分区将被缓冲数据以等待来源较慢的流分区）。当算子从每个输入流中都收到 ‘barrier’ 时，会检查其状态（如果有）并写到持久存储中。一旦完成状态写检查，算子就将 ‘barrier’ 向下游转发。请注意，在此机制中，如果算子支持，则状态写检查既可以是异步（在写入状态时继续处理），也可以是增量（仅写入更改）。 一旦所有数据接收器（Sink）都收到 ‘barrier’，当前检查点就完成了。故障恢复意味着只需恢复最新的检查点状态，并从最新记录的 ‘barrier’ 对应的偏移量重放数据源。分布式快照在我们在本文开头所要达到的所有需求中得分很高。它们实现了高吞吐量的Exactly-Once语义保证，同时还保留了连续算子模型以及低延迟和自然流量控制。 6. 结论下表总结了我们讨论的每个体系结构如何支持这些功能。 记录确认机制 微批次 事务更新 分布式快照 语义保证 At Least Once Exactly Once Exactly One Exactly One 延迟 非常低 高 低（事务延迟） 非常低 吞吐量 低 高 中到高（取决于分布式事务存储的吞吐量） 高 计算模型 流式 微批次 流式 流式 容错开销 高 低 取决于分布式事务存储的吞吐量 低 流控制 有问题 有问题 自然 自然 应用程序逻辑与容错分离 部分（超时很重要） 否（微批量大小会影响语义） 是 是 7. 实验为了说明Apache Flink的性能，我们进行了一系列实验来研究吞吐量，延迟以及容错机制的影响。下面所有实验都是在Google Compute Engine上进行，使用30个实例，每个实例包含4个内核和15 GB内存。所有Flink实验均使用截至7月24日的最新代码修订版进行，所有Storm实验均使用0.9.3版。可以在此处找到用于评估的所有代码。 7.1 吞吐量我们在有30节点120个核的集群上测量Flink和Storm在两个不同程序上的吞吐量。第一个程序是并行流式grep任务，它在流中搜索包含与正则表达式匹配的字符串的事件。 Flink实现了每个核每秒150万个元素的连续吞吐量。这样集群的总吞吐量达到每秒1.82亿个元素。测试得到的Flink延迟为零，因为作业不涉及网络，也不涉及微批处理。当开启Flink容错机制，设置每5秒进行一次Checkpoint，我们只看到吞吐量的轻微下降（小于2％），没有引入任何延迟。 Storm集群在关闭记录确认机制的情况下（因此没有任何准确性保证）实现了每核每秒约82,000个元素的吞吐量，99%的处理延迟在10毫秒左右。集群的总吞吐量为每秒57万个元素。当启用记录确认机制（保证At-Least-Once语义）时，Storm的吞吐量降至每核每秒4700个元素，延迟也增加到30-120毫秒。接下来，我们配置了Storm Trident，其微批量大小为200,000个元组。Trident实现了每个核每秒75,000个元素的吞吐量（总吞吐量与关闭容错机制的Storm的大致相同）。然而，这是以3000毫秒的延迟（99%）为代价的。 我们可以看到Flink的吞吐量比Trident高出20倍以上，吞吐量比Storm高300倍。在保持高吞吐的情况下，Flink还保证延迟为零。我们还看到，不使用微批次处理模型，高吞吐量不会以延迟为代价。Flink还链接数据源和接收器任务形成任务链，从而仅在单个JVM内交换记录句柄。 我们还进行了如下实验，将核从40个扩展到120个。跟我们预期一样，所有框架都线性扩展，因为grep是一个易于并行处理的程序。现在让我们看一个不同的实验，它按键进行流分组，从而通过网络对流进行Shuffle。我们在30台机器的集群中运行此作业，其系统配置与以前相同。Flink实现了每核每秒大约720,000个事件的吞吐量，启动检查点后降至690,000。请注意，Flink在每个检查点都要备份算子的状态，而Storm则不支持。此示例中的状态相对较小（计数和摘要，每个检查点每个算子的大小小于1M）。具有At-Least-Once语义保证的Storm具有每核每秒约2,600个事件的吞吐量。 7.2 延迟能够处理大规模事件是至关重要的。另一方面，在流处理中尤为重要的是延迟。对于欺诈检测或IT安全等应用程序，以毫秒为单位对事件进行处理意味着可以防止问题出现，而超过100毫秒的延迟通常意味着问题只能在问题发生之后才能发现，而这时候发现意义已经不大了。 当应用程序开发人员可以允许一定的延迟时，通常需要把延迟限制在一定范围内。我们测量流记录分组作业的几个延迟界限，该作业通过网络对数据进行Shuffle。下图显示了观察到的中位数延迟，以及第90百分位，第95百分位和第99百分位延迟（例如，50毫秒的第99百分位的延迟意味着99％的元素到达管道的末端不到50毫秒）。 在以最大吞吐量运行时，Flink的中位数延迟为26毫秒，第99百分位延迟为51毫秒，这意味着99％的延迟都低于51毫秒。打开Flink的检查点机制（启用Exact-Once语义保证）并没有增加可观察到的延迟。但此时，我们确实看到较高百分位数的延迟增加，观察到的延迟大约为150毫秒（译者注：没太搞懂）。出现延迟增加的原因是需要对齐流，算子等待接收所有输入的 ‘barrier’。Storm具有非常低的中位数延迟（1毫秒），并且第99百分位的延迟也是51毫秒。 对于大多数应用程序而言，让人感兴趣的是能够在可接受的延迟上维持高吞吐量，具体取决于特定应用程序的延迟要求。在Flink中，用户可以使用缓冲区超时时间（Buffer Timeout）来调整可接受的延迟。这是什么意思？Flink算子在将记录发送到下一个算子之前会暂存储在缓冲区中。通过指定缓冲区超时时间，例如10毫秒，我们可以告诉Flink在缓冲区满了时或者到达10毫秒时发送缓冲区数据。较低的缓冲区超时时间通常意味着较低的延迟，可能以吞吐量为代价。在上面的实验中，缓冲区超时时间设置为50毫秒，这解释了为什么99%的记录延迟在50毫秒以下。 下面说明了延迟如何影响Flink的吞吐量。因为较低的延迟保证意味着缓冲较少的数据，所以必然会产生一定的吞吐量成本。下图显示了不同缓冲区超时时间下的Flink吞吐量。该实验再次使用流记录分组作业。 如果指定缓冲区超时时间为零，流经算子的记录不会缓冲而是立即转发到下一个算子。在这个延迟优化设置中，Flink可以实现50%的元素延迟在0毫秒，以及99%的元素延迟在20毫秒以下。相应的吞吐量为每个核每秒24,500个事件。当我们增加缓冲区超时时间时，我们会看到延迟增加，吞吐量会同时增加，直到达到吞吐量峰值，缓冲区填充速度超过超时到期时间。缓冲区超时时间为50毫秒时，系统达到每个核每秒750,000个事件的吞吐量峰值，99%的处理延迟在50毫秒以下。 7.3 正确性与恢复开销我们最后一个实验评估了检查点机制的正确性和恢复的开销。我们运行一个需要强一致性的流式程序，并定期杀死工作节点。 我们的测试程序受到网络安全/入侵检测等用例的启发，并使用规则来检查事件序列的有效性（例如，身份验证令牌，登录，服务交互）。该程序从Kafka并行读取事件流，并通过生成的实体（例如，IP地址或用户ID）对事件进行分组。对于每个事件，程序根据一些规则检测目前为止生成实体对应事件序列是否有效（例如，’服务交互’ 必须在 ‘登录’ 之前）。对于无效序列，程序会发布警报。如果没有Exactly-Once语义保证，发生故障时将不可避免地产生无效的事件序列并导致程序发布错误警报。 我们在一个30节点的集群中运行这个程序，其中 YARN chaos monkey 进程每5分钟杀死一个随机的YARN容器。我们保留备用 Worker（TaskManagers），这样系统可以在发生故障后立即取的新资源并继续运行，而无需等待YARN配置新容器。Flink将重新启动失败的 Worker 并在后台将其加入到集群，以确保备用Worker始终可用。 为了模拟的效果，我们使用并行数据生成器将事件推送到Kafka，这些生成器每个核的速度大约为每秒30,000个事件。下图显示了数据生成器的速率（红线），以及Flink作业从Kafka读取事件并使用规则验证事件序列的吞吐量（蓝线）。 原文：https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 集群模式安装与启动]]></title>
    <url>%2Fzookeeper-setup-and-run.html</url>
    <content type="text"><![CDATA[1. 安装要在你的计算机上安装 ZooKeeper 框架，请访问该链接并下载最新版本的ZooKeeper。到目前为止，最新稳定版本的 ZooKeeper是3.4.12(ZooKeeper-3.4.12.tar.gz)。 使用以下命令提取tar文件：cd ~/opt/$ tar -zxf zookeeper-3.4.12.tar.gz 创建软连接，便于升级：$ sudo ln -s zookeeper-3.4.12/ zookeeper 创建数据目录：$ cd zookeeper$ mkdir data 2. 配置修改 conf/zoo.cfg 配置文件：tickTime = 2000dataDir = /Users/smartsi/opt/zookeeper/dataclientPort = 2181initLimit = 10syncLimit = 5server.1=101.34.82.15:2888:3888 server.2=101.34.82.16:2888:3888server.3=101.34.82.17:2888:3888 我们在三台机器上搭建 ZooKeeper 集群：101.34.82.15，10.43.28.1６，10.43.28.1７ 说明： 参数 默认值 描述 initLimit 10 对于从节点最初连接到主节点时的超时时间，单位为tick值的倍数。 syncLimit 5 对于主节点与从节点进行同步操作时的超时时间，单位为tick值的倍数。 dataDir /tmp/zookeeper 用于配置内存数据库保存的模糊快照的目录。文件信息都存放在data目录下。 clientPort 2181 表示客户端所连接的服务器所监听的端口号，默认是2181。即zookeeper对外提供访问的端口号。 server.A=B:C:D 其中A是一个数字，表示这是第几号服务器。B是这台服务器的IP地址。C表示这台服务器与集群中的Leader服务器交换信息的端口。D表示的是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于B都是一样，所以不同的zookeeper实例通信端口号不能一样，所以要给他们分配不同的端口号。 3. 创建myid文件分别在三台机器上我们创建的data目录下新建一个 myid 文件，并进行修改： 在 101.34.82.15 机器上输入1 在 101.34.82.16 机器上输入2 在 101.34.82.17 机器上输入3 需要确保每台服务器的 myid 文件中数字不同，并且和自己所在机器的 zoo.cfg 中 server.id=host:port:port 的id值一样。另外，id的范围是1～255。 4. 配置环境变量分别在三台机器的上修改 /etc/profile 配置环境变量：# ZOOKEEPERexport ZOOKEEPER_HOME=/Users/smartsi/opt/zookeeperexport PATH=$&#123;ZOOKEEPER_HOME&#125;/bin:$PATH 运行命令 source /etc/profile 使环境变量生效。 5. 启动ZooKeeper分别在三台机器的上启动ZooKeeper，进入bin目录下执行：[sjf0115@ying /Users/smartsi/opt/zookeeper/bin]$ sudo zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /Users/smartsi/opt/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 当对三台机器启动后，我们用 zkServer.sh status 命令来查看启动状态：# 101.34.82.16ZooKeeper JMX enabled by defaultUsing config: /Users/smartsi/opt/zookeeper/bin/../conf/zoo.cfgMode: leader# 101.34.82.15ZooKeeper JMX enabled by defaultUsing config: /Users/smartsi/opt/zookeeper/bin/../conf/zoo.cfgMode: follower# 101.34.82.17ZooKeeper JMX enabled by defaultUsing config: /Users/smartsi/opt/zookeeper/bin/../conf/zoo.cfgMode: follower 三台机器会选择一台做为leader，另两台为follower。 6. 连接到ZooKeeper使用如下命令即可连接到其中一台ZooKeeper服务器：[sjf0115@ying ~]$ zkCli.sh -server 101.34.82.17:2181Connecting to 101.34.82.17:2181...Welcome to ZooKeeper!...WATCHER::WatchedEvent state:SyncConnected type:None path:null 其他自动实现同步，客户端只需要和一台保持连接即可。 成功连接后，系统会输出ZooKeeper的相关配置信息和相关环境，并在屏幕上输出 Welcome to ZooKeeper! 等信息。 欢迎关注我的公众号和博客：]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive ORC文件格式]]></title>
    <url>%2Fhive-base-orc-file-format.html</url>
    <content type="text"><![CDATA[1. ORC文件格式 在Hive 0.11.0版本引入此功能 ORC 是 Optimized Row Columnar 的缩写，ORC 文件格式提供一种高效的方法来存储Hive数据。旨在解决其他Hive文件格式的局限。当Hive读取，写入和处理数据时，使用 ORC 文件格式可以提高性能。 例如，与 RCFile 文件格式相比，ORC 文件格式具有许多优点，例如： 每个任务输出文件只有一个，这样可以减轻 NameNode 的负载； 支持的Hive数据类型包括 datetime, decimal, 以及一些复杂类型(struct, list, map, union)； 存储在文件中的轻量级索引； 基于数据类型的块级别压缩：Integer类型的列用行程长度编码(Run Length Encoding)，String类型的列用字典编码(Dictionary Encoding)； 使用多个互相独立的RecordReaders并行读相同的文件； 无需扫描markers就可以分割文件； 绑定读写所需要的内存； 使用Protocol Buffers存储Metadata，可以支持添加和删除一些字段。 1.1 文件结构ORC 文件包含了多个 Stripe。除此之外，File Footer 还包含了一些额外辅助信息。在文件的末尾，PostScript 保存了压缩参数和压缩页脚的大小。Stripe 默认大小为250MB。大的 Stripe 可实现 HDFS 的高效读取。File Footer 包含了文件中的 Stripe 列表，每个 Stripe 有多少行以及每列的数据类型。还包了一些含列级聚合的计数，最小值，最大值以及总和。 下图说明了ORC文件结构： 1.2 Stripe结构从上图我们可以看出，每个 Stripe 都包含 Index data、Row data 以及 Stripe Footer。Stripe Footer 包含流位置的目录(a directory of stream locations)。Row data 在表扫描的时候会用到。 Index data 包含每列的最大值和最小值以及每列所在的行（还可以包括位字段或布隆过滤器）。行索引里面提供了偏移量，它可以跳到正确的压缩块位置以及解压缩块的字节位置。请注意，ORC索引仅用于选择 Stripe 和行组，而不用于查询。 尽管 Stripe 大小很大，具有相对频繁的行索引，可以跳过 Stripe 内很多行快速读取。在默认情况下，最大可以跳过10000行。通过过滤谓词，可以跳过大量的行，你可以根据表的 Secondary Keys 进行排序，从而大幅减少执行时间。例如，你的表的主分区是交易日期，那么你可以在 state、zip code以及last name 上进行排序。然后在一个 state 中查找记录将跳过所有其他 state 的记录。 2. 语法文件格式在表（或分区）级别指定。你可以使用HiveQL语句指定ORC文件格式，例如：CREATE TABLE Addresses ( name string, street string, city string, state string, zip int)STORED AS orc tblproperties ("orc.compress"="NONE"); 除此之外，还可以为表指定压缩算法：CREATE TABLE Addresses ( name string, street string, city string, state string, zip int)STORED AS orc tblproperties ("orc.compress"="Zlib"); 通常不需要设置压缩算法，因为Hive会设置默认的压缩算法 hive.exec.orc.default.compress=ZLIB。 我们通常的做法是将 HDFS 中的数据作为文本，在其上创建 Hive 外部表，然后将数据以 ORC 格式存储在Hive中：CREATE TABLE Addresses_ORC STORED AS ORC AS SELECT * FROM Addresses_TEXT; 3. 高级设置属性全部放在 TBLPROPERTIES 中。ORC具有通常不需要修改的属性。但是，对于特殊情况，你可以修改下表中列出的属性： 从Hive 0.14.0开始 ALTER TABLE table_name [PARTITION partition_spec] CONCATENATE 可用于将小的ORC文件合并为一个更大的文件。合并发生在 Stripe 级别，这可以避免对数据进行解压缩和解码。 参考： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC https://www.iteblog.com/archives/1014.html#HiveORCFile]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL DataFrame与RDD交互]]></title>
    <url>%2Fspark-sql-dataframe-interoperating-with-rdds.html</url>
    <content type="text"><![CDATA[Spark SQL 支持两种不同的方法将现有 RDD 转换为 Datasets。 第一种方法使用反射来推断包含特定类型对象的 RDD 的 schema。当你在编写 Spark 应用程序时，你已经知道了 schema，这种基于反射的方法会使代码更简洁，并且运行良好。 第二种方法是通过编程接口来创建 DataSet，这种方法允许构建一个 schema，并将其应用到现有的 RDD 上。虽然这种方法更详细，但直到运行时才知道列及其类型，才能构造 DataSets。 1. 使用反射推导schemaSpark SQL 支持自动将 JavaBeans 的 RDD 转换为 DataFrame。使用反射获取的 BeanInfo 定义了表的 schema。目前为止，Spark SQL 还不支持包含 Map 字段的 JavaBean。但是支持嵌套的 JavaBeans，List 以及 Array 字段。你可以通过创建一个实现 Serializable 的类并为其所有字段设置 getter 和 setter 方法来创建一个 JavaBean。 Java版本：import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.Encoder;import org.apache.spark.sql.Encoders;// 从文本文件中创建Person对象的RDDJavaRDD&lt;Person&gt; personRDD = sparkSession.read() .textFile("src/main/resources/person.txt") .javaRDD() .map(line -&gt; &#123; String[] parts = line.split(","); Person person = new Person(); person.setName(parts[0]); person.setAge(Integer.parseInt(parts[1].trim())); return person;&#125;);// 在 JavaBean 的 RDD 上应用 schema 生成 DataFrameDataset&lt;Row&gt; personDataFrame = sparkSession.createDataFrame(personRDD, Person.class);// 注册为临时视图personDataFrame.createOrReplaceTempView("people");// 运行SQlDataset&lt;Row&gt; teenagersDataFrame = sparkSession.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19");// Row中的列可以通过字段索引获取Encoder&lt;String&gt; stringEncoder = Encoders.STRING();Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDataFrame.map( (MapFunction&lt;Row, String&gt;) row -&gt; "Name: " + row.getString(0), stringEncoder);teenagerNamesByIndexDF.show();/** +------------+ | value| +------------+ |Name: Justin| +------------+ */// Row中的列可以通过字段名称获取Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDataFrame.map( (MapFunction&lt;Row, String&gt;) row -&gt; "Name: " + row.&lt;String&gt;getAs("name"), stringEncoder);teenagerNamesByFieldDF.show();/** +------------+ | value| +------------+ |Name: Justin| +------------+ */ Scala版本：// For implicit conversions from RDDs to DataFramesimport spark.implicits._// Create an RDD of Person objects from a text file, convert it to a Dataframeval peopleDF = spark.sparkContext .textFile("src/main/resources/person.txt") .map(_.split(",")) .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)) .toDF()// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by Sparkval teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")// The columns of a row in the result can be accessed by field indexteenagersDF.map(teenager =&gt; "Name: " + teenager(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// or by field nameteenagersDF.map(teenager =&gt; "Name: " + teenager.getAs[String]("name")).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// No pre-defined encoders for Dataset[Map[K,V]], define explicitlyimplicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]// Primitive types and case classes can be also defined as// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List("name", "age"))).collect()// Array(Map("name" -&gt; "Justin", "age" -&gt; 19)) 2. 使用编程方式指定Schema当 JavaBean 类不能提前定义时（例如，记录的结构以字符串编码，或者解析文本数据集，不同用户字段映射方式不同），可以通过编程方式创建 DataSet，有如下三个步骤： 从原始 RDD(例如，JavaRDD)创建 Rows 的 RDD(JavaRDD); 创建由 StructType 表示的 schema，与步骤1中创建的 RDD 中的 Rows 结构相匹配。 通过SparkSession提供的 createDataFrame 方法将 schema 应用到 Rows 的 RDD。 Java版本：import java.util.ArrayList;import java.util.List;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.types.DataTypes;import org.apache.spark.sql.types.StructField;import org.apache.spark.sql.types.StructType;// JavaRDD&lt;String&gt;JavaRDD&lt;String&gt; peopleRDD = sparkSession.sparkContext() .textFile("src/main/resources/person.txt", 1) .toJavaRDD();// JavaRDD&lt;Row&gt;JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123; String[] attributes = record.split(","); return RowFactory.create(attributes[0], attributes[1].trim());&#125;);// 字符串 schemaString schemaString = "name age";// 根据字符串 schema 产生 schemaList&lt;StructField&gt; fields = new ArrayList&lt;&gt;();for (String fieldName : schemaString.split(" ")) &#123; StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true); fields.add(field);&#125;StructType schema = DataTypes.createStructType(fields);// Dataset&lt;Row&gt;Dataset&lt;Row&gt; peopleDataFrame = sparkSession.createDataFrame(rowRDD, schema);// 临时视图peopleDataFrame.createOrReplaceTempView("people");// 运行SQLDataset&lt;Row&gt; results = sparkSession.sql("SELECT name FROM people");Dataset&lt;String&gt; namesDS = results.map( (MapFunction&lt;Row, String&gt;) row -&gt; "Name: " + row.getString(0), Encoders.STRING());namesDS.show();/** +-------------+ | value| +-------------+ |Name: Michael| | Name: Andy| | Name: Justin| +-------------+ */ Scala版本：import org.apache.spark.sql.types._// Create an RDDval peopleRDD = spark.sparkContext.textFile("src/main/resources/person.txt")// The schema is encoded in a stringval schemaString = "name age"// Generate the schema based on the string of schemaval fields = schemaString.split(" ") .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))val schema = StructType(fields)// Convert records of the RDD (people) to Rowsval rowRDD = peopleRDD .map(_.split(",")) .map(attributes =&gt; Row(attributes(0), attributes(1).trim))// Apply the schema to the RDDval peopleDF = spark.createDataFrame(rowRDD, schema)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL can be run over a temporary view created using DataFramesval results = spark.sql("SELECT name FROM people")// The results of SQL queries are DataFrames and support all the normal RDD operations// The columns of a row in the result can be accessed by field index or by field nameresults.map(attributes =&gt; "Name: " + attributes(0)).show()/** +-------------+ | value| +-------------+ |Name: Michael| | Name: Andy| | Name: Justin| +-------------+ */ Spark 版本: 2.3.1 原文：http://spark.apache.org/docs/2.3.1/sql-programming-guide.html#interoperating-with-rdds]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 多文件输出]]></title>
    <url>%2Fspark-base-multiple-output-format.html</url>
    <content type="text"><![CDATA[1. 自定义MultipleOutputFormat在Hadoop 多文件输出MultipleOutputFormat中介绍了如何在Hadoop中根据Key或者Value的值将属于不同的类型的记录写到不同的文件中。在这里面用到了MultipleOutputFormat这个类。 因为Spark内部写文件方式其实调用的是Hadoop相关API，所以我们也可以通过Spark实现多文件输出。不过遗憾的是，Spark内部没有多文件输出的函数供我们直接使用。我们可以通过调用saveAsHadoopFile函数并自定义MultipleOutputFormat类来实现多文件输出，如下所示：public class RDDMultipleTextOutputFormat&lt;K, V&gt; extends MultipleTextOutputFormat&lt;K, V&gt; &#123; @Override protected String generateFileNameForKeyValue(K key, V value, String name) &#123; return key.toString(); &#125;&#125; RDDMultipleTextOutputFormat类中的 generateFileNameForKeyValue 函数有三个参数，key和value是RDD对应的Key和Value，而name参数是每个Reduce的编号。上面例子中没有使用该参数，而是直接将同一个Key的数据输出到同一个文件中。我们来看看如何使用这个自定义的类：String appName = "MultipleTextOutputExample";SparkConf conf = new SparkConf().setAppName(appName);JavaSparkContext sc = new JavaSparkContext(conf);JavaRDD&lt;String&gt; source = sc.textFile(inputPath);// 以platform为keyJavaPairRDD&lt;String, String&gt; result = source.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123; @Override public Tuple2&lt;String, String&gt; call(String str) throws Exception &#123; String[] params = str.split("\t"); String platform = "other"; if(params.length &gt; 1 &amp;&amp; StringUtils.isNotBlank(params[1]))&#123; platform = params[1]; &#125; return new Tuple2&lt;&gt;(platform, str); &#125;&#125;);// 保存result.saveAsHadoopFile(outputPath, String.class, String.class, RDDMultipleTextOutputFormat.class); 上面示例中通过调用 saveAsHadoopFile 函数并自定义 MultipleOutputFormat 类来实现多文件输出，如下所示输出：[xiaosi@ying ~]$ sudo -uxiaosi hadoop fs -ls tmp/data_group/example/output/priceFound 3 items-rw-r--r-- 3 xiaosi xiaosi 0 2018-07-12 16:24 tmp/data_group/example/output/price/_SUCCESS-rw-r--r-- 3 xiaosi xiaosi 723754 2018-07-12 16:23 tmp/data_group/example/output/price/adr-rw-r--r-- 3 xiaosi xiaosi 799216 2018-07-12 16:23 tmp/data_group/example/output/price/ios 我们可以看到输出已经根据RDD的key将属于不同类型的记录写到不同的文件中，每个key对应一个文件，如果想每个key对应多个文件输出，需要修改一下我们自定义的RDDMultipleTextOutputFormat，如下代码所示：public class RDDMultipleTextOutputFormat&lt;K, V&gt; extends MultipleTextOutputFormat&lt;K, V&gt; &#123; @Override protected String generateFileNameForKeyValue(K key, V value, String name) &#123; return key.toString() + "/" + name; &#125;&#125; 输出如下所示:[xiaosi@ying ~]$ sudo -uxiaosi hadoop fs -ls tmp/data_group/example/output/price/Found 3 items-rw-r--r-- 3 xiaosi xiaosi 0 2018-07-16 10:00 tmp/data_group/example/output/price/_SUCCESSdrwxr-xr-x - xiaosi xiaosi 0 2018-07-16 10:00 tmp/data_group/example/output/price/adrdrwxr-xr-x - xiaosi xiaosi 0 2018-07-16 10:00 tmp/data_group/example/output/price/ios[xiaosi@ying ~]$[xiaosi@ying ~]$ sudo -uxiaosi hadoop fs -ls tmp/data_group/example/output/price/adr/Found 2 items-rw-r--r-- 3 xiaosi xiaosi 23835 2018-07-16 10:00 tmp/data_group/example/output/price/adr/part-00000-rw-r--r-- 3 xiaosi xiaosi 22972 2018-07-16 10:00 tmp/data_group/example/output/price/adr/part-00001 2. DataFrame 方式如果你使用的是Spark 1.4+，借助DataFrame API会变得更加容易。（DataFrames是在Spark 1.3中引入的，但我们需要的partitionBy（）是在1.4中引入的。） 如果你使用的是RDD，首先需要将其转换为DataFrame。拥有DataFrame后，基于特定 key 输出到多个文件中就很简单了。SparkSession sparkSession = SparkSession .builder() .appName("MultipleTextOutputExample") .config("spark.some.config.option", "some-value") .getOrCreate();JavaRDD&lt;Price&gt; priceRDD = sparkSession.read().textFile(inputPath).javaRDD().map(new Function&lt;String, Price&gt;() &#123; @Override public Price call(String str) throws Exception &#123; String[] params = str.split("\t"); Price price = new Price(); price.setDate(params[0]); price.setPlatform(params[1]); price.setAdType(params[2]); price.setChannelId(params[3]); price.setUid(params[4]); price.setPrice(params[5]); return price; &#125;&#125;);Dataset&lt;Row&gt; priceDataFrame = sparkSession.createDataFrame(priceRDD, Price.class);priceDataFrame.write().partitionBy("platform").json(outputPath); 在这个示例中，Spark将为我们在DataFrame上分区的每个 key 创建一个子目录：[xiaosi@ying ~]$ sudo -uxiaosi hadoop fs -ls tmp/data_group/example/output/price/Found 3 items-rw-r--r-- 3 xiaosi xiaosi 0 2018-07-16 15:41 tmp/data_group/example/output/price/_SUCCESSdrwxr-xr-x - xiaosi xiaosi 0 2018-07-16 15:41 tmp/data_group/example/output/price/platform=adrdrwxr-xr-x - xiaosi xiaosi 0 2018-07-16 15:41 tmp/data_group/example/output/price/platform=ios 参考： https://www.iteblog.com/archives/1281.html https://stackoverflow.com/questions/23995040/write-to-multiple-outputs-by-key-spark-one-spark-job]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Persist,Cache以及Checkpoint]]></title>
    <url>%2Fspark-base-persist-cache-checkpoint.html</url>
    <content type="text"><![CDATA[1. 概述要重用RDD（弹性分布式数据集），Apache Spark提供了许多选项，包括： Persisting Caching Checkpointing 下面我们将了解每一个的用法。重用意味着将计算和数据存储在内存中，并在不同的算子中多次重复使用。通常，在处理数据时，我们需要多次使用相同的数据集。例如，许多机器学习算法（如K-Means）在生成模型之前会对数据进行多次迭代。如果处理过程中的中间结果没有持久存储在内存中，这意味着你需要将中间结果存储在磁盘上，这会降低整体性能，因为与RAM相比，从磁盘访问数据就像是从隔壁或从其他国家获取内容。下面我们看一下在不同存储设备上的访问时间： 注意硬盘访问时间和RAM访问时间。这就是为什么Hadoop MapReduce与Spark相比速度慢的原因，因为每个MapReduce迭代都会在磁盘上读取或写入数据。Spark在内存中处理数据，如果使用不当将导致作业在执行期间性能下降。让我们首先从持久化RDD到内存开始，但首先我们需要看看为什么我们需要持久化。 假设我们执行以下Spark语句：val textFile = sc.textFile("file:///c://fil.txt")textFile.first()textFile.count() 第一行读取内存中的文件内容，读取操作是Transformation操作，因此不会有任何作业执行。Spark直到遇到Action操作才会惰性地执行DAG。接下来的两行是Action操作，它们为每个Action操作生成一个单独的作业。第二行得到RDD的第一个文本行并打印出来。第三行计算RDD中的行数。这两个Action操作都会产生结果，其内部发生的事情是Spark为每个Action生成一个单独的作业，因此RDD计算了两次。现在让我们执行以下语句：val textFile = sc.textFile("file:///c://fil.txt")textFile.cache()textFile.first()textFile.count()// again execute the same set of commandstextFile.first()textFile.count() 我们来看看Shell应用程序UI界面。如果你正在运行Spark Shell，那么默认情况下，可以通过URL http://localhost:4040 访问此接口： 每个Action都会在Spark中生成一个单独的作业。我们从上图的底部开始看（按照时间发生顺序看），前两组记录是first（）和count（）Action操作执行的作业。中间两个记录也是前面两个Action操作产生的作业，但在此之前，RDD持久存储在RAM中。由于Spark必须在第一个语句中重新计算RDD，因此Duration时间没有得到改善。但请注意最上面的2个作业，是在RDD持久化存储在RAM后执行的，这次完成每个作业的Duration时间明显减少，这是因为Spark没有从磁盘中获取数据重新计算RDD，而是处理持久化存储在RAM中的RDD，并且与访问硬盘相比访问RAM时间会更少，我们完成相同工作的时间也会更短。现在让我们关注 Persist，Cache 和 Checkpoint。 2. PersistPersist 意味着将计算出的RDD保存在RAM中并在需要时重复使用它。有几种不同级别的持久化： 持久化级别 说明 MEMORY_ONLY 将 RDD 以 Java 对象的形式存储在 JVM 中。如果没有足够的内存存储 RDD，则某些分区将不会被缓存，每次需要时都会重新计算。这是默认级别。如果你知道数据大小可以装载进内存中，可以使用此选项，否则会重新计算某些分区，会显着降低整体作业的性能。 MEMORY_AND_DISK 将 RDD 以 Java 对象的形式存储在 JVM 中。如果数据在内存中放不下，则溢写到磁盘上。需要时则会从磁盘上读取，但与重新计算不能放进内存的分区相比，花费的时间会少得多。 MEMORY_ONLY_SER 此级别与MEMORY_ONLY完全相同，但会在存储到内存之前序列化对象。这通常比 Java 对象更具空间效率，但是这种方式读取数据会消耗更多的CPU。 MEMORY_AND_DISK_SER 与 MEMORY_ONLY_SER 类似，但如果数据在内存中放不下，则溢写到磁盘上，而不是每次需要时重新计算它们。 DISK_ONLY 将 RDD 分区存储在磁盘上而不是内存上。 OFF_HEAP 分区可以存储在堆外内存上。需要启用堆外内存才能使此存储级别正常工作。与堆上相比，从堆外内存访问数据有点慢，但仍然比磁盘上访问好得多。 以下是使用上述存储级别持久保存RDD的代码。如上所述可以更改存储级别：textFile.persist(StorageLevel.MEMORY_ONLY) 3. CacheCache 与 MEMORY_ONLY 的持久化级别相同，如以下代码所示：textFile.cache()// is same as MEMORY_ONLY storage level in persisttextFile.persist(StorageLevel.MEMORY_ONLY) 4. Checkpoint最后一个是Checkpoint，这是在作业执行期间发生故障时对RDD分区的一种重用。在具有数百个节点的集群环境中运行时，节点故障很有可能发生。即使在正常计算期间，JVM 进程也可能由于多种原因而失败。无论是什么故障，重新计算丢失的分区是一种昂贵的操作。最佳策略是在出现故障时从某个 Checkpoint 恢复故障。Checkpoint 将 RDD 的某些 stage 保存在磁盘上并打破DAG的执行链条。DAG 是应用在 RDD 上的Transformations序列，并且在每个Transformation中执行一些计算。有时这些计算很昂贵，如果出现故障，则需要从头开始重新计算丢失的分区。但是如果我们将DAG某个时间点的RDD stage 保存在磁盘上，则不需要重新从头进行计算，而是将检查点作为重新计算的起点。虽然Spark具有弹性并可以通过重新计算丢失的分区从故障中恢复，但是有时重新执行非常长的转换序列代价非常昂贵，如果我们在某个时刻点对RDD进行 Checkpoint 并使用该 Checkpoint 作为起点来重新计算丢失的分区，这样可以提高性能。我们来看下图： 此作业从Spark开始并经历 stage 1到5。第一个 stage 从磁盘读取数据文件，然后stage 2到5在RDD上执行一些昂贵且复杂的计算。假设我们没有在第3个 stage 上进行 Checkpoint，并且在第4个 stege 或第5个 stage 上发生了一些故障。由于Spark具有弹性并且可以从故障中恢复，但是因为我们没有在第三个 stage 上进行 Checkpoint，所以需要从第1个 stage 开始来重新计算分区。就整体作业的性能而言，代价非常昂贵的。现在假设我们在第3个 stage 上进行 Checkpoint。Spark做的是将第3个 stage 的RDD状态保存在某些可靠的介质上，如HDFS。Checkpoint 会打破DAG执行链条，并将 Checkpoint 视为新的基线。这意味着如果在stage 4或5中发生任何节点或分区故障，不是从第一个 stage 开始计算丢失的分区，而是从 Checkpoint 开始计算。这种策略会极大地提高Spark作业在由于任何原因可能发生故障的环境中的性能。将 Checkpoint 视为新的基线，在分区或 stage 失败时会从此基线执行所有计算。 本文介绍了重用RDD的不同策略，正确使用这些策略将大大提高Spark作业的整体性能。 原文：https://www.linkedin.com/pulse/persist-cache-checkpoint-apache-spark-shahzad-aslam/]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Grouping Sets,CUBE与ROLLUP]]></title>
    <url>%2Fhive-base-grouping-sets.html</url>
    <content type="text"><![CDATA[这篇文章描述了 SELECT 语句 GROUP BY 子句的增强聚合功能 GROUPING SETS。GROUPING SETS 子句是 SELECT 语句的 GROUP BY 子句的扩展。通过 GROUPING SETS 子句，你可采用多种方式对结果分组，而不必使用多个 SELECT 语句来实现这一目的。这就意味着，能够减少响应时间并提高性能。 在Hive 0.10.0版本中添加了 Grouping sets，CUBE 和 ROLLUP 运算符以及 GROUPING__ID 函数。参见HIVE-2397，HIVE-3433，HIVE-3471和 HIVE-3613。另外在Hive 0.11.0版本进行的优化 HIVE-3552。 1. GROUPING SETSGROUP BY 中的 GROUPING SETS 子句允许我们在同一记录集中指定多个 GROUP BY 选项。所有 GROUPING SET 子句都可以逻辑表示为 UNION 连接的几个 GROUP BY 查询。下面展示了几个这样的等价示例。这有助于我们了解 GROUPING SETS 子句的思想。GROUPING SETS 子句中的空白set（）计算整体聚合。 SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b) ) 等价于:SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b), a) 等价于:SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, bUNIONSELECT a, null, SUM( c ) FROM tab1 GROUP BY a SELECT a,b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS (a,b) 等价于:SELECT a, null, SUM( c ) FROM tab1 GROUP BY aUNIONSELECT null, b, SUM( c ) FROM tab1 GROUP BY b SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a, b), a, b, ( ) ) 等价于:SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, bUNIONSELECT a, null, SUM( c ) FROM tab1 GROUP BY a, nullUNIONSELECT null, b, SUM( c ) FROM tab1 GROUP BY null, bUNIONSELECT null, null, SUM( c ) FROM tab1 2. Grouping__IDGROUPING SETS 会对 GROUP BY 子句中的列进行多维组合，结果整体展现，对于没有参与 GROUP BY 的那一列置为 NULL 值。如果列本身值就为 NULL，则可能会发生冲突。这样我们就没有办法去区分该列显示的 NULL 值是列本身就是 NULL 值，还是因为该列没有参与 GROUP BY 而被置为 NULL 值。所以需要一些方法来识别列中的NULL，GROUPING__ID 函数就是为了解决这个问题而引入的。 此函数返回一个位向量，与每列是否存在对应。用二进制形式中的每一位来标示对应列是否参与 GROUP BY。Hive2.3.0版本之前，如果某一列参与了 GROUP BY，对应位就被置为1，否则为0。在这一版本，GROUPINGID 与位向量之间的关系比较别扭，GROUPINGID实际为位向量先反转之后再转为十进制的值。这一点，在Hive2.3.0版本得到解决，如果某一列参与了 GROUP BY，对应位就被置为0，否则为1。所以在使用 GROUPING__ID 时注意一下版本号。 GROUPINGID 的值与 GROUP BY 表达式中列的取值和顺序有关，所以如果重新排列，GROUPINGID 对应的含义也会变化。 具体看一个例子（数据内容以及表结构可以在文章末尾查看）：SELECT GROUPING__ID, dt, platform, channel, SUM(pv), COUNT(DISTINCT userName)FROM tmp_read_pvGROUP BY dt, platform, channel GROUPING SETS ( dt, (dt, platform), (dt, channel), (dt, platform, channel)); 输出结果如下： 序号 GROUPING__ID 位向量 日期 平台 渠道 浏览量 用户数 1 1 100 20180627 NULL NULL 242.0 8 2 1 100 20180628 NULL NULL 282.0 9 3 3 110 20180627 adr NULL 137.0 6 4 3 110 20180627 ios NULL 105.0 2 5 3 110 20180628 NULL NULL 26.0 1 6 3 110 20180628 adr NULL 96.0 4 7 3 110 20180628 ios NULL 160.0 4 8 5 101 20180627 NULL toutiao 149.0 6 9 5 101 20180627 NULL uc 93.0 6 10 5 101 20180628 NULL NULL 96.0 1 11 5 101 20180628 NULL toutiao 89.0 7 12 5 101 20180628 NULL uc 97.0 6 13 7 111 20180627 adr toutiao 82.0 5 14 7 111 20180627 adr uc 55.0 4 15 7 111 20180627 ios toutiao 67.0 1 16 7 111 20180627 ios uc 38.0 2 17 7 111 20180628 NULL uc 26.0 1 18 7 111 20180628 adr toutiao 35.0 4 19 7 111 20180628 adr uc 61.0 3 20 7 111 20180628 ios NULL 96.0 1 21 7 111 20180628 ios toutiao 54.0 3 22 7 111 20180628 ios uc 10.0 2 Hive2.1.1版本下生成的数据 例如上面的第5，10，17，20行所示，有些字段本身值就为 NULL。 如果希望没有参与 GROUP BY 的列不显示 NULL 而是显示一个自定义值（例如，total 表示对应分组的全量），SELECT GROUPING__ID, CASE WHEN (CAST (GROUPING__ID AS INT) &amp; 1) == 0 THEN 'total' ELSE dt END, CASE WHEN (CAST (GROUPING__ID AS INT) &amp; 2) == 0 THEN 'total' ELSE platform END, CASE WHEN (CAST (GROUPING__ID AS INT) &amp; 4) == 0 THEN 'total' ELSE channel END, dt, platform, channel, SUM(pv), COUNT(DISTINCT userName)FROM tmp_read_pvGROUP BY dt, platform, channel GROUPING SETS ( dt, (dt, platform), (dt, channel), (dt, platform, channel)); 结果如下: GROUPING__ID 日期昵称 平台昵称 渠道昵称 日期 平台 渠道 浏览量 用户数 1 20180627 total total 20180627 NULL NULL 242.0 8 1 20180628 total total 20180628 NULL NULL 282.0 9 3 20180628 adr total 20180628 adr NULL 96.0 4 3 20180627 ios total 20180627 ios NULL 105.0 2 3 20180628 NULL total 20180628 NULL NULL 26.0 1 3 20180627 adr total 20180627 adr NULL 137.0 6 3 20180628 ios total 20180628 ios NULL 160.0 4 5 20180628 total NULL 20180628 NULL NULL 96.0 1 5 20180628 total toutiao 20180628 NULL toutiao 89.0 7 5 20180627 total toutiao 20180627 NULL toutiao 149.0 6 5 20180628 total uc 20180628 NULL uc 97.0 6 5 20180627 total uc 20180627 NULL uc 93.0 6 7 20180627 adr uc 20180627 adr uc 55.0 4 7 20180627 ios toutiao 20180627 ios toutiao 67.0 1 7 20180628 ios uc 20180628 ios uc 10.0 2 7 20180628 NULL uc 20180628 NULL uc 26.0 1 7 20180628 adr uc 20180628 adr uc 61.0 3 7 20180628 ios NULL 20180628 ios NULL 96.0 1 7 20180627 adr toutiao 20180627 adr toutiao 82.0 5 7 20180628 adr toutiao 20180628 adr toutiao 35.0 4 7 20180627 ios uc 20180627 ios uc 38.0 2 7 20180628 ios toutiao 20180628 ios toutiao 54.0 3 Hive2.1.1版本下生成的数据 如果对于列本身值没有为 NULL 的情况，可以使用如下简单方式来实现：SELECT GROUPING__ID, CASE WHEN dt IS NULL THEN 'total' ELSE dt END, CASE WHEN platform IS NULL THEN 'total' ELSE platform END, CASE WHEN channel IS NULL THEN 'total' ELSE channel END, SUM(pv), COUNT(DISTINCT userName)FROM tmp_read_pvGROUP BY dt, platform, channel GROUPING SETS ( dt, (dt, platform), (dt, channel), (dt, platform, channel)); 4. CUBE与ROLLUP通用语法是 WITH CUBE/ROLLUP。只能 GROUP BY 一起使用。 4.1 CUBECUBE 简称数据魔方，可以实现 Hive 多个任意维度的查询。CUBE 创建集合中所有可能组合。例如：GROUP BY a，b，c WITH CUBE 等价于GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（b，c），（a，c），（a），（b），（c），（））。 4.2 ROLLUPROLLUP 子句与 GROUP BY 一起使用用来计算维度上层次结构级别的聚合。ROLLUP 可以实现从右到左递减多级的统计。 具有 ROLLUP 的 GROUP BY a，b，c 假定层次结构为 a 向下钻取到(drilling down) b，向下钻取到 c。例如：GROUP BY a，b，c WITH ROLLUP 等价于:GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（a），（）） 5. 数据演示数据：20180627 adr toutiao d918 720180627 adr uc d918 3020180628 adr uc d918 1520180628 adr toutiao d918 1020180627 ios uc 828b 1620180628 ios uc 828b 620180628 ios toutiao 828b 1820180627 adr toutiao cece 520180628 adr toutiao cece 820180627 ios toutiao 6428 6720180627 ios uc 6428 2220180627 adr uc e962 920180627 adr uc e962 820180628 ios toutiao 953c 1320180628 ios toutiao 953c 720180627 adr toutiao f930 5420180628 adr toutiao f930 820180627 adr uc f930 420180628 adr uc f930 4020180627 adr uc 2bfa 420180627 adr toutiao 2bfa 720180628 adr uc 2bfa 620180628 adr toutiao 2bfa 920180627 adr toutiao 2f3d 520180627 adr toutiao 2f3d 420180628 ios \N 2f3d 6220180628 ios \N 2f3d 3420180628 \N uc 5f02 1220180628 \N uc 5f02 1420180628 ios uc f215 420180628 ios toutiao f215 16 演示表：CREATE EXTERNAL TABLE IF NOT EXISTS tmp_read_pv ( dt string, platform string, channel string, userName string, pv string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'LINES TERMINATED BY '\n'LOCATION '/user/xiaosi/tmp/data_group/example/input/read_pv/'; Hive版本:2.1.1 参考：https://stackoverflow.com/questions/29577887/grouping-in-hive https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Count Distinct优化]]></title>
    <url>%2Fhive-tuning-count-distinct.html</url>
    <content type="text"><![CDATA[目前，Hive底层使用MapReduce作为实际计算框架，SQL的交互方式隐藏了大部分MapReduce的细节。这种细节的隐藏在带来便利性的同时，也对计算作业的调优带来了一定的难度。未经优化的SQL语句转化后的MapReduce作业，它的运行效率可能大大低于用户的预期。本文我们就来分析一个简单语句的优化过程。 日常统计场景中，我们经常会对一段时期内的字段进行去重并统计数量，SQL语句类似于SELECT COUNT( DISTINCT id )FROM TABLE_NAMEWHERE ...; 这条语句是从一个表的符合WHERE条件的记录中统计不重复的id的总数。该语句转化为MapReduce作业后执行示意图如下，图中还列出了我们实验作业中Reduce阶段的数据规模：由于引入了DISTINCT，因此在Map阶段无法利用Combine对输出结果去重，必须将id作为Key输出，在Reduce阶段再对来自于不同Map Task、相同Key的结果进行去重，计入最终统计值。 我们看到作业运行时的Reduce Task个数为1，对于统计大数据量时，这会导致最终Map的全部输出由单个的ReduceTask处理。这唯一的Reduce Task需要Shuffle大量的数据，并且进行排序聚合等处理，这使得它成为整个作业的IO和运算瓶颈。 经过上述分析后，我们尝试显式地增大Reduce Task个数来提高Reduce阶段的并发，使每一个Reduce Task的数据处理量控制在2G左右。具体设置如下：# Hadoop1.xset mapred.reduce.tasks=100;# Hadoop2.xset mapreduce.job.reduces=100; 调整后我们发现这一参数并没有影响实际Reduce Task个数，Hive运行时输出 Number of reduce tasks determined at compile time: 1。原来Hive在处理COUNT这种全聚合(full aggregates)计算时，会忽略用户指定的Reduce Task数，而强制使用1。我们只能采用变通的方法来绕过这一限制。我们利用Hive对嵌套语句的支持，将原来一个MapReduce作业转换为两个作业，在第一阶段选出全部的非重复id，在第二阶段再对这些已去重的id进行计数。这样在第一阶段我们可以通过增大Reduce的并发数，并发处理Map输出。在第二阶段，由于id已经去重，因此 COUNT(*) 操作在Map阶段不需要输出原id数据，只输出一个合并后的计数即可。这样即使第二阶段Hive强制指定一个Reduce Task，极少量的Map输出数据也不会使单一的Reduce Task成为瓶颈。改进后的SQL语句如下：SELECT COUNT(*)FROM( SELECT DISTINCT id FROM TABLE_NAME WHERE …) t; 在实际运行时，我们发现Hive还对这两阶段的作业做了额外的优化。它将第二个MapReduce作业Map中的Count过程移到了第一个作业的Reduce阶段。这样在第一阶段Reduce就可以输出计数值，而不是去重的全部id。这一优化大幅地减少了第一个作业的Reduce输出IO以及第二个作业Map的输入数据量。最终在同样的运行环境下优化后的语句执行只需要原语句20%左右的时间。优化后的MapReduce作业流如下： 从上述优化过程我们可以看出，一个简单的统计需求，如果不理解Hive和MapReduce的工作原理，它可能会比优化后的执行过程多四、五倍的时间。我们在利用Hive简化开发的同时，也要尽可能优化SQL语句，提升计算作业的执行效率。 注：文中测试环境Hive版本为0.9 原文：http://bigdata-blog.net/2013/11/08/hive-sql%E4%BC%98%E5%8C%96%E4%B9%8B-count-distinct/]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hive 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 本地执行模式]]></title>
    <url>%2Fhive-tuning-local-mode.html</url>
    <content type="text"><![CDATA[1. 简介假设你正在运行一些复杂的 Hive 查询，我们都知道这会在后台触发 MapReduce 作业并为你提供输出。如果 Hive 中的数据比较大，这种方法比较有效，但如果 Hive 表中的数据比较少，这样会有一些问题。出现此问题的主要原因是 MapReduce 作业被触发，它是在服务器/集群上触发，因此每次运行查询时，它都会上传到服务器并在那里启动 MapReduce，然后输出。因此，为查询触发执行任务的时间消耗可能会比实际作业的执行时间要多的多。 Hive 可以通过本地模式在单台机器上处理所有的任务。对于本地模式，如果你的查询启动的 MapReduce 作业少于4个 Mapper，那么 MapReduce 作业将在本地运行，这样可以在更短的时间内输出查询结果。 Hive 0.7.0 版本开始引入 2. 配置需要满足如下三个配置条件，才能在本地模式下运行 Hive 查询： 参数 默认值 描述 hive.exec.mode.local.auto false 让Hive确定是否自动启动本地模式运行 hive.exec.mode.local.auto.inputbytes.max 134217728(128MB) 当第一个参数为true时，输入字节小于此值时才能启动本地模式 hive.exec.mode.local.auto.input.files.max 4 当一个参数为true时，任务个数小于此值时才能启动本地模式 3. Example本地模式执行如下所示：hive&gt; SET hive.exec.mode.local.auto=true;hive&gt; SET hive.exec.mode.local.auto.inputbytes.max=50000000;hive&gt; SET hive.exec.mode.local.auto.input.files.max=5;hive&gt; select count(1) from test;Automatically selecting local only mode for query...Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1...Job running in-process (local Hadoop)2018-07-25 19:04:23,559 Stage-1 map = 100%, reduce = 100%Ended Job = job_local3079326_0001MapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 18214 HDFS Write: 102 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOK55Time taken: 7.542 seconds, Fetched: 1 row(s) 远程模式执行如下所示：hive&gt; SET hive.exec.mode.local.auto=false;hive&gt; select count(1) from test;...Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1...Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12018-07-25 19:06:51,819 Stage-1 map = 0%, reduce = 0%2018-07-25 19:06:58,006 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.24 sec2018-07-25 19:07:18,447 Stage-1 map = 100%, reduce = 67%, Cumulative CPU 3.14 sec2018-07-25 19:07:22,527 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.2 secMapReduce Total cumulative CPU time: 4 seconds 200 msecEnded Job = job_1504162679223_31594449MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.2 sec HDFS Read: 18103 HDFS Write: 102 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 200 msecOK55Time taken: 89.256 seconds, Fetched: 1 row(s) 我们可以看到在本地模式下只需执行7.542s，而在远程模式下执行却需要执行89.256s。 参考：https://milindjagre.co/2015/09/04/set-hive-in-local-auto-mode/]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hive 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SparkSession:一个新的入口]]></title>
    <url>%2Fspark-sql-sparksession-new-entry-point.html</url>
    <content type="text"><![CDATA[在 Spark 1.x 中，使用 HiveContext 作为 DataFrame API 的入口显得并不直观。在 Spark 2.0 引入 SparkSession 作为一个新的入口，并且包含 SQLContext 和 HiveContext 的特性，同时为了向后兼容，两者都保留下来。SparkSession 有很多特性，在这里我们展示一些更重要的特性。 1. 创建SparkSessionSparkSession 可以使用建造者模式创建。如果 SparkContext 存在，那么 SparkSession 将会重用它，但是如果不存在就会创建一个 SparkContext。在I/O期间，在 builder 中设置的配置选项会自动传递给 Spark 和 Hadoop。 Java版本：SparkSession sparkSession = SparkSession .builder() .master("local[2]") .appName("SparkSession Example") .config("spark.some.config.option", "config-value") .getOrCreate(); Scala版本：import org.apache.spark.sql.SparkSessionval sparkSession = SparkSession.builder .master("local[2]") .appName("SparkSession Example") .config("spark.some.config.option", "config-value") .getOrCreate()import org.apache.spark.sql.SparkSessionsparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@46d6b87c 2. 统一读取数据的入口SparkSession 是读取数据的入口，类似于旧的 SQLContext.read。 Java版本:Dataset&lt;Row&gt; dataFrame = sparkSession.read().json("src/main/resources/person.json"); Scala版本:val jsonData = sparkSession.read.json("src/main/resources/person.json")jsonData: org.apache.spark.sql.DataFrame = [email: string, iq: bigint ... 1 more field] 输出:display(jsonData) email iq name matei@databricks.com 180 Matei Zaharia rxin@databricks.com 80 Reynold Xin 3. 运行SQL查询SparkSession 可以在数据上执行SQL查询，结果以 DataFrame 形式返回（即DataSet[Row]）。display(spark.sql(&quot;select * from person&quot;)) email iq name matei@databricks.com 180 Matei Zaharia rxin@databricks.com 80 Reynold Xin 4. 使用配置选项SparkSession 还可以用来设置运行时配置选项，这些选项可以触发性能优化或I/O（即Hadoop）行为。spark.conf.set(&quot;spark.some.config&quot;, &quot;abcd&quot;)res12: org.apache.spark.sql.RuntimeConfig = org.apache.spark.sql.RuntimeConfig@55d93752spark.conf.get(&quot;spark.some.config&quot;)res13: String = abcd 配置选项也可以在 SQL 中使用变量替换：%sql select &quot;$&#123;spark.some.config&#125;&quot;abcd 5. 直接使用元数据SparkSession还包含一个 catalog 方法，该方法包含操作 Metastore（即数据目录）的方法。这些方法以 Datasets 形式返回结果，所以你可以在它们上面使用相同的 Datasets API。// To get a list of tables in the current databaseval tables = spark.catalog.listTables()tables: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table] = [name: string, database: string ... 3 more fields] 输出:display(tables) name database description tableType isTemporary person default null MANAGED false smart default null MANAGED false // Use the Dataset API to filter on namesdisplay(tables.filter(_.name contains &quot;son&quot;)) name database description tableType isTemporary person default null MANAGED false // Get the list of columns for a tabledisplay(spark.catalog.listColumns(&quot;smart&quot;)) name description dataType nullable isPartition isBucket email null string true false false iq null bigint true false false name null string true false false 6. 访问底层的SparkContextSparkSession.sparkContext 返回底层的 SparkContext，用于创建 RDD 以及管理集群资源。spark.sparkContextres17: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2debe9ac 原文：https://docs.databricks.com/_static/notebooks/sparksession.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 在Spark2.0中如何使用SparkSession]]></title>
    <url>%2Fspark-sql-how-to-use-sparksession-in-spark-2-0.html</url>
    <content type="text"><![CDATA[除了有时限的交互之外，SparkSession 提供了一个单一的入口来与底层的 Spark 功能进行交互，并允许使用 DataFrame 和 Dataset API 对 Spark 进行编程。最重要的是，它减少了开发人员在与 Spark 进行交互时必须了解和构造概念的数量。 在这篇文章中我们将探讨 Spark 2.0 中的 SparkSession 的功能。 1. 探索SparkSession的统一功能首先，我们将检查 Spark 应用程序 SparkSessionZipsExample，该应用程序从 JSON 文件读取邮政编码，并使用 DataFrame API 执行一些分析，然后运行 Spark SQL 查询，而无需访问 SparkContext，SQLContext 或 HiveContext。 1.1 创建SparkSession在Spark2.0版本之前，必须创建 SparkConf 和 SparkContext 来与 Spark 进行交互，如下所示：//set up the spark configuration and create contextsval sparkConf = new SparkConf().setAppName("SparkSessionZipsExample").setMaster("local")// your handle to SparkContext to access other context like SQLContextval sc = new SparkContext(sparkConf).set("spark.some.config.option", "some-value")val sqlContext = new org.apache.spark.sql.SQLContext(sc) 而在 Spark 2.0 中，通过 SparkSession 可以实现相同的效果，而不用显式创建 SparkConf，SparkContext或 SQLContext，因为它们都被封装在 SparkSession 中。使用建造者模式，实例化 SparkSession 对象（如果不存在的话）以及相关的基础上下文。// Create a SparkSession. No need to create SparkContext// You automatically get it as part of the SparkSessionval warehouseLocation = "file:$&#123;system:user.dir&#125;/spark-warehouse"val spark = SparkSession .builder() .appName("SparkSessionZipsExample") .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .getOrCreate() 到这个时候，你可以在 Spark 作业期间通过 spark 这个变量（作为实例对象）访问其公共方法和实例。 1.2 配置Spark的运行时属性一旦 SparkSession 被实例化，你就可以配置 Spark 的运行时配置属性。例如，在下面这段代码中，我们可以改变已经存在的运行时配置选项。configMap 是一个集合，你可以使用 Scala 的 iterable 方法来访问数据。//set new runtime optionsspark.conf.set("spark.sql.shuffle.partitions", 6)spark.conf.set("spark.executor.memory", "2g")//get all settingsval configMap:Map[String, String] = spark.conf.getAll() 1.3 访问Catalog元数据通常，你可能需要访问和浏览底层的目录元数据。SparkSession 将 catalog 作为一个公开的公共实例，该实例包含可以操作该元数据的方法。这些方法以 DataSets 形式返回，因此可以使用 DataSets API 访问或查看数据。在下面代码中，我们访问所有的表和数据库。//fetch metadata data from the catalogspark.catalog.listDatabases.show(false)spark.catalog.listTables.show(false) 1.4 创建DataSets和DataFrame使用 SparkSession API 创建 DataSets 和 DataFrame 方法有许多。 快速生成 DataSets 的一种方法是使用 spark.range 方法。在学习如何操作 DataSets API 时，这种方法非常有用。//create a Dataset using spark.range starting from 5 to 100, with increments of 5val numDS = spark.range(5, 100, 5)// reverse the order and display first 5 itemsnumDS.orderBy(desc("id")).show(5)//compute descriptive stats and display themnumDs.describe().show()// create a DataFrame using spark.createDataFrame from a List or Seqval langPercentDF = spark.createDataFrame(List(("Scala", 35), ("Python", 30), ("R", 15), ("Java", 20)))//rename the columnsval lpDF = langPercentDF.withColumnRenamed("_1", "language").withColumnRenamed("_2", "percent")//order the DataFrame in descending order of percentagelpDF.orderBy(desc("percent")).show(false) 1.5 使用SparkSession API读取JSON数据和任何Scala对象一样，你可以使用 spark，SparkSession 对象来访问其公共方法和实例字段。我可以读取 JSON 或 CVS 或 TXT 文件，或者我可以读取 parquet 表。例如，在下面这段代码中，我们将读取一个邮政编码的 JSON 文件，该文件返回一个 DataFrame，Rows的集合。// read the json file and create the dataframeval jsonFile = args(0)val zipsDF = spark.read.json(jsonFile)//filter all cities whose population &gt; 40KzipsDF.filter(zipsDF.col("pop") &gt; 40000).show(10) 1.6 在SparkSession中使用Spark SQL通过 SparkSession，你可以像通过 SQLContext 一样访问所有 Spark SQL 功能。在下面的代码示例中，我们创建了一个表，并在其上运行 SQL 查询。// Now create an SQL table and issue SQL queries against it without// using the sqlContext but through the SparkSession object.// Creates a temporary view of the DataFramezipsDF.createOrReplaceTempView("zips_table")zipsDF.cache()val resultsDF = spark.sql("SELECT city, pop, state, zip FROM zips_table")resultsDF.show(10) 1.7 使用SparkSession保存和读取Hive表接下来，我们将创建一个 Hive 表，并使用 SparkSession 对象对其进行查询，就像使用 HiveContext 一样。//drop the table if exists to get around existing table errorspark.sql("DROP TABLE IF EXISTS zips_hive_table")//save as a hive tablespark.table("zips_table").write.saveAsTable("zips_hive_table")//make a similar query against the hive tableval resultsHiveDF = spark.sql("SELECT city, pop, state, zip FROM zips_hive_table WHERE pop &gt; 40000")resultsHiveDF.show(10) 正如你所看到的，输出中的结果通过使用 DataFrame API，Spark SQL和Hive查询运行完全相同。其次，让我们把注意力转向 SparkSession 自动为你创建的两个Spark开发人员环境。 2. SparkSession封装SparkContext最后，对于历史上下文，让我们简单了解一下 SparkContext 的底层功能。 如图所示，SparkContext 是一个访问 Spark 所有功能的入口；每个 JVM 仅存在一个 SparkContext。Spark Driver 使用它连接到集群管理器进行通信，提交 Spark 作业并知道要与之通信的资源管理器（YARN，Mesos或Standalone）。它允许你配置 Spark 参数。通过 SparkContext，Driver 可以访问其他上下文，如SQLContext，HiveContext和 StreamingContext 来编程Spark。 但是，在 Spark 2.0，SparkSession 可以通过单一统一的入口访问前面提到的所有 Spark 功能。除了使访问 DataFrame 和 Dataset API 更简单外，它还包含底层的上下文以操作数据。 以前通过 SparkContext，SQLContext 或 HiveContext 在早期版本的 Spark 中提供的所有功能现在均可通过 SparkSession 获得。从本质上讲，SparkSession 是一个统一的入口，用 Spark 处理数据，最大限度地减少要记住或构建的概念数量。因此，如果你使用更少的编程结构，你更可能犯的错误更少，并且你的代码可能不那么混乱。 原文：https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 正则序列化器RegexSerDe]]></title>
    <url>%2Fhive-base-how-to-use-regexserde.html</url>
    <content type="text"><![CDATA[RegexSerDe 可以从 Hive 两个jar文件的类中获取，hive-serde-&lt;version&gt;.jar中的 org.apache.hadoop.hive.contrib.serde2.RegexSerDe 以及 hive-contrib-&lt;version&gt;.jar 中的 org.apache.hadoop.hive.serde2.RegexSerDe。 1. hive.serde2.RegexSerDe下面这种格式是 Apache 的打出的 Web 日志文件格式。包含我们想要获取的两个字段信息，一个是日志时间，一个是日志Json：[2018-06-04 00:00:09 INFO price:335] &#123;&quot;os&quot;:&quot;adr&quot;,&quot;phone&quot;:&quot;187xxxx3617&quot;, &quot;business&quot;:&quot;train&quot;, &quot;price&quot;:&quot;198&quot;&#125; 我们使用 RegexSerDe 类作为 SERDE 在正则表达式的帮助下处理上面日志：CREATE EXTERNAL TABLE IF NOT EXISTS adv_push_price ( time string, line string)PARTITIONED BY( dt string)ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'WITH SERDEPROPERTIES( 'input.regex' = '\\[(\\d*-\\d*-\\d*\\s+\\d*:\\d*:\\d*)\\s+INFO.*\\]\\s+(.*)', 'input.regex.case.insensitive' = 'false', 'output.format.string' = '%1$s %2$s')LOCATION '/user/xiaosi/log/price'; 上面是一个外表，从 /user/xiaosi/log/price 路径下加载数据，并经正则表达式的处理，对应到 time 和 line 两个字段上，现在我们查看一下Hive表中的数据：hive&gt; select * from adv_push_price limit 10;OK2018-06-04 00:00:00 &#123;&quot;os&quot;:&quot;adr&quot;,&quot;phone&quot;:&quot;187xxxx3617&quot;, &quot;business&quot;:&quot;train&quot;, &quot;price&quot;:&quot;198&quot;&#125; 20180604 从上面输出中我们可以看到数据符合我们的预期。 2. hive.contrib.serde2.RegexSerDe我们也可以使用 org.apache.hadoop.hive.contrib.serde2.RegexSerDe：CREATE EXTERNAL TABLE IF NOT EXISTS adv_push_price ( time string, line string)PARTITIONED BY( dt string)ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'WITH SERDEPROPERTIES( 'input.regex' = '\\[(\\d*-\\d*-\\d*\\s+\\d*:\\d*:\\d*)\\s+INFO.*\\]\\s+(.*)', 'input.regex.case.insensitive' = 'false', 'output.format.string' = '%1$s %2$s')LOCATION '/user/xiaosi/log/price'; 在运行过程中我们遇到如下错误：Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.RegexSerDe not found at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:329) at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:364) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:106) ... 22 moreCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.RegexSerDe not found at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1626) at org.apache.hadoop.hive.ql.plan.PartitionDesc.getDeserializer(PartitionDesc.java:175) at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:295) ... 24 more 上面的意思很明确，我们找不到 org.apache.hadoop.hive.contrib.serde2.RegexSerDe 类， hive-contrib-&lt;version&gt;.jar 可以在 $HIVE_HOME/lib 文件夹中找到，但是仍需要添加到环境变量中，将 hive-contrib-&lt;version&gt;.jar 配置到 Hive 会话中。如下所示将 hive-contrib-&lt;version&gt;.jar 添加到 HIVE_AUX_JARS_PATH 环境变量，将此 jar 永久添加到Hive会话中。在 conf/hive-site.xml 添加如下配置：&lt;property&gt; &lt;name&gt;hive.aux.jars.path&lt;/name&gt; &lt;value&gt;file:///home/q/hive/hive-2.1.0/lib/hive-contrib-2.1.0.jar&lt;/value&gt;&lt;/property&gt; org.apache.hadoop.hive.serde2.RegexSerDe 对应的 hive-serde-&lt;version&gt;.jar 默认包含在 hive 执行路径中，而 org.apache.hadoop.hive.contrib.serde2.RegexSerDe 对应的 hive-serde-&lt;version&gt;.jar 却不包含在 hive 执行路径中。 如果表中和数据中定义的列数不匹配，那么我们会遇到下面的错误消息:Diagnostic Messages for this Task:Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable 64.242.88.10 - - [07/Mar/2014:16:05:49 -0800] &quot;GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1&quot; 401 12846 at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable 64.242.88.10 - - [07/Mar/2014:16:05:49 -0800] &quot;GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1&quot; 401 12846 at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:501) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176) ... 8 moreCaused by: org.apache.hadoop.hive.serde2.SerDeException: Number of matching groups doesn&apos;t match the number of columns at org.apache.hadoop.hive.serde2.RegexSerDe.deserialize(RegexSerDe.java:180) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:136) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:100) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:492) ... 9 moreFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask 查看Hive表中声明的列数及其数据类型，以及正则表达式及其输出中的字段.format.string应包含相同数量的列。 参考资料： http://hadooptutorial.info/processing-logs-in-hive/ https://blog.csdn.net/s530723542/article/details/38437257 https://www.cnblogs.com/java20130722/archive/2013/06/09/3206794.html]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 如何使用累加器Accumulator]]></title>
    <url>%2Fspark-base-how-to-use-accumulator.html</url>
    <content type="text"><![CDATA[Accumulator 是 spark 提供的累加器，累加器可以用来实现计数器（如在 MapReduce 中）或者求和。Spark 本身支持数字类型的累加器，程序员可以添加对新类型的支持。 1. 内置累加器在 Spark2.0.0 版本之前，我们可以通过调用 SparkContext.intAccumulator() 或 SparkContext.doubleAccumulator() 来创建一个 Int 或 Double 类型的累加器：Accumulator&lt;Double&gt; doubleAccumulator = sparkContext.doubleAccumulator(0.0, "Double Accumulator");Accumulator&lt;Integer&gt; intAccumulator = sparkContext.intAccumulator(0, "Int Accumulator");Accumulator&lt;Double&gt; doubleAccumulator2 = sparkContext.accumulator(0.0, "Double Accumulator 2");Accumulator&lt;Integer&gt; intAccumulator2 = sparkContext.accumulator(0, "Int Accumulator 2");java 在 Spark2.0.0 之后的版本中，之前的的 Accumulator 已被废除，用 AccumulatorV2 代替:@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)class Accumulator[T] private[spark] ( // SI-8813: This must explicitly be a private val, or else scala 2.11 doesn&apos;t compile @transient private val initialValue: T, param: AccumulatorParam[T], name: Option[String] = None, countFailedValues: Boolean = false) extends Accumulable[T, T](initialValue, param, name, countFailedValues)// Int@deprecated(&quot;use sc().longAccumulator(String)&quot;, &quot;2.0.0&quot;)def intAccumulator(initialValue: Int, name: String): Accumulator[java.lang.Integer] = sc.accumulator(initialValue, name)(IntAccumulatorParam) .asInstanceOf[Accumulator[java.lang.Integer]]@deprecated(&quot;use sc().longAccumulator(String)&quot;, &quot;2.0.0&quot;)def accumulator(initialValue: Int, name: String): Accumulator[java.lang.Integer] = intAccumulator(initialValue, name)// Double@deprecated(&quot;use sc().doubleAccumulator(String)&quot;, &quot;2.0.0&quot;)def doubleAccumulator(initialValue: Double, name: String): Accumulator[java.lang.Double] = sc.accumulator(initialValue, name)(DoubleAccumulatorParam) .asInstanceOf[Accumulator[java.lang.Double]]@deprecated(&quot;use sc().doubleAccumulator(String)&quot;, &quot;2.0.0&quot;)def accumulator(initialValue: Double, name: String): Accumulator[java.lang.Double] = doubleAccumulator(initialValue, name) 我们可以通过调用 sparkContext.sc().longAccumulator() 或 sparkContext.sc().doubleAccumulator() 来创建一个 Long 或 Double 类型的累加器：DoubleAccumulator doubleAccumulator = sparkContext.sc().doubleAccumulator("Double Accumulator");LongAccumulator longAccumulator = sparkContext.sc().longAccumulator("Long Accumulator"); 看一下这两个方法具体的实现：/** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */def longAccumulator: LongAccumulator = &#123; val acc = new LongAccumulator register(acc) acc&#125;/** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */def doubleAccumulator: DoubleAccumulator = &#123; val acc = new DoubleAccumulator register(acc) acc&#125; 通过源码我们知道分别通过创建 LongAccumulator 和 DoubleAccumulator 对象，然后进行注册来创建一个累加器。所以我们也可以使用如下方式创建一个Long类型的累加器：LongAccumulator longAccumulator = new LongAccumulator();sparkContext.sc().register(longAccumulator, "Long Accumulator"); LongAccumulator DoubleAccumulator 都继承自 AccumulatorV2 Spark内置了数值型累加器(例如，Long，Double类型)，我们还可以通过继承 AccumulatorV2 来创建我们自己类型的累加器。 2. 自定义累加器自定义累加器类型的功能在 1.x 版本中就已经提供了，但是使用起来比较麻烦，在 Spark 2.0.0 版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2 来提供更加友好的自定义类型累加器的实现方式。官方同时给出了一个实现的示例：CollectionAccumulator，这个类允许以集合的形式收集 Spark 应用执行过程中的一些信息。例如，我们可以用这个类收集 Spark 处理数据过程中的非法数据或者引起异常的异常数据，这对我们处理异常时很有帮助。当然，由于累加器的值最终要汇聚到 Driver 端，为了避免 Driver 端的出现 OOM，需要收集的数据规模不宜过大。 实现自定义类型累加器需要继承 AccumulatorV2 并覆盖下面几个方法： reset 将累加器重置为零 add 将另一个值添加到累加器中 merge 将另一个相同类型的累加器合并到该累加器中。 下面这个累加器可以用于在程序运行过程中收集一些异常或者非法数据，最终以 List[String] 的形式返回：package com.sjf.open.spark;import com.google.common.collect.Lists;import org.apache.spark.util.AccumulatorV2;import java.util.ArrayList;import java.util.List;/** * 自定义累加器 CollectionAccumulator * @author sjf0115 * @Date Created in 下午2:11 18-6-4 */public class CollectionAccumulator&lt;T&gt; extends AccumulatorV2&lt;T, List&lt;T&gt;&gt; &#123; private List&lt;T&gt; list = Lists.newArrayList(); @Override public boolean isZero() &#123; return list.isEmpty(); &#125; @Override public AccumulatorV2&lt;T, List&lt;T&gt;&gt; copy() &#123; CollectionAccumulator&lt;T&gt; accumulator = new CollectionAccumulator&lt;&gt;(); synchronized (accumulator) &#123; accumulator.list.addAll(list); &#125; return accumulator; &#125; @Override public void reset() &#123; list.clear(); &#125; @Override public void add(T v) &#123; list.add(v); &#125; @Override public void merge(AccumulatorV2&lt;T, List&lt;T&gt;&gt; other) &#123; if(other instanceof CollectionAccumulator)&#123; list.addAll(((CollectionAccumulator) other).list); &#125; else &#123; throw new UnsupportedOperationException("Cannot merge " + this.getClass().getName() + " with " + other.getClass().getName()); &#125; &#125; @Override public List&lt;T&gt; value() &#123; return new ArrayList&lt;&gt;(list); &#125;&#125; 下面我们在数据处理过程中收集非法坐标为例，来看一下我们自定义的累加器如何使用:package com.sjf.open.spark;import com.google.common.collect.Lists;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.VoidFunction;import java.io.Serializable;import java.util.List;/** * 自定义累加器示例 * @author sjf0115 * @Date Created in 下午2:11 18-6-4 */public class CustomAccumulatorExample implements Serializable&#123; public static void main(String[] args) &#123; String appName = "CustomAccumulatorExample"; SparkConf conf = new SparkConf().setAppName(appName); JavaSparkContext sparkContext = new JavaSparkContext(conf); List&lt;String&gt; list = Lists.newArrayList(); list.add("27.34832,111.32135"); list.add("34.88478,185.17841"); list.add("39.92378,119.50802"); list.add("94,119.50802"); CollectionAccumulator&lt;String&gt; collectionAccumulator = new CollectionAccumulator&lt;&gt;(); sparkContext.sc().register(collectionAccumulator, "Illegal Coordinates"); // 原始坐标 JavaRDD&lt;String&gt; sourceRDD = sparkContext.parallelize(list); // 过滤非法坐标 JavaRDD&lt;String&gt; resultRDD = sourceRDD.filter(new Function&lt;String, Boolean&gt;() &#123; @Override public Boolean call(String str) throws Exception &#123; String[] coordinate = str.split(","); double lat = Double.parseDouble(coordinate[0]); double lon = Double.parseDouble(coordinate[1]); if(Math.abs(lat) &gt; 90 || Math.abs(lon) &gt; 180)&#123; collectionAccumulator.add(str); return true; &#125; return false; &#125; &#125;); // 输出 resultRDD.foreach(new VoidFunction&lt;String&gt;() &#123; @Override public void call(String coordinate) throws Exception &#123; System.out.println("[Data]" + coordinate); &#125; &#125;); // 查看异常坐标 for (String coordinate : collectionAccumulator.value()) &#123; System.out.println("[Illegal]: " + coordinate); &#125; &#125;&#125; 结果输出:[Illegal]: 94,119.50802[Illegal]: 34.88478,185.17841 3. 累加器注意事项累加器不会改变 Spark 的懒加载（Lazy）的执行模型。如果在 RDD 上的某个操作中更新累加器，那么其值只会在 RDD 执行 action 计算时被更新一次。因此，在 transformation （例如， map()）中更新累加器时，其值并不能保证一定被更新。 Spark 中的一系列 transformation 操作会构成一个任务链，需要通过 action 操作来触发。累加器也是一样的，也只能通过 action 触发更新，所以在 action 操作之前调用 value 方法查看其数值是没有任何变化的。对于在 action 中更新的累加器，Spark 会保证每个任务对累加器只更新一次，即使重新启动的任务也不会重新更新该值。而如果在 transformation 中更新的累加器，如果任务或作业 stage 被重新执行，那么其对累加器的更新可能会执行多次。 package com.sjf.open.spark;import com.google.common.collect.Lists;import org.apache.spark.Accumulator;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.util.CollectionAccumulator;import org.apache.spark.util.DoubleAccumulator;import org.apache.spark.util.LongAccumulator;import java.io.Serializable;import java.util.List;/** * 累加器陷阱 * @author sjf0115 * @Date Created in 下午2:11 18-6-4 */public class AccumulatorTrap implements Serializable&#123; public static void main(String[] args) &#123; String appName = "AccumulatorTrap"; SparkConf conf = new SparkConf().setAppName(appName); JavaSparkContext sparkContext = new JavaSparkContext(conf); LongAccumulator evenAccumulator = sparkContext.sc().longAccumulator("Even Num Accumulator"); LongAccumulator oddAccumulator = sparkContext.sc().longAccumulator("Odd Num Accumulator"); /*LongAccumulator evenAccumulator = new LongAccumulator(); LongAccumulator oddAccumulator = new LongAccumulator(); sparkContext.sc().register(evenAccumulator, "Even Num Accumulator"); sparkContext.sc().register(oddAccumulator, "Odd Num Accumulator");*/ List&lt;Integer&gt; numList = Lists.newArrayList(); for(int i = 0;i &lt; 10;i++)&#123; numList.add(i); &#125; JavaRDD&lt;Integer&gt; numRDD = sparkContext.parallelize(numList); // transform JavaRDD&lt;Integer&gt; resultRDD = numRDD.map(new Function&lt;Integer, Integer&gt;() &#123; @Override public Integer call(Integer num) throws Exception &#123; if (num % 2 == 0) &#123; evenAccumulator.add(1L); return 0; &#125; else &#123; oddAccumulator.add(1L); return 1; &#125; &#125; &#125;); // the first action resultRDD.count(); System.out.println("Odd Num Count : " + oddAccumulator.value()); System.out.println("Even Num Count : " + evenAccumulator.value()); // the second action resultRDD.foreach(new VoidFunction&lt;Integer&gt;() &#123; @Override public void call(Integer num) throws Exception &#123; System.out.println(num); &#125; &#125;); System.out.println("Odd Num Count : " + oddAccumulator.value()); System.out.println("Even Num Count : " + evenAccumulator.value()); &#125;&#125; 在第一个 action 算子 count 执行之后，累加器输出符合我们预期的结果：Odd Num Count : 5Even Num Count : 5 在第二个 action 算子 foreach 执行之后，累加器输出结果如下：Odd Num Count : 10Even Num Count : 10 其实这个时候又执行了一次 map 操作，所以累加器各自又增加了5，最终获得的结果变成了10。 看了上面的分析以及输出结果，我们知道，那就是使用累加器的过程中只能使用一次 action 操作才能保证结果的准确性。事实上，这种情况是可以解决的，只要将任务之间的依赖关系切断就可以。我们可以调用 cache，persist 等方法将之前的依赖切断，后续的累加器就不会受之前的 transfrom 操作影响了：Odd Num Count : 5Even Num Count : 5Odd Num Count : 5Even Num Count : 5 所以在使用累加器时，为了保证准确性，最好只使用一次 action 操作。如果需要使用多次，可以使用 cache 或 persist 操作切断依赖。 参考： http://smartsi.club/2018/04/10/spark-base-shared-variables/ https://blog.csdn.net/lsshlsw/article/details/50979579]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 如何使用DataSets]]></title>
    <url>%2Fspark-sql-how-to-use-datasets-in-spark.html</url>
    <content type="text"><![CDATA[开发人员一直非常喜欢Apache Spark，它提供简单但功能强大的API，这些特性的组合使得用最少的代码就可以进行复杂的分析。我们通过引入 DataFrames 和 Spark SQL 继续推动 Spark 的可用性和性能。这些是用于处理结构化数据（例如数据库表，JSON文件）的高级API，这些 API 可让 Spark 自动优化存储和计算。在这些 API 背后，Catalyst 优化器和 Tungsten 执行引擎用 Spark 面向对象（RDD）API无法实现的方式优化应用程序，例如以原始二进制形式对数据进行操作。 Spark Datasets 是 DataFrame API 的扩展，提供了一个类型安全的，面向对象的编程接口。Spark 1.6 包含 DataSets 的API预览版，它们将成为下几个 Spark 版本的开发重点。与 DataFrame 一样，DataSets 通过将表达式和数据字段公开给查询计划器(query planner)来充分利用 Spark 的 Catalyst 优化器。DataSets 还充分利用了 Tungsten 的快速内存编码。DataSets 继承了编译时类型安全性的好处 - 这意味着线上应用程序可以在运行之前检查错误。它们还允许直接对用户自定义的类操作。 从长远来看，我们期望 DataSets 成为编写更高效 Spark 应用程序的强大方式。DataSets 可以与现有的 RDD API 一起使用，但是当数据可以用结构化的形式表示时，可以提高效率。Spark 1.6 首次提出了 Datasets，我们期望在未来的版本中改进它们。 1. 使用DatasetsDatasets 是一种强类型，不可变的可以映射到关系性 schema 的对象集合。Datasets API 的核心是一个称为 Encoder 的新概念，它负责在 JVM 对象和表格表示(tabular representation)之间进行转换。表格表示使用 Spark 的内部 Tungsten 二进制格式存储，允许对序列化数据进行操作并提高内存利用率。Spark 1.6 支持自动生成各种类型的 Encoder，包括原始类型（例如String，Integer，Long），Scala Case 类和Java Beans。 使用 RDD 的用户会发现 Dataset API 非常熟悉，因为它提供了许多相同的功能转换（例如map，flatMap，filter）。考虑下面的代码，该代码读取文本文件的行并将它们拆分为单词：# RDDval lines = sc.textFile("/wikipedia")val words = lines .flatMap(_.split(" ")) .filter(_ != "")# Datasetsval lines = sqlContext.read.text("/wikipedia").as[String]val words = lines .flatMap(_.split(" ")) .filter(_ != "") Spark2.0以上版本，sqlContext 可以使用 SparkSeesion 替换。具体细节请参阅Spark SparkSession:一个新的入口 这两种API都可以很容易地使用lambda函数表达转换操作。编译器和IDE懂得你正在使用的类型，并且可以在你构建数据管道时提供有用的提示和错误信息。 虽然这个高层次代码在语法上看起来类似，但使用 Datasets，你也可以访问完整关系执行引擎的所有功能。例如，如果你现在要执行聚合（例如计算每个词的出现次数），则可以简单有效地表达该操作，如下所示：# RDDsval counts = words .groupBy(_.toLowerCase) .map(w =&gt; (w._1, w._2.size))# Datasetsval counts = words .groupBy(_.toLowerCase) .count() 由于 Datasets 版本的 WordCount 可以充分利用内置的聚合计数，所以这种计算不仅可以用较少的代码表示，而且还可以更快地执行。正如你在下面的图表中看到的那样，Datasets 的实现比原始的 RDD 实现要快得多。相反，使用 RDD 获得相同的性能需要用户手动考虑如何以最佳并行化方式表达计算。 这个新的 Datasets API 的另一个好处是减少了内存使用量。由于 Spark 了解 Datasets 中数据的结构，因此可以在缓存 Datasets 时在内存中创建更优化的布局。在下面的例子中，我们对比使用 Datasets 和 RDD 来在内存中缓存几百万个字符串。在这两种情况下，缓存数据都可以显着提高后续查询的性能。但是，由于 Datasets Encoder 向 Spark 提供有关正在存储数据的更多信息，因此优化后缓存会减少 4.5x 的空间。 2. 使用Encoder进行快速序列化Encoder 经过高度优化，并使用运行时代码生成来构建用于序列化和反序列化的自定义字节码(use runtime code generation to build custom bytecode for serialization and deserialization)。因此，它们可以比 Java 或 Kryo 序列化更快地运行。 除了速度之外，由此产生的编码数据的序列化大小也明显更小（高达2倍），从而降低了网络传输的成本。此外，序列化的数据已经是 Tungsten 二进制格式，这意味着许多操作可以在原地完成，而不需要物化一个对象。Spark内置支持自动生成原始类型（如String，Integer，Long），Scala Case 类和 Java Beans 的 Encoder。 3. 无缝支持半结构化数据Encoder 的功能不仅仅在性能方面。它们还可以作为半结构化格式（例如JSON）和类型安全语言（如Java和Scala）之间的桥梁。例如，考虑以下有关大学的数据集：&#123;&quot;name&quot;: &quot;UC Berkeley&quot;, &quot;yearFounded&quot;: 1868, numStudents: 37581&#125;&#123;&quot;name&quot;: &quot;MIT&quot;, &quot;yearFounded&quot;: 1860, numStudents: 11318&#125;… 你可以简单地定义一个具有预期结构的类并将输入数据映射到它，而不是手动提取字段并将其转换为所需类型。列按名称自动排列，并保留类型。case class University(name: String, numStudents: Long, yearFounded: Long)val schools = sqlContext.read.json("/schools.json").as[University]schools.map(s =&gt; s"$&#123;s.name&#125; is $&#123;2015 – s.yearFounded&#125; years old") Encoder 检查你的数据与预期的模式是否匹配，在尝试错误地处理TB大小数据之前提供有用的错误消息。例如，如果我们尝试使用太小的数据类型，例如转换为对象会导致截断（即numStudents大于一个字节，最大值为255），分析器将发出AnalysisException。case class University(numStudents: Byte)val schools = sqlContext.read.json("/schools.json").as[University]org.apache.spark.sql.AnalysisException: Cannot upcast yearFounded from bigint to smallint as it may truncate 执行映射时，Encoder 自动处理复杂类型，包括嵌套类，数组和 map。 4. Java和Scala统一APIDataSets API 的另一个目标是提供可在 Scala 和 Java 中使用的统一接口。这种统一对于 Java 用户来说是个好消息，因为它确保了他们的API不会落后于 Scala 接口，代码示例可以很容易地在两种语言中使用，而库不再需要处理两种稍微不同的输入类型。Java 用户唯一的区别是他们需要指定要使用的 Encoder，因为编译器不提供类型信息。 例如，如果想要使用Java处理json数据，你可以这样做：public class University implements Serializable &#123; private String name; private long numStudents; private long yearFounded; public void setName(String name) &#123;...&#125; public String getName() &#123;...&#125; public void setNumStudents(long numStudents) &#123;...&#125; public long getNumStudents() &#123;...&#125; public void setYearFounded(long yearFounded) &#123;...&#125; public long getYearFounded() &#123;...&#125;&#125;class BuildString implements MapFunction &#123; public String call(University u) throws Exception &#123; return u.getName() + " is " + (2015 - u.getYearFounded()) + " years old."; &#125;&#125;Dataset schools = context.read().json("/schools.json").as(Encoders.bean(University.class));Dataset strings = schools.map(new BuildString(), Encoders.STRING()); 原文：https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 利用ToolRunner运行MapReduce]]></title>
    <url>%2Fhadoop-base-implementing-the-tool-interface-for-mapreduce.html</url>
    <content type="text"><![CDATA[大多数人通常使用通过静态 main 方法执行驱动程序代码创建他们的 MapReduce 作业。这种实现的缺点是大多数特定的配置（如果有的话）通常都是硬编码的(例如：设置Reducer的个数)。如果需要随时修改一些配置属性（例如：修改Reducer数量），就必须修改代码，然后重新构建你的jar文件并重新部署应用程序。这种方式很浪费时间。这可以通过在 MapReduce 驱动程序代码中实现 Tool 接口来避免。 1. Hadoop配置通过实现 Tool 接口并扩展 Configured 类，你可以通过 GenericOptionsParser 轻松的在命令行界面设置 hadoop 配置对象。这使得你的代码更加具有可移植性（并且更加简洁），因为你不需要再对任何特定配置进行硬编码。 让我们举几个例子，使用和不使用Tool接口。 1.1 不使用 Tool 接口package com.sjf.open.example;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import java.io.IOException;/** * WordCount示例 * @author sjf0115 * @Date Created in 上午10:01 18-6-1 */public class WordCountNoTool&#123; public int run(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("./xxxx &lt;input&gt; &lt;output&gt; but got " + args.length + ":"); for(String argument : args)&#123; System.out.println("Output: " + argument); &#125; System.exit(1); &#125; String inputPaths = args[0]; String outputPath = args[1]; Configuration conf = new Configuration(); conf.set("mapred.job.queue.name", "xxx"); Job job = Job.getInstance(conf); job.setJobName("word_count_example"); job.setJarByClass(WordCountNoTool.class); // mapper job.setMapperClass(mapper.class); // reducer job.setReducerClass(reducer.class); job.setNumReduceTasks(2); // input FileInputFormat.setInputPaths(job, inputPaths); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // output FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); boolean success = job.waitForCompletion(true); return success ? 0 : 1; &#125; public static class mapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] words = line.split("\\s+"); Text text = new Text(); IntWritable intWritable = new IntWritable(); for(String word : words)&#123; text.set(word); intWritable.set(1); // &lt;word, 1&gt; context.write(text, intWritable); &#125; &#125; &#125; public static class reducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for(IntWritable intWritable : values)&#123; count += intWritable.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; public static void main(String[] args) throws Exception &#123; WordCountNoTool wordCountNoTool = new WordCountNoTool(); int result = wordCountNoTool.run(args); System.exit(result); &#125;&#125; 如下方式执行 MapReduce 作业。你期望在这里只有2个参数 inputPath 和 outputPath，可以通过 main方法String数组上的索引[0]和[1]获取：hadoop jar common-tool-jar-with-dependencies.jar com.sjf.open.example.WordCountNoTool $&#123;inputPath&#125; $&#123;outputPath&#125; 在这种情况下，reducer 的个数硬编码在代码中（job.setNumReduceTasks(2)），因此无法根据需要进行修改。 1.2 使用 Tool 接口package com.sjf.open.example;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import java.io.IOException;/** * WordCount示例 * @author sjf0115 * @Date Created in 上午10:01 18-6-1 */public class WordCountWithTool extends Configured implements Tool &#123; @Override public int run(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("./xxxx &lt;input&gt; &lt;output&gt;"); System.exit(1); &#125; String inputPaths = args[0]; String outputPath = args[1]; Configuration conf = this.getConf(); conf.set("mapred.job.queue.name", "xxxx"); Job job = Job.getInstance(conf); job.setJobName("word_count_example"); job.setJarByClass(WordCountWithTool.class); // mapper job.setMapperClass(mapper.class); // reducer job.setReducerClass(reducer.class); // input FileInputFormat.setInputPaths(job, inputPaths); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // output FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); boolean success = job.waitForCompletion(true); return success ? 0 : 1; &#125; public static class mapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] words = line.split("\\s+"); Text text = new Text(); IntWritable intWritable = new IntWritable(); for(String word : words)&#123; text.set(word); intWritable.set(1); // &lt;word, 1&gt; context.write(text, intWritable); &#125; &#125; &#125; public static class reducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for(IntWritable intWritable : values)&#123; count += intWritable.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; public static void main(String[] args) throws Exception &#123; int result = ToolRunner.run(new Configuration(), new WordCountWithTool(), args); System.exit(result); &#125;&#125; ToolsRunner 通过其静态 run 方法执行 MapReduce 作业。在这个例子中，我们不需要对 reducer的个数进行硬编码，因为它可以直接可以在命令行中指定（使用-D选项）：hadoop jar common-tool-jar-with-dependencies.jar com.sjf.open.example.WordCountWithTool -D mapred.reduce.tasks=1 $&#123;inputPath&#125; $&#123;outputPath&#125; 请注意，你仍然需要提供 inputPath 和 outputPath 两个参数。GenericOptionParser 可以把通用 Tools 选项与实际作业的参数分开。无论你提供多少个通用选项，inputPath 和 outputPath 变量仍位于索引[0]和[1]处，但位于 run 方法String数组中（不是在 main 方法String数组中）。 如果不实现 Tool 接口运行 MapReduce 作业:hadoop jar common-tool-jar-with-dependencies.jar com.sjf.open.example.WordCountNoTool -D mapred.reduce.tasks=1 $&#123;inputPath&#125; $&#123;outputPath&#125; -D mapred.reduce.tasks=1 也会被 main 方法认为是一个参数，位于索引[0]处，inputPath 和 outputPath 则分别位于索引[1]和[2]处。 2. 支持通用选项可以从CLI提供一些其他有用的选项。-conf 指定应用程序配置文件-D 给指定属性设置值-fs 指定 namenode-files 指定复制到集群上的文件，以逗号分隔-libjars 指定包含在类路径中的jar文件，以逗号分隔 原文:https://hadoopi.wordpress.com/2013/06/05/hadoop-implementing-the-tool-interface-for-mapreduce-driver/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Task not serializable]]></title>
    <url>%2Fspark-base-task-not-serializable.html</url>
    <content type="text"><![CDATA[你可能会看到如下错误：org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: ... 当你在 Driver（master）上初始化变量，然后在其中一个 worker 上尝试使用它时，可能会触发上述错误。在这种情况下，Spark Streaming 会尝试序列化该对象以将其发送给 worker，如果对象不可序列化，就会失败。考虑下面的代码片段：NotSerializable notSerializable = new NotSerializable();JavaRDD&lt;String&gt; rdd = sc.textFile(&quot;/tmp/myfile&quot;);rdd.map(s -&gt; notSerializable.doSomething(s)).collect(); 这就会触发上述错误。这里有一些方法可以解决上述错误： 对该类进行序列化 仅在传递给 map 中 lambda 函数内声明实例。 将 NotSerializable 对象设置为静态，并在每台机器上创建一次。 调用 rdd.forEachPartition 并在其中创建 NotSerializable 对象，如下所示：rdd.forEachPartition(iter -&gt; &#123; NotSerializable notSerializable = new NotSerializable(); // ...Now process iter&#125;); 原文:https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/troubleshooting/javaionotserializableexception.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 在Yarn上运行Spark应用程序]]></title>
    <url>%2Fspark-running-spark-applications-on-yarn.html</url>
    <content type="text"><![CDATA[1. 部署模式在 YARN 中，每个应用程序实例都有一个 ApplicationMaster 进程，该进程是为该应用程序启动的第一个容器。应用程序负责从 ResourceManager 上请求资源。一旦分配了资源，应用程序将指示 NodeManagers 启动容器。ApplicationMasters 消除了对活跃客户端的依赖：启动应用程序的进程可以终止，并且从在集群上由 YARN 管理的进程继续协作运行。 有关指定部署模式的选项，请参阅spark-submit选项。 1.1 Cluster部署模式在 Cluster 模式下，Spark Driver 在集群主机上的 ApplicationMaster 上运行，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。 Cluster 模式不太适合使用 Spark 进行交互式操作。需要用户输入的 Spark 应用程序（如spark-shell和pyspark）需要 Spark Driver 在启动 Spark 应用程序的 Client 进程内运行。 1.2 Client部署模式在 Client 模式下，Spark Driver 在提交作业的主机上运行。ApplicationMaster 仅负责从 YARN 中请求 Executor 容器。在容器启动后，Client 与容器通信以调度工作。 模式 Client模式 Cluster模式 Driver在哪运行 Client ApplicationMaster 请求资源 ApplicationMaster ApplicationMaster 启动 executor 进程 YARN NodeManager YARN NodeManager 持久化服务 YARN ResourceManager 和 NodeManagers YARN ResourceManager 和 NodeManagers 是否支持Spark Shell Yes No 2. 在YARN上运行Spark Shell应用程序要在 YARN 上运行 spark-shell 或 pyspark 客户端，请在启动应用程序时使用 --master yarn --deploy-mode client 标志。 3. Example3.1 以Cluster模式运行以Cluster模式运行WordCount:spark-submit \--class com.sjf.example.batch.WordCount \--master yarn \--deploy-mode cluster \--driver-memory 10g \--executor-memory 12g \--num-executors 20 \--executor-cores 2 \$&#123;RUN_HOME&#125;/spark-example-jar-with-dependencies.jar \$&#123;input_path&#125; $&#123;output_path&#125; 该命令会打印状态，直到作业完成或按下 control-C。在 Cluster 模式下终止 spark-submit 进程不会像在 Client 模式下那样终止 Spark 应用程序。要监视正在运行的应用程序的状态，请运行 yarn application -list。 3.2 以Client模式运行spark-submit \--class com.sjf.example.batch.WordCount \--master yarn \--deploy-mode client \--driver-memory 10g \--executor-memory 12g \--num-executors 20 \--executor-cores 2 \$&#123;RUN_HOME&#125;/spark-example-jar-with-dependencies.jar \$&#123;input_path&#125; $&#123;output_path&#125; 原文：https://www.cloudera.com/documentation/enterprise/5-14-x/topics/cdh_ig_running_spark_on_yarn.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.5发布中的新功能]]></title>
    <url>%2Fapache-flink-1-5-0-release.html</url>
    <content type="text"><![CDATA[Flink 1.5.0 是 1.x.y 系列的第六个主要版本。与往常一样，它兼容之前 1.x.y 版本中使用 @Public 注解标注过的 API。 最新版本已经可以下载，开发者可以通过 Flink 邮件列表或 JIRA 进行反馈。以下将列出最新版本的主要特性和改进。 1. 流式处理进一步演化Flink 正在给流式处理领域带来另一次重大飞跃。流式处理不仅意味着更加快速的分析，更是一种构建快速连续数据处理管道的原则性方法。流式处理正在成为构建数据驱动型和数据密集型应用程序的典范——它将数据处理逻辑和应用程序及业务逻辑汇集在了一起。 新版本对底层的一些基础组件进行了改进，包括： 重新设计并实现了 Flink 的大部分处理模型（FLIP-6）。尽管此项工作尚未全部完工，但 Flink 1.5 已经可以支持更为顺畅的 Kubernetes 部署，并可以将与外部系统的通信（与外部服务代理的交互）切换到 HTTP/REST。同时，Flink 1.5 简化了在常见集群管理器（如 YARN、Mesos）上进行的部署，并提供动态资源分配功能。 流式广播状态（FLINK-4940）。可以将广播流（如上下文数据、机器学习模型、规则 / 模式、触发器等）与可能带有键控状态（KeyedState）的流（如特征向量、状态机等）连接在一起。而在 Flink 1.5 之前，很难做到这一点。 为了改善对实时应用程序的支持，Flink 团队对 Flink 的 网络栈（FLINK-7315）进行了重大改进。Flink 1.5 在保持高吞吐量的同时实现了更低的延迟。另外，新版本还改进了回压情况下检查点的稳定性。 流式 SQL 越来越被认为是一种简单而强大的方式，用于执行流式分析、构建数据管道、进行特征工程或基于变更数据增量更新应用程序状态。新版本 添加了用于流式 SQL 查询的 SQL CLI（FLIP-24），让流式 SQL 更易于使用。 2. 新特性和改进2.1 重写 Flink 部署和处理模型重写 Flink 部署和处理模型的工作已经进行了一年多，来自多个组织的贡献者（如 Artisans、阿里巴巴和 Dell EMC）合作设计和实现了该特性，这是 Flink 项目启动以来对核心组件做出的最重大的一项改进。 简单地说，这些改进增加了对 YARN 和 Mesos 调度器动态资源分配和动态资源释放的支持，以更好的利用资源、进行故障恢复和动态扩展。此外，新版本还简化了在容器管理基础设施（如 Kubernetes）上进行的部署，所有对 JobManager 的请求都通过 REST 发起，包括提交和取消作业、请求作业状态，获取保存点等。 此次改进也为 Flink 将来与 Kubernetes 更好的集成奠定了基础。在稍后的版本中，有可能在不先启动 Flink 集群的情况下，将作业塞进 Docker，并作为容器部署的一部分。此外，此次改进向支持应用程序的并行性自动调整卖出了一大步。 需要注意的是，这些改进对 Flink API 没有任何影响。 2.2 广播状态对广播状态的支持（即在某个函数的所有并行实例中复制状态）是一直广受开发者期待的特性。广播状态的典型应用场景包括两个流，一个是控制或配置流，负责管理规则、模式或其他配置消息，另一个是常规的数据流。常规数据流的处理是通过控制流的消息来配置的，规则或模式被广播到函数的所有并行实例中，并应用于常规流的所有事件上。 当然，广播状态也可以有保存点或进行保存点恢复，就像 Flink 的其他状态一样，也具有一次性（exactly once）状态一致性保证。此外，广播状态为实现 Flink CEP 库的“动态模式”特性带来了可能性。 2.3 Flink 网络栈的改进分布式流式应用程序的性能在很大程度上取决于通过网络连接传输事件的组件。在流式处理环境中，延迟和吞吐量是最为重要的两个性能指标。 Flink 1.5 从两个方面对 Flink 的网络栈进行了改进，即使用基于信用（credit based）的流量控制和改善传输延迟。基于信用的流量控制在最大程度上减少“线上”数据量，同时保持了高吞吐量。这显著减少了在回压情况下用于完成检查点的时间。此外，Flink 现在能够在不降低吞吐量的情况下实现更低的延迟。 2.4 任务本地状态恢复Flink 的检查点机制将应用程序状态的副本写入到远程的持久化存储中，并在发生故障时将其加载回去。这种机制确保应用程序在发生故障时不会丢失状态。不过如果真的发生故障，可能需要一段时间才能从远程存储中加载状态以恢复应用程序。 Flink 社区正在不断努力提高检查点和恢复效率。以前版本使用了异步和增量检查点，在新版本中，主要提高了故障恢复的效率。 任务本地状态恢复主要利用了这样的一个事实——作业的失败通常是由单个操作、任务管理器或机器失效引起的。在将操作状态写入远程存储时，Flink 也会在每台机器的本地磁盘上保留一份副本。在进行失效备援时，调度程序会尝试将任务重新分配给以前的机器，并从本地磁盘而不是远程存储加载状态，从而加快恢复速度。 2.5 扩展对 SQL 和 Table API 的 Join 支持在 1.5.0 版本中，Flink 增加对基于窗口的外连接的支持。如下查询允许对有限时间范围内的基于事件时间或处理时间的表进行连接。 对于不应该在有限时间间隔内连接两个流式表的情况，Flink SQL 提供了非窗口内部连接支持。这样可以实现完全匹配，而这在许多标准 SQL 语句中是很常见的。 2.6 SQL CLI 客户端几个月前，Flink 社区开始致力于添加一项服务，用于执行流和批处理 SQL 查询（FLIP-24）。新的 SQL CLI 客户端就是这项工作的第一个成果，并提供了一个 SQL shell 用于查询数据流。 3. 其他特性和改进 OpenStack 提供了用于在资源池上创建公共和私有云的软件。Flink 现在支持 OpenStack 的类 S3 文件系统 Swift，用于保存检查点和保存点。Swift 可以在没有 Hadoop 依赖的情况下使用。 改进从连接器读取或向连接器写入 JSON 消息。现在可以通过解析一个标准的 JSON 模式来配置序列化器和反序列化器。SQL CLI 客户端能够读取来自 Kafka 的 JSON 记录。 应用程序可以在无需手动触发保存点的情况下进行伸缩。实际上，Flink 仍然会保存一个保存点，然后停止应用程序并重新调整并行度。 改进了 watermark 和延迟的度量标准，Flink 现在捕获所有操作器（包括数据源在内）的最小化 watermark。此外，为了更好地与常用指标系统集成，延迟度量指标进行了重新设计。 FileInputFormat（和其他多种输入格式）现在支持从多个路径读取文件。 BucketingSink 支持自定义扩展规范。 CassandraOutputFormat 可用于发送 Row 对象。 Kinesis 消费者客户端允许更大程度的定制化。 欢迎关注我的公众号： 原文:Flink 1.5重磅发布：处理模型重构，延迟更低！]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Realease</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Spark {{JAVA_HOME}}找不到]]></title>
    <url>%2Fspark-base-java-home-no-such-file.html</url>
    <content type="text"><![CDATA[在 Yarn 上使用 Spark，以 cluster 模式运行：sudo -uxiaosi spark-submit \ --class com.sjf.example.sql.SparkHiveExample \ --master yarn \ --deploy-mode cluster \ --driver-memory 10g \ --executor-memory 12g \ --num-executors 20 \ --executor-cores 2 \ --queue xiaosi \ --conf spark.driver.extraJavaOptions=&quot;-XX:MaxPermSize=6144m -XX:PermSize=1024m&quot; \ $&#123;baseDir&#125;/$&#123;jarDir&#125; 出现了以下异常：Application application_1504162679223_27868973 failed 2 times due to AM Container for appattempt_1504162679223_27868973_000002 exited with exitCode: 127 due to: Exception from container-launch: org.apache.hadoop.util.Shell$ExitCodeException:org.apache.hadoop.util.Shell$ExitCodeException:at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)at org.apache.hadoop.util.Shell.run(Shell.java:379)at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:200)at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)at java.util.concurrent.FutureTask.run(FutureTask.java:266)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745).Failing this attempt.. Failing the application. 而且 ApplicationMaster 所在机器的日志里面有下面的信息提示：/bin/bash: &#123;&#123;JAVA_HOME&#125;&#125;/bin/java: No such file or directory 发现换一台机器提交作业就没有问题，怀疑是版本的问题，经过对比，原来是我编译Spark所使用的Hadoop版本和线上Hadoop版本不一致导致的，当前使用Hadoop版本是2.7，而线上是使用的2.2。后来使用线上Hadoop版本重新编译了Spark，这个问题就解决了。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 任务运行失败]]></title>
    <url>%2Fhadoop-base-task-failure.html</url>
    <content type="text"><![CDATA[1. 常见情况任务运行失败最常见的情况是 map 任务或 reduce 任务中的用户代码抛出运行异常。如果发生这种情况，任务 JVM 会在退出之前向其父 application master 发送错误报错。错误报告最后被记入用户日志中。application master 会将此次任务尝试标记为 failed (失败)，并释放容器以便资源可以为其他任务使用。 任务运行失败另一种常见情况是任务 JVM 突然退出，可能由于 JVM 软件缺陷而导致 MapReduce 用户代码由于特殊原因造成 JVM 退出。在这种情况下，节点管理器会注意到进程已经退出，并通知 application master 将此次任务尝试标记为失败。 一旦 application master 注意到已经有一段时间没有收到进度的更新，便会将任务标记为失败。在此之后，任务 JVM 进程将被自动杀死。任务被认为失败的超时时间间隔通常为10分钟，可以以作业为基础（或以集群为基础）进行设置，对应的属性为 mapreduce.task.timeout ，单位为毫秒。 超时设置为0，将关闭超时判定，所以长时间运行的任务永远不会被标记为失败。在这种情况下，被挂起的任务永远不会释放它的容器并随着时间的推移，最终降低整个集群的效率。因此，尽量避免这种设置。 2. 失败重试application master 被告知一个任务尝试失败后，将重新调度该任务的执行。application master 会试图避免在以前失败过的节点管理器上重新调度该任务。此外，如果一个任务失败过4次，将不会再重试，整个作业都会失败，如下表。 Attempt State Node attempt_1504162679223_24764734_r_000057_0 FAILED /default-rack/l-hp609.data.cn2:8042 attempt_1504162679223_24764734_r_000057_1 FAILED /default-rack/l-hp143.data.cn2:8042 attempt_1504162679223_24764734_r_000057_2 FAILED /default-rack/l-hp618.data.cn2:8042 attempt_1504162679223_24764734_r_000057_3 FAILED /default-rack/l-hp272.data.cn2:8042 上述作业在任务失败之后会在不同节点管理器上重新调度该任务，如果任务重试４次之后还是失败则整个作业会失败：18/05/21 00:24:52 INFO mapreduce.Job: Job job_1504162679223_24764734 failed with state FAILED due to: Task failed task_1504162679223_24764734_r_000057Job failed as tasks failed. failedMaps:0 failedReduces:1 上面的４次是可以设置的：对于 map 任务，运行任务的最多尝试次数由 mapreduce.map.maxattempts 属性控制；对于 reduce 任务，则由 mapreduce.reduce.maxattempts 属性控制。默认情况下，如果任何任务失败次数大于４（或最多尝试次数被配置为４），整个作业都会失败。 3. 任务失败容忍对于一些应用程序，我们不希望一旦有少数几个任务失败就终止运行整个作业，因为即使有任务失败，作业的一些结果可能还是可用的。在这种情况下，可以为作业设置在不触发作业的情况下任务失败的最大百分比。针对 map 任务和 reduce 任务的设置可以通过 mapreduce.map.failures.maxpercent 和 mapreduce.reduce.failures.maxpercent 两个属性来完成。 4. Killed任务任务尝试也是可以终止的（killed），这与失败不同。任务尝试可以被终止是因为它是一个推测执行任务或因为它所处的节点管理器失败，导致application master 将它上面运行的所有任务尝试标记为 killed 。被中止的任务尝试不会计入任务运行尝试次数（由 mapreduce.map.maxattempts 和 mapreduce.reduce.maxattempts 属性控制），因为尝试被中止并不是任务的过错。 用户也可以使用 Web UI 或命令行来中止或取消任务尝试。也可以采用相同的机制来中止作业。 来自:Hadoop权威指南]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Block 与 InputSplit 的区别与联系]]></title>
    <url>%2Fhadoop-base-block-and-inputsplit.html</url>
    <content type="text"><![CDATA[相信大家都知道，HDFS 将文件按照一定大小的块进行切割，（我们可以通过 dfs.blocksize 参数来设置 HDFS 块的大小，在 Hadoop 2.x 上，默认的块大小为 128MB。）也就是说，如果一个文件大小大于 128MB，那么这个文件会被切割成很多块，这些块分别存储在不同的机器上。当我们启动一个 MapReduce 作业去处理这些数据的时候，程序会计算出文件有多少个 Splits，然后根据 Splits 的个数来启动 Map 任务。那么 HDFS 块和 Splits 到底有什么关系？ 为了简便起见，下面介绍的文件为普通文本文件。 1. HDFS 块现在我有一个名为 iteblog.txt 的文件，如下：[iteblog@iteblog.com /home/iteblog]$ ll iteblog.txt-rw-r--r-- 1 iteblog iteblog 454669963 May 15 12:07 iteblog.txt 很明显，这个文件大于一个 HDFS 块大小，所有如果我们将这个文件存放到 HDFS 上会生成 4 个 HDFS 块，如下（注意下面的输出做了一些删除操作）：[iteblog@iteblog.com /home/iteblog]$ hadoop -put iteblog.txt /tmp[iteblog@iteblog.com /home/iteblog]$ hdfs fsck /tmp/iteblog.txt -files -blocks/tmp/iteblog.txt 454669963 bytes, 4 block(s): OK0. BP-1398136447-192.168.246.60-1386067202761:blk_8133964845_1106679622318 len=134217728 repl=31. BP-1398136447-192.168.246.60-1386067202761:blk_8133967228_1106679624701 len=134217728 repl=32. BP-1398136447-192.168.246.60-1386067202761:blk_8133969503_1106679626977 len=134217728 repl=33. BP-1398136447-192.168.246.60-1386067202761:blk_8133970122_1106679627596 len=52016779 repl=3 可以看出 iteblog.txt 文件被切成 4 个块了，前三个块大小正好是 128MB（134217728），剩下的数据存放到第 4 个 HDFS 块中。 如果文件里面有一行记录的偏移量为 134217710，长度为 100，HDFS 如何处理？ 答案是这行记录会被切割成两部分，一部分存放在 block 0 里面；剩下的部分存放在 block 1 里面。具体的，偏移量为134217710，长度为18的数据存放到 block 0 里面；偏移量134217729，长度为82的数据存放到 block 1 里面。 可以将这部分的逻辑以下面的图概括： 说明： 图中的红色块代表一个文件 中间的蓝色矩形块代表一个 HDFS 块，矩形里面的数字代表 HDFS 块的编号，读整个文件的时候是从编号为0的 HDFS 块开始读，然后依次是1,2,3… 最下面的一行矩形代表文件里面存储的内容，每个小矩形代表一行数据，里面的数字代表数据的编号。红色的竖线代表 HDFS 块边界(block boundary)。 从上图我们可以清晰地看出，当我们往 HDFS 写文件时，HDFS 会将文件切割成大小为 128MB 的块，切割的时候不会判断文件里面存储的到底是什么东西，所以逻辑上属于一行的数据会被切割成两部分，这两部分的数据被物理的存放在两个不同的 HDFS 块中，正如上图中的第5、10以及14行被切割成2部分了。 2. File Split现在我们需要使用 MapReduce 来读取上面的文件，由于是普通的文本文件，所以可以直接使用 TextInputFormat 来读取。下面是使用 TextInputFormat获取到的 FileSplit 信息：scala&gt; FileInputFormat.addInputPath(job,new Path(&quot;/tmp/iteblog.txt&quot;));scala&gt; val format = new TextInputFormat;scala&gt; val splits = format.getSplits(job)scala&gt; splits.foreach(println)hdfs://iteblogcluster/tmp/iteblog.txt:0+134217728hdfs://iteblogcluster/tmp/iteblog.txt:134217728+134217728hdfs://iteblogcluster/tmp/iteblog.txt:268435456+134217728hdfs://iteblogcluster/tmp/iteblog.txt:402653184+52016779 可以看出，每个 FileSplit 的起始偏移量和上面 HDFS 每个文件块一致。但是具体读数据的时候，MapReduce 是如何处理的呢？我们现在已经知道，在将文件存储在 HDFS 的时候，文件被切割成一个一个 HDFS Block，其中会导致一些逻辑上属于一行的数据会被切割成两部分，那 TextInputFormat 遇到这样的数据是如何处理的呢？ 对于这种情况，TextInputFormat 会做出如下两种操作： 在初始化 LineRecordReader 的时候，如果 FileSplit 的起始位置 start 不等于0， 说明这个 Block 块不是第一个 Block，这时候一律丢掉这个 Block 的第一行数据。 在读取每个 Block 的时候，都会额外地多读取一行，如果出现数据被切割到另外一个 Block 里面，这些数据能够被这个任务读取。 使用图形表示可以概括如下： 说明： 图中的红色虚线代表 HDFS 块边界(block boundary)； 蓝色的虚线代表Split 读数的边界。 从图中可以清晰地看出： 当程序读取 Block 0 的时候，虽然第五行数据被分割并被存储在 Block 0 和 Block 1 中，但是，当前程序能够完整的读取到第五行的完整数据。 当程序读取 Block 1 的时候，由于其 FileSplit 的起始位置 start 不等于0，这时候会丢掉第一行的数据，也就是说 Block 1 中的第五行部分数据会被丢弃，而直接从第六行数据读取。这样做的原因是，Block 1 中的第五行部分数据在程序读取前一个 Block 的时候已经被读取了，所以可以直接丢弃。 其他剩下的 Block 读取逻辑和这个一致。 3. 总结从上面的分析可以得出以下的总结 Split 和 HDFS Block 是一对多的关系； HDFS block 是数据的物理表示，而 Split 是 block 中数据的逻辑表示； 满足数据本地性的情况下，程序也会从远程节点上读取少量的数据，因为存在行被切割到不同的 Block 上。 原文：https://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=2650715088&amp;idx=1&amp;sn=90c6ca52fcb9ba9a1f79764095a8a635&amp;pass_ticket=S4Ar8TbFuzTw%2F%2BRLzzms8abC%2BhdeIHSX65rbkBbw7R3doqhY%2FQQLwo9QJ0V8arTy]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 抽样Sampling]]></title>
    <url>%2Fhive-base-how-to-use-sampling.html</url>
    <content type="text"><![CDATA[1. Block 抽样Block 抽样功能在 Hive 0.8 版本开始引入。具体参阅JIRA - Input Sampling By Splits block_sample: TABLESAMPLE (n PERCENT) 该语句允许至少抽取 n% 大小的数据（注意：不是行数，而是数据大小）做为输入，仅支持 CombineHiveInputFormat ，不能够处理一些特殊的压缩格式。如果抽样失败，MapReduce 作业的输入将是整个表或者是分区的数据。由于在 HDFS 块级别进行抽样，所以抽样粒度为块大小。例如如果块大小为256MB，即使 n% 的输入仅为100MB，那也会得到 256MB 的数据。 在下面例子中 0.1% 或更多的输入数据用于查询：SELECT *FROM source TABLESAMPLE(0.1 PERCENT) s; 如果希望在不同的块中抽取相同大小的数据，可以改变下面的参数：set hive.sample.seednumber=&lt;INTEGER&gt;; 或者可以指定要读取的总长度，但与 PERCENT 抽样具有相同的限制。（从Hive 0.10.0开始 - https://issues.apache.org/jira/browse/HIVE-3401）block_sample: TABLESAMPLE (ByteLengthLiteral)ByteLengthLiteral : (Digit)+ (&apos;b&apos; | &apos;B&apos; | &apos;k&apos; | &apos;K&apos; | &apos;m&apos; | &apos;M&apos; | &apos;g&apos; | &apos;G&apos;) 在下面例子中中 100M 或更多的输入数据用于查询：SELECT *FROM source TABLESAMPLE(100M) s; Hive 还支持按行数对输入进行限制，但它与上述两种行为不同。首先，它不需要 CombineHiveInputFormat，这意味着这可以在 non-native 表上使用。其次，用户给定的行数应用到每个 InputSplit 上。 因此总行数还取决于输入 InputSplit 的个数（不同 InputSplit 个数得到的总行数也会不一样）。（从Hive 0.10.0开始 - https://issues.apache.org/jira/browse/HIVE-3401）block_sample: TABLESAMPLE (n ROWS) 例如，以下查询将从每个输入 InputSplit 中取前10行：SELECT * FROM source TABLESAMPLE(10 ROWS); 因此如果有20个 InputSplit 就会输出200条记录。 2. 分桶表抽样语法：table_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname]) TABLESAMPLE 子句允许用户编写对抽样数据的查询，而不是对整个表格进行查询。TABLESAMPLE 子句可以添加到任意表中的 FROM 子句中。桶从1开始编号。colname 表明在哪一列上对表的每一行进行抽样。colname 可以是表中的非分区列，也可以使用 rand() 表明在整行上抽样而不是在单个列上。表中的行在 colname 上进行分桶，并随机分桶到编号为1到y的桶上。返回属于第x个桶的行。下面的例子中，返回32个桶中的第3个桶中的行，s 是表的别名：SELECT * FROM source TABLESAMPLE(BUCKET 3 OUT OF 32 ON rand()) s; 通常情况下，TABLESAMPLE 将扫描整个表并抽取样本。但是，这并不是一种有效率的方式。相反，可以使用 CLUSTERED BY 子句创建该表，表示在该表的一组列上进行哈希分区/分簇。如果 TABLESAMPLE子 句中指定的列与 CLUSTERED BY 子句中的列匹配，则 TABLESAMPLE 仅扫描表中所需的哈希分区。 所以在上面的例子中，如果使用 CLUSTERED BY id INTO 32 BUCKETS 创建表 source（根据id将数据分到32个桶中）：TABLESAMPLE(BUCKET 3 OUT OF 16 ON id) 会返回第3个和第19个簇，因为每个桶由（32/16）= 2个簇组成（创建表时指定了32个桶，会对应32个簇）。为什么选择3和19呢，因为要返回的是第3个桶，而每个桶由原来的2个簇组成，3%16=3 19%16=3，第3个桶就由原来的第3个和19个簇组成。另一个例子:TABLESAMPLE(BUCKET 3 OUT OF 64 ON id) 会返回第三个簇的一半，因为每个桶将由（32/64）= 1/2个簇组成。 原文:https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Sampling]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Yarn上的调度器]]></title>
    <url>%2Fhadoop-scheduler-of-yarn.html</url>
    <content type="text"><![CDATA[1. 引言Yarn在Hadoop的生态系统中担任了资源管理和任务调度的角色。在讨论其构造器之前先简单了解一下Yarn的架构。 上图是Yarn的基本架构，其中 ResourceManager 是整个架构的核心组件，负责集群上的资源管理，包括内存、CPU以及集群上的其他资； ApplicationMaster 负责在生命周期内的应用程序调度； NodeManager 负责本节点上资源的供给和隔离；Container 可以抽象的看成是运行任务的一个容器。本文讨论的调度器是在 ResourceManager 进行调度，接下来在了解一下 FIFO 调度器、Capacity 调度器、Fair 调度器三个调度器。 2. FIFO调度器 上图显示了 FIFO 调度器的实现（执行过程示意图）。FIFO 调度器是先进先出（First In First Out）调度器。FIFO 调度器是 Hadoop 使用最早的一种调度策略，可以简单的将其理解为一个 Java 队列，这就意味着在集群中同时只能有一个作业运行。所有的应用程序按照提交顺序来执行，在上一个 Job 执行完成之后，下一个 Job 按照队列中的顺序执行。FIFO调度器以独占集群全部资源的方式来运行作业，这样的好处是 Job 可以充分利用集群的全部资源，但是对于运行时间短，优先级高或者交互式查询类的MR Job 需要等待它之前的 Job 完成才能被执行，这也就导致了如果前面有一个比较大的 Job 在运行，那么后面的 Job 将会被阻塞。因此，虽然 FIFO 调度器实现简单，但是并不能满足很多实际场景的要求。这也就促使 Capacity 调度器和 Fair 调度器的诞生。 增加作业优先级的功能后，可以通过设置 mapred.job.priority 属性或 JobClinet 的 setJobPriority 方法来设置优先级。在作业调度器选择要运行的下一个作业时，FIFO 调度器中不支持优先级抢占，所以高优先级的作业会受阻于前面已经开始，长时间运行的低优先级的作业。 3. Capacity调度器 上图显示了 Capacity 调度器的实现（执行过程示意图）。Capacity 调度器也称之为容器调度器。可以将它理解为一个资源队列。资源队列需要用户自己分配。例如因为 Job 需要要把整个集群分成了AB两个队列，A队列又可以继续分，比如将A队列再分为1和2两个子队列。那么队列的分配就可以参考下面的树形结构：—A[60%] |—A.1[40%] |—A.2[60%]—B[40%] 上述的树形结构可以理解为A队列占用集群全部资源的60%，B队列占用40%。A队列又分为1，2两个子队列，A.1占据40%，A.2占据60%，也就是说此时A.1和A.2分别占用A队列的40%和60%的资源。虽然此时已经对集群的资源进行了分配，但并不是说A提交了任务之后只能使用集群资源的60%，而B队列的40%的资源处于空闲。只要是其它队列中的资源处于空闲状态，那么有任务提交的队列就可以使用分配给空闲队列的那些资源，使用的多少依据具体配置。参数的配置会在后文中提到。 3.1 Capacity调度器的特性(1) 层次化的队列设计，这种层次化的队列设计保证了子队列可以使用父队列的全部资源。这样通过层次化的管理可以更容易分配和限制资源的使用。 (2) 容量，给队列设置一个容量(资源占比)，确保每个队列不会占用集群的全部资源。 (3) 安全，每个队列都有严格的访问控制。用户只能向自己的队列提交任务，不能修改或者访问其他队列的任务。 (4) 弹性分配，可以将空闲资源分配给任何队列。当多个队列出现竞争的时候，则会按照比例进行平衡。 (5) 多租户租用，通过队列的容量限制，多个用户可以共享同一个集群，colleagues 保证每个队列分配到自己的容量，并且提高利用率。 (6) 可操作性，Yarn支持动态修改容量、权限等的分配，这些可以在运行时直接修改。还提供管理员界面，来显示当前的队列状态。管理员可以在运行时添加队列；但是不能删除队列。管理员还可以在运行时暂停某个队列，这样可以保证当前队列在执行期间不会接收其他任务。如果一个队列被设置成了stopped，那么就不能向他或者子队列提交任务。 (7) 基于资源的调度，以协调不同资源需求的应用程序，比如内存、CPU、磁盘等等。 3.2 Capacity调度器的参数配置(1) capacity：队列的资源容量（百分比）。当系统非常繁忙时，应保证每个队列的容量得到满足，如果每个队列应用程序较少，可与其他队列共享剩余资源。注意，所有队列的容量之和应小于100。 (2) maximum-capacity：队列的资源使用上限（百分比）。由于资源共享，因此一个队列使用的资源量可能超过其容量，可以通过该参数来限制最多使用资源量。（这也是前文提到的队列可以占用资源的最大百分比） (3) user-limit-factor：每个用户最多可使用的资源量（百分比）。比如，如果该值为30，那么在任何时候每个用户使用的资源量都不能超过该队列容量的30%。 (4) maximum-applications ：集群中或者队列中同时处于等待和运行状态的应用程序数目上限(cluster or at the same time in a waiting in the queue and application number of the running condition limit)，这是一个强限制，一旦集群中应用程序数目超过该上限，后续提交的应用程序将被拒绝，默认值为 10000。所有队列的数目上限可通过参数 yarn.scheduler.capacity.maximum-applications 设置（可看做默认值），而单个队列可通过参数 yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-applications 设置。 (5) maximum-am-resource-percent：集群中用于运行应用程序 ApplicationMaster 的最大资源比例，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，默认是0.1，表示10%。所有队列的 ApplicationMaster 资源比例上限可通过参数 yarn.scheduler.capacity. maximum-am-resource-percent 设置（可看做默认值），而单个队列可通过参数 yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-am-resource-percent 设置。 (6) state ：队列状态可以为 STOPPED 或者 RUNNING，如果一个队列处于 STOPPED 状态，用户不可以将应用程序提交到该队列或者它的子队列中，类似的，如果 ROOT 队列处于 STOPPED 状态，用户不可以向集群中提交应用程序，但是处于 RUNNING 状态的应用程序仍可以正常运行，以便队列可以优雅地退出。 (7) acl_submit_applications：指定哪些Linux用户/用户组可向队列提交应用程序。需要注意的是，该属性具有继承性，即如果一个用户可以向某个队列提交应用程序，那么它可以向它的所有子队列提交应用程序。配置该属性时，用户之间或用户组之间用 ， 分割，用户和用户组之间用空格分割，比如 user1,user2 group1,group2。 (8) acl_administer_queue：指定队列的管理员，管理员可控制该队列的所有应用程序，例如杀死任意一个应用程序等。同样，该属性具有继承性，如果一个用户可以向某个队列提交应用程序，则它可以向它的所有子队列提交应用程序。 4. Fair调度器 上图显示了 Fair 调度器的实现（执行过程示意图）。Fair 调度器也称之为公平调度器。Fair 调度器是一种队列资源分配方式，在整个时间线上，所有的 Job 平分资源。默认情况下，Fair 调度器只是对内存资源做公平的调度和分配。当集群中只有一个任务在运行时，那么此任务会占用集群的全部资源。当有其他的任务提交后，那些释放的资源将会被分配给新的 Job，所以每个任务最终都能获取几乎一样多的资源。 Fair 调度器也可以在多个队列上工作，如上图所示，例如有两个用户A和B，他们分别拥有一个队列。当A启动一个 Job 而B没有提交任何任务时，A会获得集群全部资源；当B启动一个 Job 后，A的任务会继续运行，不过队列A会慢慢释放它的一些资源，一会儿之后两个任务会各自获得集群一半的资源。如果此时B再启动第二个 Job 并且其它任务也还在运行时，那么它将会和B队列中的的第一个 Job 共享队列B的资源，也就是队列B的两个 Job 会分别使用集群四分之一的资源，而队列A的 Job 仍然会使用集群一半的资源，结果就是集群的资源最终在两个用户之间平等的共享。 4.1 Fair调度器参数配置(1) yarn.scheduler.fair.allocation.file： allocation 文件的位置，allocation 文件是一个用来描述队列以及它们属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为 fair-scheduler.xml。 (2) yarn.scheduler.fair.user-as-default-queue：如果没有指定队列名称时，是否将与 allocation 有关的 username 作为默认的队列名称。如果设置成 false(且没有指定队列名称) 或者没有设定，所有的 jobs 将共享 default 队列。默认值为 true。 (3) yarn.scheduler.fair.preemption：是否使用抢占模式(优先权，抢占)，默认为 fasle，在此版本中此功能为测试性的。 (4) yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中发送多个容器分配信息。默认值为 false。 (5) yarn.scheduler.fair.max.assign：如果 yarn.scheduler.fair.assignmultiple 为true，那么在一次心跳中最多发送分配容器的个数。默认为-1，无限制。 (6) yarn.scheduler.fair.locality.threshold.node：0~1之间一个float值，表示在等待获取满足 node-local 条件的容器时，最多放弃不满足 node-local 的容器机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。 (7) yarn.scheduler.fair.locality.threashod.rack：同上，满足 rack-local。 (8) yarn.scheduler.fair.sizebaseweight：是否根据应用程序的大小(Job的个数)作为权重。默认为 false，如果为 true，那么复杂的应用程序会获取更多的资源。 5. 总结如果业务逻辑比较简单或者刚接触 Hadoop 的时建议使用 FIFO 调度器；如果需要控制部分应用程序的优先级，同时又想要充分利用集群资源的情况下，建议使用 Capacity 调度器；如果想要多用户或者多队列公平的共享集群资源，那么就选用Fair调度器。希望大家能够根据业务所需选择合适的调度器。 原文：http://www.cobub.com/en/the-selection-and-use-of-hadoop-yarn-scheduler/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next主题添加版权信息]]></title>
    <url>%2Fhexo-next-add-copyright-information.html</url>
    <content type="text"><![CDATA[1. 开启版权声明主题配置文件下,搜索关键字 post_copyright , 将 enable 改为 true：# Declare license on postspost_copyright: enable: true license: CC BY-NC-SA 4.0 license_url: https://creativecommons.org/licenses/by-nc-sa/4.0/ 这样设置之后出现一个问题:文章的链接并不是我设置域名之后的链接，对 next/layout/_macro/ 下的 post-copyright.swig 做如下修改:&lt;li class=&quot;post-copyright-link&quot;&gt; &lt;strong&gt;&#123;&#123; __(&apos;post.copyright.link&apos;) + __(&apos;symbol.colon&apos;) &#125;&#125;&lt;/strong&gt; &lt;a href=&quot;http://smartsi.club/&#123;&#123; post.path | default(post.permalink) &#125;&#125;&quot; title=&quot;&#123;&#123; post.title &#125;&#125;&quot;&gt;http://smartsi.club/&#123;&#123; post.path | default(post.permalink) &#125;&#125;&lt;/a&gt;&lt;/li&gt; 不知道还有没有什么更好的方法解决这个问题，欢迎留言指教。 最近发现一个解决办法，修改_config.yml文件，将url改为自定义域名：url: http://smartsi.club 就不需要像上面那样麻烦了。 update:20180919 2. 自定义文章底部版权声明在目录 next/layout/_macro/ 下添加 my-copyright.swig：&#123;% if page.copyright %&#125;&lt;div class=&quot;post-copyright-information&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src=&quot;https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/sweetalert/dist/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); $(&quot;.fa-clipboard&quot;).click(function()&#123; clipboard.on(&apos;success&apos;, function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, icon: &quot;success&quot;, showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 在目录 next/source/css/_common/components/post/ 下添加 post-copyright-information.styl：.post-copyright-information &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.post-copyright-information p&#123;margin:0;&#125;.post-copyright-information span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.post-copyright-information .raw &#123; margin-left: 1em; width: 5em;&#125;.post-copyright-information a &#123; color: #808080; border-bottom:0;&#125;.post-copyright-information a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.post-copyright-information:hover .fa-clipboard &#123; color: #000;&#125;.post-copyright-information .post-url:hover &#123; font-weight: normal;&#125;.post-copyright-information .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.post-copyright-information .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改 next/layout/_macro/post.swig: 在代码：&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;wechat-subscriber.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 之前添加增加如下代码：&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改 next/source/css/_common/components/post/post.styl 文件，在最后一行增加代码：@import &quot;my-post-copyright&quot; 保存重新生成即可。 如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加 copyright: true 的设置：---layout: postauthor: sjf0115title: Hexo Next主题添加版权信息date: 2018-05-04 18:33:01tags: - Hexocategories: Hexocopyright: truepermalink: hexo-next-add-copyright-information--- 原文:https://segmentfault.com/a/1190000009544924#articleHeader19]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 容错的改进与零数据丢失]]></title>
    <url>%2Fspark-streaming-improved-driver-fault-tolerance-and-zero-data-loss.html</url>
    <content type="text"><![CDATA[实时流处理系统必须可以7*24小时工作，因此它需要具备从各种系统故障中恢复过来的能力。最开始，Spark Streaming就支持从driver和worker故障中恢复。然而，从有些数据源导入数据时可能存在故障恢复以后丢失数据的情况。在Spark 1.2版本中，我们已经在Spark Streaming中对预写日志（也被称为journaling）作了初步支持，改进了恢复机制，使得更多数据源零数据丢失有了可靠的保证。本文将详细地描述这个特性的工作机制，以及开发者如何在Spark Streaming应用中使用这个机制。 1. 背景Spark和它的RDD抽象设计允许无缝地处理集群中任何worker节点的故障。鉴于Spark Streaming建立于Spark之上，因此其worker节点也具备了同样的容错能力。然而，Spark Streaming的长时间正常运行需求需要其应用程序必须也具备从driver进程（协调各个worker的主要应用进程）故障恢复的能力。使Spark driver能够容错是件很棘手的事情，因为它可能是任意计算模式实现的任意用户程序。不过Spark Streaming应用程序在计算上有一个内在的结构 - 在每段micro-batch数据周期性地执行同样的Spark计算。这种结构允许把应用的状态（亦称checkpoint）周期性地保存到可靠的存储空间中，并在driver重新启动时恢复该状态。 对于文件这样的源数据，这个driver恢复机制足以做到零数据丢失，因为所有的数据都保存在了像HDFS或S3这样的容错文件系统中了。但对于像Kafka和Flume等其它数据源，有些接收到的数据还只缓存在内存中，尚未被处理，它们就有可能会丢失。这是由于Spark应用的分布式操作引起的。当driver进程失败时，所有在standalone/yarn/mesos集群运行的executor，连同它们在内存中的所有数据，也同时被终止。对于Spark Streaming来说，从诸如Kafka和Flume的数据源接收到的所有数据，在它们处理完成之前，一直都缓存在executor的内存中。纵然driver重新启动，这些缓存的数据也不能被恢复。为了避免这种数据损失，我们在Spark 1.2发布版本中引进了预写日志（Write Ahead Logs）功能。 2. 预写日志预写日志（也称作journal）通常被用于数据库和文件系统中，用来保证任何数据操作的持久性。这个操作的思想是首先将操作记入一个持久的日志，然后才对数据施加这个操作。假如在施加操作的中间系统失败了，通过读取日志并重新施加前面预定的操作，系统就得到了恢复。下面让我们看看如何利用这样的概念保证接收到的数据的持久性。 像Kafka和Flume这样的数据源使用接收器（Receiver）来接收数据。它们作为长驻运行任务在executor中运行，负责从数据源接收数据，并且在数据源支持时，还负责确认收到的数据。收到的数据被保存在executor的内存中，然后driver在executor中运行来处理任务。 当启用了预写日志以后，所有收到的数据同时还保存到了容错文件系统的日志文件中。因此即使Spark Streaming失败，这些接收到的数据也不会丢失。另外，接收数据的正确性只在数据被预写到日志以后接收器才会确认，已经缓存但还没有保存的数据可以在driver重新启动之后由数据源再发送一次。这两个机制确保了零数据丢失，即所有的数据或者从日志中恢复，或者由数据源重发。 3. 配置如果需要启用预写日志功能，可以通过如下动作实现： 通过streamingContext.checkpoint(path-to-directory)设置检查点的目录。这个目录可以在任何与HadoopAPI口兼容的文件系统中设置，它既用作保存流检查点，又用作保存预写日志。设置SparkConf的属性 spark.streaming.receiver.writeAheadLog.enable为真（默认值是假）。 在日志被启用以后，所有接收器都获得了能够从可靠收到的数据中恢复的优势。我们建议禁止内存中的复制机制（in-memory replication）（通过在输入流中设置适当的持久等级(persistence level)），因为用于预写日志的容错文件系统很可能也复制了数据。 此外，如果希望可以恢复缓存的数据，就需要使用支持acking的数据源（就像Kafka，Flume和Kinesis一样），并且实现了一个可靠的接收器，它在数据可靠地保存到日志以后，才向数据源确认正确。内置的Kafka和Flume轮询接收器已经是可靠的了。 最后，请注意在启用了预写日志以后，数据接收吞吐率会有轻微的降低。由于所有数据都被写入容错文件系统，文件系统的写入吞吐率和用于数据复制的网络带宽，可能就是潜在的瓶颈了。在此情况下，最好创建更多的接收器增加接收的并行度，和/或使用更好的硬件以增加容错文件系统的吞吐率。 4. 实现细节让我们更深入地探讨一下这个问题，弄清预写日志到底是如何工作的。首先，我们重温一下常用的Spark Streaming的架构。 在一个Spark Streaming应用开始时（也就是driver开始时），相关的StreamingContext（所有流功能的基础）使用SparkContext启动接收器成为长驻运行任务。这些接收器接收并保存流数据到Spark内存中以供处理。用户传送数据的生命周期如下图所示（请参考下列图示）。 接收数据（蓝色箭头）——接收器将数据流分成一系列小块，存储到executor内存中。另外，在启用以后，数据同时还写入到容错文件系统的预写日志。 通知driver（绿色箭头）——接收块中的元数据（metadata）被发送到driver的StreamingContext。这个元数据包括：（i）定位其在executor内存中数据位置的块reference id，（ii）块数据在日志中的偏移信息（如果启用了）。 处理数据（红色箭头）——每批数据的间隔，流上下文使用块信息产生弹性分布数据集RDD和它们的作业（job）。StreamingContext通过运行任务处理executor内存中的块来执行作业。 周期性地设置检查点（橙色箭头）——为了恢复的需要，流计算（换句话说，即 StreamingContext提供的DStreams ）周期性地设置检查点，并保存到同一个容错文件系统中另外的一组文件中。 当一个失败的driver重启时，下列事情出现（参考下一个图示）。 恢复计算（橙色箭头）——使用检查点信息重启driver，重新构造上下文并重启接收器。 恢复元数据块（绿色箭头）——为了保证能够继续下去所必备的全部元数据块都被恢复。 未完成作业的重新形成（红色箭头）——由于失败而没有处理完成的批处理，将使用恢复的元数据再次产生RDD和对应的作业。 读取保存在日志中的块数据（蓝色箭头）——在这些作业执行时，块数据直接从预写日志中读出。这将恢复在日志中可靠地保存的所有必要数据。 重发尚未确认的数据（紫色箭头）——失败时没有保存到日志中的缓存数据将由数据源再次发送。因为接收器尚未对其确认。 因此通过预写日志和可靠的接收器，Spark Streaming就可以保证没有输入数据会由于driver的失败（或换言之，任何失败）而丢失。 5. 未来的发展方向有关预写日志的某些未来发展方向包括： 类似Kafka这样的系统可以通过复制数据保持可靠性。允许预写日志两次高效地复制同样的数据：一次由Kafka，而另一次由Spark Streaming。Spark未来版本将包含Kafka容错机制的原生支持，从而避免第二个日志。预写日志写入性能的改进（尤其是吞吐率）。 6. 进一步研究的参考关于检查点和预写日志更多的信息，请参考Spark Streaming Programming GuideSpark的Meetup talk中有关的主题JIRA – SPARK-3129 译文：https://www.csdn.net/article/2015-03-03/2824081原文：https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 子查询]]></title>
    <url>%2Fhive-base-sub-queries.html</url>
    <content type="text"><![CDATA[1. FROM中的子查询SELECT ... FROM (subquery) name ...SELECT ... FROM (subquery) AS name ... (Note: Only valid starting with Hive 0.13.0) Hive仅在FROM子句中支持子查询（从Hive 0.12版本开始）。必须为子查询指定名称，因为FROM子句中的每个表都必须具有名称。子查询 SELECT 列表中的列必须具有独一无二的名称。子查询 SELECT 列表中的列可以在外部查询中使用，就像使用表中的列一样。子查询也可以是带 UNION 的查询表达式。Hive支持任意级别的子查询。 在Hive 0.13.0及更高版本（HIVE-6519）中可选关键字 AS 可以包含的子查询名称之前。使用简单子查询的示例：SELECT colFROM ( SELECT a+b AS col FROM t1) t2 包含UNION ALL的子查询示例：SELECT t3.colFROM ( SELECT a+b AS col FROM t1 UNION ALL SELECT c+d AS col FROM t2) t3 2. WHERE中的子查询从Hive 0.13开始，WHERE子句中支持某些类型的子查询。可以将这些子查询的结果视为 IN 和 NOT IN 语句中的常量（我们也称这些子查询为不相关子查询，因为子查询不引用父查询中的列）。 SELECT Id, LastName, FirstName, Address, CityFROM PersonsWHERE Id IN ( SELECT PersonId FROM Orders); 也可以支持 EXISTS 和 NOT EXISTS 子查询：SELECT *FROM PersonsWHERE EXISTS ( SELECT * FROM Orders WHERE Orders.PersonId = Persons.Id); 有一些限制： 子查询仅支持在表达式的右侧。 IN/NOT IN 子查询只能选择一列。 EXISTS/NOT EXISTS 必须有一个或多个相关谓词。 对父查询的引用仅在子查询的WHERE子句中支持。 原文：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SubQueries#LanguageManualSubQueries-SubqueriesintheWHEREClause]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之Shuffle调优]]></title>
    <url>%2Fspark-performance-shuffle-tuning.html</url>
    <content type="text"><![CDATA[1. 调优概述大多数 Spark 作业的性能主要就是消耗在了 shuffle 环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对 shuffle 过程进行调优。但是也必须提醒大家的是，影响一个 Spark 作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle 调优只能在整个 Spark 的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解 shuffle 的原理，以及相关参数的说明，同时给出各个参数的调优建议。 2. ShuffleManager发展概述在Spark的源码中，负责 shuffle 过程的执行、计算和处理的组件主要就是 ShuffleManager，也即 shuffle 管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的 shuffle 计算引擎是 HashShuffleManager。HashShuffleManager 有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的 ShuffleManager 改成了 SortShuffleManager。SortShuffleManager 相较于 HashShuffleManager 来说，有了一定的改进。主要就在于，每个 Task 在进行 shuffle 操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个 Task 就只有一个磁盘文件。在下一个 stage 的 shuffle read task 拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下 HashShuffleManager 和 SortShuffleManager 的原理。 3. HashShuffleManager运行原理3.1 未经优化的HashShuffleManager 上图说明了未经优化的 HashShuffleManager 的原理。这里我们先明确一个假设前提：每个 Executor 只有1个CPU core，也就是说，无论这个 Executor 上分配多少个 task 线程，同一时间都只能执行一个 task 线程。 我们先从 shuffle write 开始说起。shuffle write 阶段，主要就是在一个 stage 结束计算之后，为了下一个 stage 可以执行 shuffle 类的算子（比如reduceByKey），而将每个 task 处理的数据按 key 进行“分类”。所谓“分类”，就是对相同的 key 执行 hash 算法，从而将相同 key 都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游 stage 的一个 task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。 那么每个执行 shuffle write 的 task，要为下一个 stage 创建多少个磁盘文件呢？很简单，下一个 stage 的 task 有多少个，当前 stage 的每个 task 就要创建多少份磁盘文件。比如下一个 stage 总共有 100 个 task，那么当前 stage 的每个 task 需要创建 100 份磁盘文件。如果当前 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 Task，那么每个 Executor 上总共需要创建 500 个磁盘文件，所有 Executor 上会创建 5000 个磁盘文件。由此可见，未经优化的 shuffle write 操作所产生的磁盘文件的数量是极其惊人的。 接着我们来说说 shuffle read。shuffle read，通常就是一个 stage 刚开始时要做的事情。此时该 stage 的每一个 task 就需要将上一个 stage 的计算结果中的所有相同 key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行 key 的聚合或连接等操作。由于 shuffle write 的过程中，上游 stage 的每个 task 给下游 stage 的每个 task 都创建了一个磁盘文件，因此 shuffle read 的过程中，每个 task 都要从上游 stage 的所有 task 所在节点拉取属于自己的那一个磁盘文件。 shuffle read 的拉取过程是一边拉取一边进行聚合的。每个 shuffle read task 都会有一个自己的 buffer 缓冲，每次都只能拉取与 buffer 缓冲相同大小的数据，然后通过内存中的一个 Map 进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到 buffer 缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 3.2 优化后的HashShuffleManager 上图说明了优化后的 HashShuffleManager 的原理。这里说的优化，是指我们可以设置一个参数 spark.shuffle.consolidateFiles。该参数默认值为 false，将其设置为 true 即可开启优化机制。通常来说，如果我们使用 HashShuffleManager，建议开启这个选项。 开启 consolidate 机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件了。此时引入了一个 shuffleFileGroup 的概念，每个 shuffleFileGroup 会对应一批磁盘文件，磁盘文件的数量与下游 stage 的 task 数量是相同的。一个 Executor 上有多少个 CPU core，就可以并行执行多少个 task。而第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内。 当 Executor 的 CPU core 执行完一批 task，接着执行下一批 task 时，下一批 task 就会复用之前已有的 shuffleFileGroup，包括其中的磁盘文件。也就是说，此时 task 会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能。 假设第二个 stage 有 100 个 task，第一个 stage 有 50 个 task，总共还是有 10 个 Executor，每个 Executor 执行 5 个 task。那么原本使用未经优化的 HashShuffleManager 时，每个 Executor 会产生 500 个磁盘文件，所有 Executor 会产生 5000 个磁盘文件的。但是此时经过优化之后，每个 Executor 此时只会创建 100 个磁盘文件(假设每个 Executor 只有1个CPU core，所以每个 Executor 中并行任务只有一个)，所有 Executor 只会创建 1000 个磁盘文件。 每个 Executor 创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。 4. SortShuffleManager运行原理SortShuffleManager 的运行机制主要分成两种，一种是普通运行机制，另一种是 bypass 运行机制。当 shuffle read task 的数量小于等于 spark.shuffle.sort.bypassMergeThreshold 参数的值时（默认为200），就会启用 bypass 机制。 4.1 普通运行机制 上图说明了普通的 SortShuffleManager 的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的 shuffle 算子，可能选用不同的数据结构。如果是 reduceByKey 这种聚合类的 shuffle 算子，那么会选用 Map 数据结构，一边通过 Map 进行聚合，一边写入内存；如果是 join 这种普通的 shuffle 算子，那么会选用 Array 数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过 Java 的 BufferedOutputStream 实现的。BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 一个 task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是 merge 过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个 task 就只对应一个磁盘文件，也就意味着该 task 为下游 stage 的 task 准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个 task 的数据在文件中的 start offset 与 end offset。 SortShuffleManager 由于有一个磁盘文件 merge 的过程，因此大大减少了文件数量。比如第一个 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 task，而第二个 stage 有 100 个task。由于每个 task 最终只有一个磁盘文件，因此此时每个 Executor 上只有5个磁盘文件，所有 Executor 只有 50 个磁盘文件。 4.2 bypass运行机制 上图说明了 bypass SortShuffleManager 的原理。bypass 运行机制的触发条件如下： shuffle map task 数量小于 spark.shuffle.sort.bypassMergeThreshold 参数的值。 不是聚合类的shuffle算子（比如reduceByKey）。 此时 task 会为每个下游 task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的 hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read的性能会更好。 而该机制与普通 SortShuffleManager 运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write 过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 5. shuffle相关参数调优以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。 (1) spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置 shuffle write task 的 BufferedOutputStream 的 buffer 缓冲大小。将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少 shuffle write 过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 (2) spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置 shuffle read task 的 buffer 缓冲大小，而这个 buffer 缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 (3) spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task 从 shuffle write task 所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如60次），以避免由于 JVM 的 full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的 shuffle 过程，调节该参数可以大幅度提升稳定性。 (4) spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加 shuffle 操作的稳定性。 (5) spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了 Executor 内存中，分配给 shuffle read task 进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给 shuffle read 的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 (6) spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置 ShuffleManager 的类型。Spark 1.5以后，有三个可选项：hash、sort 和 tungsten-sort。HashShuffleManager 是Spark 1.2以前的默认选项，Spark 1.2以及之后的版本默认为 SortShuffleManager 了。tungsten-sort 与 sort类似，但是使用了 tungsten 计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于 SortShuffleManager 默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，使用默认的 SortShuffleManager 就可以；如果你的业务逻辑不需要对数据进行排序，建议参考后面的几个参数调优，通过 bypass 机制或优化的 HashShuffleManager 来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort 要慎用，因为之前发现了一些相应的bug。 (7) spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当 ShuffleManager 为 SortShuffleManager 时，如果 shuffle read task 的数量小于阈值（默认是200），shuffle write 过程中不会进行排序操作，而是直接按照未经优化的 HashShuffleManager 的方式去写数据，但是最后会将每个 task 产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用 SortShuffleManager 时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于 shuffle read task 的数量。那么此时就会自动启用 bypass 机制，map-side 就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 (8) spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用 HashShuffleManager，该参数有效。如果设置为 true，那么就会开启 consolidate 机制，会大幅度合并 shuffle write 的输出文件，对于 shuffle read task 数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要 SortShuffleManager 的排序机制，那么除了使用 bypass 机制，还可以尝试将 spark.shffle.manager 参数手动指定为hash，使用 HashShuffleManager，同时开启 consolidate 机制。在实践中尝试过，发现其性能比开启了 bypass 机制的 SortShuffleManager 要高出10%~30%。 原文:https://tech.meituan.com/spark-tuning-pro.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内部原理之内存管理]]></title>
    <url>%2Fspark-internal-memory-management.html</url>
    <content type="text"><![CDATA[Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。 1. 堆内和堆外内存规划作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。 图 1 . 堆内和堆外内存示意图 1.1 堆内内存堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。 Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程： (1) 申请内存： Spark 在代码中 new 一个对象实例 JVM 从堆内内存分配空间，创建对象并返回对象引用 Spark 保存该对象的引用，记录该对象占用的内存 (2) 释放内存： Spark 记录该对象释放的内存，删除该对象的引用 等待 JVM 的垃圾回收机制释放该对象占用的堆内内存 我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。 对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。 虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。 1.2 堆外内存为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。 在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。 1.3 内存管理接口Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存: 清单 1 . 内存管理接口的主要方法//申请存储内存def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请展开内存def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请执行内存def acquireExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long//释放存储内存def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit//释放执行内存def releaseExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit//释放展开内存def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit 我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。 MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（Unified Memory Manager）方式，1.6 之前采用的静态管理（Static Memory Manager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 2 小节会分别对这两种方式进行介绍。 2. 内存空间分配2.1 静态内存管理在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示： 图 2 . 静态内存管理图示——堆内 可以看到，可用的堆内内存的大小需要按照下面的方式计算： 清单 2 . 可用堆内内存空间可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。 堆外的空间分配较为简单，只有存储内存和执行内存，如图 3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。 图 3 . 静态内存管理图示——堆外 静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。 2.2 统一内存管理Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图 4 和图 5 所示 图 4 . 统一内存管理图示——堆内 图 5 . 统一内存管理图示——堆外 其中最重要的优化在于动态占用机制，其规则如下： 设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block） 执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间 存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂[4] 图 6 . 动态占用机制图示 凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。 3. 存储内存管理3.1 RDD 的持久化机制弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。 Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管 理 （存储内存的其他应用场景，如缓存 broadcast 数据，暂时不在本文的讨论范围之内）。 RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。 图 7 . Storage 模块示意图 在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等 7 种不同的 存储级别 ，而存储级别是以下 5 个变量的组合： 清单 3 . 存储级别class StorageLevel private(private var _useDisk: Boolean, //磁盘private var _useMemory: Boolean, //这里其实是指堆内内存private var _useOffHeap: Boolean, //堆外内存private var _deserialized: Boolean, //是否为非序列化private var _replication: Int = 1 //副本个数) 通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式： 存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。 存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。 副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。 3.2 RDD 缓存的过程RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（Iterator）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续。 RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。 因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图 8 所示。 图 8. Spark Unroll 示意图 在图 3 和图 5 中可以看到，在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。 3.3 淘汰和落盘由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。 存储内存的淘汰规则为： 被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存 新旧 Block 不能属于同一个 RDD，避免循环淘汰 旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题 遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。 落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。 4. 执行内存管理4.1 多任务间内存分配Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。 4.2 Shuffle 的内存占用执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用： (1) Shuffle Write 若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。 若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。 (2) Shuffle Read 在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。 如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。 在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。 Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划[9]，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。 Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成： 页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。 页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。 有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升[10]。 Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。 5. 结束语Spark 的内存管理是一套复杂的机制，且 Spark 的版本更新比较快，笔者水平有限，难免有叙述不清、错误的地方，若读者有好的建议和更深的理解，还望不吝赐教。 6. 参考资源 Spark Cluster Mode Overview Spark Sort Based Shuffle 内存分析 Spark OFF_HEAP Unified Memory Management in Spark 1.6 Tuning Spark: Garbage Collection Tuning Spark Architecture 《Spark 技术内幕：深入解析 Spark 内核架构于实现原理》第 8 章 Storage 模块详解 Spark Sort Based Shuffle 内存分析 Project Tungsten: Bringing Apache Spark Closer to Bare Metal Spark Tungsten-sort Based Shuffle 分析 探索 Spark Tungsten 的秘密 Spark Task 内存管理（on-heap&amp;off-heap） 原文：https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 如何使用压缩]]></title>
    <url>%2Fhadoop-how-to-use-compression.html</url>
    <content type="text"><![CDATA[就如上一篇文章介绍的那样，如果输入文件是压缩文件，当 MapReduce 程序读取压缩文件时，根据文件名的后缀来选择 codes，输入文件自动解压缩（我们不需要指定压缩文件是哪一种压缩格式）。 下面我们列出了一些代码，为 Hadoop 中常用的压缩格式设置输出压缩。 1. 常用压缩格式1.1 Gzip对于最终输出，我们可以使用FileOutputFormat上的静态方便方法来设置属性：FileOutputFormat.setCompressOutput(job, true);FileOutputFormat.setOutputCompressorClass(job, GzipCodec,class); 或者Configuration conf = new Configuration();conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);conf.setClass("mapreduce.output.fileoutputformat.compress.codec", GzipCodec.class, CompressionCodec.class); 对于 Map 输出：Configuration conf = new Configuration();conf.setBoolean("mapreduce.map.output.compress",true);conf.setClass("mapreduce.map.output.compress.codec", GzipCodec.class, CompressionCodec.class);Job job = Job.getInstance(conf); 1.2 LZO对于最终输出：FileOutputFormat.setCompressOutput(conf, true);FileOutputFormat.setOutputCompressorClass(conf, LzoCodec.class); 为了使LZO可分割，我们需要生成一个LZO索引文件。 对于 Map 输出：Configuration conf = new Configuration();conf.setBoolean("mapreduce.map.output.compress",true);conf.setClass("mapreduce.map.output.compress.codec", LzoCodec.class, CompressionCodec.class);Job job = Job.getInstance(conf); 1.3 Snappy对于最终输出：conf.setOutputFormat(SequenceFileOutputFormat.class);SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);SequenceFileOutputFormat.setCompressOutput(conf, true);conf.set("mapreduce.output.fileoutputformat.compress.codec","org.apache.hadoop.io.compress.SnappyCodec"); 对于Map输出：Configuration conf = new Configuration();conf.setBoolean("mapreduce.map.output.compress", true);conf.set("mapreduce.map.output.compress.codec","org.apache.hadoop.io.compress.SnappyCodec"); 2. 实验与结果2.1 Filesystem counters文件系统计数器用于分析实验结果。以下是典型的内置文件系统计数器。 FILE_BYTES_READ 是本地文件系统读取的字节数。假设所有的 map 输入数据都来自 HDFS，那么在 map 阶段，FILE_BYTES_READ 应该为零。另一方面，reducer 的输入文件是 reduce 端本地磁盘上的数据，它们是从 map 端磁盘拉取过来的。因此，reduce 端 FILE_BYTES_READ 表示 reducer 读取的总字节数。 FILE_BYTES_WRITTEN 由两部分组成。第一部分来自 mappers。所有的 mappers 都会将中间输出溢出到磁盘上。mappers 写入磁盘的所有字节将包含在 FILE_BYTES_WRITTEN 中。第二部分来自 reducers。 在 shuffle 阶段，所有 reducers 将从 mappers 中获取中间数据，合并并溢写到 reducer 端的磁盘上。reducers 写入磁盘的所有字节也将包含在 FILE_BYTES_WRITTEN 中。 HDFS_BYTES_READ 表示作业启动时 mappers 从 HDFS 上读取的字节数。这些数据不仅包括源文件的内容，还包括关于 splits 的元数据。 HDFS_BYTES_WRITTEN 表示写入 HDFS 的字节。这是最终输出的字节数。 请注意，由于 HDFS 和本地文件系统是不同的文件系统，因此来自两个文件系统的数据不会重叠。 2.2 压缩比较(1) 没有压缩 (2) 只压缩输入 我们可以看到 HDFS_BYTES_READ 明显减少。这表明 mappers 从 HDFS 上读取的总字节数显着减少。 (3) 只压缩map中间输出 我们可以看到 FILE_BYTES_READ 和 FILE_BYTES_WRITTEN 显着减少。这意味着本地文件系统节点之间的数据传输显着减少。 (4) 只压缩最终输出 我们可以看到 HDFS_BYTES_WRITTEN 显着减少。这表明 HDFS 的最终输出显着降低。 2.3 不同压缩格式的比较：gzip，lzo 正如我们所看到的，LZO 文件略大于对应的 gzip 文件，但都比原来未压缩文件小得多。另外，LZO 文件压缩速度快了近五倍，解压速度快了两倍。 我们还可以看到 Snappy 文件比相应的 LZO 文件大，但仍然是原来未压缩文件的一半。另外，Snappy 的压缩和解压缩速度都比 LZO 更快。总之，Snappy 在压缩和解压缩时间方面速度更快，但在压缩比方面效率更低。 原文：http://comphadoop.weebly.com/how-to-use-compression.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 大量小文件问题的优化]]></title>
    <url>%2Fhadoop-small-files-problem.html</url>
    <content type="text"><![CDATA[1. HDFS上的小文件问题小文件是指文件大小明显小于 HDFS 上块（block）大小（默认64MB，在Hadoop2.x中默认为128MB）的文件。如果存储小文件，必定会有大量这样的小文件，否则你也不会使用 Hadoop，这样的文件给 Hadoop 的扩展性和性能带来严重问题。当一个文件的大小小于 HDFS 的块大小（默认64MB）就认定为小文件，否则就是大文件。为了检测输入文件的大小，可以浏览Hadoop DFS 主页 ，并点击 Browse filesystem（浏览文件系统）。 首先，HDFS 中任何一个文件，目录或者数据块在 NameNode 节点内存中均以一个对象形式表示（元数据），而这受到 NameNode 物理内存容量的限制。每个元数据对象约占 150 byte，所以如果有1千万个小文件，每个文件占用一个block，则 NameNode 大约需要2G空间。如果存储1亿个文件，则 NameNode 需要20G空间，这毫无疑问1亿个小文件是不可取的。 其次，处理小文件并非 Hadoop 的设计目标，HDFS 的设计目标是流式访问大数据集（TB级别）。因而，在 HDFS 中存储大量小文件是很低效的。访问大量小文件经常会导致大量的 seek，以及不断的在 DatanNde 间跳跃去检索小文件。这不是一个很有效的访问模式，严重影响性能。 最后，处理大量小文件速度远远小于处理同等大小的大文件的速度。每一个小文件要占用一个 slot，而任务启动将耗费大量时间甚至大部分时间都耗费在启动任务和释放任务上。 2. MapReduce上的小文件问题Map任务一般一次只处理一个块的输入（input）（默认使用FileInputFormat）。如果文件非常小，并且有很多，那么每一个 Map 任务都仅仅处理非常小的输入数据，并会产生大量的 Map 任务，每一个 Map 任务都会额外增加 bookkeeping 开销。一个1GB大小的文件拆分成16个64M大小的块，相对于拆分成10000个100KB的块，后者每一个小文件启动一个 Map 任务，作业的运行时间将会十倍甚至百倍慢于前者。 Hadoop 中有一些特性可以用来减轻 bookkeeping 开销：可以在一个 JVM 中允许 task JVM 重用，以支持在一个 JVM 中运行多个 Map 任务，以此来减少 JVM 的启动开销(译者注：MR1中通过设置 mapred.job.reuse.jvm.num.tasks 属性)。 3. 为什么会产生大量的小文件至少在两种场景下会产生大量的小文件： 这些小文件都是一个大逻辑文件的一部分。由于 HDFS 在2.x版本开始支持对文件进行追加，所以在此之前保存无边界文件（例如，日志文件）一种常用的方式就是将这些数据以块的形式写入HDFS中（译者注：持续产生的文件，例如日志每天都会生成）。 文件本身就是很小。设想一下，我们有一个很大的图片语料库，每一个图片都是一个单独的文件，并且没有一种很好的方法来将这些文件合并为一个大的文件。 4. 解决方案这两种情况需要有不同的解决方式。 4.1 第一种情况对于第一种情况，文件是许多记录组成的，那么可以通过调用 HDFS 的 sync() 方法(和 append 方法结合使用)，每隔一定时间生成一个大文件。或者，可以通过写一个 MapReduce 程序来来合并这些小文件。 4.2 第二种情况对于第二种情况，就需要容器通过某种方式来对这些文件进行分组。Hadoop提供了一些选择： 4.2.1 HAR FileHadoop Archives （HAR files）是在 0.18.0 版本中引入到 HDFS 中的，它的出现就是为了缓解大量小文件消耗 NameNode 内存的问题。HAR 文件是通过在 HDFS 上构建一个分层文件系统来工作。HAR 文件通过 hadoop archive 命令来创建，而这个命令实际上是运行 MapReduce 作业来将小文件打包成少量的 HDFS 文件（译者注：将小文件进行合并成几个大文件）。对于客户端来说，使用 HAR 文件系统没有任何的变化：所有原始文件都可见以及可以访问（只是使用 har://URL，而不是 hdfs://URL），但是在 HDFS 中中文件个数却减少了。 读取 HAR 文件不如读取 HDFS 文件更有效，并且实际上可能更慢，因为每个 HAR 文件访问需要读取两个索引文件以及还要读取数据文件本。 尽管 HAR 文件可以用作 MapReduce 的输入，但是 Map 没有办法直接对共同驻留在 HDFS 块上的 HAR 所有文件操作。可以考虑通过创建一种 input format，充分利用 HAR 文件的局部性优势，但是目前还没有这种input format。需要注意的是：MultiFileInputSplit，即使在 HADOOP-4565 进行了改进，选择节点本地分割中的文件，但始终还是需要每个小文件的搜索。在目前看来，HAR 可能最好仅用于存储文档。 4.2.2 SequenceFile通常解决”小文件问题”的回应是：使用 SequenceFile。这种方法的思路是，使用文件名作为 key，文件内容作为 value，如下图。 在实践中这种方式非常有效。我们回到10,000个100KB大小的小文件问题上，你可以编写一个程序将合并为一个 SequenceFile，然后你可以以流式方式处理（直接处理或使用 MapReduce） SequenceFile。这样会带来两个优势： SequenceFiles 是可拆分的，因此 MapReduce 可以将它们分成块，分别对每个块进行操作； 与 HAR 不同，它们支持压缩。在大多数情况下，块压缩是最好的选择，因为它直接对几个记录组成的块进行压缩，而不是对每一个记录进行压缩。 将现有数据转换为 SequenceFile 可能会很慢。但是，完全可以并行创建一个一个的 SequenceFile 文件。Stuart Sierra 写了一篇关于将 tar 文件转换为 SequenceFile 的文章，像这样的工具是非常有用的，我们应该多看看。向前看，最好设计好数据管道，如果可能的话，将源数据直接写入 SequenceFile，而不是作为中间步骤写入小文件。 与 HAR 文件不同，没有办法列出 SequenceFile 中的所有键，所以不能读取整个文件。Map File，类似于对键进行排序的 SequenceFile，维护部分索引，所以他们也不能列出所有的键，如下图。 4.2.3 HBase如果你产生很多小文件，根据访问模式的不同，应该进行不同类型的存储。HBase 将数据存储在 Map Files（带索引的 SequenceFile）中，如果你需要随机访问来执行 MapReduce 流式分析，这是一个不错的选择。如果延迟是一个问题，那么还有很多其他选择 - 参见Richard Jones对键值存储的调查。 原文：http://blog.cloudera.com/blog/2009/02/the-small-files-problem/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 数据压缩简介]]></title>
    <url>%2Fhadoop-data-compress.html</url>
    <content type="text"><![CDATA[文件压缩带来两大好处：它减少了存储文件所需的空间，并加速了数据在网络或者磁盘上的传输速度。在处理大量数据时，这两项节省可能非常重要，因此需要仔细考虑如何在 Hadoop 中使用压缩。 1. 要压缩什么？1.1 压缩输入文件如果输入文件是压缩的，那么从HDFS读入的字节就会减少，这意味着读取数据的时间会减少。对于提升作业执行的性能是有帮助的。 如果输入文件被压缩，在 MapReduce 读取时会自动解压缩，根据文件扩展名来确定使用哪个编解码器。例如，以 .gz 结尾的文件可以被识别为 gzip 压缩文件，因此使用 GzipCodec 进行读取。 1.2 压缩输出文件通常我们需要将输出存储为历史文件。如果每天的输出文件很大，并且我们需要存储历史结果以供将来使用，那么这些累积结果将占用大量的 HDFS 空间。但是，这些历史文件可能不会非常频繁地被使用，导致浪费 HDFS 空间。因此，在 HDFS 上存储之前，需要压缩输出。 1.3 压缩Map输出即使你的 MapReduce 应用程序读取和写入未压缩的数据，它也可能从压缩 Map 阶段的中间输出中受益。由于 Map 输出被写入磁盘并通过网络传输到 Reducer 节点，所以通过使用 LZO 或 Snappy 等快速压缩器，由于减少了传输的数据量从而获得性能提升。 2. 常见压缩格式 2.1 Gzipgzip 是 Hadoop 内置压缩方法，基于 DEFLATE 算法，组合 LZ77 和 Huffman 编码。 2.2 Bzip2bzip2 能够进行高质量的数据压缩。它利用先进的压缩技术，能够把普通的数据文件压缩10%至15%，压缩的速度和解压的效率都非常高！支持大多数压缩格式，包括tar、gzip 等等。 2.3 LZOLZO压缩格式由许多较小（大约256K）的压缩数据块组成，因此允许作业沿着块边界进行分割。此外，设计时考虑了速度要素，目的是达到与硬盘读取速度相当的压缩速度：压缩速度是 gzip 的５倍，解压缩速度是 gzip 的2倍。同一个文件用 LZO 压缩后比用 gzip 压缩后大50％，但比压缩前小20-50％，这对改善性能非常有利，Map 阶段完成时间要快四倍。 2.4 SnappySnappy 是一个压缩/解压库。它的目标不是最大压缩率,也不关心与任何其他压缩库的兼容性。相反，它旨在提供非常高的速度和合理的压缩。例如，与 zlib 的最快压缩模式相比，Snappy 对于大多数输入都快了一个数量级，但是生成的压缩文件都要比 zlib 模式大20％到100％。在一个64位，单核酷睿i7处理器上，Snappy 压缩速度在 250 MB/秒以上，解压缩速度在 500 MB/秒以上。Snappy 广泛应用于 Google 内部，BigTable，MapReduce 以及内部 RPC 系统各个地方都在使用。 3. 折衷所有压缩算法都在空间与时间上进行权衡：更快的压缩和解压缩速度通常以更少的空间节省为代价，意味着耗费更大的空间。上表中列出的工具通常会在压缩时通过提供九种不同的选项来控制这种权衡：-1表示对速度进行优化，-9表示对空间进行优化。 不同的工具具有非常不同的压缩特性。Gzip 是一个通用压缩器，空间与时间权衡的更好一些。Bzip2 比 gzip 压缩更有效（压缩后文件更小），但速度较慢。 Bzip2 的解压缩速度比压缩速度快，但它仍然比其他方法慢。另一方面，LZO 和 Snappy 都对速度进行了优化，并且比 gzip 快一个数量级，但是压缩效率较低。解压缩方面 Snappy 明显快于LZO。 4. 有关压缩和输入拆分的问题当考虑如何压缩由 MapReduce 处理的数据时，重要的是要了解压缩格式是否支持分割。考虑存储在 HDFS 中大小为 1GB 的未压缩文件。如果 HDFS 块大小为 64MB（MR1默认64MB，MR2默认128MB），文件将存储为16个块，并且使用此文件作为输入的 MapReduce 作业将创建16个 InputSplit（输入拆分），每一个 InputSplit 作为一个独立 Map 任务的输入单独进行处理。 假设我们有一个大小为 1GB 的 gzip 压缩文件，和以前一样，HDFS 将文件存储为16块。然而，无法为每个块创建 InputSplit，因为不能从 gzip 数据流中的任意位置开始读取，因此 Map 任务不可能独立于其他 Map 任务而只读取一个 InputSplit 中的数据。gzip 格式使用 DEFLATE 算法存储压缩数据，DEFLATE 算法将数据存储为一系列压缩的数据块。问题在于，用任何方法也不能区分每个块的开始位置，每个块的开始位置保证了允许从流中的任意位置能够读到下一个块的开始位置，这就意味着能够读出单个块的数据。因此，gzip 不支持拆分。 在这种情况下，MapReduce 不会尝试对压缩文件进行分割，因为 MapReduce 知道输入文件是通过 gzip 压缩（通过查看文件扩展名），并且知道 gzip 不支持分割。这种情况下 MapReduce 还是会继续工作的，但是以牺牲数据局部性的特性为代价：单个 Map 将会处理 16个 HDFS 块，大部分都不会在 Map 本地节点。此外，使用较少的 Mapper，作业的粒度变小，因此可能运行较长时间。 假设示例中的文件是一个 LZO 文件，我们也会遇到同样的问题，因为底层的压缩格式不能提供一种方法与流同步读取。但是，可以使用 Hadoop LZO 库附带的索引器工具处理 LZO 文件。该工具建立分割点的索引，当使用恰当的 MapReduce 输入格式时，可以有效地使他们进行拆分。另一方面，bzip2 文件在块之间提供了同步标记（pi的48位近似），因此它支持拆分。 5. IO密集型与CPU密集型在 HDFS 中存储压缩数据能够进一步分配你的硬件，因为压缩数据通常是原始数据大小的25％。此外，由于 MapReduce 作业几乎都是IO密集型，存储压缩数据意味着整体上更少的IO处理，意味着作业运行更快。然而，有两个注意事项： 一些压缩格式不能拆分来并行处理 一些解压速度比较慢，作业变为CPU密集型，抵消你在IO上的收益。 gzip 压缩格式说明了第一个注意事项。假设有一个 1.1GB 的 gzip 文件，并且集群中块大小为 128MB。这个文件分割为 9 个 HDFS 块，每个大约128MB。为了在 MapReduce 作业中并行处理这些数据，每个块将由不同的 Mapper 负责。但这意味着第二个 Mapper 将在文件中大约 128MB 的任意字节处开始。gzip 用于解压缩输入的上下文字典在这为空，这意味着 gzip 解压缩器将无法正确解释字节。结果就是，Hadoop 中的大型 gzip 文件需要由单个 Mapper 处理，这违背了并行性的目的。 Bzip2压缩格式说明了作业成为CPU密集型的第二个注意事项。Bzip2文件压缩效果良好，也可以拆分，但是解压缩算法速度比较慢，无法跟上在 Hadoop 作业中常见的流式磁盘读取。虽然 Bzip2 压缩效果良好，节省了存储空间，但是正在运行的作业需要花费时间等待CPU完成解压数据，这会降低整体速度并抵消其他收益。 6. 总结6.1 需要压缩原因 数据存储但不经常处理。这是通常的 DWH 场景。在这种情况下，空间节省可能比处理开销更重要； 压缩因子非常高，节省了大量的IO； 解压缩非常快（例如 Snappy）使我们有一定的收益； 数据已经到达压缩状态(Data already arrived compressed) 6.2 不需要压缩原因 压缩数据不可拆分。 必须注意的是，现在许多格式都是以块级压缩构建的，以实现文件的拆分和部分处理； 数据在集群中创建，压缩需要很长时间。必须注意，压缩通常比解压缩需要更多的CPU； 数据几乎没有冗余，没有必要进行压缩； 原文地址：http://comphadoop.weebly.com/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark UI 之 Streaming 标签页]]></title>
    <url>%2Fspark-streaming-ui-streaming-tab.html</url>
    <content type="text"><![CDATA[这篇博文将重点介绍为理解 Spark Streaming 应用程序而引入的新的可视化功能。我们已经更新了 Spark UI 中的 Streaming 标签页来显示以下信息： 时间轴视图和事件率统计，调度延迟统计以及以往的批处理时间统计 每个批次中所有JOB的详细信息此外，为了理解在 Streaming 操作上下文中作业的执行情况，有向无环执行图的可视化增加了 Streaming 的信息。 让我们通过一个从头到尾分析Streaming应用程序的例子详细看一下上面这些新的功能。 1. 处理趋势的时间轴和直方图当我们调试一个 Spark Streaming 应用程序的时候，我们更希望看到数据正在以什么样的速率被接收以及每个批次的处理时间是多少。Streaming标签页中新的UI能够让你很容易的看到目前的值和之前1000个批次的趋势情况。当你在运行一个 Streaming 应用程序的时候，如果你去访问 Spark UI 中的 Streaming 标签页，你将会看到类似下面图一的一些东西（红色的字母，例如[A]，是我们的注释，并不是UI的一部分）。 第一行（标记为 [A]）展示了 Streaming 应用程序当前的状态；在这个例子中，应用已经以1秒的批处理间隔运行了将近40分钟;在它下面是输入速率（Input rate）的时间轴（标记为 [B]），显示了Streaming应用从它所有的源头以大约49 events每秒的速度接收数据。在这个例子中，时间轴显示了在中间位置（标记为[C]）平均速率有明显的下降，在时间轴快结束的地方应用又恢复了。如果你想得到更多详细的信息，你可以点击 Input Rate旁边（靠近[B]）的下拉列表来显示每个源头各自的时间轴，正如下面图2所示： 图2显示了这个应用有两个来源，(SocketReceiver-0和 SocketReceiver-1)，其中的一个导致了整个接收速率的下降，因为它在接收数据的过程中停止了一段时间。 这一页再向下（在图1中标记为 [D] ），处理时间（Processing Time）的时间轴显示，这些批次大约在平均20毫秒内被处理完成，和批处理间隔（在本例中是1s）相比花费的处理时间更少，意味着调度延迟（被定义为：一个批次等待之前批次处理完成的时间，被标记为 [E]）几乎是零，因为这些批次在创建的时候就已经被处理了。调度延迟是你的Streaming引用程序是否稳定的关键所在，UI的新功能使得对它的监控更加容易。 3. 批次细节再次参照图1，你可能很好奇，为什么向右的一些批次花费更长的时间才能完成（注意图1中的[F]）。你可以通过UI轻松的分析原因。首先，你可以点击时间轴视图中批处理时间比较长的点，这将会在页面下方产生一个关于完成批次的详细信息列表。 它将显示这个批次的所有主要信息（在上图3中以绿色高亮显示）。正如你所看到的，这个批次较之其他批次有更长的处理时间。另一个很明显的问题是：到底是哪个spark job引起了这个批次的处理时间过长。你可以通过点击Batch Time（第一列中的蓝色链接），这将带你看到对应批次的详细信息，向你展示输出操作和它们的spark job，正如图4所示。 图4显示有一个输出操作，它产生了3个spark job。你可以点击job ID链接继续深入到stages和tasks做更深入的分析。 4. Streaming RDDs的有向无环执行图一旦你开始分析批处理job产生的stages和tasks，更加深入的理解执行图将非常有用。正如之前的博文所说，Spark1.4.0加入了有向无环执行图（execution DAG ）的可视化（DAG即有向无环图），它显示了RDD的依赖关系链以及如何处理RDD和一系列相关的stages。如果在一个Streaming应用程序中，这些RDD是通过DStreams产生的，那么可视化将展示额外的Streaming语义。让我们从一个简单的Streaming字数统计（word count）程序开始，我们将统计每个批次接收的字数。程序示例 NetworkWordCount 。它使用DStream操作flatMap, map和 reduceByKey 来计算字数。任一个批次中一个Spark job的有向无环执行图将会是如下图5所示。 可视化展示中的黑点代表着在批处理时16:06:50由DStream产生的RDD。蓝色阴影的正方形是指用来转换RDD的DStream操作，粉色的方框代表这些转换操作执行的阶段。总之图5显示了如下信息： 数据是在批处理时间16:06:50通过一个socket文本流（ socket text stream ）接收的。 Job用了两个stage和flatMap , map , reduceByKey 转换操作来计算数据中的字数 尽管这是一个简单的图表，它可以通过增加更多的输入流和类似window操作和updateStateByKey操作等高级的DStream转换而变得更加复杂。例如，如果我们通过一个含三个批次的移动窗口来计算字数（即使用reduceByKeyAndWindow），它的数据来自两个socket文本流，那么，一个批处理job的有向无环执行图将会像如下图6所示。 图6显示了于一个跨3个批次统计字数的Spark job的许多相关信息： 前三个stage实际上是各自统计窗口中3个批次的字数。这有点像上面例子 NetworkWordCount 的第一个stage，使用的是map和flatmap操作。不过要注意以下不同点： 这里有两个输入RDD，分别来自两个socket文本流，这两个RDD通过union结合成一个RDD，然后进一步转换，产生每个批次的中间统计结果。 其中的两个stage都变灰了，因为两个较旧批次的中间结果已经缓存在内存中，因此不需要再次计算，只有最近的批次需要从头开始计算。最后一个右边的stage使用reduceByKeyAndWindow 来联合每个批次的统计字数最终形成一个“窗口”的字数。这些可视化使得开发人员不仅能够监控Streaming应用程序的状态和趋势，而且能够理解它们与底层spark job和执行计划的关系。 5. 未来方向Spark1.5.0中备受期待的一个重要提升是关于每个批次( JIRA , PR )中输入数据的更多信息。例如：如果你正在使用Kafka，批处理详细信息页面将会显示这个批次处理的topics, partitions和offsets，预览如下图： 原文发布时为2015.07.08 原文：https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html 译文：https://www.csdn.net/article/2015-07-15/2825214]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 启用压缩]]></title>
    <url>%2Fhive-enable-compression.html</url>
    <content type="text"><![CDATA[对于数据密集型任务，I/O操作和网络数据传输需要花费相当长的时间才能完成。通过在 Hive 中启用压缩功能，我们可以提高 Hive 查询的性能，并节省 HDFS 集群上的存储空间。 1. Hive中的可用压缩编解码器要在 Hive 中启用压缩，首先我们需要找出 Hadoop 集群上可用的压缩编解码器，我们可以使用下面的 set 命令列出可用的压缩编解码器。hive&gt; set io.compression.codecs;io.compression.codecs= org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec 2. 在中间数据上启用压缩提交后，一个复杂的 Hive 查询通常会转换为一系列多阶段 MapReduce 作业，这些作业将通过 Hive 引擎进行链接以完成整个查询。因此，这里的 ‘中间输出’ 是指前一个 MapReduce 作业的输出，将会作为下一个 MapReduce 作业的输入数据。 可以通过使用 Hive Shell 中的 set 命令或者修改 hive-site.xml 配置文件来修改 hive.exec.compress.intermediate 属性，这样我们就可以在 Hive Intermediate 输出上启用压缩。 &lt;property&gt; &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.intermediate.compression.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;description/&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.intermediate.compression.type&lt;/name&gt; &lt;value&gt;BLOCK&lt;/value&gt; &lt;description/&gt;&lt;/property&gt; 或者我们可以使用 set 命令在 hive shell 中设置这些属性，如下所示:hive&gt; set hive.exec.compress.intermediate=true;hive&gt; set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;hive&gt; set hive.intermediate.compression.type=BLOCK; 3. 在最终输出上启用压缩通过设置以下属性，我们可以在 Hive shell 中的最终输出上启用压缩:&lt;property&gt; &lt;name&gt;hive.exec.compress.output&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* &lt;/description&gt;&lt;/property&gt; 或者hive&gt; set hive.exec.compress.output=true;hive&gt; set mapreduce.output.fileoutputformat.compress=true;hive&gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec; hive&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK; 4. Example在下面的 shell 代码片段中，我们在 hive shell 中将压缩属性设置为 true 后，根据现有表 tmp_order_id 创建一个压缩后的表 tmp_order_id_compress。 为了对比，我们先根据现有表 tmp_order_id 创建一个不经压缩的表 tmp_order_id_2:CREATE TABLE tmp_order_id_2 ROW FORMAT DELIMITED LINES TERMINATED BY &apos;\n&apos;AS SELECT * FROM tmp_order_id; 我们看一下不设置压缩属性输出:[xiaosi@ying:~]$ sudo -uxiaosi hadoop fs -ls /user/hive/warehouse/xiaosidata.db/tmp_order_id_2/Found 1 items-rwxr-xr-x 3 xiaosi xiaosi 2565 2018-04-18 20:38 /user/hive/warehouse/hivedata.db/tmp_order_id_2/000000_0 在 Hive Shell 中设置压缩属性：hive&gt; set hive.exec.compress.output=true;hive&gt; set mapreduce.output.fileoutputformat.compress=true;hive&gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;hive&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;hive&gt; set hive.exec.compress.intermediate=true; 根据现有表 tmp_order_id 创建一个压缩后的表 tmp_order_id_compress:hive&gt; CREATE TABLE tmp_order_id_compress ROW FORMAT DELIMITED LINES TERMINATED BY &apos;\n&apos; &gt; AS SELECT * FROM tmp_order_id;Query ID = xiaosi_20180418204750_445d742f-be89-47fb-9d2c-2b0eeac76c09Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1504162679223_21624651, Tracking URL = http://xxx:9981/proxy/application_1504162679223_21624651/Kill Command = /home/xiaosi/hadoop-2.2.0/bin/hadoop job -kill job_1504162679223_21624651Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-04-18 20:47:59,575 Stage-1 map = 0%, reduce = 0%2018-04-18 20:48:04,711 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.28 secMapReduce Total cumulative CPU time: 1 seconds 280 msecEnded Job = job_1504162679223_21624651Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to directory hdfs://cluster/user/hive/warehouse/hivedata.db/.hive-staging_hive_2018-04-18_20-47-50_978_2339056359585839454-1/-ext-10002Moving data to directory hdfs://cluster/user/hive/warehouse/hivedata.db/tmp_order_id_compressMapReduce Jobs Launched:Stage-Stage-1: Map: 1 Cumulative CPU: 1.28 sec HDFS Read: 6012 HDFS Write: 1436 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 280 msecOKTime taken: 14.902 seconds 我们在看一下设置压缩属性后输出:[xiaosi@ying:~]$ sudo -uxiaosi hadoop fs -ls /user/hive/warehouse/hivedata.db/tmp_order_id_compress/Found 1 items-rwxr-xr-x 3 xiaosi xiaosi 1343 2018-04-18 20:48 /user/hive/warehouse/hivedata.db/tmp_order_id_compress/000000_0.gz 因此，我们可以使用 gzip 格式创建输出文件。 原文：http://hadooptutorial.info/enable-compression-in-hive/]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hive 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 共享变量]]></title>
    <url>%2Fspark-base-shared-variables.html</url>
    <content type="text"><![CDATA[通常情况下，传递给 Spark 操作（例如 map 或 reduce）的函数是在远程集群节点上执行的，函数中使用的变量，在多个节点上执行时是同一变量的多个副本。这些变量被拷贝到每台机器上，并且在远程机器上对变量的更新不会回传给驱动程序。在任务之间支持通用的，可读写的共享变量是效率是非常低的。所以，Spark 提供了两种类型的共享变量 : 广播变量（broadcast variables）和 累加器（accumulators）。 1. 广播变量广播变量允许程序员将一个只读的变量缓存到每台机器上，而不是给每个任务中传递一个副本。例如，使用它们我们可以以更有效的方式将一个比较大的输入数据集的副本传递给每个节点。Spark 还试图使用高效的广播算法来分发广播变量，以降低通信成本。 Spark 的 action 操作通过一系列 stage 进行执行，这些 stage 由分布式的 shuffle 操作拆分。Spark 会自动广播每个 stage 中任务所需的公共数据。这种情况下广播的数据以序列化的形式进行缓存，并在运行每个任务之前进行反序列化。这意味着只有当跨多个 stage 的任务需要相同的数据，或者以反序列化形式缓存数据非常重要时，显式创建广播变量才是有用的。 广播变量通过在一个变量 v 上调用 SparkContext.broadcast（v） 创建。广播变量是 v 的一个包装，广播变量的值可以通过调用 value 方法来访问。下面的代码显示了这一点： Java版本：Broadcast&lt;int[]&gt; broadcastVar = sc.broadcast(new int[] &#123;1, 2, 3&#125;);broadcastVar.value();// returns [1, 2, 3] Scala版本:scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)scala&gt; broadcastVar.valueres0: Array[Int] = Array(1, 2, 3) Python版本：&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 创建广播变量后，运行在集群上的任意函数中的值 v 可以使用广播变量来代替，以便 v 在节点上最多分发一次（v is not shipped to the nodes more than once）。另外，对象 v 在广播之后不应该被修改，以保证分发到所有的节点上的广播变量具有同样的值（例如，如果该变量稍后被传送到新的节点）。 2. 累加器累加器是一种仅通过关联和交换操作进行 add 的变量，因此可以在并行计算中得到高效的支持。累加器可以用来实现计数器（如在 MapReduce 中）或者求和。Spark 本身支持数字类型的累加器，程序员可以添加对新类型的支持。 作为使用者，你可以创建命名或未命名的累加器。如下图所示，命名累加器（在此为 counter 实例）会在 Web UI 中展示。 Spark 在 Tasks 任务表中显示由任务修改的每个累加器的值。 跟踪 UI 中的累加器对于理解运行的 stage 的进度很有用（注意：Python尚未支持）。 数值型的累加器可以通过调用 SparkContext.longAccumulator() 或 SparkContext.doubleAccumulator() 来创建，分别累加 Long 或 Double 类型的值。运行在集群上的任务可以使用 add 方法进行累加数值。但是，它们无法读取累加器的值。只有驱动程序可以通过使用 value 方法读取累加器的值。 下面的代码显示了一个累加器，用于累加数组的元素： Java版本:LongAccumulator accum = jsc.sc().longAccumulator();sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -&gt; accum.add(x));// ...// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 saccum.value();// returns 10 Scala版本:scala&gt; val accum = sc.longAccumulator("My Accumulator")accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))...10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 sscala&gt; accum.valueres2: Long = 10 此代码使用了内置的 Long 类型的累加器，我们还可以通过继承 AccumulatorV2 来创建我们自己的类型。 备注:在2.0.0之前的版本中，通过继承AccumulatorParam来实现，而2.0.0之后的版本需要继承AccumulatorV2来实现自定义类型的累加器。 AccumulatorV2 抽象类有几个方法必须重写： reset 将累加器重置为零 add 将另一个值添加到累加器中 merge 将另一个相同类型的累加器合并到该累加器中。 其他必须被覆盖的方法包含在API文档中。 例如，假设我们有一个表示数学上向量的 MyVector 类，我们可以这样写： Java版本:class VectorAccumulatorV2 implements AccumulatorV2&lt;MyVector, MyVector&gt; &#123; private MyVector myVector = MyVector.createZeroVector(); public void reset() &#123; myVector.reset(); &#125; public void add(MyVector v) &#123; myVector.add(v); &#125; ...&#125;// Then, create an Accumulator of this type:VectorAccumulatorV2 myVectorAcc = new VectorAccumulatorV2();// Then, register it into spark context:jsc.sc().register(myVectorAcc, "MyVectorAcc1"); Scala版本：class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] &#123; private val myVector: MyVector = MyVector.createZeroVector def reset(): Unit = &#123; myVector.reset() &#125; def add(v: MyVector): Unit = &#123; myVector.add(v) &#125; ...&#125;// Then, create an Accumulator of this type:val myVectorAcc = new VectorAccumulatorV2// Then, register it into spark context:sc.register(myVectorAcc, "MyVectorAcc1") 请注意，当程序员定义自己的 AccumulatorV2 类型时，返回值类型可以与添加的元素的类型不同。 对于在 action 中更新的累加器，Spark 会保证每个任务对累加器只更新一次，即使重新启动的任务也不会重新更新该值。而如果在 transformation 中更新的累加器，如果任务或作业 stage 被重新执行，那么其对累加器的更新可能会执行多次。 累加器不会改变 Spark 的懒加载（Lazy）的执行模型。如果在 RDD 上的某个操作中更新累加器，那么其值只会在 RDD 执行 action 计算时被更新一次。因此，在 transformation （例如， map()）中更新累加器时，其值并不能保证一定被更新。下面的代码片段演示了这个属性： Java版本:LongAccumulator accum = jsc.sc().longAccumulator();data.map(x -&gt; &#123; accum.add(x); return f(x); &#125;);// Here, accum is still 0 because no actions have caused the `map` to be computed. Scala版本:val accum = sc.longAccumulatordata.map &#123; x =&gt; accum.add(x); x &#125;// Here, accum is still 0 because no actions have caused the map operation to be computed. Python版本:accum = sc.accumulator(0)def g(x): accum.add(x) return f(x)data.map(g)# Here, accum is still 0 because no actions have caused the `map` to be computed. Spark 版本:2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#shared-variables]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用split命令分割文件]]></title>
    <url>%2Flinux-split-file-usage.html</url>
    <content type="text"><![CDATA[1. 概述split 命令可以将一个大文件分割成很多个小文件。在默认情况下将按照每1000行切割成一个小文件，默认前缀为 x。没有输入或输入为 - 时，从标准输入中读取。 2. 语法：split [OPTION]... [INPUT [PREFIX]] 3. 选项-l : 指定每多少行切成一个小文件。-b : 指定每多少字节切成一个小文件。-C : 每一输出档中，单行的最大 byte 数。-d ： 使用数字作为后缀。 4. 实例4.1 按行分割xiaosi@ying:~/test/input$ split -6 a.txt 或者xiaosi@ying:~/test/input$ split -l 6 a.txt 执行以上命令后，会将原来的大文件 a.txt 切割成多个以 x 开头的小文件。而在这些小文件中，每个文件都至多6行数据(最后一个文件有可能不满６行)。xiaosi@ying:~/test/input$ ll总用量 32drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:19 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xaa-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xab-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xac-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xad-rw-rw-r-- 1 xiaosi xiaosi 132 4月 8 18:19 xaexiaosi@ying:~/test/input$ cat a.txt | wc -l28xiaosi@ying:~/test/input$ cat xae | wc -l4 4.2 按文件大小分割xiaosi@ying:~/test/input$ split -b50M b.txt 执行以上命令后，会将原来的大文件 b.txt 切割成多个以 x 开头的小文件。而在这些小文件中，每个文件大小都为50M(最后一个文件有可能不满50M)。xiaosi@ying:~/test/input$ ll总用量 322296drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:25 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 165000000 4月 8 11:53 b.txt-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:25 xaa-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:25 xab-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:25 xac-rw-rw-r-- 1 xiaosi xiaosi 7713600 4月 8 18:25 xad 4.3 修改后缀上述示例中，文件被分割成多个带有字母的后缀文件，如果想用数字后缀可使用 -d 参数，同时可以使用 -a 来指定后缀的长度：xiaosi@ying:~/test/input$ split -b50M b.txt -d -a 3 执行以上命令后，会将原来的大文件 b.txt 切割成多个以 x 开头后面为数字的小文件：xiaosi@ying:~/test/input$ ll总用量 322296drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:36 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 165000000 4月 8 11:53 b.txt-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:36 x000-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:36 x001-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:36 x002-rw-rw-r-- 1 xiaosi xiaosi 7713600 4月 8 18:36 x003 4.4 指定输出文件名前缀xiaosi@ying:~/test/input$ split -b50M b.txt split_ 执行以上命令后，会将原来的大文件 b.txt 切割成多个以 split_ 开头的小文件：xiaosi@ying:~/test/input$ ll总用量 322296drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:41 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 165000000 4月 8 11:53 b.txt-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:41 split_aa-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:41 split_ab-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:41 split_ac-rw-rw-r-- 1 xiaosi xiaosi 7713600 4月 8 18:41 split_ad]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux 命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 使用spark-submit部署应用程序]]></title>
    <url>%2Fspark-base-launching-applications-with-spark-submit.html</url>
    <content type="text"><![CDATA[1. 简介Spark的 bin 目录中的 spark-submit 脚本用于在集群上启动应用程序。可以通过一个统一的接口使用 Spark 所有支持的集群管理器，因此不必为每个集群管理器专门配置你的应用程序。 2. 语法xiaosi@yoona:~/opt/spark-2.1.0-bin-hadoop2.7$ spark-submit --helpUsage: spark-submit [options] &lt;app jar | python file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (&quot;client&quot;) or on one of the worker machines inside the cluster (&quot;cluster&quot;) (Default: client). --class CLASS_NAME Your application&apos;s main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: &quot;default&quot;). --num-executors NUM Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM. --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. 3. 打包应用依赖如果你的代码依赖于其他项目，则需要将它们与应用程序一起打包，以便将代码分发到 Spark 集群上。为此，需要创建一个包含代码及其依赖关系的 assembly jar（或 Uber jar）。sbt 和 Maven 都有 assembly 插件。创建 assembly jar 时，将 Spark 和 Hadoop 的依赖设置为 provided。他们不需要打包，因为它们在运行时由集群管理器提供。一旦你有一个 assembly jar，你可以调用 bin/spark-submit 脚本，如下所示，同时传递你的 jar。 对于Python，你可以使用 spark-submit 的 --py-files 参数来添加 .py， .zip 或 .egg 文件来与应用程序一起分发。如果你依赖于多个 Python 文件，我们建议将它们打包成一个 .zip 或 .egg 文件。 4. 使用spark-submit启动应用程序用户应用程序打包成功后，就可以使用 bin/spark-submit 脚本启动应用程序。脚本负责设置 Spark 及其依赖关系的 classpath，并且可以支持不同集群管理器和部署模式（Spark所支持的）：./bin/spark-submit \ --class &lt;main-class&gt; \ --master &lt;master-url&gt; \ --deploy-mode &lt;deploy-mode&gt; \ --conf &lt;key&gt;=&lt;value&gt; \ ... # other options &lt;application-jar&gt; \ [application-arguments] 一些常用的选项： --class: 应用程序入口 (例如：com.sjf.open.spark.Java.JavaWordCount 包含包名的全路径名称) --master: 集群的 master URL (例如：spark://23.195.26.187:7077) --deploy-mode: 是在工作节点(cluster)上还是在本地作为一个外部的客户端(client)部署你的 driver (默认: client) --conf: 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。 application-jar: 包含应用程序和所有依赖关系的 jar 路径。URL必须在集群内部全局可见，例如，对所有节点上可见的 hdfs：// 路径或 file：// 路径。 application-arguments: 传递给主类 main 方法的参数（如果有的话） Example:bin/spark-submit --class com.sjf.open.spark.Java.JavaWordCount --master local common-tool-jar-with-dependencies.jar /home/xiaosi/click_uv.txt 常见的部署策略是将你的应用程序从与工作节点机器物理位置相同的网关机器（例如，独立EC2集群中的主节点）提交。在这种设置中， client 模式比较合适。在 client 模式中，驱动程序作为集群的客户端直接在 spark-submit 进程内启动。应用程序的输入和输出直接连到控制台。因此，这个模式特别适合那些涉及 REPL（例如，Spark shell）的应用程序。 如果你提交应用程序的机器远离工作节点机器（例如在笔记本电脑本地提交），则通常使用 cluster 模式来最小化 drivers 和 executors 之间的网络延迟。目前，对于 Python 应用程序而言，在独立模式上不支持集群模式。 对于Python应用程序，只需在 &lt;application-jar&gt; 位置传递一个 .py 文件来代替 JAR，然后使用 --py-files 参数将 Python 的 .zip，.egg 或 .py 文件添加到搜索路径。 有几个可用选项是特定用于集群管理器。例如，对于具有集群部署模式的Spark独立集群，可以指定 --supervise 参数以确保如果驱动程序以非零退出码失败时，可以自动重新启动。如果要列举 spark-submit 所有可用选项，可以使用 spark-submit --help 命令来查看。以下是常见选项的几个示例： # 在本地运行 8 核./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master local[8] \ /path/to/examples.jar \ 100# 以客户端部署模式在Spark独立集群上运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000# 在集群部署模式下使用supervise在Spark独立集群上运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000# 在 YARN 集群上运行export HADOOP_CONF_DIR=XXX./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ # can be client for client mode --executor-memory 20G \ --num-executors 50 \ /path/to/examples.jar \ 1000# 在 Spark 独立集群上运行Python程序./bin/spark-submit \ --master spark://207.184.161.138:7077 \ examples/src/main/python/pi.py \ 1000# 在集群部署模式下使用supervise在Mesos集群上运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master mesos://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ http://path/to/examples.jar \ 1000 5. Master Urls传递给Spark的master url 可以采用如下格式： Master URL 描述 local 使用一个线程本地运行 Spark。 local[K] 使用K个工作线程本地运行 Spark（理想情况下，设置这个值的数量为你机器内核数量）。 local[K,F] 使用K工作线程和F个 maxFailures 在本地运行 Spark（有关此变量的解释，请参阅spark.task.maxFailures） local[*] 使用与你机器上的逻辑内核一样多的工作线程在本地运行 Spark。 local[*,F] 使用与你机器上的逻辑内核一样多的工作线程以及F个 maxFailures 在本地运行 Spark。 spark://HOST:PORT 连接到给定的Spark独立集群主机。端口必须是主机配置可使用的端口，默认情况下为7077。 spark://HOST1:PORT1,HOST2:PORT2 使用 Zookeeper 连接到具有备用 masters 的给定 Spark 独立集群。该列表必须包含使用 Zookeeper 搭建的高可用集群中的所有 master 主机。端口必须是每个 master 可以配置使用的端口，默认情况下为7077。 mesos://HOST:PORT 连接到给定的Mesos集群。端口必须是主机配置可使用的端口，默认为5050。或者，对于使用ZooKeeper的Mesos集群，借助 --deploy-mode cluster 参数使用 mesos://zk:// .... 提交。 yarn 以客户端模式还是以集群模式连接到YARN群集具体取决于 --deploy-mode 的值。可以根据HADOOP_CONF_DIR或YARN_CONF_DIR变量找到集群位置 6. 从文件加载配置spark-submit 脚本可以从 properties 文件加载默认 Spark 配置选项，并将它们传递到应用程序。默认情况下，spark 从 spark 目录下的 conf/spark-defaults.conf 配置文件中读取配置选项。有关更多详细信息，请参考加载默认配置。 以这种方式加载 Spark 默认配置可以避免在 spark-submit 上添加配置选项。例如，如果默认配置文件中设置了 spark.master 属性，那么可以安全地从 spark-submit 中省略 --master 参数。一般来说，在 SparkConf 上显式设置的配置选项拥有最高优先级，然后是传递到 spark-submit 的配置选项，最后是默认配置文件中的配置选项。 如果不清楚配置选项来自哪里，可以通过使用 --verbose 选项运行 spark-submit 打印出细粒度的调试信息。 7. 高级依赖管理使用 spark-submit 时，包含在 --jars 选项中的应用程序 jar 以及其他 jar 将自动分发到集群。在 --jars 之后提供的 URL 列表必须用逗号分隔。该列表会包含在 driver 和 executor 的 classpath 中。--jars 不支持目录的形式。 Spark使用如下URL来允许以不同策略分发 jar： file : 绝对路径和 file:/URI 通过 driver 的HTTP文件服务器提供，每个 executor 从 driver HTTP服务器上拉取文件。 hdfs : http :, https :, ftp： 正如你希望的一样，从这些URI拉取文件和 JAR。 local : 以 local:/ 开头的URI应该作为每个工作节点上的本地文件存在。这意味着不会产生网络IO，适用于推送大文件或者JAR到每个工作线程或通过 NFS，GlusterFS 等方式共享这些大文件或者jar。 请注意，JAR和文件被复制到 executor 节点上每个 SparkContext 的工作目录下。随着时间的推移，这可能会占用大量的空间，需要定时清理。使用 YARN，清理会自动执行；使用 Spark 独立集群，可以使用 spark.worker.cleanup.appDataTtl 属性配置自动清理。 用户还可以通过用 --packages 提供以逗号分隔的 maven 坐标列表来包含任何其他依赖项。使用此命令时将处理所有传递依赖性。可以使用配置选项 --repositories 以逗号分隔的方式添加其他存储库（或SBT中的解析器）。pyspark，spark-shell和 spark-submit 都可以使用这些命令来包含 Spark 的 Packages。 对于Python，等价的 --py-files 选项可用于将 .egg，.zip 和 .py 库分发给执行程序。 Spark版本:2.3.0 原文:http://spark.apache.org/docs/2.3.0/submitting-applications.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之数据倾斜调优]]></title>
    <url>%2Fspark-performance-data-skew-tuning.html</url>
    <content type="text"><![CDATA[1. 为何要处理数据倾斜1.1 什么是数据倾斜对Spark/Hadoop这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。 何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 1.2 数据倾斜是如何造成的在Spark中，同一个Stage的不同Partition可以并行处理，而具体依赖关系的不同Stage之间是串行处理的。假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。 由于同一个Stage内的所有Task执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该Task所处理的数据量决定。 Stage的数据来源主要分为如下两类： 从数据源直接读取。如读取HDFS，Kafka 读取上一个Stage的Shuffle数据 2. 如何缓解/消除数据倾斜2.1 尽量避免数据源的数据倾斜以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。 Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。 但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的PV信息置于同一个Partition中。此时，如果产生了数据倾斜，则需要通过其它方式处理。 2.2 调整并行度分散同一个Task的不同Key2.2.1 原理Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。 如果调整Shuffle时的并行度，使得原本被分配到同一Task的不同Key发配到不同Task上处理，则可降低原Task所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。 2.2.2 案例现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5亿条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如下INSERT OVERWRITE TABLE testSELECT CASE WHEN id &lt; 940000000 THEN (9500000 + (CAST (RAND() * 8 AS INTEGER)) * 12 ) ELSE CAST(id/100 AS INTEGER) END, nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000; 通过上述处理，一份可能造成后续数据倾斜的测试数据即以准备好。接下来，使用Spark读取该测试数据，并通过groupByKey(12)对id分组处理，且Shuffle并行度为12。代码如下public class SparkDataSkew &#123; public static void main(String[] args) &#123; SparkSession sparkSession = SparkSession.builder() .appName("SparkDataSkewTunning") .config("hive.metastore.uris", "thrift://hadoop1:9083") .enableHiveSupport() .getOrCreate(); Dataset&lt;Row&gt; dataframe = sparkSession.sql( "select * from test"); dataframe.toJavaRDD() .mapToPair((Row row) -&gt; new Tuple2&lt;Integer, String&gt;(row.getInt(0),row.getString(1))) .groupByKey(12) .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123; int id = tuple._1(); AtomicInteger atomicInteger = new AtomicInteger(0); tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet()); return new Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get()); &#125;).count(); sparkSession.stop(); sparkSession.close(); &#125;&#125; 本次实验所使用集群节点数为4，每个节点可被Yarn使用的CPU核数为16，内存为16GB。使用如下方式提交上述应用，将启动4个Executor，每个Executor可使用核数为12（该配置并非生产环境下的最优配置，仅用于本文实验），可用内存为12GB。spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar GroupBy Stage的Task状态如下图所示，Task 8处理的记录数为4500万，远大于（9倍于）其它11个Task处理的500万记录。而Task 8所耗费的时间为38秒，远高于其它11个Task的平均时间（16秒）。整个Stage的时间也为38秒，该时间主要由最慢的Task 8决定。 在这种情况下，可以通过调整Shuffle并行度，使得原来被分配到同一个Task（即该例中的Task 8）的不同Key分配到不同Task，从而降低Task 8所需处理的数据量，缓解数据倾斜。 通过groupByKey(48)将Shuffle并行度调整为48，重新提交到Spark。新的Job的GroupBy Stage所有Task状态如下图所示。 从上图可知，记录数最多的Task 20处理的记录数约为1125万，相比于并行度为12时Task 8的4500万，降低了75%左右，而其耗时从原来Task 8的38秒降到了24秒。 在这种场景下，调整并行度，并不意味着一定要增加并行度，也可能是减小并行度。如果通过groupByKey(11)将Shuffle并行度调整为11，重新提交到Spark。新Job的GroupBy Stage的所有Task状态如下图所示。 从上图可见，处理记录数最多的Task 6所处理的记录数约为1045万，耗时为23秒。处理记录数最少的Task 1处理的记录数约为545万，耗时12秒。 2.2.3 总结(1) 适用场景 大量不同的Key被分配到了相同的Task造成该Task数据量过大。 (2) 解决方案 调整并行度。一般是增大并行度，但有时如本例减小并行度也可达到效果。 (3) 优势 实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用 spark.default.parallelism 设置。如果是Spark SQL，还可通过 SET spark.sql.shuffle.partitions=[num_tasks] 设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。 (4) 劣势 适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般 2.3 自定义Partitioner2.3.1 原理使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。 2.3.2 案例以上述数据集为例，继续将并发度设置为12，但是在groupByKey算子上，使用自定义的Partitioner（实现如下）.groupByKey(new Partitioner() &#123; @Override public int numPartitions() &#123; return 12; &#125; @Override public int getPartition(Object key) &#123; int id = Integer.parseInt(key.toString()); if(id &gt;= 9500000 &amp;&amp; id &lt;= 9500084 &amp;&amp; ((id - 9500000) % 12) == 0) &#123; return (id - 9500000) / 12; &#125; else &#123; return id % 12; &#125; &#125;&#125;) 由下图可见，使用自定义Partition后，耗时最长的Task 6处理约1000万条数据，用时15秒。并且各Task所处理的数据集大小相当。 2.3.3 总结(1) 适用场景 大量不同的Key被分配到了相同的Task造成该Task数据量过大。 (2) 解决方案 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。 (3) 优势 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。 (4) 劣势 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 2.4 将Reduce side Join转变为Map side Join2.4.1 原理通过Spark的Broadcast机制，将Reduce侧Join转化为Map侧Join，避免Shuffle从而完全消除Shuffle带来的数据倾斜。 2.4.2 案例通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。INSERT OVERWRITE TABLE testSELECT CAST(CASE WHEN id &lt; 980000000 THEN (95000000 + (CAST (RAND() * 4 AS INT) + 1) * 48 ) ELSE CAST(id/10 AS INT) END AS STRING), nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000; 使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。INSERT OVERWRITE TABLE test_newSELECT CAST(CAST(id/10 AS INT) AS STRING), nameFROM student_delta_externalWHERE id BETWEEN 950000000 AND 950500000; 直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id; 该SQL对应的DAG如下图所示。从该图可见，该执行过程总共分为三个Stage，前两个用于从Hive中读取数据，同时二者进行Shuffle，通过最后一个Stage进行Join并将结果写入表test_join中。 从下图可见，最近Join Stage各Task处理的数据倾斜严重，处理数据量最大的Task耗时7.1分钟，远高于其它无数据倾斜的Task约2s秒的耗时。 接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过CACHE TABLE test_new将小表test_new进行cache。现通过如下SQL进行Join。CACHE TABLE test_new;INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id; 通过如下DAG图可见，该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。 并且数据倾斜仍然存在。如下图所示，最慢的Task耗时为7.1分钟，远高于其它Task的约2秒。 正确的使用Broadcast实现Map侧Join的方式是，通过SET spark.sql.autoBroadcastJoinThreshold=104857600;将Broadcast的阈值设置得足够大。 再次通过如下SQL进行Join。SET spark.sql.autoBroadcastJoinThreshold=104857600;INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id; 通过如下DAG图可见，该方案只包含一个Stage。 并且从下图可见，各Task耗时相当，无明显数据倾斜现象。并且总耗时为1.5分钟，远低于Reduce侧Join的7.3分钟。 2.4.3 总结(1) 适用场景 参与Join的一边数据集足够小，可被加载进Driver并通过Broadcast方法广播到各个Executor中。 (2) 优势 避免了Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。 (4) 劣势 要求参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。 2.5 为skew的key增加随机前/后缀2.5.1 原理为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。 2.5.2 案例通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。 对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。INSERT OVERWRITE TABLE testSELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000 + (CAST (RAND() * 2 AS INT) + 1) * 48 ) ELSE CAST(id/100 AS INT) END AS STRING), nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000;INSERT OVERWRITE TABLE test_newSELECT CAST(CAST(id/100 AS INT) AS STRING), nameFROM student_delta_externalWHERE id BETWEEN 950000000 AND 950500000; 通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。public class SparkDataSkew&#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setAppName("DemoSparkDataFrameWithSkewedBigTableDirect"); sparkConf.set("spark.default.parallelism", parallelism + ""); JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); leftRDD.join(rightRDD, parallelism) .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2())) .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123; AtomicInteger atomicInteger = new AtomicInteger(); iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet()); &#125;); javaSparkContext.stop(); javaSparkContext.close(); &#125;&#125; 从下图可看出，整个Join耗时1分54秒，其中Join Stage耗时1.7分钟。 通过分析Join Stage的所有Task可知，在其它Task所处理记录数为192.71万的同时Task 32的处理的记录数为992.72万，故它耗时为1.7分钟，远高于其它Task的约10秒。这与上文准备数据集时，将id为9500048为9500096对应的数据量设置非常大，其它id对应的数据集非常均匀相符合。 现通过如下操作，实现倾斜Key的分散处理 将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD 将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD 将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD 将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD 对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD 通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集具体实现代码如下public class SparkDataSkew&#123; public static void main(String[] args) &#123; int parallelism = 48; SparkConf sparkConf = new SparkConf(); sparkConf.setAppName("SolveDataSkewWithRandomPrefix"); sparkConf.set("spark.default.parallelism", parallelism + ""); JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); String[] skewedKeyArray = new String[]&#123;"9500048", "9500096"&#125;; Set&lt;String&gt; skewedKeySet = new HashSet&lt;String&gt;(); List&lt;String&gt; addList = new ArrayList&lt;String&gt;(); for(int i = 1; i &lt;=24; i++) &#123; addList.add(i + ""); &#125; for(String key : skewedKeyArray) &#123; skewedKeySet.add(key); &#125; Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys = javaSparkContext.broadcast(skewedKeySet); Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList); JavaPairRDD&lt;String, String&gt; leftSkewRDD = leftRDD .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1())) .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + "," + tuple._1(), tuple._2())); JavaPairRDD&lt;String, String&gt; rightSkewRDD = rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1())) .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream() .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + "," + tuple._1(), tuple._2())) .collect(Collectors.toList()) .iterator() ); JavaPairRDD&lt;String, String&gt; skewedJoinRDD = leftSkewRDD .join(rightSkewRDD, parallelism) .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(",")[1], tuple._2()._2())); JavaPairRDD&lt;String, String&gt; leftUnSkewRDD = leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1())); JavaPairRDD&lt;String, String&gt; unskewedJoinRDD = leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2())); skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123; AtomicInteger atomicInteger = new AtomicInteger(); iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet()); &#125;); javaSparkContext.stop(); javaSparkContext.close(); &#125;&#125; 从下图可看出，整个Join耗时58秒，其中Join Stage耗时33秒。 通过分析Join Stage的所有Task可知 由于Join分倾斜数据集Join和非倾斜数据集Join，而各Join的并行度均为48，故总的并行度为96 由于提交任务时，设置的Executor个数为4，每个Executor的core数为12，故可用Core数为48，所以前48个Task同时启动（其Launch时间相同），后48个Task的启动时间各不相同（等待前面的Task结束才开始） 由于倾斜Key被加上随机前缀，原本相同的Key变为不同的Key，被分散到不同的Task处理，故在所有Task中，未发现所处理数据集明显高于其它Task的情况 实际上，由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。而本实验受限于可用总核数为48，可同时运行的总Task数为48，故而该方案只是将总耗时减少一半（效率提升一倍）。如果资源充足，可并发执行Task数增多，该方案的优势将更为明显。在实际项目中，该方案往往可提升数倍至10倍的效率。 2.5.3 总结(1) 适用场景 两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。 (2) 解决方案 将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。 (3) 优势 相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。 (4) 劣势 如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。 2.6 大表随机添加N种随机前缀，小表扩大N倍2.6.1 原理如果出现数据倾斜的Key比较多，上一种方法将这些大量的倾斜Key分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。 2.6.2 案例这里给出示例代码，读者可参考上文中分拆出少数倾斜Key添加随机前缀的方法，自行测试。public class SparkDataSkew &#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setAppName("ResolveDataSkewWithNAndRandom"); sparkConf.set("spark.default.parallelism", parallelism + ""); JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); List&lt;String&gt; addList = new ArrayList&lt;String&gt;(); for(int i = 1; i &lt;=48; i++) &#123; addList.add(i + ""); &#125; Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList); JavaPairRDD&lt;String, String&gt; leftRandomRDD = leftRDD.mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(new Random().nextInt(48) + "," + tuple._1(), tuple._2())); JavaPairRDD&lt;String, String&gt; rightNewRDD = rightRDD .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream() .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + "," + tuple._1(), tuple._2())) .collect(Collectors.toList()) .iterator() ); JavaPairRDD&lt;String, String&gt; joinRDD = leftRandomRDD .join(rightNewRDD, parallelism) .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(",")[1], tuple._2()._2())); joinRDD.foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123; AtomicInteger atomicInteger = new AtomicInteger(); iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet()); &#125;); javaSparkContext.stop(); javaSparkContext.close(); &#125;&#125; 2.6.3 总结(1) 适用场景 一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。 (2) 优势 对大部分场景都适用，效果不错。 (3) 劣势 需要将一个数据集整体扩大N倍，会增加资源消耗。 3. 总结对于数据倾斜，并无一个统一的一劳永逸的方法。更多的时候，是结合数据特点（数据集大小，倾斜Key的多少等）综合使用上文所述的多种方法。 原文:http://www.infoq.com/cn/articles/the-road-of-spark-performance-tuning]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 Input DStreams和Receivers]]></title>
    <url>%2Fspark-streaming-input-dstreams-and-receivers.html</url>
    <content type="text"><![CDATA[1. 输入DStream与Receiver输入 DStreams 表示从 source 中获取输入数据流的 DStreams。在入门示例中，lines 表示输入DStream，它代表从netcat服务器获取的数据流。每一个输入DStream(除 file stream)都与一个 Receiver (接收器)相关联，接收器从 source 中获取数据，并将数据存入 Spark 内存中来进行处理。输入 DStreams 表示从数据源获取的原始数据流。Spark Streaming 提供了两类内置的流源（streaming sources）： 基础数据源(Basic sources)：在 StreamingContext API 中可以直接使用的数据源。例如：文件系统(file system)和套接字连接(socket connections)。 高级数据源(Advanced sources)：例如 Kafka，Flume，Kinesis 等数据源可通过额外的utility classes获得。这些需要额外依赖。 我们将稍后讨论这两类数据源。 请注意，如果希望在流应用程序中并行的接收多个数据流，你可以创建多个输入 DStream（在性能调优部分中进一步讨论）。这需要创建多个接收器（Receivers），来同时接收多个数据流。但请注意，Spark 的 worker/executor 是一个长期运行的任务，因此会占用分配给 Spark Streaming 应用程序的其中一个核（core）。因此，记住重要的一点，Spark Streaming 应用程序需要分配足够的核（或线程，如果在本地运行）来处理接收的数据，以及来运行接收器。 注意 当在本地运行 Spark Streaming 程序时，不要使用 local 或 local [1] 作为 master 的 URL。这两个都意味着只会有一个线程用于本地任务运行。如果使用基于接收器（例如套接字，Kafka，Flume等）的输入 DStream，那么唯一的那个线程会用于运行接收器，不会有其他线程来处理接收到的数据。因此，在本地运行时，始终使用 local [n] 作为 master 的 URL，其中 n &gt; 要运行的接收器的数目。 将逻辑扩展到集群上运行，分配给 Spark Streaming 应用程序的核数量必须大于接收器的数量。否则系统将只接收数据，而无法处理。 2. 源2.1 基础数据源在入门实例中我们已经了解到 ssc.socketTextStream（...），它通过 TCP 套接字连接从数据服务器获取文本数据创建 DStream。除了套接字，StreamingContext API 也提供了把文件作为输入源创建 DStreams 的方法。 2.1.1 File Streams可以从与 HDFS API 兼容的任何文件系统（即，HDFS，S3，NFS等）上的文件读取数据，DStream 可以使用如下命令创建： Java:streamingContext.fileStream&lt;KeyClass, ValueClass, InputFormatClass&gt;(dataDirectory); Scala:streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) Spark Streaming 会监视 dataDirectory 目录并处理在该目录中创建的任何文件（不支持嵌套目录中写入的文件）。 注意 所有文件必须具有相同的数据格式 通过原子地移动或重命名它们到数据目录中，来在dataDirectory目录下创建文件。 一旦移动到dataDirectory目录后，不能进行更改。因此，如果文件被连续追加数据，新的数据将不会被读取。 对于简单的文本文件，有一个更简单的方法：streamingContext.textFileStream（dataDirectory） 文件流不需要运行接收器（Receiver），因此不需要分配核。 fileStream 在 Python API 中不可用，只有 textFileStream 可用。 2.1.2 基于自定义的Receivers的流可以使用通过自定义的接收器接收的数据流创建 DStream。有关详细信息，请参阅自定义接收器指南。 2.1.3 RDD队列作为一个流要使用测试数据测试 Spark Streaming 应用程序，还可以使用 streamingContext.queueStream（queueOfRDDs） 基于 RDD 队列创建 DStream。 推送到队列中的每个 RDD 将被视为 DStream 中的一批次数据，并像流一样处理。 2.2 高级数据源这类数据源需要使用非Spark库的外部接口，其中一些需要复杂依赖（例如，Kafka和Flume）。因此，为了尽量减少依赖的版本冲突问题，这些数据源本身不能创建 DStream 的功能，它是通过 依赖 单独的类库实现创建 DStream 的功能。 请注意，这些高级源在 Spark Shell 中不可用，因此基于这些高级数据源的应用程序无法在 shell 中测试。如果你真的想在 Spark shell 中使用它们，那么你必须下载相应的 Maven 组件的JAR及其依赖项，并将其添加到 classpath 中。 介绍一下常用的高级数据源： Kafka：Spark Streaming 2.1.0与Kafka代理版本0.8.2.1或更高版本兼容。 有关更多详细信息，请参阅Kafka集成指南。 Flume：Spark Streaming 2.1.0与Flume 1.6.0兼容。 有关更多详细信息，请参阅Flume集成指南。 Kinesis：Spark Streaming 2.1.0与Kinesis Client Library 1.2.1兼容。 有关更多详细信息，请参阅Kinesis集成指南。 3. 自定义数据源这在Python中还不支持。 输入DStreams也可以从自定义数据源中创建。如果你这样做，需要实现一个自定义接收器（Receiver），可以从自定义数据源接收数据，并推送到Spark。有关详细信息，请参阅自定义接收器指南。 4. Receiver的可靠性基于Receiver的可靠性，可以分为两种数据源。如Kafka和Flume之类的数据源允许传输的数据被确认。如果从这些可靠源接收数据，并且被确认正确的接收数据，则可以确保不会由于任何种类的故障而丢失数据。这样就出现了两种接收器（Receiver）： 可靠的接收器 - 当数据被接收并存储在Spark中，同时备份副本，可靠的接收器正确地向可靠的源发送确认。 不可靠的接收器 - 不可靠的接收器不会向数据源发送确认。这可以用在不支持确认机制的数据源上，或者甚至是可靠的数据源当你不想或者不需要进行复杂的确认的时候。 Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#input-dstreams-and-receivers]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 DStreams]]></title>
    <url>%2Fspark-streaming-discretized-streams-dstreams.html</url>
    <content type="text"><![CDATA[离散流或者 DStreams 是 Spark Streaming 提供的基本抽象，它代表一个连续的数据流。从 source 中获取输入流，或者是输入流通过转换算子处理后生成的数据流。在内部，DStreams 由一系列连续的 RDD 组成。它是 Spark 中一个不可改变的抽象，分布式数据集的抽象（更多细节参见Spark编程指南）。DStream 中的每个 RDD 包含来自特定间隔的数据，如下图所示： 对 DStream 应用的任何操作都会转换为对 DStream 底层的 RDD 操作。例如，在之前的示例中将行数据流转换单词数据流，flatMap 操作应用于 lines 这个 DStreams 中的每个 RDD，生成 words 这个 DStreams 的 RDD。过程如下图所示： 这些底层的 RDD 转换操作由 Spark 引擎计算。DStream 操作隐藏了大部分细节，并为开发人员提供了更高级别的API以方便使用。这些操作将在后面的章节中详细讨论。 Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#discretized-streams-dstreams]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 初始化StreamingContext]]></title>
    <url>%2Fspark-streaming-initializing-streamingcontext.html</url>
    <content type="text"><![CDATA[为了初始化 Spark Streaming 程序，必须创建一个 StreamingContext 对象，它是 Spark Streaming 所有流操作的主要入口。StreamingContext 对象可以用 SparkConf 对象创建。 可以使用SparkConf对象创建JavaStreamingContext对象（对于Scala和Python语言来说，创建 StreamingContext对象）： Java版本:SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(seconds)); Scala版本:import org.apache.spark._import org.apache.spark.streaming._val conf = new SparkConf().setAppName(appName).setMaster(master)val ssc = new StreamingContext(conf, Seconds(1)) Python:from pyspark import SparkContextfrom pyspark.streaming import StreamingContextsc = SparkContext(master, appName)ssc = StreamingContext(sc, 1) appName 参数是应用程序在集群UI上显示的名称。master 是Spark，Mesos或YARN集群URL，或者是以本地模式运行的特殊字符串local [*]。 实际上，当在集群上运行时，如果你不想在程序中硬编码 master(即在程序中写死)，而是希望使用 spark-submit 启动应用程序时得到 master 的值。对于本地测试和单元测试，你可以传递 local [*] 来运行 Spark Streaming 进程。注意，这里内部创建的 JavaSparkContext（所有Spark功能的起始点），可以通过 jsc.sparkContext 访问。 JavaStreamingContext对象也可以从现有的JavaSparkContext创建。对于Scala来说，StreamingContext对象也可以从现有的 SparkContext 创建： Java版本:SparkConf conf = new SparkConf().setAppName("socket-spark-stream").setMaster("local[2]");JavaSparkContext sparkContext = new JavaSparkContext(conf);JavaStreamingContext jsc = new JavaStreamingContext(sparkContext, Durations.seconds(seconds)); Scala版本:import org.apache.spark.streaming._val sc = ... // existing SparkContextval ssc = new StreamingContext(sc, Seconds(1)) 批处理间隔必须根据应用程序和可用群集资源的延迟要求进行设置。 有关更多详细信息，请参阅“性能调优”部分。 定义上下文后，您必须执行以下操作： 通过创建输入DStreams定义输入源 通过对DStreams应用转换操作（transformation）和输出操作（output）来定义流计算 可以使用streamingContext.start()方法接收和处理数据 可以使用streamingContext.awaitTermination()方法等待流计算完成（手动或由于任何错误），来防止应用退出 可以使用streamingContext.stop（）手动停止处理。 注意点: 一旦上下文已经开始，则不能设置或添加新的流计算。 上下文停止后，无法重新启动。 在同一时间只有一个StreamingContext可以在JVM中处于活动状态。 在StreamingContext上调用stop()方法，也会关闭SparkContext对象。如果只想关闭StreamingContext对象，设置stop()的可选参数为false。 一个SparkContext可以重复利用创建多个StreamingContext，只要在创建下一个StreamingContext之前停止前一个StreamingContext（而不停止SparkContext）即可。 Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#initializing-streamingcontext]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 Example]]></title>
    <url>%2Fspark-streaming-first-example.html</url>
    <content type="text"><![CDATA[1. 概述Spark Streaming 是 Spark Core API的一个扩展，它对实时流式数据的处理具有可扩展性、高吞吐量、可容错性等特点。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等许多源中提取，并且可以使用由诸如map，reduce，join或者 window 等高级函数组成的复杂算法来处理。最后，处理后的数据可以推送到文件系统、数据库、实时仪表盘中。事实上，你可以将处理后的数据应用到 Spark 的机器学习算法、 图处理算法中去。 它的内部工作原理如下图所示。Spark Streaming 接收实时输入数据流，并将数据分成多个批次，然后由 Spark 引擎处理，批量生成最终结果数据流。 Spark Streaming 提供了一个叫做离散流(discretized stream)或称作 DStream 的高级抽象，它表示连续的数据流。DStreams 可以从如 Kafka，Flume和 Kinesis 等数据源的输入数据流创建，也可以通过对其他 DStreams 应用高级操作来创建。在内部，DStream 表示为 RDD 序列，即由一系列的 RDD 组成。 本文章介绍如何使用 DStreams 编写 Spark Streaming 程序。 可以在Scala，Java或Python（在Spark 1.2中介绍）中编写Spark Streaming程序，本文只要使用Java作为演示示例，其他可以参考原文。 2. Example在我们进入如何编写自己的Spark Streaming程序之前，让我们快速看看一个简单的Spark Streaming程序的具体样子。 假设我们要计算从监听TCP套接字的数据服务器接收的文本数据中的统计文本中包含的单词数。 首先，我们创建一个JavaStreamingContext对象，这是所有流功能的主要入口点。 我们创建一个具有两个执行线程的本地StreamingContext，并且批处理间隔为1秒。 import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaStreamingContext;SparkConf conf = new SparkConf().setAppName("socket-spark-stream").setMaster("local[2]");JavaSparkContext sparkContext = new JavaSparkContext(conf);JavaStreamingContext jsc = new JavaStreamingContext(sparkContext, Durations.seconds(1)); 使用此context，我们可以创建一个DStream，表示来自TCP源的流数据，指定主机名（例如localhost）和端口（例如7777）: import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;private static String hostName = "localhost";private static int port = 7777;// 以端口7777作为输入源创建DStreamJavaReceiverInputDStream&lt;String&gt; lines = jsc.socketTextStream(hostName, port); lines DStream表示从数据服务器接收的数据流。 此流中的每个记录都是一行文本。 然后，我们要将每行文本切分为单词：// 从DStream中将每行文本切分为单词JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String x) &#123; return Arrays.asList(x.split(" ")).iterator(); &#125;&#125;); flatMap是一个DStream操作，通过从源DStream中的每个记录生成多个新记录来创建新的DStream。 在我们例子中，每一行将被拆分成多个单词，并且单词数据流用 words 这个DStream来表示。 注意，我们使用FlatMapFunction对象定义了一个转换操作。 正如我们将会发现，在Java API中有许多这样的类帮主我们定义DStream转换操作。 下一步，我们计算单词的个数：// 在每个批次中计算单词的个数JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;&gt;(s, 1); &#125;&#125;);JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125;&#125;);// 将此DStream中生成的每个RDD的前10个元素打印到控制台wordCounts.print(); 使用PairFunction对象将words 这个DStream进一步映射（一对一变换）为（word，1）键值对的DStream。 然后，使用Function2对象，计算得到每批次数据中的单词出现的频率。 最后，wordCounts.print()将打印每秒计算的词频。 这只是设定好了要进行的计算，系统收到数据时计算就会开始。要开始接收数据，必须显式调用StreamingContext的start()方法。这样，SparkStreaming 就会开始把Spark作业不断的交给SparkContext去调度。执行会在另一个线程中进行，所以需要调用awaitTermination来等待流计算完成，来防止应用退出。 // 启动流计算环境StreamingContext并等待完成jsc.start();// 等待作业完成jsc.awaitTermination(); 注意一个Streaming context 只启动一次，所以只有在配置好所有DStream以及所需的操作之后才能启动。 如果你已经下载和构建了Spark环境，你就能够用如下的方法运行这个例子。首先，你需要运行Netcat作为数据服务器：xiaosi@yoona:~$ nc -lk 7777hello I am yoona hello... 然后，在不同的终端，你能够用如下方式运行例子:xiaosi@yoona:~/opt/spark-2.1.0-bin-hadoop2.7$ bin/spark-submit --class com.sjf.open.spark.stream.SocketSparkStreaming /home/xiaosi/code/Common-Tool/target/common-tool-jar-with-dependencies.jar 输出信息：-------------------------------------------Time: 1488348756000 ms-------------------------------------------(am,1)(,1)(yoona,1)(hello,2)(I,1) 3. Maven依赖与Spark类似，Spark Streaming通过Maven Central提供。 要编写自己的Spark Streaming程序，您必须将以下依赖项添加到Maven项目中。&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 对于Spark Streaming核心API中不存在的来源（如Kafka，Flume和Kinesis）获取数据，您必须将相应的组件 spark-streaming-xyz_2.11 添加到依赖项中。 例如，一些常见的如下： Source Artifact Kafka spark-streaming-kafka-0-8_2.11 Flume spark-streaming-flume_2.11 Kinesis spark-streaming-kinesis-asl_2.11 [Amazon Software License] 为了获取最新的列表，请访问Apache repository Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#a-quick-example]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming与Kafka如何保证数据零丢失]]></title>
    <url>%2Fspark-streaming-kafka-integration-achieve-zero-data-loss.html</url>
    <content type="text"><![CDATA[Spark Streaming 是一种构建在 Spark 上的实时计算框架，它扩展了 Spark 处理大规模流式数据的能力。Spark Streaming 的优势在于： 能运行在1000+的结点上，并达到秒级延迟。 使用基于内存的 Spark 作为执行引擎，具有高效和容错的特性。 能集成 Spark 的批处理和交互查询。 为实现复杂的算法提供和批处理类似的简单接口。 为此，Spark Streaming受到众多企业的追捧，并将其大量用于生产项目；然而，在使用过程中存在一些辣手的问题。本文将介绍使用Spark Streaming进行实时处理的一个关于保证数据零丢失的经验。 在Spark Streaming的生产实践中，要做到数据零丢失，你需要满足以下几个先决条件： 输入的数据源是可靠的； 数据接收器是可靠的； 元数据持久化; 启用了WAL特性（Write ahead log）； Exactly-Once。 下面将简单地介绍这些先决条件。 1. 输入的数据源是可靠的Spark Streaming实时处理数据零丢失，需要类似Kafka的数据源： 支持在一定时间范围内重新消费； 支持高可用消费； 支持消费确认机制； 具有这些特征的数据源，可以使得消费程序准确控制消费位置，做到完全掌控消费情况的程度，为数据零丢失打下基础。 2. 数据接收器是可靠的Spark Streaming可以对已经接收的数据进行确认。输入的数据首先被接收器（Receivers）所接收，然后存储到Spark内部。数据一旦存储到Spark中，接收器可以对它进行确认。这种机制保证了在接收器突然挂掉的情况下也不会丢失数据：因为数据虽然被接收，但是没有被持久化的情况下是不会发送确认消息的。所以在接收器恢复的时候，数据可以被原端重新发送。 3. 元数据持久化可靠的数据源和接收器可以让实时计算程序从接收器挂掉的情况下恢复。但是更棘手的问题是，如果Driver挂掉如何恢复？使用Checkpoint应用程序元数据的方法可以解决这一问题。为此，Driver可以将应用程序的重要元数据（包含：配置信息、计算代码、未处理的batch数据）持久化到可靠的存储中，比如HDFS、S3；然后Driver可以利用这些持久化的数据进行恢复。 由于有了元数据的Checkpoint，所以Driver可以利用他们重构应用程序，而且可以计算出Driver挂掉的时候应用程序执行到什么位置。 通过持久化元数据，并能重构应用程序，貌似解决了数据丢失的问题，然而在以下场景任然可能导致数据丢失： 1）两个Exectuor已经从接收器中接收到输入数据，并将它缓存到Exectuor的内存中； 2）接收器通知输入源数据已经接收； 3）Exectuor根据应用程序的代码开始处理已经缓存的数据； 4）这时候Driver突然挂掉了； 5）从设计的角度看，一旦Driver挂掉之后，它维护的Exectuor也将全部被kill； 6）既然所有的Exectuor被kill了，所以缓存到它们内存中的数据也将被丢失。结果，这些已经通知数据源但是还没有处理的缓存数据就丢失了； 7）缓存的时候不可能恢复，因为它们是缓存在Exectuor的内存中，所以数据被丢失了。 这对于很多关键型的应用程序来说还是无法容忍。这时，Spark团队再次引入了WAL解决以上这些问题。 4. WAL（Write ahead log）启用了WAL机制，所以已经接收的数据被接收器写入到容错存储中，比如HDFS或者S3。由于采用了WAl机制，Driver可以从失败的点重新读取数据，即使Exectuor中内存的数据已经丢失了。在这个简单的方法下，Spark Streaming提供了一种即使是Driver挂掉也可以避免数据丢失的机制。 虽然WAL可以确保数据不丢失,它并不能对所有的数据源保证exactly-once语义。以下场景任然比较糟糕： 1）接收器接收到输入数据，并把它存储到WAL中； 2）接收器在更新Zookeeper中Kafka的偏移量之前突然挂掉了； 3）Spark Streaming假设输入数据已成功收到（因为它已经写入到WAL中），然而Kafka认为数据被没有被消费，因为相应的偏移量并没有在Zookeeper中更新； 4）过了一会，接收器从失败中恢复； 5）那些被保存到WAL中但未被处理的数据被重新读取； 6）一旦从WAL中读取所有的数据之后，接收器开始从Kafka中消费数据。因为接收器是采用Kafka的High-Level Consumer API实现的，它开始从Zookeeper当前记录的偏移量开始读取数据，但是因为接收器挂掉的时候偏移量并没有更新到Zookeeper中，所有有一些数据被处理了2次。 除了上面描述的场景，WAL还有其他两个不可忽略的缺点: 1）WAL减少了接收器的吞吐量，因为接受到的数据必须保存到可靠的分布式文件系统中。 2）对于一些输入源来说，它会重复相同的数据。比如当从Kafka中读取数据，你需要在Kafka的brokers中保存一份数据，而且你还得在Spark Streaming中保存一份。 5. Exactly-Once为了解决由WAL引入的性能损失，并且保证 exactly-once 语义，新版的Spark中引入了名为Kafka direct API。这个想法对于这个特性是非常明智的。Spark driver只需要简单地计算下一个batch需要处理Kafka中偏移量的范围，然后命令Spark Exectuor直接从Kafka相应Topic的分区中消费数据。换句话说，这种方法把Kafka当作成一个文件系统，然后像读文件一样来消费Topic中的数据。 在这个简单但强大的设计中: 1）不再需要Kafka接收器，Exectuor直接采用Simple Consumer API从Kafka中消费数据。 2）不再需要WAL机制，我们仍然可以从失败恢复之后从Kafka中重新消费数据； 3）Exactly-Once语义得以保存，我们不再从WAL中读取重复的数据。 原文： Spark Streaming And Kafka：可靠实时计算]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop中的Secondary Sort]]></title>
    <url>%2Fhadoop-basics-secondary-sort-in-mapreduce.html</url>
    <content type="text"><![CDATA[我们首先提出了一个查询问题，为了解决这个问题，需要在数据集的多个字段上进行排序。然后，我们将研究 MapReduce Shuff 阶段的工作原理，然后再实现我们的二次排序以获得我们想要的查询结果。 1. 查询如果我们想查看确定 state 和 city 的所有捐款的 id，捐赠者的 state，捐助者的 city 和捐款总额 total。按以下优先顺序排列结果： state - 按字母顺序升序排序（不区分大小写） city - 按字母顺序升序排序（不区分大小写） total - 按数字顺序降序排序 可以用SQL如下实现：SELECT donation_id, donor_state, donor_city, totalFROM donationsWHERE donor_state IS NOT NULL AND donor_city IS NOT NULLORDER BY lower(donor_state) ASC, lower(donor_city) ASC, total DESC; 2. 理解Shuffle阶段现在我们需要深入了解 Shuffle 阶段： 如何和在哪里工作 有哪些工具可以根据我们的需求进行自定义以及调整 以下是使用2个 mapper 和2个 reducer 任务的工作流程图： 关于不同编号的步骤的一些细节： (1) mapper 的 map 方法从 InputFormat 提供的分片中接收所有 （key，value） 键值对。这是我们通常在 Mapper 中编写的最重要的方法。 (2) 使用指定的分区器为每个用户的 map 方法输出进行分区。默认情况下，在 MapReduce 中使用 HashPartitioner。它使用 key 的 hashCode（） 值并对 reducer 的个数进行取模。这将根据 key 随机确定（key，value） 键值对存储在每个 Reducer 的不同分区中。所有具有相同 key 的键值对位于同一个分区中，并在相同的 reducer 中结束。 (3) 在写入磁盘之前，使用指定的 Sort Comparator 对数据进行排序。同一分区记录全部写入同一个临时文件。 (4) reducer 从所有 mapper 中拉取所有分配给他们的分区。分区可以写入本地临时文件，或者足够小时存储在内存中。这个过程也被称为 Shuffle，因为分区正在洗牌。 (5) Sort Comparator 在合并所有内存和磁盘中的分区时再次使用。每个 reducer 都有一个所有（key, value）键值对完全排序的列表，这些键值对是分区器分配给它们的所有键的。 (6) Group Comparator 用于将值分组成列表。每个 “不同” key，都将调用带有参数（key，list&lt;values&gt;）的 reduce 方法。 3. 二次排序二次排序是一种可用于在多个字段上排序数据的技术。它依赖于使用一个复合键，它将包含我们想要用于排序的所有值。 在本文中，我们将阅读我们的 donations Sequence File，并在 shuffling和 reducing 之前将每个捐赠记录映射为（CompositeKey，DonationWritable） 键值对。 本文中使用的所有类都可以在GitHub上查看：https://github.com/nicomak/blog/tree/master/donors/src/main/java/mapreduce/donation/secondarysort。 为了得到查询结果而执行的 MapReduce 二次排序作业位于同一个包的 OrderByCompositeKey.java 文件中。 package mapreduce.donation.secondarysort;import java.io.IOException;import org.apache.commons.lang.StringUtils;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import data.writable.DonationWritable;public class OrderByCompositeKey &#123; public static final Log LOG = LogFactory.getLog(OrderByCompositeKey.class); /** * This mapper simply outputs a (CompositeKey, DonationWritable) pair for each donation row. * It partitions map outputs by the natural key (the 'state' field), because of our NaturalKeyPartitioner class. * Within these partitions, rows are sorted by secondary key, because of our FullKeyComparator class, * which sorts on the full composite key, in the order of ('state', 'city', 'total'). * * @author Nicomak * */ public static class CompositeKeyCreationMapper extends Mapper&lt;Object, DonationWritable, CompositeKey, DonationWritable&gt; &#123; private CompositeKey compositeKey = new CompositeKey(); @Override public void map(Object key, DonationWritable donation, Context context) throws IOException, InterruptedException &#123; // Ignore entries with empty values for better readability of results if (StringUtils.isEmpty(donation.donor_state) || StringUtils.isEmpty(donation.donor_city)) &#123; return; &#125; compositeKey.set(donation.donor_state, donation.donor_city, donation.total); context.write(compositeKey, donation); &#125; &#125; /** * This reducer will fetch the partitions (from different mappers) and then sort them by ('state', 'city', 'total') order again, * because we used our FullKeyComparator as the sort comparator class. * After that, it will group all sorted partition data by natural key ('state') because we used our NaturalKeyComparator * as the grouping comparator. * The groups which are created here are lists of donation rows with the same 'state', and ordered by 'city' * These groups are passed to the "reduce" function in order or natural key, and their content is sorted in order of secondary key. * So the output of the reducer will be rows ordered by ('state', 'city', 'total'). * * @author Nicomak * */ public static class ValueOutputReducer extends Reducer&lt;CompositeKey, DonationWritable, Text, Text&gt; &#123; private Text outputKey = new Text(); private Text outputValue = new Text(); @Override public void reduce(CompositeKey key, Iterable&lt;DonationWritable&gt; donations, Context context) throws IOException, InterruptedException &#123; for (DonationWritable donation : donations) &#123; outputKey.set(donation.donation_id); outputValue.set(String.format("%s %s %.2f", donation.donor_state, donation.donor_city, donation.total)); context.write(outputKey, outputValue); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Job job = Job.getInstance(new Configuration(), "Secondary Sorting"); job.setJarByClass(OrderByCompositeKey.class); // Mapper configuration job.setMapperClass(CompositeKeyCreationMapper.class); job.setInputFormatClass(SequenceFileInputFormat.class); job.setMapOutputKeyClass(CompositeKey.class); job.setMapOutputValueClass(DonationWritable.class); // Partitioning/Sorting/Grouping configuration job.setPartitionerClass(NaturalKeyPartitioner.class); job.setSortComparatorClass(FullKeyComparator.class); job.setGroupingComparatorClass(NaturalKeyComparator.class); // Reducer configuration job.setReducerClass(ValueOutputReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setNumReduceTasks(1); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 3.1 Composite Key我们的查询想要对3个值进行排序，所以我们创建了一个名为 CompositeKey 的 WritableComparable 类，它具有以下3个属性： state（String） - 这个被用作分区的 natural key（或主键） city（String） - 在同一个分区内对具有相同 state natural key的进行排序的辅助键（译者注：即同一分区内 state 相同将根据 city 进行排序） total（float） - 当 city 相同时进一步排序的另一个辅助键（译者注：在同一分区内 state 和 city 均相同则根据 total 进行排序） package mapreduce.donation.secondarysort;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class CompositeKey implements WritableComparable&lt;CompositeKey&gt; &#123; public String state; public String city; public float total; public CompositeKey() &#123; &#125; public CompositeKey(String state, String city, float total) &#123; super(); this.set(state, city, total); &#125; public void set(String state, String city, float total) &#123; this.state = (state == null) ? "" : state; this.city = (city == null) ? "" : city; this.total = total; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(state); out.writeUTF(city); out.writeFloat(total); &#125; @Override public void readFields(DataInput in) throws IOException &#123; state = in.readUTF(); city = in.readUTF(); total = in.readFloat(); &#125; @Override public int compareTo(CompositeKey o) &#123; int stateCmp = state.toLowerCase().compareTo(o.state.toLowerCase()); if (stateCmp != 0) &#123; return stateCmp; &#125; else &#123; int cityCmp = city.toLowerCase().compareTo(o.city.toLowerCase()); if (cityCmp != 0) &#123; return cityCmp; &#125; else &#123; return Float.compare(total, o.total); &#125; &#125; &#125;&#125; 我在这个类中实现了 compareTo（），但它只是默认的自然排序，所有字段都按升序比较。我们的查询想要对 total 字段进行降序排序，为此我们将在下一段中创建一个特定的 Sort Comparator。 3.2 Sort Comparator如图所示，如果我们希望我们的结果在 CompositeKey 的所有3个属性上进行按照我们期望的方式进行排序，我们必须使用按照 [state，city，-total] 优先级顺序的 Sort Comparator。正如我们在前一部分中所做的那样，我们创建了一个继承 WritableComparator 并为我们的排序需求实现 compare（） 方法的类：import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class FullKeyComparator extends WritableComparator &#123; public FullKeyComparator() &#123; super(CompositeKey.class, true); &#125; @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable wc1, WritableComparable wc2) &#123; CompositeKey key1 = (CompositeKey) wc1; CompositeKey key2 = (CompositeKey) wc2; int stateCmp = key1.state.toLowerCase().compareTo(key2.state.toLowerCase()); if (stateCmp != 0) &#123; return stateCmp; &#125; else &#123; int cityCmp = key1.city.toLowerCase().compareTo(key2.city.toLowerCase()); if (cityCmp != 0) &#123; return cityCmp; &#125; else &#123; return -1 * Float.compare(key1.total, key2.total); &#125; &#125; &#125;&#125; 然后，我们使用 job.setSortComparatorClass（FullKeyComparator.class） 将此类设置为 Sort Comparator。 现在使用单个 reducer 将给我们完全排序的结果。仅使用一个 reducer 时，实现 Composite Key 和 Sort Comparator 就足以对多个字段进行排序。 3.3 Partitioner如果我们使用多个 reducer，会发生什么？ 默认分区器 HashPartitioner 将根据 CompositeKey 对象的 hashCode 值将其分配给 reducer。无论我们是重写了 hashcode() 方法（正确使用所有属性的哈希）还是不重写（使用默认 Object 的实现，使用内存中地址），都将 “随机” 对所有 keys 进行分区。 二次排序不会这样的。因为合并来自 mappers 的所有分区后，reducer 的 key 可能会像这样（第一列）： 在第一个输出列中，在一个 reducer 内，对于给定 state 的数据按城市名称排序，然后按总捐赠量降序排列。但这种排序没有什么意义，因为有些数据丢失了。例如，Reducer 0 有2个排序的 Los Angeles key，但来自 Reducer 1 的 Los Angeles 条目应该放在这两个 key 之间。 因此，当使用多个 reducers 时，我们想要的是将具有相同 state 的所有 （key，value） 键值对发送给同一个 reducer，就像第二列显示的那样。最简单的方法是创建我们自己的 NaturalKeyPartitioner，类似于默认的 HashPartitioner，但仅基于 state 的 hashCode，而不是完整的 CompositeKey 的 hashCode：import org.apache.hadoop.mapreduce.Partitioner;import data.writable.DonationWritable;public class NaturalKeyPartitioner extends Partitioner&lt;CompositeKey, DonationWritable&gt; &#123; @Override public int getPartition(CompositeKey key, DonationWritable value, int numPartitions) &#123; // Automatic n-partitioning using hash on the state name return Math.abs(key.state.hashCode() &amp; Integer.MAX_VALUE) % numPartitions; &#125;&#125; 我们使用 job.setPartitionerClass（NaturalKeyPartitioner.class） 将此类设置为作业的分区器。 3.4 Group ComparatorGroup Comparator 决定每次调用 reduce 方法时如何对这些值分组（译者注：一个分组调用一次 reduce 方法）。 继续使用上图中的 Reducer 0 的例子。如果合并分区后，一个 reducer 中的（key，value）键值对必须如下处理： 可以完成的可能分组如下： 说明： 没有相同 (state,city,total) 组合的 keys。因此，对于第一个分组，每个记录调用一次 reduce 方法。 第二个是根据 state, city 分组。B 和 C 键值对的 key 有相同的 state 和 city，因此它们组合在一起在一个 reducer() 中调用。传递给函数的 key 是分组中第一个键值对的 key，因此它依赖于排序。 第三个只查看 state。B， C ，D 键值对中的 key 都具有相同的 state，因此它们被组合在一起以在一个 reducer() 中调用。 在某些情况下分组可能很有用。 例如，如果你想在每个捐赠输出旁边打印给定城市的所有捐款总和，则可以使用上述示例中的第二个分组。这样做，可以在输出所有值之前，将 reduce() 函数中的所有 “总计” 字段求和。 对于我们的查询，我们只需要打印出每个记录的字段，对于分组无关紧要。调用 reduce() 函数4次，3次或2次仍然会只打印出 A，B ，C 和 D 记录的 （id，state，city，total） 字段。 对于这个作业，它对性能没有任何影响。 让我们按照 state（natural key）进行分组，只是为了使用我们自己的 Group Comparator：import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class NaturalKeyComparator extends WritableComparator &#123; public NaturalKeyComparator() &#123; super(CompositeKey.class, true); &#125; @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable wc1, WritableComparable wc2) &#123; CompositeKey key1 = (CompositeKey) wc1; CompositeKey key2 = (CompositeKey) wc2; return key1.state.compareTo(key2.state); &#125;&#125; 然后我们可以通过调用 job.setGroupingComparatorClass（NaturalKeyComparator.class） 来使用这个比较器。 4. 作业运行与结果4.1 Job 1 : With a single reducer输出结果:$ hadoop jar donors.jar mapreduce.donation.secondarysort.OrderByCompositeKey donors/donations.seqfile donors/output_secondarysort$ hdfs dfs -ls -h donors/output_secondarysort-rw-r--r-- 2 hduser supergroup 0 2015-12-28 16:00 donors/output_secondarysort/_SUCCESS-rw-r--r-- 2 hduser supergroup 74.6 M 2015-12-28 16:00 donors/output_secondarysort/part-r-00000$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | head -n 8c8e871528033bd9ce6b267ed8df27698 AA Canada 100.006eb5a716f73260c53a76a5d2aeaf3820 AA Canada 100.0092db424b01676e462eff4c9361799c18 AA Canada 98.36e0f266ed8875df71f0012fdaf50ae22e AA Canada 1.64d9064b2494941725d0f93f6ca781cdc7 AA DPO 50.0083b85744490320c8154f1f5bcd703296 AA DPO 25.007133a67b51c1ee61079fa47e3b9e5160 AA Fremont 50.00f3475f346f1483dfb57efc152d3fbced AA Helsinki\, FINLAND 153.39$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | tail -n 810df7888672288077dc4f60e4a83bcc2 WY Wilson 114.980b4dda44ac5dc29a522285db56082986 WY Wilson 100.004276bc7e6df5f4643675e65bb2323281 WY Wilson 100.00519da9b977281b7623d655b8ee0c8ea5 WY Wilson 91.4792469f6f000a6cd66ba18b5fe03e6871 WY Wilson 23.32f0a9489e53a203e0f7f47e6a350bb19a WY Wilson 1.688aed3aba4473c0f9579927d0940c540f WY Worland 75.001a497106ff2e2038f41897248314e6c6 WY Worland 50.00 分析: 由于只使用1个 reducer ，所以只有1个输出文件。该文件以状态 “AA” 开始，该 state 不是 USA state，但看起来是对国外城市的重新组合。该文件以 “WY” （怀俄明州）城市 Wilson 和 Worland 结束。所有内容都按照查询中的要求排序。 4.2 Job 2 : With 3 reducers, using default partitioner这次我们通过设置 job.setNumReduceTasks（3） 来使用3个 reducer，并且我们注释掉 job.setPartitionerClass（NaturalKeyPartitioner.class） 这一行来查看会发生什么。 输出结果:$ hadoop jar donors.jar mapreduce.donation.secondarysort.OrderByCompositeKey donors/donations.seqfile donors/output_secondarysort$ hdfs dfs -ls -h donors/output_secondarysort-rw-r--r-- 2 hduser supergroup 0 2015-12-28 15:36 donors/output_secondarysort/_SUCCESS-rw-r--r-- 2 hduser supergroup 19.4 M 2015-12-28 15:36 donors/output_secondarysort/part-r-00000-rw-r--r-- 2 hduser supergroup 38.7 M 2015-12-28 15:36 donors/output_secondarysort/part-r-00001-rw-r--r-- 2 hduser supergroup 16.5 M 2015-12-28 15:36 donors/output_secondarysort/part-r-00002$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | tail -n 5234600cf0c052b95e544f690a8deecfc WY Wilson 158.0010df7888672288077dc4f60e4a83bcc2 WY Wilson 114.980b4dda44ac5dc29a522285db56082986 WY Wilson 100.004276bc7e6df5f4643675e65bb2323281 WY Wilson 100.001a497106ff2e2038f41897248314e6c6 WY Worland 50.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00001 | tail -n 5a31e6d2ddcffef3fb0c81a9b5be8a62b WY Wilson 177.60ab277b46c65df53305ceee436e775f86 WY Wilson 150.00519da9b977281b7623d655b8ee0c8ea5 WY Wilson 91.4792469f6f000a6cd66ba18b5fe03e6871 WY Wilson 23.328aed3aba4473c0f9579927d0940c540f WY Worland 75.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00002 | tail -n 5db2334f876e2a661dc66ec79b49a7073 WY Wilson 319.44f740650269f523ac94a8bc54c40ffcb8 WY Wilson 294.70dfdba1ed68a130da5337e28b45a469d1 WY Wilson 286.64e5cf931220ab071083d174461ef50411 WY Wilson 278.19f0a9489e53a203e0f7f47e6a350bb19a WY Wilson 1.68 分析: 通过查看每个输出的最后5行，我们可以注意到所有输出都与 Wilson 城市有关。其中两项输出有 Worland 的条目。 正如前面所解释的，每个输出的结果都是按照 state 和 city 的上升顺序排列，并且 donation 降序排列。但是不可能查看给定 state 或 city 的所有排序捐赠，因为它们分布在多个文件中。 4.3 Job 3 : With 3 reducers, using NaturalKeyPartitioner对于这个作业，我们只需重新设置 job.setPartitionerClass（NaturalKeyPartitioner.class） 即可使用我们的自定义分区器，同时保留3个 Reducer 任务。 输出结果:$ hadoop jar donors.jar mapreduce.donation.secondarysort.OrderByCompositeKey donors/donations.seqfile donors/output_secondarysort$ hdfs dfs -ls -h donors/output_secondarysort-rw-r--r-- 2 hduser supergroup 0 2015-12-28 16:37 donors/output_secondarysort/_SUCCESS-rw-r--r-- 2 hduser supergroup 23.2 M 2015-12-28 16:37 donors/output_secondarysort/part-r-00000-rw-r--r-- 2 hduser supergroup 22.8 M 2015-12-28 16:37 donors/output_secondarysort/part-r-00001-rw-r--r-- 2 hduser supergroup 28.7 M 2015-12-28 16:37 donors/output_secondarysort/part-r-00002$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | tail -n 5eeac9e18795680ccc00f69a42ec4fbc5 VI St. Thomas 10.00634f02e6ffc99fddd2b2c9cda7b7677c VI St. Thomas 10.009f71710178a4fab17fb020bc994be60b VI Zurich / Switzerland 75.00f81b2dc07c0cc3ccea5941dc928247a5 VI Zurich / Switzerland 50.008337fac8a67e50ad4a0dfe7decc4b8e9 VI Zurich / Switzerland 50.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00001 | tail -n 59f3ea230d660a20ec91b287b8a0f6693 WI Wrightstown 5.008b95de5d2b2027c0a3d2631c9f0f6f9b WI Wrightstown 5.009db66b1c0c71dd438a8f979dd04cdfb8 WI Wrightstown 5.00c7c60406d260915cb35a7e267c28becc WI Wrightstown 5.00c2d98a640c301cf277247302ad5014ca WI Wrightstown 5.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00002 | tail -n 5519da9b977281b7623d655b8ee0c8ea5 WY Wilson 91.4792469f6f000a6cd66ba18b5fe03e6871 WY Wilson 23.32f0a9489e53a203e0f7f47e6a350bb19a WY Wilson 1.688aed3aba4473c0f9579927d0940c540f WY Worland 75.001a497106ff2e2038f41897248314e6c6 WY Worland 50.00 分析: 与 Job2 的输出相比，我们可以看到只有一个 reducer（r-00002） 具有来自 WY state（怀俄明州）的条目。因此，如果你只对来自 WY 的捐款感兴趣，那么你要查找的结果是完整且正确地排序在一个文件中的。 其他 reducer 输出以不同 state（”VI” 和 “WI”）结束，因为每个 state 都是独占一个 reducer。这是因为我们告诉我们的 NaturalKeyPartitioner 将 state 字段视为划分的决定性值。 4.4 Performance Comparison这里是本文中描述的3个作业的比较表。取自资源管理器用户界面的总时间。其他值来自MR历史服务器UI。所有指标均为2次执行的平均值。 使用3个 reducer 时，我们可以观察到总执行时间显着改善。Job2和3比Job1更快。在Job2和Job3中，每个 reducer 的shuffling/merging的时间更长一些，但是实际时间要短得多。 5. 结论在这一部分中，我们学习了如何使用一些工具在 Shuffle 阶段对分区，排序和分组进行更多控制。 我们看到了如何实现二次排序，这有助于我们： 当使用单个 reducer 时，对多个字段的数据集进行完全排序 当使用多个 reducer 时，在辅助键上对有相同 natural key 的记录进行排序。 使用多个 reducer 可以加快排序过程，但代价是只能对多个 reducer 实现在 natural key 上的部分排序。 5.1 新定义事后看来，回顾我们应用于复合 key 的不同工具的效果，除了 “在多个字段上进行排序” 外，我们还可以给出二次排序的更一般而精确的定义： 二次排序是一种技术，用于控制 Reducer 的输入对 如何 传递给 reduce 函数。 上面的 如何 可以理解为以何种顺序（Sort Comparator）以及基于 key 的对值进行分组的方式（Group Comparator） 根据这个定义，使用 Secondary Sort，我们可以对 Reducer 内的数据进行全面控制，这就是输出文件内部始终排好序的原因。 5.2 下一步使用 Secondary Sort，我们可以控制 Reducer 内的数据，但我们无法控制如何将已排序的 map 输出分发给 reducer。我们已经定义了一个分区器来确保不同的 reducer 管理他们自己的 natural keys 并保证在二级键的排序。但它并没有解决在所有输出中对所有 natural keys 进行排序的问题。 在下一篇文章中，我们将学习如何使用全排序（Total Order Sorting）来做到这一点。 原文： http://blog.ditullio.fr/2015/12/28/hadoop-basics-secondary-sort-in-mapreduce/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux rsync配置指南]]></title>
    <url>%2Frsync-config-usage.html</url>
    <content type="text"><![CDATA[1. 概述rsync 命令是一个远程数据同步工具，可通过 LAN/WAN 快速同步多台主机间的文件。rsync 使用所谓的 “rsync算法” 来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。rsync 是一个功能非常强大的工具，其命令也有很多功能特色选项。 2. 安装在 ubuntu 下安装 rsync 通过以步骤可以实现：sudo apt-get install rsync xinetd 默认情况下 ubuntu 安装了 rsync，因此只需安装 xinetd即可:sudo apt-get install xinetd 3. 配置(1) 编辑 /etc/default/rsync 启动 rsync 作为使用 xinetd 的守护进程：# 打开rsyncsudo vim /etc/default/rsync# 编辑rsyncRSYNC_ENABLE=inetd (2) 创建 /etc/xinetd.d/rsync, 通过 xinetd 使 rsync 开始工作# 创建并打开文件sudo vim /etc/xinetd.d/rsync# 编辑内容service rsync&#123; disable = no socket_type = stream wait = no user = root server = /usr/bin/rsync server_args = --daemon log_on_failure += USERID&#125; (3) 创建 /etc/rsyncd.conf ,并填写配置信息# 创建并打开文件sudo vim /etc/rsyncd.conf# 编辑配置信息max connections = 2log file = /var/log/rsync.logtimeout = 300[share] # 模块名comment = Public Share# path为需要同步的文件夹路径path = /home/shareread only = nolist = yesuid = rootgid = root# 必须和 rsyncd.secrets中的用户名对应auth users = testsecrets file = /etc/rsyncd.secrets (4) 创建 /etc/rsyncd.secrets，配置用户名和密码.# 创建并打开文件sudo vim /etc/rsyncd.secrets# 配置用户名和密码，密码可以任意设置test:123 (5) 修改 rsyncd.secrets 文件的权限sudo chmod 600 /etc/rsyncd.secrets (6) 启动/重启 xinetdsudo /etc/init.d/xinetd restart 4. 测试在客户端运行下面的命令以及输入密码，确认 rsync 是否配置成功：xiaosi@ying:/etc/apt$ rsync test@123.206.187.64::sharePassword:drwxr-xr-x 4,096 2018/03/20 18:44:51 .-rw-r--r-- 17 2018/03/20 18:44:51 remote_content.txt test 是在服务器中 rsyncd.secrets 文件中配置的用户名。 xx.xx.xx.xx 是服务器的ip地址，也可以填写服务器对应的域名。share 是 rsyncd.conf 中定义的模块 5. 问题在测试的时候出现如下问题：xiaosi@ying:/etc/apt$ rsync test@xxx:xxx:xxx:xxx::sharersync: failed to connect to xxx:xxx:xxx:xxx (xxx:xxx:xxx:xxx): Connection refused (111)rsync error: error in socket IO (code 10) at clientserver.c(128) [Receiver=3.1.1] 首先判断 873 端口是否开放，如果没有开启一下：telnet 192.168.xxx.xxx 873 或者查看 rsync 服务是否启动：ubuntu@VM-0-7-ubuntu:~$ ps -ef | grep rsyncroot 18848 1 0 17:29 ? 00:00:00 rsync --daemon --config=/etc/rsyncd.confubuntu 18850 12214 0 17:29 pts/0 00:00:00 grep --color=auto rsync 如果没有启动，启动一下 rsync 服务：sudo rsync --daemon --config=/etc/rsyncd.conf 转载于： https://segmentfault.com/a/1190000010310496 参考于： http://ju.outofmemory.cn/entry/27665]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux 命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux rsync命令使用指南]]></title>
    <url>%2Frsync-command-usage.html</url>
    <content type="text"><![CDATA[1. 概述rsync 命令是一个远程数据同步工具，可通过 LAN/WAN 快速同步多台主机间的文件。rsync 使用所谓的 “rsync算法” 来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。rsync 是一个功能非常强大的工具，其命令也有很多功能特色选项。 2. 安装与配置请参考： 3. 语法rsync [OPTION...] SRC... [DEST]rsync [OPTION...] [USER@]HOST:SRC... [DEST]rsync [OPTION...] SRC... [USER@]HOST:DESTrsync [OPTION...] [USER@]HOST::SRC... [DEST]rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]rsync [OPTION...] SRC... [USER@]HOST::DESTrsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 3.1 本地模式rsync [OPTION...] [USER@]HOST:SRC... [DEST] 拷贝本地文件。当 SRC 和 DEST 路径信息都不包含有单个冒号 : 分隔符时就启动这种工作模式。例如：xiaosi@ying:~$ rsync -a adv_push adv_push_backup/xiaosi@ying:~$ ll adv_push_backup/总用量 20drwxrwxr-x 3 xiaosi xiaosi 4096 3月 20 15:29 ./drwxr-xr-x 83 xiaosi xiaosi 12288 3月 20 15:29 ../drwxrwxr-x 2 xiaosi xiaosi 4096 3月 20 15:28 adv_push/ 3.2 通过远程Shell访问-Pullrsync [OPTION...] [USER@]HOST:SRC... [DEST] 使用一个远程Shell程序(如rsh、ssh)来实现将远程机器的内容拷贝到本地机器。当 SRC 地址路径包含单个冒号 : 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a ubuntu@xxx.xxx.xxx.xxx:test/remote_content.txt /home/xiaosi/data/share/ubuntu@xxx.xxx.xxx.xxx&apos;s password: 在本地机器上查看从服务器拷贝到本地上的文件：xiaosi@ying:~$ cd /home/xiaosi/data/share/xiaosi@ying:~/data/share$ ll总用量 12drwxrwxr-x 2 xiaosi xiaosi 4096 3月 20 19:41 ./drwxr-xr-x 14 xiaosi xiaosi 4096 3月 20 19:24 ../-rw-rw-r-- 1 xiaosi xiaosi 17 3月 20 15:57 remote_content.txt ubuntu@xxx.xxx.xxx.xxx 中 ubuntu 是服务器登录名 xxx.xxx.xxx.xxx 是服务器IP地址 password 是服务器登录名 ubuntu 对应的密码 以 Shell 方式访问，首先在服务端启动 ssh 服务： service sshd start 3.3 通过远程Shell访问-Pushrsync [OPTION...] SRC... [USER@]HOST:DEST 使用一个远程shell程序(如rsh、ssh)来实现将本地机器的内容拷贝到远程机器。当 DEST 路径地址包含单个冒号 : 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a /home/xiaosi/data/exception.txt ubuntu@xxx.xxx.xxx.xxx:tmpubuntu@xxx.xxx.xxx.xxx&apos;s password: 在服务器上查看从本地拷贝到服务器上的文件：ubuntu@VM-0-7-ubuntu:~/tmp$ lltotal 24drwxrwxr-x 5 ubuntu ubuntu 4096 Mar 20 19:34 ./drwxr-xr-x 11 ubuntu ubuntu 4096 Mar 20 19:32 ../...-rw-rw-r-- 1 ubuntu ubuntu 80 Mar 20 11:03 exception.txt... ubuntu@xxx.xxx.xxx.xxx 中 ubuntu 是服务器登录名 xxx.xxx.xxx.xxx 是服务器IP地址 password 是服务器登录名 ubuntu 对应的密码 以 Shell 方式访问，首先在服务端启动 ssh 服务： service sshd start 3.4 通过rsync进程访问-Pullrsync [OPTION...] [USER@]HOST::SRC... [DEST]rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] 从远程 rsync 服务器中拷贝文件到本地机。当 SRC 路径信息包含 :: 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a test@xxx.xxx.xxx.xxx::share/remote_content.txt /home/xiaosi/data/share 在本地机器上查看从服务器拷贝到本地上的文件：xiaosi@ying:~/data/share$ ll总用量 12drwxrwxr-x 2 xiaosi xiaosi 4096 3月 20 19:06 ./drwxr-xr-x 14 xiaosi xiaosi 4096 3月 20 19:06 ../-rw-r--r-- 1 xiaosi xiaosi 17 3月 20 18:44 remote_content.txt 以后台服务方式访问 请参考：Linux rsync配置指南 test@xxx.xxx.xxx.xxx 中 test 是以后台方式访问配置的用户 3.5 通过rsync进程访问-Pushrsync [OPTION...] SRC... [USER@]HOST::DESTrsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 从本地机器拷贝文件到远程 rsync 服务器中。当 DEST 路径信息包含 :: 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a /home/xiaosi/data/sources.list.backup test@xxx:xxx:xxx:xxx::share/config 在服务器上查看从本地拷贝到服务器上的文件：ubuntu@VM-0-7-ubuntu:/home/share/config$ lltotal 12drwxr-xr-x 2 root root 4096 Mar 20 19:02 ./drwxr-xr-x 3 root root 4096 Mar 20 19:02 ../-rw-r--r-- 1 root root 2981 Mar 20 16:30 sources.list.backup 以后台服务方式访问 请参考：Linux rsync配置指南 test@xxx.xxx.xxx.xxx 中 test 是以后台方式访问配置的用户 3.6 查阅模式只使用一个 SRC 参数，而不使用 DEST 参数将列出源文件而不是进行复制。例如：xiaosi@ying:~$ rsync -a adv_pushdrwxrwxr-x 4,096 2018/03/20 15:28:15 adv_push-rw-rw-r-- 353 2018/03/14 17:02:35 adv_push/adv_push_20180307.txt-rw-rw-r-- 353 2018/03/14 17:02:33 adv_push/adv_push_20180308.txt 4. 选项-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。-l, --links 保留软链结。-L, --copy-links 想对待常规文件一样处理软链结。--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。--safe-links 忽略指向SRC路径目录树以外的链结。-H, --hard-links 保留硬链结。-p, --perms 保持文件权限。-o, --owner 保持文件属主信息。-g, --group 保持文件属组信息。-D, --devices 保持设备文件信息。-t, --times 保持文件时间信息。-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, --dry-run现实哪些文件将被传输。-w, --whole-file 拷贝文件，不进行增量检测。-x, --one-file-system 不要跨越文件系统边界。-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。--delete 删除那些DST中SRC没有的文件。--delete-excluded 同样删除接收端那些被该选项指定排除的文件。--delete-after 传输结束以后再删除。--ignore-errors 及时出现IO错误也进行删除。--max-delete=NUM 最多删除NUM个文件。--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。--force 强制删除目录，即使不为空。--numeric-ids 不将数字的用户和组id匹配为用户名和组名。--timeout=time ip超时时间，单位为秒。-I, --ignore-times 不跳过那些有同样的时间和长度的文件。--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T --temp-dir=DIR 在DIR中创建临时文件。--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 --partial。--progress 显示备份过程。-z, --compress 对备份的文件在传输时进行压缩处理。--exclude=PATTERN 指定排除不需要传输的文件模式。--include=PATTERN 指定不排除而需要传输的文件模式。--exclude-from=FILE 排除FILE中指定模式的文件。--include-from=FILE 不排除FILE指定模式匹配的文件。--version 打印版本信息。--address 绑定到特定的地址。--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。--port=PORT 指定其他的rsync服务端口。--blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。--progress 在传输时现实传输过程。--log-format=formAT 指定日志文件格式。--password-file=FILE 从FILE中得到密码。--bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, --help 显示帮助信息。 参考: http://man.linuxde.net/rsync]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux 命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 与 Kafka 整合的改进]]></title>
    <url>%2Fimprovements-to-kafka-integration-of-spark-streaming.html</url>
    <content type="text"><![CDATA[Apache Kafka 正在迅速成为最受欢迎的开源流处理平台之一。我们在 Spark Streaming 中也看到了同样的趋势。因此，在 Apache Spark 1.3 中，我们专注于对 Spark Streaming 与 Kafka 集成进行重大改进。主要增加如下： 为 Kafka 新增了 Direct API - 这允许每个 Kafka 记录在发生故障时只处理一次，并且不使用 Write Ahead Logs。这使得 Spark Streaming + Kafka 流水线更高效，同时提供更强大的容错保证。 为 Kafka 新增了 Python API - 这样你就可以在 Python 中处理 Kafka 数据。 在本文中，我们将更详细地讨论这些改进。 1. Direct APISpark Streaming 自成立以来一直支持 Kafka，Spark Streaming 与 Kafka 在生产环境中的很多地方一起使用。但是，Spark 社区要求更好的容错保证和更强的可靠性语义。为了满足这一需求，Spark 1.2 引入了 Write Ahead Logs（WAL）。它可以确保在发生故障时从任何可靠的数据源（即Flume，Kafka和Kinesis等事务源）接收的数据不会丢失（即至少一次语义）。即使对于像 plain-old 套接字这样的不可靠（即非事务性）数据源，它也可以最大限度地减少数据的丢失。 然而，对于允许从数据流中的任意位置重放数据流的数据源（例如 Kafka），我们可以实现更强大的容错语义，因为这些数据源让 Spark Streaming 可以更好地控制数据流的消费。Spark 1.3 引入了 Direct API 概念，即使不使用 Write Ahead Logs 也可以实现 exactly-once 语义。让我们来看看集成 Apache Kafka 的 Spark Direct API 的细节。 2. 我们是如何构建它？从高层次的角度看，之前的 Kafka 集成与 Write Ahead Logs（WAL）一起工作如下： (1) 运行在 Spark workers/executors 上的 Kafka Receivers 连续不断地从 Kafka 中读取数据，这用到了 Kafka 高级消费者API。 (2) 接收到的数据存储在 Spark 的 worker/executor的内存上，同时写入到 WAL（拷贝到HDFS）上。Kafka Receiver 只有在数据保存到日志后才会更新 Zookeeper中的 Kafka 偏移量。 (3) 接收到的数据及其WAL存储位置信息也可靠地存储。在出现故障时，这些信息用于从故障中恢复，重新读取数据并继续处理。 虽然这种方法可以确保从 Kafka 接收的数据不会丢失，但是在失败的时候，某些记录仍然有可能会被多次被处理（即 at-least-once 语义）。这种情况在一些接收到的数据被可靠地保存到 WAL 中，但是在更新 Zookeeper 中相应的 Kafka 偏移量之前失败时会发生(译者注：即已经保存到WAL，但是还没有来得及更新 Zookeeper 中的 Kafka 偏移量)。从而导致了不一致的情况 - Spark Streaming 认为数据已被接收，但 Kafka 认为数据还未成功发送，因为 Zookeeper 中的偏移未更新。因此，在系统从故障中恢复后，Kafka 会再一次发送数据。 出现这种不一致的原因是两个系统无法对描述已发送内容的信息进行原子更新。为了避免这种情况，只需要一个系统来维护已发送或接收的内容的一致性视图。此外，这个系统需要有从故障中恢复时重放数据流的一切控制权。因此，我们决定所有消费的偏移量信息只保存在 Spark Streaming 中，这些信息可以使用 Kafka 的 Simple Consumer API 根据故障需要重放任意偏移量的数据来从故障中恢复。 为了构建这个系统，新的 Direct Kafka API 采用与 Receivers 和 WAL 完全不同的方法。与使用 Receivers 连续接收数据并将其存储在 WAL 中不同，我们只需在给出每个批次开始时要使用的偏移量范围。之后，在执行每个批次的作业时，将从 Kafka 中读取与偏移量范围对应的数据进行处理（与读取HDFS文件的方式类似）。这些偏移量也能可靠地保存（）并用于重新计算数据以从故障中恢复。 请注意，Spark Streaming 可以在失败以后重新读取和处理来自 Kafka 的流片段以从故障中恢复。但是，由于 RDD 转换的 exactly-once 语义，最终重新计算的结果与在没有失败的结果完全相同。 因此，Direct API 消除了对 Kafka 的 WAL 和 Receivers 的依赖，同时确保每个 Kafka 记录都被 Spark Streaming 有效地接收一次。这允许我们用端到端的 exactly-once 语义将 Spark Streaming 与 Kafka 进行整合。总的来说，它使得这样的流处理流水线更加容错，高效并且更易于使用。 3. 如何来使用新的API相比之前的更加容易使用：// Define the Kafka parameters, broker list must be specifiedval kafkaParams = Map("metadata.broker.list" -&gt; "localhost:9092,anotherhost:9092")// Define which topics to read fromval topics = Set("sometopic", "anothertopic")// Create the direct stream with the Kafka parameters and topicsval kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](streamingContext, kafkaParams, topics) 由于这种 Direct API 没有使用 Receivers，因此你不必担心如何创建多个输入 DStream 来创建多个 Receivers。你也不需要考虑每个 Receiver 消费的 Kafka partition 的数量。每个 Kafka partition 将自动的并行读取。此外，每个 Kafka partition 与 RDD partition 一一对应，从而简化了并行模型。 除了新的流处理API之外，我们还引入了 KafkaUtils.createRDD()，它可用于在 Kafka 数据上运行批处理作业。 // Define the offset ranges to read in the batch jobval offsetRanges = Array( OffsetRange("some-topic", 0, 110, 220), OffsetRange("some-topic", 1, 100, 313), OffsetRange("another-topic", 0, 456, 789))// Create the RDD based on the offset rangesval rdd = KafkaUtils.createRDD[String, String, StringDecoder, StringDecoder](sparkContext, kafkaParams, offsetRanges) 如果你想了解更多关于API和它如何实现的细节，请看下面的内容: Spark Streaming + Kafka Integration Guide Exactly-once Spark Streaming from Kafka Direct API 完整 word count example: Scala 和 Java Fault-tolerance Semantics in Spark Streaming Programming Guide 4. Python 中的Kafka API在 Spark 1.2 中，添加了 Spark Streaming 的基本 Python API，因此开发人员可以使用 Python 编写分布式流处理应用程序。在 Spark 1.3 中，扩展了 Python API 来包含Kafka。借此，在 Python 中使用 Kafka 编写流处理应用程序变得轻而易举。这是一个示例代码。kafkaStream = KafkaUtils.createStream(streamingContext,"zookeeper-server:2181", "consumer-group", &#123;"some-topic": 1&#125;)lines = kafkaStream.map(lambda x: x[1]) 查看完整的示例和 python文档。运行该示例的说明可以在 Kafka 集成指南中找到。请注意，对于使用 Kafka API 运行示例或任何 python 应用程序，你必须将 Kafka Maven 依赖关系添加到路径中。这可以在 Spark 1.3 中轻松完成，因为你可以直接将 Maven 依赖关系添加到 spark-submit （推荐的方式来启动Spark应用程序）。 原文：https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 持久化]]></title>
    <url>%2Fspark-base-rdd-persistence.html</url>
    <content type="text"><![CDATA[1. 概述Spark 中最重要的功能之一是在操作之间将数据集持久化(缓存)在内存中。当你持久化一个 RDD 时，每个节点都会保存 RDD 的任意分区，RDD在内存中计算时该数据集（或从其派生的数据集）上的其他 Action 可以重用它。这样可以使后面的 Action 操作执行的更快（通常超过10倍）。缓存是迭代算法和快速交互的关键工具。 可以使用 RDD 上的 persist() 或 cache() 方法来标记要持久化的 RDD (译者注：执行这两个方法不会立即持久化 RDD)。当 RDD 第一次在 action 操作中计算时，将持久化(缓存)到节点的内存中。Spark 的缓存是可容错的 - 如果 RDD 的任意分区丢失，将使用最初创建的转换操作自动重新计算。 2. 存储级别除此之外，可以使用不同的持久化级别来存储每个持久化的 RDD，从而允许你，例如，将数据集存储在磁盘上，或者以 Java 序列化对象形式存储在内存中(以节省空间)，或者在不同机器节点上进行备份。通过将 StorageLevel 对象传递给 persist() 方法来设置持久化级别。cache() 方法使用默认存储级别，即 StorageLevel.MEMORY_ONLY（将反序列化的对象存储在内存中）。 持久化级别 说明 MEMORY_ONLY 将 RDD 以 Java 对象形式存储在 JVM 中。如果没有足够的内存存储 RDD，则某些分区将不会被缓存，每次需要时都会重新计算。这是默认级别。 MEMORY_AND_DISK 将 RDD 以 Java 对象形式存储在 JVM 中。如果数据在内存中放不下，则溢写到磁盘上．需要时则会从磁盘上读取 MEMORY_ONLY_SER (Java and Scala) 此级别与MEMORY_ONLY完全相同，但会在存储到内存之前序列化对象。这通常比 Java 对象更具空间效率，特别是在使用快速序列化器的情况下，但是这种方式读取数据会消耗更多的CPU。 MEMORY_AND_DISK_SER (Java and Scala) 与 MEMORY_ONLY_SER 类似，但如果数据在内存中放不下，则溢写到磁盘上，而不是每次需要时重新计算它们。 DISK_ONLY 将 RDD 分区存储在磁盘上而不是内存上。 MEMORY_ONLY_2, MEMORY_AND_DISK_2等 与上面的储存级别相同，只不过将持久化数据存为两份，在两个集群节点上备份每个分区。 OFF_HEAP（实验中） 与 MEMORY_ONLY_SER 类似，但将数据存储在 堆外内存 中。 这需要启用堆内存。 在 Python 中，存储对象始终使用 Pickle 库进行序列化，因此选择什么样的序列化级别是无关紧要的。Python 中的可用存储级别包括MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY和DISK_ONLY_2。 在 Shuffle 操作中(例如，reduceByKey)，即使用户没有主动对调用 persist，Spark也会对一些中间数据进行持久化。这样做是为了如果一个节点在 Shuffle 过程中发生故障避免重新计算整个输入。如果要重用，我们仍然建议用户对生成的 RDD 调用 persist。 3. 选择存储级别Spark 的存储级别旨在提供内存使用率和CPU效率之间的不同权衡。我们建议通过以下过程来选择一个： 如果你的 RDD 适合于默认存储级别（MEMORY_ONLY），那就保持不动。 这是CPU效率最高的选项，允许 RDD 上的操作尽可能快地运行。 如果不是，请尝试使用 MEMORY_ONLY_SER 并选择一个 快速的序列化库，这种方式更加节省空间，并仍然能够快速访问。 不要溢写到磁盘，除非在数据集上的计算操作成本较高，或者需要过滤大量的数据。否则，重新计算分区可能与从磁盘读取分区一样快。 如果要快速故障恢复（例如，使用Spark为Web应用程序提供服务），请使用副本存储级别（例如，MEMORY_ONLY_2）。所有存储级别通过重新计算丢失的数据来提供完整的容错能力，但副本数据可让你继续在 RDD 上运行任务，而无需重新计算丢失的分区。 4. 清除数据Spark会自动监视每个节点的缓存使用情况，并以最近最少使用（LRU）方式丢弃旧的数据分区。如果你想手动删除 RDD，而不是等待它自动从缓存中删除，请使用 RDD.unpersist() 方法。 Spark版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#rdd-persistence]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 与 Kafka0.8 整合]]></title>
    <url>%2Fspark-streaming-kafka-0-8-integration.html</url>
    <content type="text"><![CDATA[在这里我们解释如何配置 Spark Streaming 以接收来自 Kafka 的数据。有两种方法，一种为使用 Receivers 和 Kafka 高级API的旧方法，以及不使用 Receivers 的新方法（在 Spark 1.3 中引入）。它们具有不同的编程模型，性能特征和语义保证。就目前的 Spark 版本而言，这两种方法都被为稳定的API。 Kafka0.8 在 Spark2.3.0 版本中已经被弃用 1. 基于Receiver的方法这种方法使用 Receiver 来接收数据。Receiver 是使用 Kafka 高级消费者API实现的。与所有接收方一样，通过 Receiver 从 Kafka 接收的数据存储在 Spark executors 中，然后由 Spark Streaming 启动的作业处理数据。 但是，在默认配置下，这种方法可能会在失败时丢失数据（请参阅接收器的可靠性）。为确保零数据丢失，你不得不另外启用 Spark Streaming 中的 Write Ahead Logs （在 Spark 1.2 中引入），同时将所有收到的 Kafka 数据保存在分布式文件系统（例如HDFS）的 Write Ahead Logs 中，以便在发生故障时恢复所有数据。有关 Write Ahead Logs 的更多详细信息，请参阅流编程指南中的部署章节。 接下来，我们将讨论如何在流应用程序中使用这种方法。 1.1 引入对于使用 SBT/Maven 项目定义的 Scala/Java 应用程序，请引入如下工件（请参阅主编程指南中的Linking部分以获取更多信息）。groupId = org.apache.sparkartifactId = spark-streaming-kafka-0-8_2.11version = 2.3.0 对于Python应用程序，在部署应用程序时，必须添加上述库及其依赖项。请参阅下面的部署小节。 1.2 编程在流应用程序代码中，导入 KafkaUtils 并创建一个输入 DStream，如下所示。 Scala版本:import org.apache.spark.streaming.kafka._val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) Java版本：import org.apache.spark.streaming.kafka.*;JavaPairReceiverInputDStream&lt;String, String&gt; kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]); Python：from pyspark.streaming.kafka import KafkaUtilskafkaStream = KafkaUtils.createStream(streamingContext, \ [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) 默认情况下，Python API会将 Kafka 数据解码为 UTF8 编码的字符串。你可以指定自定义解码函数，将 Kafka 记录中的字节数组解码为任意任意数据类型。 查看API文档。 请记住: Kafka 中的 topic partition 区与 Spark Streaming 中生成的 RDD partition 没有相关性。因此增加 KafkaUtils.createStream() 中特定 topic partition 的数量仅仅增加了在单个接收器中消费 topic 使用的线程数。但是这并没有增加 Spark 在处理数据的并行度。 可以用不同的 groups 和 topics 来创建多个 Kafka 输入 DStream，用于使用多个接收器并行接收数据。之后可以利用 union 来合并成一个 Dstream。 如果你使用 HDFS 等副本文件系统去启用 Write Ahead Logs，那么接收到的数据已经在日志中备份。因此，输入流的存储级别为 StorageLevel.MEMORY_AND_DISK_SER（即使用KafkaUtils.createStream（…，StorageLevel.MEMORY_AND_DISK_SER））。 1.3 部署与任何 Spark 应用程序一样，spark-submit 用于启动你的应用程序。但是，Scala/Java 应用程序和 Python 应用程序的细节略有不同。 对于 Scala 和 Java 应用程序，如果你使用 SBT 或 Maven 进行项目管理，需要将 spark-streaming-kafka-0-8_2.11 及其依赖项打包到应用程序 JAR 中。同时确保 spark-core_2.11 和 spark-streaming_2.11 被标记为 provided 依赖关系，因为这些已经存在 Spark 的安装中。最后使用 spark-submit 启动你的应用程序。 对于缺乏 SBT/Maven 项目管理的 Python 应用程序，可以使用 –packages 直接将 spark-streaming-kafka-0-8_2.11 及其依赖添加到 spark-submit 中（请参阅应用程序提交指南）。即，./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 ... 或者，你也可以从 Maven 仓库中下载 spark-streaming-kafka-0-8-assembly 的JAR，并将其添加到 spark-submit -jars 中。 2. 不使用Receiver的方法这种新的没有接收器的 “直接” 方法已在 Spark 1.3 中引入，以确保更强大的端到端保证。这个方法不使用接收器接收数据，而是定期查询 Kafka 每个 topic+partition 中的最新偏移量，并相应地定义了要在每个批次中要处理的偏移量范围。当处理数据的作业启动后，Kafka 的简单消费者API用于从 Kafka 中读取定义的偏移量范围（类似于从文件系统读取文件）。请注意，此特征是在 Spark 1.3 中为 Scala 和 Java API 引入的，Python API 在 Spark 1.4 中引入。 与基于 Receiver 的方法相比，该方法具有以下优点： 简化并行：不需要创建多个 Kafka 输入 Stream 然后将其合并。使用 directStream ， Spark Streaming 将创建与可以消费的 Kafka partition 一样多的 RDD partition，这些 partition 将全部从 Kafka 并行读取数据。 因此，Kafka 和 RDD partition 之间有一对一的映射关系，这更易于理解和调整。 效率：在第一种方法中实现零数据丢失需要将数据存储在 Write Ahead Log 中，这会进行数据的拷贝。这样效率比较低下，因为数据被有效地复制了两次 - 一次是 Kafka 进行的，另一次是通过 Write Ahead Log 进行的。因为没有 Receiver，所以第二种方法不存在这个问题，因此不需要 Write Ahead Log。只要我们 Kafka 的数据保留足够长的时间，就可以从 Kafka 恢复信息。 Exactly-once 语义：第一种方法使用 Kafka 的高级API在 Zookeeper 中存储消费的偏移量。这是传统的从 Kafka 上消费数据的方式。尽管这种方法（结合 Write Ahead Log 使用）可以确保零数据丢失（即 at-least once 语义），但在某些失败情况下，有一些记录可能会消耗两次。发生这种情况是因为 Spark Streaming 可靠接收的数据与 Zookeeper 跟踪的偏移之间不一致。因此，在第二种方法中，我们使用不使用 Zookeeper 的简单 Kafka API。在其检查点内，Spark Streaming 跟踪偏移量。这消除了 Spark Streaming 和 Zookeeper/Kafka 之间的不一致性，因此 Spark Streaming 每条记录在即使发生故障时也可以确切地收到一次。为了实现输出结果的 exactly-once 语义，将数据保存到外部数据存储区的输出操作必须是幂等的，或者是保存结果和偏移量的原子事务（请参阅主程序中输出操作的语义指南获取更多信息）。 请注意，这种方法的一个缺点是它不会更新 Zookeeper 中的偏移量，因此基于 Zookeeper 的 Kafka 监控工具不会显示进度。但是，你可以在每个批次中访问由此方法处理的偏移量，并自己更新 Zookeeper（请参见下文）。 接下来，我们将讨论如何在流应用程序中使用这种方法。 ２.1 引入对于使用 SBT/Maven 项目定义的 Scala/Java 应用程序，请引入如下工件（请参阅主编程指南中的Linking部分以获取更多信息）。groupId = org.apache.sparkartifactId = spark-streaming-kafka-0-8_2.11version = 2.3.0 ２.2 编程在流应用程序代码中，导入 KafkaUtils 并创建一个输入 DStream，如下所示。 Scala版本:import org.apache.spark.streaming.kafka._val directKafkaStream = KafkaUtils.createDirectStream[ [key class], [value class], [key decoder class], [value decoder class] ]( streamingContext, [map of Kafka parameters], [set of topics to consume]) Java版本:import org.apache.spark.streaming.kafka.*;JavaPairInputDStream&lt;String, String&gt; directKafkaStream = KafkaUtils.createDirectStream(streamingContext, [key class], [value class], [key decoder class], [value decoder class], [map of Kafka parameters], [set of topics to consume]); Python版本:from pyspark.streaming.kafka import KafkaUtils directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], &#123;"metadata.broker.list": brokers&#125;) 你还可以将 messageHandler 传递给 createDirectStream 来访问 MessageAndMetadata，其包含了当前消息的元数据，并可以将其转换为任意所需的类型。 在 Kafka 参数中，必须指定 metadata.broker.list 或 bootstrap.servers。默认情况下，它将从每个 Kafka 分区的最新偏移量开始消费。如果你将 Kafka 参数中的 auto.offset.reset 配置为 smallest，那么它将从最小偏移量开始消费。 你也可以使用 KafkaUtils.createDirectStream 的其他变体从任意偏移量开始消费。此外，如果你想访问每个批次中消费的偏移量，你可以执行以下操作： Scala版本：// Hold a reference to the current offset ranges, so it can be used downstreamvar offsetRanges = Array.empty[OffsetRange]directKafkaStream.transform &#123; rdd =&gt; offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd&#125;.map &#123; ...&#125;.foreachRDD &#123; rdd =&gt; for (o &lt;- offsetRanges) &#123; println(s"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;") &#125; ...&#125; Java版本:// Hold a reference to the current offset ranges, so it can be used downstreamAtomicReference&lt;OffsetRange[]&gt; offsetRanges = new AtomicReference&lt;&gt;();directKafkaStream.transformToPair(rdd -&gt; &#123; OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges(); offsetRanges.set(offsets); return rdd;&#125;).map( ...).foreachRDD(rdd -&gt; &#123; for (OffsetRange o : offsetRanges.get()) &#123;System.out.println( o.topic() + " " + o.partition() + " " + o.fromOffset() + " " + o.untilOffset()); &#125; ...&#125;); Python版本:offsetRanges = []def storeOffsetRanges(rdd): global offsetRanges offsetRanges = rdd.offsetRanges() return rdddef printOffsetRanges(rdd): for o in offsetRanges: print "%s %s %s %s" % (o.topic, o.partition, o.fromOffset, o.untilOffset)directKafkaStream \ .transform(storeOffsetRanges) \ .foreachRDD(printOffsetRanges) 如果你希望基于 Zookeeper 的 Kafka 监视工具显示流应用程序的进度，你可以使用上面来更新 Zookeeper。 请注意，HasOffsetRanges 的类型转换只有在 directKafkaStream 的第一个方法调用中使用才会成功，而不是放在后面的方法链中。你可以使用 transform() 替换 foreachRDD() 作为调用的第一个方法来访问偏移量，然后再调用其他的Spark方法。但是，请注意，RDD partition 与 Kafka partition 之间的一对一映射经过任意 shuffle 或重新分区的方法（例如， reduceByKey（）或window（）之后不会保留。 另外需要注意的是，由于此方法不使用 Receivers，因此与 receiver 相关的配置（即 spark.streaming.receiver.* 形式的配置）将不再适用于由此方法创建的输入DStream（将应用于其他输入DStreams）。相反，使用 spark.streaming.kafka.* 配置。一个重要的配置是 spark.streaming.kafka.maxRatePerPartition，每个 Kafka partition 使用 direct API 读取的最大速率（每秒消息数）。 2.3 部署这与第一种方法相同。 Spark版本： 2.3.0Kafka版本：0.8 原文：http://spark.apache.org/docs/2.3.0/streaming-kafka-0-8-integration.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Stream</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内部原理之运行原理]]></title>
    <url>%2Fspark-internal-operating-principle-one.html</url>
    <content type="text"><![CDATA[在大数据领域，只有深挖数据科学领域，走在学术前沿，才能在底层算法和模型方面走在前面，从而占据领先地位。 Spark的这种学术基因，使得它从一开始就在大数据领域建立了一定优势。无论是性能，还是方案的统一性，对比传统的 Hadoop，优势都非常明显。Spark 提供的基于 RDD 的一体化解决方案，将 MapReduce、Streaming、SQL、Machine Learning、Graph Processing 等模型统一到一个平台下，并以一致的API公开，并提供相同的部署方案，使得 Spark 的工程应用领域变得更加广泛。 1. Spark 专业术语定义1.1 Application：Spark应用程序指的是用户编写的Spark应用程序，包含了Driver功能代码和分布在集群中多个节点上运行的Executor代码。 Spark应用程序，由一个或多个作业JOB组成，如下图所示。 1.2 Driver：驱动程序Spark 中的 Driver 即运行上述 Application 的 Main() 函数并且创建 SparkContext，其中创建 SparkContext 的目的是为了准备 Spark 应用程序的运行环境。在 Spark 中由 SparkContext 负责和 ClusterManager 通信，进行资源的申请、任务的分配和监控等；当 Executor 部分运行完毕后，Driver 负责将 SparkContext 关闭。通常 SparkContext 代表 Driver，如下图所示。 1.3 Cluster Manager：资源管理器指的是在集群上获取资源的外部服务，常用的有：Standalone，Spark 原生的资源管理器，由 Master 负责资源的分配；Haddop Yarn，由 Yarn 中的 ResearchManager 负责资源的分配；Messos，由 Messos 中的 Messos Master 负责资源管理，如下图所示。 1.4 Executor：执行器Application 运行在 Worker 节点上的一个进程，该进程负责运行 Task，并且负责将数据存在内存或者磁盘上，每个 Application 都有各自独立的一批 Executor，如下图所示。 1.5 Worker：计算节点集群中任何可以运行 Application 代码的节点，类似于 Yarn 中的 NodeManager 节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，在Spark on Messos模式中指的就是Messos Slave节点，如下图所示。 1.6 RDD：弹性分布式数据集Resillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作），如下图所示。 1.7 窄依赖父RDD每一个分区最多被一个子RDD的分区所用；表现为一个父RDD的分区对应于一个子RDD的分区，或两个父RDD的分区对应于一个子RDD 的分区。如图所示。 1.8 宽依赖父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。如图所示。 常见的窄依赖有：map、filter、union、mapPartitions、mapValues、join（父RDD是hash-partitioned ：如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖）。 常见的宽依赖有groupByKey、partitionBy、reduceByKey、join（父RDD不是hash-partitioned ：除此之外的，rdd 的join api是宽依赖）。 1.9 DAG：有向无环图Directed Acycle graph，反应RDD之间的依赖关系，如图所示。 1.10 DAGScheduler：有向无环图调度器基于 DAG 划分 Stage 并以 TaskSet 的形势把 Stage 提交给 TaskScheduler；负责将作业拆分成不同阶段的具有依赖关系的多批任务；最重要的任务之一就是：计算作业和任务的依赖关系，制定调度逻辑。在 SparkContext 初始化的过程中被实例化，一个 SparkContext 对应创建一个 DAGScheduler。 1.11 TaskScheduler：任务调度器将 Taskset 提交给 worker（集群）运行并回报结果；负责每个具体任务的实际物理调度。如图所示。 1.12 Job：作业由一个或多个调度阶段所组成的一次计算作业；包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation。如图所示。 1.13 Stage：调度阶段一个任务集对应的调度阶段；每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业分为多个阶段；Stage分成两种类型ShuffleMapStage、ResultStage。如图所示。 1.14 TaskSet：任务集 由一组关联的，但相互之间没有Shuffle依赖关系的任务所组成的任务集。如图所示。 一个Stage创建一个TaskSet； 为Stage的每个Rdd分区创建一个Task,多个Task封装成TaskSet 1.15 Task：任务被送到某个Executor上的工作任务；单个分区数据集上的最小处理流程单元。如图所示 总体如图所示： 2. Spark运行基本流程 3. Spark运行架构特点3.1 Executor进程专属每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。如图所示。 3.2 支持多种资源管理器Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了，Spark支持资源管理器包含： Standalone、On Mesos、On YARN、Or On EC2。如图所示。 3.3 Job提交就近原则提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack（机架）里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。如图所示。 3.4 移动程序而非移动数据的原则执行Task采用了数据本地性和推测执行的优化机制。关键方法：taskIdToLocations、getPreferedLocations。如图所示。 4. Spark核心原理透视4.1 计算流程 4.2 从代码构建DAG图Val lines1 = sc.textFile(inputPath1).map(···).map(···)Val lines2 = sc.textFile(inputPath2).map(···)Val lines3 = sc.textFile(inputPath3)Val dtinone1 = lines2.union(lines3)Val dtinone = lines1.join(dtinone1)dtinone.saveAsTextFile(···)dtinone.filter(···).foreach(···) Spark的计算发生在RDD的Action操作，而对Action之前的所有Transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。 Spark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。 4.3 将DAG划分为Stage核心算法Application多个job多个Stage：Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。 划分依据：Stage划分的依据就是宽依赖，何时产生宽依赖，reduceByKey, groupByKey等算子，会导致宽依赖的产生。 核心算法：从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分。Spark内核会从触发Action操作的那个RDD开始从后往前推，首先会为最后一个RDD创建一个stage，然后继续倒推，如果发现对某个RDD是宽依赖，那么就会将宽依赖的那个RDD创建一个新的stage，那个RDD就是新的stage的最后一个RDD。然后依次类推，继续继续倒推，根据窄依赖或者宽依赖进行stage的划分，直到所有的RDD全部遍历完成为止。 4.4 将DAG划分为Stage剖析从HDFS中读入数据生成3个不同的RDD，通过一系列transformation操作后再将计算结果保存回HDFS。可以看到这个DAG中只有join操作是一个宽依赖，Spark内核会以此为边界将其前后划分成不同的Stage. 同时我们可以注意到，在图中Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作，通过map操作生成的partition可以不用等待整个RDD计算结束，而是继续进行union操作，这样大大提高了计算的效率。 4.5 提交Stages调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说，提交调度阶段的工作到此就完成了。而TaskScheduler的具体实现则会在得到计算资源的时候，进一步通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算。 4.6 监控Job、Task、ExecutorDAGScheduler监控Job与Task：要保证相互依赖的作业调度阶段能够得到顺利的调度执行，DAGScheduler需要监控当前作业调度阶段乃至任务的完成情况。这通过对外暴露一系列的回调函数来实现的，对于TaskScheduler来说，这些回调函数主要包括任务的开始结束失败、任务集的失败，DAGScheduler根据这些任务的生命周期信息进一步维护作业和调度阶段的状态信息。 DAGScheduler监控Executor的生命状态：TaskScheduler通过回调函数通知DAGScheduler具体的Executor的生命状态，如果某一个Executor崩溃了，则对应的调度阶段任务集的ShuffleMapTask的输出结果也将标志为不可用，这将导致对应任务集状态的变更，进而重新执行相关计算任务，以获取丢失的相关数据。 4.7 获取任务执行结果结果DAGScheduler：一个具体的任务在Executor中执行完毕后，其结果需要以某种形式返回给DAGScheduler，根据任务类型的不同，任务结果的返回方式也不同。 两种结果，中间结果与最终结果：对于FinalStage所对应的任务，返回给DAGScheduler的是运算结果本身，而对于中间调度阶段对应的任务ShuffleMapTask，返回给DAGScheduler的是一个MapStatus里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个调度阶段的任务获取输入数据的依据。 两种类型，DirectTaskResult与IndirectTaskResult：根据任务结果大小的不同，ResultTask返回的结果又分为两类，如果结果足够小，则直接放在DirectTaskResult对象内中，如果超过特定尺寸则在Executor端会将DirectTaskResult先序列化，再把序列化的结果作为一个数据块存放在BlockManager中，然后将BlockManager返回的BlockID放在IndirectTaskResult对象中返回给TaskScheduler，TaskScheduler进而调用TaskResultGetter将IndirectTaskResult中的BlockID取出并通过BlockManager最终取得对应的DirectTaskResult。 4.8 任务调度总体诠释 原文： https://www.toutiao.com/i6511498014832460301/?tt_from=weixin&amp;utm_campaign=client_share&amp;timestamp=1520998005&amp;app=news_article&amp;utm_source=weixin&amp;iid=26380623414&amp;utm_medium=toutiao_android&amp;wxshare_count=1]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 RDD操作]]></title>
    <url>%2Fspark-base-rdd-operations.html</url>
    <content type="text"><![CDATA[RDD支持两种类型的操作： 转换操作(transformations): 从现有数据集创建一个新数据集 动作操作(actions): 在数据集上进行计算后将值返回给驱动程序 例如，map 是一个转换操作，传递给每个数据集元素一个函数并返回一个新 RDD 表示返回结果。另一方面，reduce 是一个动作操作，使用一些函数聚合 RDD 的所有元素并将最终结果返回给驱动程序（尽管还有一个并行的 reduceByKey 返回一个分布式数据集）。 在 Spark 中，所有的转换操作(transformations)都是惰性(lazy)的，它们不会马上计算它们的结果。相反，它们仅仅记录应用到基础数据集(例如一个文件)上的转换操作。只有当 action 操作需要返回一个结果给驱动程序的时候， 转换操作才开始计算。 这个设计能够让 Spark 运行得更加高效。例如，我们知道：通过 map 创建的新数据集将在 reduce 中使用，并且仅仅返回 reduce 的结果给驱动程序，而不必将比较大的映射后的数据集返回。 1. 基础为了说明 RDD 基础知识，请考虑以下简单程序： Java版本:JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());int totalLength = lineLengths.reduce((a, b) -&gt; a + b); Scala版本:val lines = sc.textFile("data.txt")val lineLengths = lines.map(s =&gt; s.length)val totalLength = lineLengths.reduce((a, b) =&gt; a + b) Python版本:lines = sc.textFile("data.txt")lineLengths = lines.map(lambda s: len(s))totalLength = lineLengths.reduce(lambda a, b: a + b) 第一行定义了一个来自外部文件的基本 RDD。这个数据集并未加载到内存中或做其他处理：lines 仅仅是一个指向文件的指针。第二行将 lineLengths 定义为 map 转换操作的结果。其次，由于转换操作的惰性(lazy)，lineLengths 并没有立即计算。最后，我们运行 reduce，这是一个动作操作。此时，Spark 把计算分成多个任务(task)，并让它们运行在多台机器上。每台机器都运行 map 的一部分以及本地 reduce。然后仅仅将结果返回给驱动程序。 如果稍后还会再次使用 lineLength，我们可以在运行 reduce 之前添加： Java版本:lineLengths.persist(StorageLevel.MEMORY_ONLY()); Scala版本:lineLengths.persist() Python版本:lineLengths.persist() 这将导致 lineLength 在第一次计算之后被保存在内存中。 2. 传递函数给SparkSpark 的 API 很大程度上依赖于运行在集群上的驱动程序中的函数。 2.1 Java版本在 Java 中，函数由 org.apache.spark.api.java.function 接口实现。创建这样的函数有两种方法： 在你自己类中实现 Function 接口，作为匿名内部类或命名内部类，并将其实例传递给Spark。 使用 lambda 表达式 来简洁地定义一个实现。 虽然本指南的大部分内容都使用 lambda 语法进行简明说明，但很容易以长格式使用所有相同的API。例如，我们可以按照以下方式编写我们的代码： 匿名内部类JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new Function&lt;String, Integer&gt;() &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;);int totalLength = lineLengths.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;); 或者命名内部类class GetLength implements Function&lt;String, Integer&gt; &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());int totalLength = lineLengths.reduce(new Sum()); 请注意，Java中的匿名内部类也可以访问封闭范围内的变量，只要它们标记为final。 Spark会将这些变量的副本发送给每个工作节点，就像其他语言一样。 2.2 Scala版本有两种推荐的的方法可以做到这一点： 匿名函数语法，可用于短片段代码。 全局单例对象中的静态方法。例如，您可以定义对象 MyFunctions，然后传递 MyFunctions.func1，如下所示： object MyFunctions &#123; def func1(s: String): String = &#123; ... &#125;&#125;myRdd.map(MyFunctions.func1) 虽然也可以在类实例中传递方法的引用（与单例对象相反），但这需要将包含该类的对象与方法一起发送。 例如，考虑： class MyClass &#123; def func1(s: String): String = &#123; ... &#125; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(func1) &#125;&#125; 在这里，如果我们创建一个新的 MyClass 实例并调用 doStuff 方法，那么其中的 map 会引用该 MyClass 实例的 func1 方法，因此需要将整个对象发送到集群。它类似于编写 rdd.map（x =&gt; this.func1（x）） 。 以类似的方式，访问外部对象的字段将引用整个对象：class MyClass &#123; val field = "Hello" def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(x =&gt; field + x) &#125;&#125; 等价于 rdd.map（x =&gt; this.field + x），它引用了 this 对象的所有东西 。为了避免这个问题，最简单的方法是将字段复制到本地变量中，而不是从外部访问它：def doStuff(rdd: RDD[String]): RDD[String] = &#123; val field_ = this.field rdd.map(x =&gt; field_ + x)&#125; 3. 使用键值对虽然大多数 Spark 操作可以在任意类型对象的 RDD 上工作，但是还是几个特殊操作只能在键值对的 RDD 上使用。最常见的是分布式 shuffle 操作，例如按键分组或聚合元素。 3.1 Java版本在 Java 中，使用 Scala 标准库中的 scala.Tuple2 类来表示键值对。可以如下简单地调用 new Tuple2（a，b） 来创建一个元组，然后用 tuple._1（） 和 tuple._2（） 访问它的字段。 键值对的 RDD 由 JavaPairRDD 类表示。你可以使用特殊版本的 map 操作（如 mapToPair 和 flatMapToPair）从 JavaRDD 来构建 JavaPairRDD。JavaPairRDD 具有标准的 RDD 函数以及特殊的键值对函数。 例如，以下代码在键值对上使用 reduceByKey 操作来计算每行文本在文件中的出现次数：JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaPairRDD&lt;String, Integer&gt; pairs = lines.mapToPair(s -&gt; new Tuple2(s, 1));JavaPairRDD&lt;String, Integer&gt; counts = pairs.reduceByKey((a, b) -&gt; a + b); 例如，我们也可以使用 counts.sortByKey（） 来按字母顺序来对键值对排序，最后使用 counts.collect（） 将结果作为对象数组返回到驱动程序。 3.2 Scala版本在 Scala 中，这些操作在包含 Tuple2 对象的 RDD 上可以自动获取（内置元组，通过简单写入（a，b）创建）。在 PairRDDFunctions 类中提供了键值对操作，该类自动包装元组的 RDD。 例如，以下代码在键值对上使用 reduceByKey 操作来计算每行文本在文件中的出现次数：val lines = sc.textFile("data.txt")val pairs = lines.map(s =&gt; (s, 1))val counts = pairs.reduceByKey((a, b) =&gt; a + b) 例如，我们也可以使用 counts.sortByKey（） 来按字母顺序来对键值对排序，最后使用 counts.collect（） 将结果作为对象数组返回到驱动程序。 在键值对操作时使用一个自定义对象作为 key 的时候，你需要确保自定义 equals() 方法和 hashCode() 方法是匹配的。更加详细的内容，查看 Object.hashCode()) 文档)中的契约概述。 4. 转换操作下面列出了Spark支持的一些常见转换函数。 有关详细信息，请参阅RDD API文档（Scala，Java，Python，R）和RDD函数doc（Scala，Java）。 4.1 map(func) 映射将函数应用于 RDD 中的每个元素，将返回值构成新的 RDD。List&lt;String&gt; aList = Lists.newArrayList("a", "B", "c", "b");JavaRDD&lt;String&gt; rdd = sc.parallelize(aList);// 小写转大写JavaRDD&lt;String&gt; upperLinesRDD = rdd.map(new Function&lt;String, String&gt;() &#123; @Override public String call(String str) throws Exception &#123; if (StringUtils.isBlank(str)) &#123; return str; &#125; return str.toUpperCase(); &#125;&#125;);// A B C B 4.2 filter(func) 过滤返回通过选择那些 func 函数返回 true 的元素形成一个新的 RDD。List&lt;String&gt; list = Lists.newArrayList("a", "B", "c", "b");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);// 只返回以a开头的字符串JavaRDD&lt;String&gt; filterRDD = rdd.filter(new Function&lt;String, Boolean&gt;() &#123; @Override public Boolean call(String str) throws Exception &#123; return !str.startsWith("a"); &#125;&#125;);// B c b 4.3 flatMap(func) 一行转多行类似于 map 函数，但是每个输入项可以映射为0个输出项或更多输出项（所以 func 应该返回一个序列而不是一个条目）。List&lt;String&gt; list = Lists.newArrayList("a 1", "B 2");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);// 一行转多行 以空格分割JavaRDD&lt;String&gt; resultRDD = rdd.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) throws Exception &#123; if (StringUtils.isBlank(s)) &#123; return null; &#125; String[] array = s.split(" "); return Arrays.asList(array).iterator(); &#125;&#125;);// a// 1// B// 2 4.4 distinct([numTasks]))去重List&lt;String&gt; aList = Lists.newArrayList("1", "3", "2", "3");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);// 去重JavaRDD&lt;String&gt; rdd = aRDD.distinct(); // 1 2 3 4.5 union(otherDataset) 并集生成一个包含两个 RDD 中所有元素的 RDD。如果输入的 RDD 中有重复数据，union() 操作也会包含这些重复的数据．List&lt;String&gt; aList = Lists.newArrayList("1", "2", "3");List&lt;String&gt; bList = Lists.newArrayList("3", "4", "5");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 并集JavaRDD&lt;String&gt; rdd = aRDD.union(bRDD); // 1 2 3 3 4 5 4.6 intersection(otherDataset) 交集求两个 RDD 共同的元素的 RDD。 intersection() 在运行时也会去掉所有重复的元素，尽管 intersection() 与 union() 的概念相似，但性能却差的很多，因为它需要通过网络混洗数据来发现共同的元素。List&lt;String&gt; aList = Lists.newArrayList("1", "2", "3");List&lt;String&gt; bList = Lists.newArrayList("3", "4", "5");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 交集JavaRDD&lt;String&gt; rdd = aRDD.intersection(bRDD); // 3 4.7 subtract(otherDataset) 差集subtract 接受另一个 RDD 作为参数，返回一个由只存在第一个 RDD 中而不存在第二个 RDD 中的所有元素组成的 RDDList&lt;String&gt; aList = Lists.newArrayList("1", "2", "3");List&lt;String&gt; bList = Lists.newArrayList("3", "4", "5");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 差集JavaRDD&lt;String&gt; rdd = aRDD.subtract(bRDD); // 1 2 4.8 groupByKey 分组根据键值对 key 进行分组。 在（K，V）键值对的数据集上调用时，返回（K，Iterable ）键值对的数据集。 如果分组是为了在每个 key 上执行聚合（如求总和或平均值），则使用 reduceByKey 或 aggregateByKey 会有更好的性能。默认情况下，输出中的并行级别取决于父 RDD 的分区数。你可以传递一个可选参数 numTasks 来设置任务数量。 Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Banana", 9);Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("Apple", 4);List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Lists.newArrayList();list.add(t1);list.add(t2);list.add(t3);list.add(t4);JavaPairRDD&lt;String, Integer&gt; rdd = sc.parallelizePairs(list);// 分组JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupRDD = rdd.groupByKey();// Apple --- 4// Pear --- 5// Banana --- 10 9 4.9 reduceByKey(func, [numTasks]) 根据key聚合当在（K，V）键值对的数据集上调用时，返回（K，V）键值对的数据集，使用给定的reduce函数 func 聚合每个键的值，该函数类型必须是（V，V）=&gt; V。类似于 groupByKey，可以通过设置可选的第二个参数来配置reduce任务的数量。 Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Banana", 9);Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("Apple", 4);List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Lists.newArrayList();list.add(t1);list.add(t2);list.add(t3);list.add(t4);JavaPairRDD&lt;String, Integer&gt; rdd = sc.parallelizePairs(list);// 分组计算JavaPairRDD&lt;String, Integer&gt; reduceRDD = rdd.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125;&#125;);// Apple --- 4// Pear --- 5// Banana --- 19 4.10 sortByKey([ascending], [numPartitions])根据key进行排序。在（K，V）键值对的数据集调用，其中 K 实现 Ordered 接口，按照升序或降序顺序返回按键排序的（K，V）键值对的数据集。Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Apple", 4);List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Lists.newArrayList();list.add(t1);list.add(t2);list.add(t3);JavaPairRDD&lt;String, Integer&gt; rdd = sc.parallelizePairs(list);// 根据key排序JavaPairRDD&lt;String, Integer&gt; sortRDD = rdd.sortByKey(true);// Apple --- 4// Banana --- 10// Pear --- 5 4.11 coalesce(numPartitions) 合并分区对 RDD 分区进行合并，合并后分区数目为 numPartitions。大型数据集过滤之后可以对高效地运行操作很有帮助。List&lt;String&gt; list = Lists.newArrayList("1", "2", "3", "4");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);JavaRDD&lt;String&gt; coalesceRDD = rdd.coalesce(2, false);System.out.println("分区个数:" + coalesceRDD.partitions().size());// 分区个数:1JavaRDD&lt;String&gt; coalesceRDD2 = rdd.coalesce(2, true);System.out.println("分区个数:" + coalesceRDD2.partitions().size());// 分区个数:2 如果可选参数 shuff 为 false 时，传入的参数大于现有的分区数目，RDD 的分区数不变，也就是说不经过shuffle，是无法将 RDD 的分区数增大。 4.12 repartition(numPartitions) 重新分区对 RDD 中的数据重新洗牌来重新分区，分区数目可以增大也可以减少，并在各分区之间进行数据平衡。这总是通过网络混洗所有数据。List&lt;String&gt; list = Lists.newArrayList("1", "2", "3", "4");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);JavaRDD&lt;String&gt; coalesceRDD = rdd.repartition(2);System.out.println("分区个数:" + coalesceRDD.partitions().size());// 分区个数:2 我们从源码看到 repartition 只是 coalesce(numPartitions, shuffle = true) 的一个简易实现：def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; coalesce(numPartitions, shuffle = true)&#125; 4.13 cartesian(otherDataset) 笛卡尔积对两个 RDD 中的所有元素进行笛卡尔积操作。在类型为 T 和 U 的两个数据集上调用时，返回（T，U）键值对（所有元素对）数据集。List&lt;String&gt; aList = Lists.newArrayList("1", "2");List&lt;String&gt; bList = Lists.newArrayList("3", "4");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 笛卡尔积JavaPairRDD&lt;String, String&gt; cartesianRDD = aRDD.cartesian(bRDD);// (1, 3)// (1, 4)// (2, 3)// (2, 4) 4.14 cogroup(otherDataset, [numPartitions])在类型（K，V）和（K，W）两个数据集上调用时，返回（K，（Iterable ，Iterable ））元组的数据集。这个操作也被称为 groupWith。Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Apple", 4);Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("Banana", 2);Tuple2&lt;String, Integer&gt; t5 = new Tuple2&lt;String, Integer&gt;("Apple", 11);Tuple2&lt;String, Integer&gt; t6 = new Tuple2&lt;String, Integer&gt;("Banana", 7);List&lt;Tuple2&lt;String, Integer&gt;&gt; list1 = Lists.newArrayList();list1.add(t1);list1.add(t2);list1.add(t3);List&lt;Tuple2&lt;String, Integer&gt;&gt; list2 = Lists.newArrayList();list2.add(t4);list2.add(t5);list2.add(t6);JavaPairRDD&lt;String, Integer&gt; rdd1 = sc.parallelizePairs(list1);JavaPairRDD&lt;String, Integer&gt; rdd2 = sc.parallelizePairs(list2);JavaPairRDD&lt;String, Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt;&gt; cogroupRDD = rdd1.cogroup(rdd2);cogroupRDD.foreach(new VoidFunction&lt;Tuple2&lt;String, Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt;&gt; group) throws Exception &#123; String key = group._1; Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt; value = group._2; System.out.println(key + " --- " + value.toString()); &#125;&#125;);// Apple --- ([4],[11])// Pear --- ([5],[])// Banana --- ([10],[2, 7]) 5. 动作操作 (Action)下面列出了Spark支持的一些常见操作。 5.1 reduce接收一个函数作为参数，这个函数要操作两个相同元素类型的RDD并返回一个同样类型的新元素．List&lt;String&gt; aList = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(aList);String result = rdd.reduce(new Function2&lt;String, String, String&gt;() &#123; @Override public String call(String v1, String v2) throws Exception &#123; return v1 + "#" + v2; &#125;&#125;);System.out.println(result); // aa#bb#cc#dd 5.2 collect将整个RDD的内容返回．List&lt;String&gt; list = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);List&lt;String&gt; collect = rdd.collect();System.out.println(collect); // [aa, bb, cc, dd] 5.3 take(n)返回 RDD 中的n个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合．需要注意的是，这些操作返回元素的顺序与你的预期可能不一样．List&lt;String&gt; list = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);List&lt;String&gt; collect = rdd.take(3);System.out.println(collect); // [aa, bb, cc] 5.4 takeSample有时需要在驱动器程序中对我们的数据进行采样，takeSample(withReplacement, num, seed) 函数可以让我们从数据中获取一个采样，并指定是否替换． 5.5 saveAsTextFile(path)将数据集的元素写入到本地文件系统，HDFS 或任何其他 Hadoop 支持的文件系统中的给定目录的文本文件（或文本文件集合）中。Spark 在每个元素上调用 toString 方法将其转换为文件中的一行文本。List&lt;String&gt; list = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);rdd.saveAsTextFile("/home/xiaosi/output"); 输出:xiaosi@ying:~/output$ cat *aabbccdd 5.6 saveAsSequenceFile(path)将数据集的元素写入到本地文件系统，HDFS 或任何其他 Hadoop 支持的文件系统中的给定路径下的 Hadoop SequenceFile中。这在实现 Hadoop 的 Writable 接口的键值对的 RDD 上可用。 在 Scala 中，它也可用于可隐式转换为 Writable 的类型（Spark包含Int，Double，String等基本类型的转换）。 5.7 foreach(func)在数据集的每个元素上运行函数 func。这通常用于副作用，如更新累加器或与外部存储系统交互。 修改foreach（）之外的变量而不是累加器可能会导致未定义的行为。有关更多详细信息，请参阅了解闭包 Spark版本:2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#rdd-operations]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 创建RDD]]></title>
    <url>%2Fspark-base-build-resilient-distributed-datasets.html</url>
    <content type="text"><![CDATA[Spark的核心概念是弹性分布式数据集（RDD），RDD 是一个可容错、并行操作的分布式元素集合。有两种方法可以创建 RDD 对象： 在驱动程序中并行化操作已存在集合来创建 RDD 从外部存储系统中引用数据集（如：共享文件系统、HDFS、HBase 或者其他 Hadoop 支持的数据源）。 1. 并行化集合在你驱动程序的现有集合上调用 JavaSparkContext 的 parallelize 方法创建并行化集合(Parallelized collections)。集合的元素被复制以形成可以并行操作的分布式数据集。 例如，下面是如何创建一个包含数字1到5的并行化集合： Java版本：List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5);JavaRDD&lt;Integer&gt; rdd = sc.parallelize(list); Scala版本：val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data) Python版本：data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) RDD 一旦创建，分布式数据集（distData）就可以并行操作。例如，我们可以调用 distData.reduce（（a，b） - &gt; a + b） 来实现对列表元素求和。我们稍后介绍分布式数据集的操作。 并行化集合的一个重要参数是将数据集分割成多少分区的 partitions 个数。Spark 集群中每个分区运行一个任务(task)。典型场景下，一般为每个CPU分配2－4个分区。但通常而言，Spark 会根据你集群的情况，自动设置分区数。当然，你可以给 parallelize 方法传递第二个参数来手动设置分区数（如：sc.parallelize(data, 10)）。 Spark代码里有些地方仍然使用分片（slice）这个术语(分区的同义词)，主要为了保持向后兼容。 2. 外部数据集Spark 可以从 Hadoop 支持的任何存储数据源创建分布式数据集，包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等。Spark 也支持文本文件，SequenceFiles 以及任何其他 Hadoop 输入格式。 文本文件 RDD 可以使用 SparkContext 的 textFile 方法创建。该方法根据URL获取文件（机器的本地路径，或 hdfs:// ， s3n:// 等等），并按行读取。下面是一个示例调用： Java版本：JavaRDD&lt;String&gt; distFile = sc.textFile("data.txt"); Scala版本：scala&gt; val distFile = sc.textFile("data.txt")distFile: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[10] at textFile at &lt;console&gt;:26 Python版本：&gt;&gt;&gt; distFile = sc.textFile(&quot;data.txt&quot;) 一旦创建完成，就可以在 distFiile 上做数据集操作。例如，我们可以用下面的方式使用 map 和 reduce 操作将所有行的长度相加：distFile.map(s -&gt; s.length()).reduce((a, b) -&gt; a + b); Spark读文件时一些注意事项： (1) 如果使用本地文件系统路径，在所有工作节点上该文件必须都能用相同的路径访问到。要么能复制文件到所有的工作节点，要么能使用网络的方式共享文件系统。 (2) Spark 所有基于文件的输入方法，包括 textFile，能很好地支持文件目录，压缩文件和通配符。例如，你可以使用:textFile(&quot;/my/directory&quot;)textFile(&quot;/my/directory/*.txt&quot;)textFile(&quot;/my/directory/*.gz&quot;) (3) textFile 方法也可以选择第二个可选参数来控制文件分区数目，默认情况下，Spark 为每一个文件块创建一个分区（HDFS中分块大小默认为128MB），你也可以通过传递一个较大数值来请求更多分区。注意的是，分区数目不能少于分块数目。 除了文本文件，Spark 的 Java API 还支持其他几种数据格式： (1) JavaSparkContext.wholeTextFiles 可以读取包含多个小文本文件的目录，并将它们以（文件名，内容）键值对返回。这与 textFile 相反，textFile 将在每个文件中每行返回一条记录。JavaPairRDD&lt;String, String&gt; rdd = sc.wholeTextFiles("/home/xiaosi/wholeText");List&lt;Tuple2&lt;String, String&gt;&gt; list = rdd.collect();for (Tuple2&lt;?, ?&gt; tuple : list) &#123; System.out.println(tuple._1() + ": " + tuple._2());&#125; (2) 对于 SequenceFiles，可以使用 SparkContext 的 sequenceFile[K，V] 方法，其中 K 和 V 是文件中的键和值的类型。这些应该是 Hadoop 的 Writable 接口的子类，如 IntWritable 和 Text。 (3) 对于其他 Hadoop InputFormats，你可以使用 JavaSparkContext.hadoopRDD 方法，该方法采用任意 JobConf 和输入格式类，键类和值类。将这些设置与使用输入源的 Hadoop 作业相同。你还可以使用基于“新” MapReduce API（org.apache.hadoop.mapreduce）的 InputFormats 的 JavaSparkContext.newAPIHadoopRDD。 (4) JavaRDD.saveAsObjectFile 和 SparkContext.objectFile 支持保存一个 RDD，保存格式是一个简单的 Java 对象序列化格式。这是一种效率不高的专有格式，如 Avro，它提供了简单的方法来保存任何一个 RDD。 Spark版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#resilient-distributed-datasets-rdds]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 初始化]]></title>
    <url>%2Fspark-base-initializing-spark.html</url>
    <content type="text"><![CDATA[1. 初始化Spark 程序必须做的第一件事是创建一个 JavaSparkContext 对象(Scala和Python中是SparkContext对象)，这告诉了 Spark 如何访问集群。要创建 SparkContext，你首先需要构建一个包含有关应用程序信息的 SparkConf 对象。 Java版本：private static String appName = "JavaWordCountDemo";private static String master = "local";// 初始化Sparkprivate static SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);private static JavaSparkContext sc = new JavaSparkContext(conf); Scala版本：val conf = new SparkConf().setAppName(appName).setMaster(master)new SparkContext(conf) 每个 JVM 只能有一个 SparkContext 处于活跃状态。在创建新的 SparkContext 之前，必须先调用 stop() 方法停止之前活跃的 SparkContext。 Python版本：conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) appName 参数是应用程序在集群UI上显示的名称。master 是 Spark，Mesos 或 YARN 集群的 URL，或以本地模式运行的特殊字符串 local。实际上，当在集群上运行时，你不需要在程序中写死 master，而是使用 spark-submit 启动应用程序并以参数传递进行接收。但是，对于本地测试和单元测试，你可以通过 local 来运行 Spark 进程。 2. 使用Shell在 Spark shell 中，已经为你创建了一个专有的 SparkContext，可以通过变量 sc 访问。你自己创建的 SparkContext 将无法工作。可以用 --master 参数来设置 SparkContext 要连接的集群，用 --jars 来设置需要添加到 classpath 中的 JAR 包，如果有多个 JAR 包使用逗号分割符连接它们。你还可以通过 --packages 参数提供逗号分隔的 maven 坐标列表，将依赖关系（例如Spark Packages）添加到 shell 会话中。依赖项存在的任何可选存储库（例如Sonatype）可以传递给 --repositories 参数。例如：在一个拥有 4 核的环境上运行 bin/spark-shell，使用： ./bin/spark-shell --master local[4] 或者，还可以将 code.jar 添加到其 classpath 中，请使用：./bin/spark-shell --master local[4] --jars code.jar 使用maven坐标来包含依赖项：./bin/spark-shell --master local[4] --packages &quot;org.example:example:0.1&quot; 可以执行 spark-shell --help 获取完整的选项列表。spark-shell 调用的是更常用的spark-submit脚本。 Spark 版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#initializing-spark]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 引入Spark]]></title>
    <url>%2Fspark-base-linking-with-spark.html</url>
    <content type="text"><![CDATA[1. Java版Spark 2.3.0 支持用于简洁编写函数的 lambda 表达式，你也可以使用 org.apache.spark.api.java.function 包中的类。 请注意，在 Spark 2.2.0 中删除了对 Java 7 的支持。 要在 Java 中编写 Spark 应用程序，需要在 Spark 上添加依赖项。Spark可通过 Maven 仓库获得：groupId = org.apache.sparkartifactId = spark-core_2.11version = 2.3.0 另外，如果希望访问 HDFS 集群，需要根据你的 HDFS 版本添加 hadoop-client 的依赖：groupId = org.apache.hadoopartifactId = hadoop-clientversion = &lt;your-hdfs-version&gt; 最后，你需要将一些 Spark 类导入到程序中。 添加以下行：import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.SparkConf; 2. Scala版默认情况下，Spark 2.3.0 在 Scala 2.11 上构建并分布式运行。（Spark 可以与其他版本的 Scala 一起构建。）要在 Scala 中编写应用程序，需要使用兼容的 Scala 版本（例如2.11.X）。 要编写 Spark 应用程序，需要在 Spark 上添加依赖项。Spark 可通过 Maven 仓库获得：groupId = org.apache.sparkartifactId = spark-core_2.11version = 2.3.0 另外，如果希望访问 HDFS 集群，则需要根据你的 HDFS 版本添加 hadoop-client 的依赖：groupId = org.apache.hadoopartifactId = hadoop-clientversion = &lt;your-hdfs-version&gt; 最后，需要将一些 Spark 类导入到程序中。 添加以下行：import org.apache.spark.SparkContextimport org.apache.spark.SparkConf 备注 在 Spark 1.3.0 之前，需要明确导入 org.apache.spark.SparkContext._ 以启用基本的隐式转换。 备注 Spark版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#linking-with-spark]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 第一个Spark程序WordCount]]></title>
    <url>%2Fspark-first-application-word-count.html</url>
    <content type="text"><![CDATA[1 Maven 依赖&lt;spark.version&gt;2.1.0&lt;/spark.version&gt;&lt;!-- spark --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2. JavaWordCountpackage com.sjf.open.spark;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.sql.SparkSession;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;import java.util.List;import java.util.regex.Pattern;/** * Created by xiaosi on 17-2-13. * * Spark 测试程序 WordCount * */public final class JavaWordCount &#123; private static final Pattern SPACE = Pattern.compile(" "); public static void main(String[] args) throws Exception &#123; if (args.length &lt; 1) &#123; System.err.println("Usage: JavaWordCount &lt;file&gt;"); System.exit(1); &#125; SparkSession spark = SparkSession.builder().appName("JavaWordCount").getOrCreate(); JavaRDD&lt;String&gt; lines = spark.read().textFile(args[0]).javaRDD(); JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) &#123; return Arrays.asList(SPACE.split(s)).iterator(); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;); List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect(); for (Tuple2&lt;?, ?&gt; tuple : output) &#123; System.out.println(tuple._1() + ": " + tuple._2()); &#125; spark.stop(); &#125;&#125; 3. 命令行执行使用Maven 进行打包：mvn cleanmvn package 使用上述命令打包后，会在项目根目录下的target目录生成jar包。打完jar包后，我们可以使用spark-submit提交任务：bin/spark-submit --class com.sjf.open.spark.JavaWordCount --master local /home/xiaosi/code/Common-Tool/target/common-tool-jar-with-dependencies.jar /home/xiaosi/a.txtee: 1aa: 3dd: 2vvv: 1ff: 2bb: 3cc: 1 4. Idea本地调试运行配置]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 分布式数据流的轻量级异步快照]]></title>
    <url>%2Flightweight-asynchronous-snapshots-for-distributed-dataflows.html</url>
    <content type="text"><![CDATA[1. 概述分布式有状态流处理支持在云中部署和执行大规模连续计算，主要针对低延迟和高吞吐量。这种模式的一个最根本的挑战就是在可能的失败情况下提供处理保证。现有方法依赖于可用于故障恢复的周期性全局状态快照。这些方法有两个主要缺点。首先，他们经常拖延影响数据摄取的整体计算过程。其次，持久化存储所有传输中的记录以及算子状态，这会导致比所需的快照要更大。 因此，提出了一种新的分布式快照的算法，即在 Apache Flink 中的异步屏障快照（Asynchronous Barrier Snapshotting (ABS)）。这是一种适用于现代数据流执行引擎的轻量级算法，可最大限度地减少空间需求，让快照发生时对系统的影响降到最低。这种算法不会停止流处理，它只会引入很少的运行时间开销，而且对于整个无环图的拓扑结构，只对有状态的算子进行快照，因此快照的大小只会占用很小的空间。该算法不会对执行产生重大影响，保证线性可伸缩性，并且可以在频繁的快照下正常运行。 这里所说的新型的快照算法，既适用于有向无环图，也适用于有向有环图。本文重点关注在有向无环图中的应用。 2. Apache Flink SystemApache Flink 围绕通用运行时引擎进行架构，可以统一处理批处理和流式作业。Flink 中的作业被编译成任务的有向图。数据元素从外部数据源获取，并以流水线方式通过任务图。基于接收到的输入，任务不断操作其内部状态，并产生新的输出。 2.1 流处理编程模型可以从外部来源（例如消息队列，套接字流，自定义生成器）或通过调用其他 DataStream 上的操作来创建 DataStreams。DataStreams 支持多种算子，如 map，filter 和 reduce 等形式的高阶函数，这些函数在每个记录上逐步应用并生成新的 DataStream。每个算子可以通过将并行实例放置在相应流的不同分区上运行来并行化，从而允许分布式执行流转换。 下面的代码示例中显示了如何在 Apache Flink 中实现简单的 Word Count 程序。在此程序中，从文本文件中读取单词，并将每个单词的当前计数打印到标准输出上。这是一个有状态的流处理程序，所以数据源需要知道它们在文件中的当前偏移量，并且需要计数器来将每个单词的当前计数保持在内部状态中。 2.2 分布式数据流执行当用户执行一个应用程序时，所有的 DataStream 算子都将编译成一个执行图，原理上为一个有向图 G =（T，E），其中顶点 T 表示任务，边 E 表示两个任务之间的 data channels。上图就描绘了一个 Word Count 例子的执行图。如图所示，算子的每个实例都封装在相应的任务上。任务可以进一步细分为没有 input channels 的 Source 以及没有 output channels 的 Sink。此外，M 表示任务在并行执行期间传输的所有记录的集合。每个任务 t ∈ T 封装了一个算子实例的独立运行，其由以下内容组成： 一组 input channels 和 output channels: It , Ot ⊆ E 算子状态 St 用户自定义函数 ft 。在执行过程中，每个任务消耗输入记录，更新算子状态并根据其用户自定义函数生成新的记录。对于流入算子的每一条数据 r ∈ M，通过 UDF，产生一个新的状态值 st’，同时产生一个新的输出的集合 D ⊆ M。 3. Asynchronous Barrier Snapshotting为了提供一致性结果，分布式处理系统需要对失败任务进行恢复。提供这种弹性的一种方法是定期捕获执行图的快照，然后可以用它来从故障中恢复。快照是执行图的全局状态，捕获所有必要信息以从该特定执行状态重新开始计算。 3.1 问题定义我们定义了一个执行图 G =（T，E） 的全局快照 G * =（T *，E *），其中 T * 和 E * 分别表示所有任务和边的状态集合。更详细地说，T * 由所有算子状态 St * ∈ T * 组成， ∀t ∈ T， E * 是所有 channels 状态 e * ∈ E * 的集合，其中 e * 由在 e 上传输的记录组成。 我们确保每个快照 G * 都保留某些属性，例如最终性 Termination 和可行性 Feasibility，以便在故障恢复后保证结果的正确性。最终性保证，如果所有进程都处于活跃状态，那么快照算法最终会在启动后的有限时间内完成。可行性表达了快照的意义，即在快照过程中关于计算的信息不会丢失。 3.2 非循环数据流的ABS当一个执行过程被分成多个阶段 (stage)，在不保留 channels 状态的情况下执行快照是可行的。stage 将注入的数据流和所有相关的计算划分为一系列可能的执行过程，其中所有先前的输入和生成的输出已经完全处理。在一个 Stage 结束时的算子状态集合反映了整个执行历史，因此它可以用于快照。我们算法背后的核心思想是在保持连续数据摄入的同时使用分段快照创建一致性快照（create identical snapshots with staged snapshotting）。 在我们的方法中，在持续的数据流执行中模拟 stage 是通过向数据流中周期性注入特殊屏障 barrier 标记完成的，这些标记在整个执行图中一直传输到 sink。全局快照是随着每个任务接收表示执行 stage 的 barrier 而逐步构建的。 我们进一步为我们的算法做出以下假设： 网络通道是准确可靠的，遵循 FIFO 先进先出的顺序，并且可以被阻塞以及解除阻塞。当一个 channels 被阻塞时，所有的消息都会被缓存，但是不会被传递，直到它被解除阻塞。 任务可以触发其通道组件上的操作，例如阻塞，解除阻塞和发送消息。所有输出通道都支持广播消息。 在 source 任务中注入的消息(即 stage barrier )被解析为 Nil。 算法执行过程如下： (1) 中央协调器周期性的给所有 source 注入 stage barrier（黑色实线）。当 source 接收到 barrier 时，会为当前的状态生成一个快照，然后将 barrier 广播到它的所有输出中（图（a））。 (2) 当一个非 source 任务接收到其中一个输入的 barrier 时，它会阻塞该输入，直到它接收到来自所有输入的 barrier（第9行 图2（b））。 (3) 当收到来自所有输入的 barrier 时，该任务会生成当前状态的一个快照并将其 barrier 广播到其输出（第12-13行 图2（c））。 (4) 然后，该任务解除输入通道的阻塞来继续后续的计算（第15行，图2（d））。完整的全局快照 G * =（T *，E *） 仅包含所有算子状态 T *，其中 E * = 0。 伪代码如下： 如前所述，快照算法应该保证最终性和可行性。最终性由通道和非循环执行图属性保证。channels 的可靠性确保只要任务存活，发送的每个 barrier 最终都会被接收的。此外，由于始终存在来自 source 的一条路径，因此 DAG 拓扑中的每个任务最终都将从其所有 input channels 接收 barrier 并生成快照。 对于可行性，足以证明在全局快照中的算子状态仅反映了直到最后阶段处理的记录的历史过程。这可以通过 channels 的 FIFO 顺序特性以及 barrier 全部接收之前阻塞 input channels 保证 stage 的 post-shot 记录不会在快照生成之前处理。 3.3 循环数据流的ABS在存在有向循环的执行图中的情况下，上面的 ABS 算法不会终止而会导致死锁，因为一个循环中的任务将无限期地等待接收来自其所有输入的 barrier。此外，在循环中传输的记录不会包含在快照中，因此违反了可行性。因此，为了可行性需要在快照中包含在循环中生成的所有记录，并在恢复时将这些记录重新传输。我们处理循环图的方法继承了基本算法，而不会像上面算法中看到的那样引起任何额外的 channels 阻塞。首先，我们通过静态分析来识别执行图中循环中的 back-edge L。根据控制流图理论，有向图中的 back-edge 是指在深度优先搜索中已经访问过的顶点的边。执行图 G（T，E \ L） 是一个包含拓扑中所有任务的 DAG。从该 DAG 的角度来看，该算法与以前一样运行，但是，我们需要在快照期间对下游 back-edge 接收的记录进行备份。barrier 将循环中的所有记录都推送到下游日志中，以便将它们包含在一致的快照中。 4. 故障恢复在这提供关于故障恢复操作的简要说明。有几种故障恢复方案可用于一致性快照。最简单的是，整个执行图可以从上一个全局快照重新启动，每个任务 t ，如下所示： 从持久性存储中检索与快照 St 相关联的状态并将其设置为其初始状态 恢复备份的日志以及处理所包含的记录 从其 input channels 开始摄取记录 仅通过重新调度上游任务依赖（其包含到失败任务的 output channels）以及它们各自直到 source 的上游任务，重新恢复调度部分图也是可能的。下图显示了一个恢复示例。为了提供 exactly-once 的语义，应该在所有下游节点中忽略重复记录以避免重新计算。为了达到这个目的，我们用来自source 的序列号标记记录，因此，每个下游节点都可以丢弃序号小于他们已经处理的记录。 5. 实现我们向 Apache Flink 提供了 ABS 算法的实现，以便为流式运行提供 exactly-once 处理语义。在我们当前的实现中，阻塞通道将所有传入的记录存储在磁盘上，而不是将它们保存在内存中以增加可扩展性。虽然此技术可确保鲁棒性，但会增加 ABS 算法的运行时影响。 为了区分算子状态和数据，我们引入了一个显式的 OperatorState 接口，该接口包含更新状态以及对状态进行检查点的方法。我们为 Apache Flink 支持的有状态运行时算子（例如基于偏移量的源或聚合）提供了 OperatorState 实现。 6. 评估评估的目标是将 ABS 的运行时间开销与 Naiad 中采用的全局同步快照算法进行比较，并测试算法在大数量节点上的可伸缩性。用于评估的执行拓扑结构（如下图）由6个不同的算子组成，其并行度等于集群节点的个数，转换为 6 * 集群大小 个任务顶点。执行包含3个完整的网络 shuffle，以突显 ABS 中通道阻塞的可能影响。source 产生总共10亿条记录，这些记录在 source 实例中均匀分布。拓扑中算子的状态是按键的聚合以及 source 的偏移量。在 Amazon EC2 集群上使用多达40个 m3.medium 实例在运行实验。 我们测量了在不同快照间隔下 ABS 和同步快照两种快照方案运行的运行时间开销。我们实现了在 Apache Flink Naiad 上使用的同步快照算法，以便在相同终端上执行进行比较。该实验在10节点集群上运行。为了评估我们算法的可伸缩性，我们处理固定数量的输入记录（10亿），同时将我们拓扑的并行度从5个增加到40个节点。 在下图中，我们描述了两种算法对基线的运行时影响（无容错）。当快照时间间隔较小时，同步快照的性能影响尤其明显。这是由于系统花费更多时间来获取全局快照而不是处理数据。ABS 对运行时的影响要低得多，因为它可以持续运行而不会阻碍整体执行，同时保持相当稳定的吞吐率。当快照时间间隔变大时，同步算法的影响逐渐变小。 在下图中，我们使用3秒快照间隔的 ABS 拓扑与基准（无容错）进行比较可扩展性。很明显，基准作业和 ABS 都实现了线性可扩展性。 7. 总结我们的目的是解决在分布式数据流系统上执行定期全局快照的问题。我们引入了 ABS，这是一种新的快照技术，可实现良好的吞吐量。ABS 是第一种考虑非循环执行拓扑的最小可能状态的算法。此外，我们通过仅存储需要在恢复时重新处理的记录来扩展 ABS 以在循环执行图上使用。我们在 Apache Flink 上实现了 ABS，并对比同步快照算法评估了我们算法的性能。在早期阶段，ABS 显示出良好的结果，对整体执行吞吐量影响较小并具有线性可扩展性。 原文: http://pdfs.semanticscholar.org/541e/cf8c5b9db97eb5fcd1ffdf863d948b8954cc.pdf]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink HDFS Connector]]></title>
    <url>%2Fflink-stream-hdfs-connector.html</url>
    <content type="text"><![CDATA[此连接器提供一个 Sink，将分区文件写入 Hadoop FileSystem 支持的任何文件系统。要使用此连接器，添加以下依赖项：&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-filesystem_2.11&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 分桶文件Sink分桶(Bucketing)行为以及写入数据操作都可以配置，我们稍后会讲到。下面展示如何通过默认配置创建分桶Sink，输出到按时间切分的滚动文件中： Java版本:DataStream&lt;String&gt; input = ...;input.addSink(new BucketingSink&lt;String&gt;("/base/path")); Scala版本:val input: DataStream[String] = ...input.addSink(new BucketingSink[String]("/base/path")) 这里唯一需要的参数是这些分桶存储的基本路径 /base/path。可以通过指定自定义 bucketer，writer 和 batch size 来进一步配置 sink。 默认情况下，分桶 Sink 根据元素到达时的系统时间来进行切分，并使用 yyyy-MM-dd--HH 时间格式来命名这些分桶。这个时间格式会跟当前系统时间一起传递给 SimpleDateFormat 来命名分桶路径。用户还可以为 bucketer 指定时区以格式化分桶路径。每当遇到一个新的时间就会创建一个新的分桶。例如，如果你有一个包含分钟的最细粒度时间格式，那么你每分钟都会获得一个新的分桶。每个分桶本身就是一个包含部分文件的目录：Sink 的每个并行实例都会创建自己的那部分文件，当部分文件变得太大时，会紧挨着其他文件创建一个新的部分文件。当一个分桶最近没有被写入数据时被视为非活跃，将刷写(flush)并关闭打开的部分文件。默认情况下，Sink 每分钟都会检查非活跃的分桶，并关闭一分钟以上没有写入数据的分桶。可以在 BucketingSink上 使用 setInactiveBucketCheckInterval() 和 setInactiveBucketThreshold() 配置这些行为。 你还可以在 BucketingSink上 上使用 setBucketer() 指定自定义的 bucketer。如果需要，bucketer 可以使用元素或元组的属性来确定 bucket目录。 默认的 writer 是StringWriter。对传入的元素调用 toString()，并将它们写入部分文件，并用换行符进行分隔。要在 BucketingSink 上指定一个自定义的 writer，使用 setWriter() 方法即可。如果要写入 Hadoop SequenceFiles 文件中，可以使用提供的 SequenceFileWriter，并且可以配置使用压缩格式。 有两个配置参数可以指定何时应关闭部分文件并启动一个新的部分文件： 通过设置批量大小(batch size)（默认部件文件大小为384 MB）。 通过设置批次滚动时间间隔（默认滚动间隔为Long.MAX_VALUE）。 当满足这两个条件中的任何一个时，会启动一个的部分文件。 Java版本:DataStream&lt;Tuple2&lt;IntWritable,Text&gt;&gt; input = ...;BucketingSink&lt;String&gt; sink = new BucketingSink&lt;String&gt;("/base/path");sink.setBucketer(new DateTimeBucketer&lt;String&gt;("yyyy-MM-dd--HHmm", ZoneId.of("America/Los_Angeles")));sink.setWriter(new SequenceFileWriter&lt;IntWritable, Text&gt;());sink.setBatchSize(1024 * 1024 * 400); // this is 400 MB,sink.setBatchRolloverInterval(20 * 60 * 1000); // this is 20 minsinput.addSink(sink); Scala版本:val input: DataStream[Tuple2[IntWritable, Text]] = ...val sink = new BucketingSink[String]("/base/path")sink.setBucketer(new DateTimeBucketer[String]("yyyy-MM-dd--HHmm", ZoneId.of("America/Los_Angeles")))sink.setWriter(new SequenceFileWriter[IntWritable, Text]())sink.setBatchSize(1024 * 1024 * 400) // this is 400 MB,sink.setBatchRolloverInterval(20 * 60 * 1000); // this is 20 minsinput.addSink(sink) 上面例子将创建一个 Sink，写入遵循下面格式的分桶文件中：/base/path/&#123;date-time&#125;/part-&#123;parallel-task&#125;-&#123;count&#125; 其中 date-time 是从日期/时间格式获得的字符串，parallel-task 是并行 Sink 实例的索引，count 是由于批次大小或者滚动时间间隔而创建的部分文件的运行编号。 Flink 版本:1.7 原文:HDFS Connector]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 窗口触发器与Evictors]]></title>
    <url>%2Fflink-stream-windows-trigger-and-evictor.html</url>
    <content type="text"><![CDATA[1. 窗口触发器触发器(Trigger)决定了窗口(请参阅窗口概述)博文)什么时候使用窗口函数处理窗口内元素。每个窗口分配器都带有一个默认的触发器。如果默认触发器不能满足你的要求，可以使用 trigger(...) 指定自定义的触发器。 触发器接口有五个方法来对不同的事件做出响应： (1) 当每个元素被添加窗口时调用onElement()方法：public abstract TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx) throws Exception; (2) 当注册的处理时间计时器被触发时调用onProcessingTime()方法：public abstract TriggerResult onProcessingTime(long time, W window, TriggerContext ctx) throws Exception; (3) 当注册的事件时间计时器被触发时调用onEventTime()方法：public abstract TriggerResult onEventTime(long time, W window, TriggerContext ctx) (4) 在窗口合并时onMerge()方法与状态触发器相关，并且合并两个触发器的状态：public void onMerge(W window, OnMergeContext ctx) (5) 在清除（removal）窗口时调用clear() 方法：public abstract void clear(W window, TriggerContext ctx) 以上方法有两件事要注意: (1) 前三个函数决定了如何通过返回一个 TriggerResult 对象来对其调用事件进行操作。TriggerResult可以是，CONTINUE（什么都不做），FIRE_AND_PURGE（触发计算，然后清除窗口中的元素），FIRE（触发计算），PURGE（清除窗口中的元素）。 (2) 上面任何方法都可以用于注册处理时间计时器或事件时间计时器以供将来的操作使用。 1.1 触发与清除一旦触发器确定窗口准备好可以处理数据，就将触发，即，它返回 FIRE 或 FIRE_AND_PURGE。这是窗口算子发出当前窗口结果的信号。给定一个带有 ProcessWindowFunction 的窗口，所有的元素都被传递给 ProcessWindowFunction (可能在将所有元素传递给 evictor 之后)。带有 ReduceFunction， AggregateFunction 或者 FoldFunction 的窗口只是简单地发出他们急切希望得到的聚合结果。 触发器触发时，可以是 FIRE 或 FIRE_AND_PURGE 。FIRE 保留窗口中的内容，FIRE_AND_PURGE 会删除窗口中的内容。默认情况下，内置的触发器只返回 FIRE，不会清除窗口状态。 清除只是简单地删除窗口的内容，并保留窗口的元数据信息以及完整的触发状态。 1.2 窗口分配器的默认触发器窗口分配器的默认触发器适用于许多情况。例如，所有的事件时间窗口分配器都有一个 EventTimeTrigger 作为默认触发器。一旦 watermark 到达窗口末尾，这个触发器就会被触发。 全局窗口(GlobalWindow)的默认触发器是永不会被触发的 NeverTrigger。因此，在使用全局窗口时，必须自定义一个触发器。 通过使用 trigger() 方法指定触发器，将会覆盖窗口分配器的默认触发器。例如，如果你为 TumblingEventTimeWindows 指定 CountTrigger，那么不会再根据时间进度触发窗口，而只能通过计数。目前为止，如果你希望基于时间以及计数进行触发，则必须编写自己的自定义触发器。 1.3 内置触发器和自定义触发器Flink带有一些内置触发器: EventTimeTrigger 根据 watermarks 度量的事件时间进度进行触发。 ProcessingTimeTrigger 基于处理时间触发。 CountTrigger 一旦窗口中的元素数量超过给定限制就会触发。 PurgingTrigger 将其作为另一个触发器的参数，并将其转换为带有清除功能(transforms it into a purging one)。 如果需要实现一个自定义的触发器，你应该看看Trigger抽象类。请注意，API仍在发展中，在Flink未来版本中可能会发生改变。 2. 窗口驱逐器Flink 窗口模型还允许在窗口分配器和触发器之外指定一个可选的驱逐器(Evictor)。可以使用 evictor(...) 方法来完成。驱逐器能够在触发器触发之后，窗口函数使用之前或之后从窗口中清除元素。Evictor 接口有两种方法： void evictBefore(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext);void evictAfter(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext); evictBefore() 包含驱逐逻辑，在窗口函数之前使用。而 evictAfter() 在窗口函数之后使用。在使用窗口函数之前被逐出的元素将不被处理。 Flink带有三种内置驱逐器: CountEvictor：在窗口维护用户指定数量的元素，如果多于用户指定的数量，从窗口缓冲区的开头丢弃多余的元素。 DeltaEvictor：使用 DeltaFunction 和一个阈值，来计算窗口缓冲区中的最后一个元素与其余每个元素之间的差值，并删除差值大于或等于阈值的元素。 TimeEvictor：以毫秒为单位的时间间隔（interval）作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。 默认情况下，所有内置的驱逐器在窗口函数之前使用。指定驱逐器可以避免预聚合(pre-aggregation)，因为窗口内所有元素必须在窗口计算之前传递给驱逐器。Flink 不保证窗口内元素的顺序。这意味着虽然驱逐器可以从窗口开头移除元素，但这些元素不一定是先到的还是后到的。 Flink版本:1.4 原文: https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#triggers]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 窗口函数]]></title>
    <url>%2Fflink-stream-windows-function.html</url>
    <content type="text"><![CDATA[在定义窗口分配器之后，我们需要在每个窗口上指定我们要执行的计算。这是窗口函数的责任，一旦系统确定窗口准备好处理数据，窗口函数就处理每个窗口中的元素。 窗口函数可以是 ReduceFunction， AggregateFunction, FoldFunction 或 ProcessWindowFunction。前两个函数执行效率更高，因为 Flink 可以在每个窗口中元素到达时增量地聚合。ProcessWindowFunction 将获得一个窗口内所有元素的迭代器以及元素所在窗口的附加元信息。 使用 ProcessWindowFunction 的窗口转换操作不能像其他那样有效率，是因为 Flink 在调用该函数之前必须在内部缓存窗口中的所有元素。这可以通过将 ProcessWindowFunction 与 ReduceFunction， AggregateFunction 或 FoldFunction 组合使用来获得窗口元素的增量聚合以及WindowFunction接收的附加窗口元数据。 1. ReduceFunctionReduceFunction 指定如何组合输入数据的两个元素以产生相同类型的输出元素。Flink 使用 ReduceFunction 增量聚合窗口的元素。 ReduceFunction可以如下定义和使用: Java版本:DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123; public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2) &#123; return new Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1); &#125;&#125;); Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce &#123; (v1, v2) =&gt; (v1._1, v1._2 + v2._2) &#125; 上述示例获得窗口中的所有元素元组的第二个字段之和。 2. FoldFunctionFoldFunction 指定窗口的输入元素如何与输出类型的元素合并。FoldFunction 会被每一个加入到窗口中的元素和当前的输出值增量地调用，第一个元素与一个预定义的输出类型的初始值合并。 FoldFunction 可以如下定义和使用: Java版本:DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .fold("", new FoldFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;&gt; &#123; public String fold(String acc, Tuple2&lt;String, Long&gt; value) &#123; return acc + value.f1; &#125;&#125;); Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .fold("") &#123; (acc, v) =&gt; acc + v._2 &#125; 上述示例将所有输入元素的Long值追加到初始化为空的字符串中。 备注 fold()不能应用于会话窗口或者其他可合并的窗口中。 3. AggregateFunctionAggregateFunction 是 ReduceFunction 的通用版本，具有三种类型：输入类型（IN），累加器类型（ACC）和输出类型（OUT）。输入类型是输入流中元素的类型，AggregateFunction 有一个用于将一个输入元素添加到累加器的方法。该接口还具有创建初始累加器的方法，用于将两个累加器合并到一个累加器中，并从累加器中提取输出（类型为OUT）。我们将在下面的例子中看到它是如何工作的。 与 ReduceFunction 相同，Flink 将在窗口到达时递增地聚合窗口的输入元素。 Java版本:/** * The accumulator is used to keep a running sum and a count. The &#123;@code getResult&#125; method * computes the average. */private static class AverageAggregate implements AggregateFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123; @Override public Tuple2&lt;Long, Long&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0L, 0L); &#125; @Override public Tuple2&lt;Long, Long&gt; add(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123; return new Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); &#125; @Override public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123; return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125;&#125;DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(new AverageAggregate()); Scala版本:/** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] &#123; override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2)&#125;val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(new AverageAggregate) 上面的例子计算窗口中元素的第二个字段的平均值。 4. ProcessWindowFunctionProcessWindowFunction 获得一个窗口内所有元素的 Iterable，以及一个可以访问时间和状态信息的 Context 对象，这使得它可以提供比其他窗口函数更大的灵活性。这是以牺牲性能和资源消耗为代价的，因为元素不能增量地聚合，而是需要在内部进行缓冲，直到窗口被认为准备好进行处理为止。 ProcessWindowFunction 的结构如下所示： Java版本:public abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; implements Function &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception; /** * The context holding window metadata. */ public abstract class Context implements java.io.Serializable &#123; /** * Returns the window that is being evaluated. */ public abstract W window(); /** Returns the current processing time. */ public abstract long currentProcessingTime(); /** Returns the current event-time watermark. */ public abstract long currentWatermark(); /** * State accessor for per-key and per-window state. * * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt;If you use per-window state you have to ensure that you clean it up * by implementing &#123;@link ProcessWindowFunction#clear(Context)&#125;. */ public abstract KeyedStateStore windowState(); /** * State accessor for per-key global state. */ public abstract KeyedStateStore globalState(); &#125;&#125; Scala版本:abstract class ProcessWindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** * The context holding window metadata */ abstract class Context &#123; /** * Returns the window that is being evaluated. */ def window: W /** * Returns the current processing time. */ def currentProcessingTime: Long /** * Returns the current event-time watermark. */ def currentWatermark: Long /** * State accessor for per-key and per-window state. */ def windowState: KeyedStateStore /** * State accessor for per-key global state. */ def globalState: KeyedStateStore &#125;&#125; 关键参数是通过 KeySelector 提取为 keyBy（） 调用指定的键。在元组索引键或字符串字段引用的情况下，此键类型始终为元组，并且必须手动将其转换为正确大小的元组以提取关键字段。一个 ProcessWindowFunction 可以像这样定义和使用： Java版本:DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .process(new MyProcessWindowFunction());/* ... */public class MyProcessWindowFunction implements ProcessWindowFunction&lt;Tuple&lt;String, Long&gt;, String, String, TimeWindow&gt; &#123; void process(String key, Context context, Iterable&lt;Tuple&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) &#123; long count = 0; for (Tuple&lt;String, Long&gt; in: input) &#123; count++; &#125; out.collect("Window: " + context.window() + "count: " + count); &#125;&#125; Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .process(new MyProcessWindowFunction())/* ... */class MyProcessWindowFunction extends ProcessWindowFunction[(String, Long), String, String, TimeWindow] &#123; def apply(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]): () = &#123; var count = 0L for (in &lt;- input) &#123; count = count + 1 &#125; out.collect(s"Window $&#123;context.window&#125; count: $count") &#125;&#125; 上述示例显示了一个 ProcessWindowFunction，用于统计窗口中的元素。另外，窗口函数将有关窗口的信息添加到输出中。 备注 使用ProcessWindowFunction进行简单聚合（如count）的效率非常低。 下一节将展示ReduceFunction或AggregateFunction如何与ProcessWindowFunction组合以获得增量聚合以及ProcessWindowFunction的附加信息。 5. 使用增量聚合的ProcessWindowFunctionProcessWindowFunction 可以与 ReduceFunction ， AggregateFunction 或 FoldFunction 组合使用，以便在元素到达窗口时增量聚合元素。当窗口关闭时，ProcessWindowFunction提供聚合结果。这允许增量计算窗口，同时也可以访问 ProcessWindowFunction 额外的窗口元信息。 备注 你也可以使用传统WindowFunction而不是ProcessWindowFunction进行增量窗口聚合。 5.1 使用ReduceFunction的增量窗口聚合以下示例展现了如何将增量式 ReduceFunction 与 ProcessWindowFunction 结合以返回窗口中的最小事件以及窗口的开始时间。 Java版本:DataStream&lt;SensorReading&gt; input = ...;input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .reduce(new MyReduceFunction(), new MyWindowFunction());// Function definitionsprivate static class MyReduceFunction implements ReduceFunction&lt;SensorReading&gt; &#123; public SensorReading reduce(SensorReading r1, SensorReading r2) &#123; return r1.value() &gt; r2.value() ? r2 : r1; &#125;&#125;private static class MyWindowFunction implements WindowFunction&lt;SensorReading, Tuple2&lt;Long, SensorReading&gt;, String, TimeWindow&gt; &#123; public void apply(String key, TimeWindow window, Iterable&lt;SensorReading&gt; minReadings, Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out) &#123; SensorReading min = minReadings.iterator().next(); out.collect(new Tuple2&lt;Long, SensorReading&gt;(window.getStart(), min)); &#125;&#125; Scala版本:val input: DataStream[SensorReading] = ...input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .reduce( (r1: SensorReading, r2: SensorReading) =&gt; &#123; if (r1.value &gt; r2.value) r2 else r1 &#125;, ( key: String, window: TimeWindow, minReadings: Iterable[SensorReading], out: Collector[(Long, SensorReading)] ) =&gt; &#123; val min = minReadings.iterator.next() out.collect((window.getStart, min)) &#125; ) 5.2 使用AggregateFunction的增量窗口聚合以下示例显示了如何将增量式 AggregateFunction 与 ProcessWindowFunction 结合来计算平均值，并将键与平均值一起输出。 Java版本:DataStream&lt;Tuple2&lt;String, Long&gt; input = ...;input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction());// Function definitions/** * The accumulator is used to keep a running sum and a count. The &#123;@code getResult&#125; method * computes the average. */private static class AverageAggregate implements AggregateFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123; @Override public Tuple2&lt;Long, Long&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0L, 0L); &#125; @Override public Tuple2&lt;Long, Long&gt; add(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123; return new Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); &#125; @Override public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123; return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125;&#125;private static class MyProcessWindowFunction implements ProcessWindowFunction&lt;Double, Tuple2&lt;String, Double&gt;, String, TimeWindow&gt; &#123; public void apply(String key, Context context, Iterable&lt;Double&gt; averages, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out) &#123; Double average = averages.iterator().next(); out.collect(new Tuple2&lt;&gt;(key, average)); &#125;&#125; Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction())// Function definitions/** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] &#123; override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2)&#125;class MyProcessWindowFunction extends ProcessWindowFunction[Double, (String, Double), String, TimeWindow] &#123; def apply(key: String, context: Context, averages: Iterable[Double], out: Collector[(String, Double]): () = &#123; var count = 0L for (in &lt;- input) &#123; count = count + 1 &#125; val average = averages.iterator.next() out.collect((key, average)) &#125;&#125; 5.3 使用FoldFunction的增量窗口聚合以下示例展现了增量式 FoldFunction 如何与 WindowFunction 结合以提取窗口中的事件数，并返回窗口的键和结束时间。 Java版本:DataStream&lt;SensorReading&gt; input = ...;input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .fold(new Tuple3&lt;String, Long, Integer&gt;("",0L, 0), new MyFoldFunction(), new MyWindowFunction())// Function definitionsprivate static class MyFoldFunction implements FoldFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt; &gt; &#123; public Tuple3&lt;String, Long, Integer&gt; fold(Tuple3&lt;String, Long, Integer&gt; acc, SensorReading s) &#123; Integer cur = acc.getField(2); acc.setField(2, cur + 1); return acc; &#125;&#125;private static class MyWindowFunction implements WindowFunction&lt;Tuple3&lt;String, Long, Integer&gt;, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt; &#123; public void apply(String key, TimeWindow window, Iterable&lt;Tuple3&lt;String, Long, Integer&gt;&gt; counts, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) &#123; Integer count = counts.iterator().next().getField(2); out.collect(new Tuple3&lt;String, Long, Integer&gt;(key, window.getEnd(),count)); &#125;&#125; Scala版本:val input: DataStream[SensorReading] = ...input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .fold ( ("", 0L, 0), (acc: (String, Long, Int), r: SensorReading) =&gt; &#123; ("", 0L, acc._3 + 1) &#125;, ( key: String, window: TimeWindow, counts: Iterable[(String, Long, Int)], out: Collector[(String, Long, Int)] ) =&gt; &#123; val count = counts.iterator.next() out.collect((key, window.getEnd, count._3)) &#125; ) 备注 Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#window-functions]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 2.3.0 重要特性介绍]]></title>
    <url>%2Fintroducing-apache-spark-2-3.html</url>
    <content type="text"><![CDATA[为了继续实现 Spark 更快，更轻松，更智能的目标，Spark 2.3 在许多模块都做了重要的更新，比如 Structured Streaming 引入了低延迟的持续处理；支持 stream-to-stream joins；通过改善 pandas UDFs 的性能来提升 PySpark；支持第四种调度引擎 Kubernetes clusters（其他三种分别是自带的独立模式Standalone，YARN、Mesos）。除了这些比较具有里程碑的重要功能外，Spark 2.3 还有以下几个重要的更新： 引入 DataSource v2 APIs [SPARK-15689, SPARK-20928] 矢量化的 ORC reader [SPARK-16060] Spark History Server v2 with K-V store [SPARK-18085] 基于 Structured Streaming 的机器学习管道API模型 [SPARK-13030, SPARK-22346, SPARK-23037] MLlib 增强 [SPARK-21866, SPARK-3181, SPARK-21087, SPARK-20199] Spark SQL 增强 [SPARK-21485, SPARK-21975, SPARK-20331, SPARK-22510, SPARK-20236] 这篇文章将简单地介绍上面一些高级功能和改进，更多的特性请参见 Spark 2.3 release notes。 1. 毫秒延迟的持续流处理出于某些原因的考虑，Spark 2.0 引入的 Structured Streaming 将微批次处理从高级 API 中解耦出去。首先，它简化了 API 的使用，API 不再负责进行微批次处理。其次，开发者可以将流看成是一个没有边界的表，并基于这些 表 运行查询。 不过，为了给开发者提供更多的流式处理体验，Spark 2.3 引入了毫秒级延迟的持续流式处理模式。 从内部来看，Structured Streaming 引擎基于微批次增量执行查询，时间间隔视具体情况而定，不过这样的延迟对于真实世界的流式应用来说都是可接受的。 在持续模式下，流处理器持续不断地从数据源拉取和处理数据，而不是每隔一段时间读取一个批次的数据，这样就可以及时地处理刚到达的数据。如下图所示，延迟被降低到毫秒级别，完全满足了低延迟的要求。 持续模式目前支持的 Dataset 操作包括 Projection、Selection 以及除 current_timestamp()、current_date()、聚合函数之外的 SQL 操作。它还支持将 Kafka 作为数据源和数据池（Sink），也支持将控制台和内存作为数据池。 开发者可以根据实际的延迟需求来选择使用持续模式还是微批次模式，总之，Structured Streaming 为开发者提供了容错和可靠性方面的保证。 简单地说，Spark 2.3 的持续模式所能做到的是： 端到端的毫秒级延迟 至少一次处理保证 支持 Dataset 的映射操作 2. 流到流的连接Spark 2.0 的 Structured Streaming 已经可以支持 DataFrame/Dataset 的连接操作，但只是流到静态数据集的连接，而 Spark 2.3 带来了期待已久的流到流的连接，支持内连接和外连接，可用在大量的实时场景中。 广告变现是流到流连接的一个典型应用场景。例如，广告 impression 流和用户点击流包含相同的键（如 adld）和相关数据，而你需要基于这些数据进行流式分析，找出哪些用户的点击与 adld 相关。 虽然看起来很简单，但实际上流到流的连接解决了一些技术性难题： 将迟到的数据缓冲起来，直到在另一个流中找到与之匹配的数据。 通过设置水位（Watermark）防止缓冲区过度膨胀。 用户可以在资源消耗和延迟之间作出权衡。 静态连接和流连接之间的 SQL 语法是一致的。 3. Spark 和 KubernetesSpark 和 Kubernetes 这两个开源项目之间的功能组合也在意料之内，用于提供大规模分布式的数据处理和编配。在 Spark 2.3 中，用户可在 Kubernetes 集群上原生地运行 Spark，从而更合理地使用资源，不同的工作负载可共享 Kubernetes 集群。 Spark 可以使用 Kubernetes 的所有管理特性，如资源配额、可插拔的授权和日志。另外，要在已有的 Kubernetes 集群上启动 Spark 工作负载就像创建一个 Docker 镜像那么简单。 4. 用于 PySpark 的 Pandas UDFPandas UDF，也被称为向量化的 UDF，为 PySpark 带来重大的性能提升。Pandas UDF 以 Apache Arrow 为基础，完全使用 Python 开发，可用于定义低开销、高性能的 UDF。 Spark 2.3 提供了两种类型的 Pandas UDF：标量和组合 map。来自 Two Sigma 的 Li Jin 在之前的一篇博客中通过四个例子介绍了如何使用 Pandas UDF。 一些基准测试表明，Pandas UDF 在性能方面比基于行的 UDF 要高出一个数量级。 包括 Li Jin 在内的一些贡献者计划在 Pandas UDF 中引入聚合和窗口功能。 5. MLlib 方面的改进Spark 2.3 带来了很多 MLlib 方面的改进，包括算法、特性、性能、伸缩性和可用性。 首先，可通过 Structured Streaming 作业将 MLlib 的模型和管道部署到生产环境，不过一些已有的管道可能需要作出修改。 其次，为了满足深度学习图像分析方面的需求，Spark 2.3 引入了 ImageSchema，将图像表示成 Spark DataFrame，还提供工具用于加载常用的图像格式。 最后，Spark 2.3 带来了改进过的 Python API，用于开发自定义算法，包括 UnaryTransformer 以及用于保存和加载算法的自动化工具。 译文：https://mp.weixin.qq.com/s/SJ2P4oJtvsMgzCSH3DH4vA 原文：https://databricks.com/blog/2018/02/28/introducing-apache-spark-2-3.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 用于外部数据访问的异步IO]]></title>
    <url>%2Fflink-stream-asynchronous-io-for-external-data-access.html</url>
    <content type="text"><![CDATA[1. 异步IO操作的必要性当与外部系统交互时（例如，使用存储在数据库中数据丰富流事件），需要注意与外部系统的通信延迟并不决定流应用程序的整体工作。访问外部数据库中的数据（例如在 MapFunction 中）通常意味着同步交互：将请求发送到数据库，MapFunction 会等待直到收到响应。在许多情况下，这个等待时间占了该函数绝大部分时间。 与外部数据库进行异步交互意味着一个并行函数实例可以并发地处理多个请求和并发地接收多个响应。那样的话，可以通过发送其他请求和接收响应来重叠等待时间。至少，等待时间可以被多个请求平摊，这在很多情况下会导致更高的流吞吐量。 通过扩展 MapFunction 到一个很高的并发度来提高吞吐量在一定程度上是可行的，但是常常会导致很高的资源成本：有更多的并行 MapFunction 实例意味着更多的任务、线程、Flink内部网络连接、与数据库之间的网络连接、缓存以及通常的内部开销。 2. 前提条件如上面的部分所述，实现数据库（或key/value存储系统）适当的异步I/O访问需要该数据库的客户端支持异步请求。许多流行的数据库提供这样的客户端。在没有这样的客户端的情况下，可以尝试创建多个客户端并使用线程池处理同步调用，从而将同步客户端转换为有限的并发客户端。但是，这种方法通常比适当的异步客户端效率低。 3. Async I/O APIFlink 的异步 I/O API允许用户在数据流中使用异步请求客户端。API处理与数据流的集成，以及处理顺序，事件时间，容错等。 假设有一个用于目标数据库的异步客户端，要实现一个通过异步I/O来操作数据库还需要三个步骤： 实现调度请求的 AsyncFunction 获取操作结果并把它传递给 ResultFuture 的 callBack 将异步 I/O 操作作为转换操作应用于 DataStream 以下代码示例说明了基本模式： Java版本:// This example implements the asynchronous request and callback with Futures that have the// interface of Java 8's futures (which is the same one followed by Flink's Future)/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */class AsyncDatabaseRequest extends RichAsyncFunction&lt;String, Tuple2&lt;String, String&gt;&gt; &#123; /** The database specific client that can issue concurrent requests with callbacks */ private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception &#123; client = new DatabaseClient(host, post, credentials); &#125; @Override public void close() throws Exception &#123; client.close(); &#125; @Override public void asyncInvoke(final String str, final ResultFuture&lt;Tuple2&lt;String, String&gt;&gt; resultFuture) throws Exception &#123; // 发出异步请求，返回结果的 Future Future&lt;String&gt; resultFuture = client.query(str); // 一旦客户端的请求完成，执行回调函数 // 回调函数只是将结果转发给 resultFuture resultFuture.thenAccept( (String result) -&gt; &#123; resultFuture.complete(Collections.singleton(new Tuple2&lt;&gt;(str, result))); &#125;); &#125;&#125;// create the original streamDataStream&lt;String&gt; stream = ...;// apply the async I/O transformationDataStream&lt;Tuple2&lt;String, String&gt;&gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100); Scala版本:/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */class AsyncDatabaseRequest extends AsyncFunction[String, (String, String)] &#123; /** The database specific client that can issue concurrent requests with callbacks */ lazy val client: DatabaseClient = new DatabaseClient(host, post, credentials) /** The context used for the future callbacks */ implicit lazy val executor: ExecutionContext = ExecutionContext.fromExecutor(Executors.directExecutor()) override def asyncInvoke(str: String, resultFuture: ResultFuture[(String, String)]): Unit = &#123; // issue the asynchronous request, receive a future for the result val resultFuture: Future[String] = client.query(str) // set the callback to be executed once the request by the client is complete // the callback simply forwards the result to the result future resultFuture.onSuccess &#123; case result: String =&gt; resultFuture.complete(Iterable((str, result))) &#125; &#125;&#125;// create the original streamval stream: DataStream[String] = ...// apply the async I/O transformationval resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100) 重要提示ResultFuture是在第一次调用 ResultFuture.complete 时已经完成。所有后续的 complete 调用都将被忽略。 以下两个参数控制异步操作： 超时：超时定义了异步请求在被认为失败之前可能需要多长时间。该参数防止死亡/失败请求。 容量：该参数定义可以同时进行多少个异步请求。尽管异步I/O方法通常会有更好的吞吐量，但是算子仍然可能是流应用程序中的瓶颈。限制并发请求的数量可以确保算子不会积压不断增长的未处理请求，但一旦容量耗尽，它将触发背压。 4. 结果顺序由 AsyncFunction 发出的并发请求经常是以无序的形式完成，取决于哪个请求先完成。为了控制结果记录发出的顺序，Flink 提供了两种模式： Unordered：异步请求结束后立即输出结果记录。在经过异步I/O算子之后，流中记录的顺序与之前会不一样。当使用处理时间作为基本时间特性时，该模式具有最低延迟和最低开销的特性。在这种模式下使用 AsyncDataStream.unorderedWait（...） 函数。 Ordered：在这种情况下，保留流的顺序。结果记录输出的顺利与异步请求触发的顺序(算子输入记录的顺序)一致。为此，算子必须缓冲结果记录，直到其前面所有的记录输出（或超时）为止。这通常会导致在检查点中出现一定量的额外延迟和一些开销，因为与 Unordered 模式相比，结果的记录在检查点状态中保持较长的一段时间。在这种模式下使用 AsyncDataStream.orderedWait（...） 函数。 5. 事件时间当流式应用程序使用事件时间时，异步 I/O 算子能正确处理 watermarks。这意味着对于两个顺序模式具体如下： Unordered： watermarks 不会超过记录，反之亦然，这意味着 watermarks 建立起顺序边界。记录只在 watermarks 之间无序排列。只有在发布 watermarks 后才会发出某个 watermarks 后发生的记录。反过来，只有在发布 watermarks 前的所有输入结果记录之后才会发送 watermarks。这意味着，在有 watermarks 的情况下，Unordered 模式与 Ordered 模式一样，都引入了延迟和开销。该开销取决于 watermarks 发送频率。 Ordered：保存记录的 watermarks 顺序，就像保存记录之间的顺序一样。与处理时间相比，开销没有显着变化。 请记住，提取时间是事件时间的特例，自动生成的 watermarks 基于数据源的处理时间。 6. 容错保证异步 I/O 算子提供 exactly-once 语义容错保证。它将检查点中正在进行的异步请求记录存储起来，并在从故障中恢复时恢复/重新触发请求。 原文： https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/asyncio.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 窗口概述]]></title>
    <url>%2Fflink-stream-windows-overall.html</url>
    <content type="text"><![CDATA[Windows(窗口)是处理无限数据流的核心。窗口将流分解成有限大小的”桶”，在上面我们可以进行计算。本文将重点介绍 Flink 中的窗口，以及常见的窗口类型。 一个窗口化的 Flink 程序一般结构如下。第一个片段指的是指定 key 的数据流（keyed streams），而第二个未指定key的数据流。可以看出，唯一的区别是指定 key 的数据流调用了 keyBy() 以及 window() 方法变为未指定 key 数据流下的 windowAll() 方法。 Keyed Windows：stream .keyBy(...) &lt;- keyed versus non-keyed windows .window(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness()] &lt;- optional, else zero .reduce/fold/apply() &lt;- required: &quot;function&quot; Non-Keyed Windows：stream .windowAll(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness()] &lt;- optional, else zero .reduce/fold/apply() &lt;- required: &quot;function&quot; 在上面，方括号 [...] 中的命令是可选的。这表明 Flink 允许你可以以多种不同的方式自定义你的窗口逻辑，以便更好的满足你的需求。 1. 窗口生命周期一旦属于这个窗口的第一个元素到达，就会创建该窗口，当时间(事件时间或处理时间)到达规定结束时间和用户指定的可允许延迟的时间后，窗口将会被完全删除。Flink 保证仅对基于时间的窗口进行删除，并不适用于其他类型的窗口，例如，全局窗口（具体请参阅下面的窗口分配器）。举个例子，使用基于事件时间的窗口策略，每隔5分钟创建一个不重叠的窗口，并且允许可以有1分钟的延迟时间。当第一个带有时间戳的元素落入12:00至12:05时间间隔内时，Flink 创建一个新窗口，当时间戳到达 12:06 时，窗口将被删除。 每个窗口都有一个触发器和一个函数(例如 WindowFunction， ReduceFunction 或 FoldFunction)。函数用于窗口的计算，而触发器指定了窗口什么时候使用该函数。触发策略可能是”当窗口中元素个数大于4时”，或”当 watermark 到达窗口末尾时”。触发器还可以决定在创建窗口和删除窗口之间的什么时间内清除窗口内容。在这里，清除仅指清除窗口中的元素，而不是窗口（窗口元数据）。这意味着新数据仍然可以添加到窗口中。 你还可以指定一个 Evictor，在触发器触发之后以及在应用该函数之前和/或之后从窗口中移除元素。 2. Keyed vs Non-Keyed Windows使用窗口我们要做的第一件事就是你的数据流是否指定 key。必须在定义窗口之前完成。使用 keyBy() 可以将无限数据流分解成不同 key 上的数据流。 在指定 key 的数据流中，事件的任何属性都可以用作 key，如何指定 key 可以参阅 (Flink1.4 定义keys的几种方法)。在指定 key 的数据流可以允许通过多个任务并行执行窗口计算，因为每个逻辑数据流可以独立于其它进行。有相同 key 的所有元素将被发送到相同的并行任务上。 在未指定 key 的数据流中，原始数据流不会被分割成多个逻辑数据流，并且所有窗口逻辑将由单个任务执行，即并行度为1。 3. 窗口分配器在确定数据流是否指定 key 之后，下一步就是定义窗口分配器（WindowAssigners）。窗口分配器定义了元素如何分配给窗口（译者注：即指定元素分配给哪个窗口）。可以通过在 window()(指定key数据流)或 windowAll()(未指定key数据流)中指定你选择的窗口分配器来完成。 窗口分配器负责将每个传入的元素分配给一个或多个窗口。Flink 内置了一些用于解决常见问题的窗口分配器，例如，滚动窗口，滑动窗口，会话窗口和全局窗口等。你还可以通过继承 WindowAssigner 类实现自定义窗口分配器。所有内置窗口分配器(全局窗口除外)根据时间将元素分配给窗口，可以是处理时间，也可以是事件时间。请参阅Flink1.4 事件时间与处理时间，了解处理时间和事件时间之间的差异以及如何生成时间戳和watermarks。 在下文中，我们将展示 Flink 的内置窗口分配器的工作原理以及它们在 DataStream 程序中的使用方式。下面分配器运行图中，紫色圆圈表示数据流中的元素，根据某些key分区（在我们这个例子中为 user1，user2 和 user3），x轴显示时间进度。 3.1 滚动窗口滚动窗口分配器将每个元素分配给固定大小的窗口。滚动窗口大小固定且不重叠。例如，如果指定大小为5分钟的滚动窗口，每五分钟都会启动一个新窗口，如下图所示: 以下代码显示如何使用滚动窗口： Java版本:DataStream&lt;T&gt; input = ...;// 基于事件事件的滚动窗口input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// 基于处理时间的滚动窗口input .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// daily tumbling event-time windows offset by -8 hours.input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...// tumbling event-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// tumbling processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// daily tumbling event-time windows offset by -8 hours.input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 也可以通过使用 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x) 来指定时间间隔。 如上面例子中所示，滚动窗口分配器还可以使用一个可选的偏移量参数，用来改变窗口的对齐方式。例如，没有偏移量的情况下，窗口大小为1小时的滚动窗口与 epoch （指的是一个特定的时间：1970-01-01 00:00:00 UTC）对齐，那么你将获得如1：00：00.000 - 1：59：59.999，2：00：00.000 - 2：59：59.999等窗口。如果你想改变，你可以给一个偏移量。以15分钟的偏移量为例，那么你将获得1：15：00.000 - 2：14：59.999，2：15：00.000 - 3：14：59.999等窗口。偏移量的一个重要应用是将窗口调整为 timezones 而不是 UTC-0。例如，在中国，你必须指定 Time.hours(-8) 的偏移量。 3.2 滑动窗口滑动窗口分配器将每个元素分配给固定窗口大小的窗口。类似于滚动窗口分配器，窗口的大小由 window size 参数配置。还有一个window slide参数来控制滑动窗口的滑动频率(译者注：窗口滑动大小)。因此，如果滑动大小小于窗口大小，则滑动窗口会重叠。在这种情况下，元素会被分配到多个窗口中。 例如，窗口大小为10分钟，滑动大小为5分钟的窗口。这样，每5分钟会生成一个窗口，每个窗口包含最后10分钟内到达的事件，如下图所示。 以下代码显示如何使用滑动窗口: Java版本:DataStream&lt;T&gt; input = ...;// sliding event-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// sliding processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// sliding processing-time windows offset by -8 hoursinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...// sliding event-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// sliding processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// sliding processing-time windows offset by -8 hoursinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 也可以通过使用 Time.milliseconds(x)，Time.seconds(x)，Time.minutes(x) 来指定时间间隔。 如上面例子所示，滑动窗口分配器也可以使用一个可选的偏移量参数，用来改变窗口的对齐方式。例如，没有偏移量的情况下，窗口大小为1小时，滑动大小为30分钟的滑动窗，你将获得如1：00：00.000 - 1：59：59.999，1：30：00.000 - 2：29：59.999等窗口。你可以给一个偏移量，以15分钟的偏移量为例，那么你将获得1：15：00.000 - 2：14：59.999，1：45：00.000 - 2：44：59.999等窗口。 3.3 会话窗口会话窗口分配器通过活动会话对元素进行分组。与滚动窗口和滑动窗口相比，会话窗口不会重叠，也没有固定的开始和结束时间。当会话窗口在一段时间内没有接收到元素时会关闭，即当发生不活动的会话间隙时。会话窗口分配器需要配置一个会话间隙，定义了所需的不活动时长。当此时间段到期时，当前会话关闭，后续元素被分配到新的会话窗口。 以下代码显示如何使用会话窗口: Java版本:DataStream&lt;T&gt; input = ...;// event-time session windowsinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// processing-time session windowsinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...// event-time session windowsinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// processing-time session windowsinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 也可以通过使用 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x) 来指定时间间隔。 由于会话窗口没有固定的开始时间和结束时间，因此它们的执行与滚动窗口和滑动窗口不同。在内部，会话窗口算子为每个到达记录创建一个新窗口，如果它们之间的距离比定义的间隙要小，则窗口会合并在一起。为了可合并，会话窗口算子需要一个合并触发器和合并窗口函数，例如 ReduceFunction 或 WindowFunction（FoldFunction无法合并）。 3.4 全局窗口全局窗口分配器将具有相同 key 的所有元素分配给同一个全局窗口。仅在你指定自定义触发器时，这个窗口才起作用。否则，不会执行任何计算，因为全局窗口没有我们可以处理聚合元素的自然结束的点（译者注：即本身自己不知道窗口的大小，计算多长时间的元素）。 以下代码显示如何使用会话窗口: Java版本:DataStream&lt;T&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;) 备注: Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 Operator概述]]></title>
    <url>%2Fflink-stream-operators-overall.html</url>
    <content type="text"><![CDATA[算子(Operator)将一个或多个 DataStream 转换为新的 DataStream。程序可以将多个转换组合成复杂的数据流拓扑。 本节将介绍基本转换(transformations)操作，应用这些转换后的有效物理分区以及深入了解 Flink 算子链。 1. DataStream Transformations1.1 MapDataStream → DataStream 输入一个元素并生成一个对应的元素。下面是一个将输入流的值加倍的 map 函数： Java版本：DataStream&lt;Integer&gt; dataStream = //...dataStream.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer value) throws Exception &#123; return 2 * value; &#125;&#125;); scala版本：dataStream.map &#123; x =&gt; x * 2 &#125; 1.2 FlatMapDataStream → DataStream 输入一个元素并生成零个，一个或多个元素。下面是个将句子拆分为单词的 flatMap 函数： Java版本：dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for(String word: value.split(" "))&#123; out.collect(word); &#125; &#125;&#125;); Scala版本:dataStream.flatMap &#123; str =&gt; str.split(" ") &#125; 1.3 FilterDataStream → DataStream 为每个元素计算一个布尔值的函数并保留函数返回 true 的那些元素。下面是一个筛选出零值的 filter 函数： Java版本:dataStream.filter(new FilterFunction&lt;Integer&gt;() &#123; @Override public boolean filter(Integer value) throws Exception &#123; return value != 0; &#125;&#125;); Scala版本:dataStream.filter &#123; _ != 0 &#125; 1.4 KeyByDataStream → KeyedStream 逻辑上将一个流分成不相交的分区，每个分区包含相同键的元素。在内部，这是通过哈希分区实现的。参阅博文Flink1.4 定义keys的几种方法来了解如何指定键。这个转换返回一个 KeyedStream。 Java版本:dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple Scala版本:dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple 备注 在以下情况，不能指定为key： POJO类型，但没有覆盖hashCode()方法并依赖于Object.hashCode()实现。 任意类型的数组。 1.5 ReduceKeyedStream → DataStream 键控数据流的”滚动” reduce。将当前元素与上一个 reduce 后的值组合，并生成一个新值。下面是一个创建局部求和流的 reduce 函数： Java版本:keyedStream.reduce(new ReduceFunction&lt;Integer&gt;() &#123; @Override public Integer reduce(Integer value1, Integer value2) throws Exception &#123; return value1 + value2; &#125;&#125;); Scala版本:keyedStream.reduce &#123; _ + _ &#125; 1.6 FoldKeyedStream → DataStream 在具有初始值的键控数据流上”滚动” fold。将当前元素与上一个 fold 后的值组合，并生成一个新值。下面是 fold 函数在在序列（1,2,3,4,5）的演示，生成序列 “start-1”，”start-1-2”，”start-1-2-3”，… : Java版本:DataStream&lt;String&gt; result = keyedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String current, Integer value) &#123; return current + "-" + value; &#125; &#125;); Scala版本:val result: DataStream[String] = keyedStream.fold("start")((str, i) =&gt; &#123; str + "-" + i &#125;) 1.7 AggregationsKeyedStream → DataStream 在键控数据流上滚动聚合。min 和 minBy 之间的差别是 min 返回最小值，而 minBy 返回在该字段上具有最小值的元素（max 和 maxBy 相同）。 Java版本:keyedStream.sum(0);keyedStream.sum("key");keyedStream.min(0);keyedStream.min("key");keyedStream.max(0);keyedStream.max("key");keyedStream.minBy(0);keyedStream.minBy("key");keyedStream.maxBy(0);keyedStream.maxBy("key"); Scala版本:keyedStream.sum(0)keyedStream.sum("key")keyedStream.min(0)keyedStream.min("key")keyedStream.max(0)keyedStream.max("key")keyedStream.minBy(0)keyedStream.minBy("key")keyedStream.maxBy(0)keyedStream.maxBy("key") 1.8 WindowKeyedStream → WindowedStream 可以在已分区的 KeyedStream 上定义窗口。窗口根据某些特性（例如，在最近5秒内到达的数据）对每个键的数据进行分组。请参阅窗口以获取窗口的详细说明。 Java版本:dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data Scala版本:dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data 1.9 WindowAllDataStream → AllWindowedStream 可以在常规的 DataStream 上定义窗口。窗口根据某些特征（例如，在最近5秒内到达的数据）对所有流事件进行分组。请参阅窗口以获取窗口的详细说明。 警告 在很多情况下是非并行转换。所有记录将被收集到windowAll算子的一个任务中。 Java版本:dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data Scala版本:dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data 1.10 Window ApplyWindowedStream → DataStreamAllWindowedStream → DataStream 将常规函数应用于整个窗口。以下是手动对窗口元素求和的函数。 注意 如果你使用的是windowAll转换，则需要使用AllWindowFunction。 Java版本：windowedStream.apply (new WindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Tuple, Window&gt;() &#123; public void apply (Tuple tuple, Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;);// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply (new AllWindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Window&gt;() &#123; public void apply (Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;); Scala版本:windowedStream.apply &#123; WindowFunction &#125;// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply &#123; AllWindowFunction &#125; 1.11 Window ReduceWindowedStream → DataStream 在窗口中应用功能性 reduce 函数并返回 reduce 后的值。 Java版本:windowedStream.reduce (new ReduceFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return new Tuple2&lt;String,Integer&gt;(value1.f0, value1.f1 + value2.f1); &#125;&#125;); Scala版本:windowedStream.reduce &#123; _ + _ &#125; 1.12 Window FoldWindowedStream → DataStream 将功能性 fold 函数应用于窗口并返回 fold 后值。例如，应用于序列（1,2,3,4,5）时，将序列 fold 为字符串 start-1-2-3-4-5： Java版本:windowedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; public String fold(String current, Integer value) &#123; return current + "-" + value; &#125;&#125;); Scala版本:val result: DataStream[String] = windowedStream.fold("start", (str, i) =&gt; &#123; str + "-" + i &#125;) 1.13 Aggregations on windowsWindowedStream → DataStream 聚合一个窗口的内容。min 和 minBy 之间的差别是 min 返回最小值，而 minBy 返回该字段中具有最小值的元素（max 和 maxBy 相同）。 Java版本:windowedStream.sum(0);windowedStream.sum("key");windowedStream.min(0);windowedStream.min("key");windowedStream.max(0);windowedStream.max("key");windowedStream.minBy(0);windowedStream.minBy("key");windowedStream.maxBy(0);windowedStream.maxBy("key"); Scala版本:windowedStream.sum(0)windowedStream.sum("key")windowedStream.min(0)windowedStream.min("key")windowedStream.max(0)windowedStream.max("key")windowedStream.minBy(0)windowedStream.minBy("key")windowedStream.maxBy(0)windowedStream.maxBy("key") 1.14 UnionDataStream* → DataStream 合并两个或更多数据流，创建一个包含所有流中所有元素的新流。 注意 如果你与自己进行合并，你将在结果流中获取每个元素两次。 Java版本:dataStream.union(otherStream1, otherStream2, ...); Scala版本:dataStream.union(otherStream1, otherStream2, ...) 1.15 Window JoinDataStream,DataStream → DataStream 在给定的键和公共窗口上对两个数据流进行 join。 Java版本:dataStream.join(otherStream) .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new JoinFunction () &#123;...&#125;); Scala版本:dataStream.join(otherStream) .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply &#123; ... &#125; 1.16 Window CoGroupDataStream,DataStream → DataStream 在给定键和公共窗口上对两个数据流进行组合。 Java版本:dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new CoGroupFunction () &#123;...&#125;); Scala版本:dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply &#123;&#125; 1.17 SplitDataStream → SplitStream 根据一些标准将流分成两个或更多流。 Java版本:SplitStream&lt;Integer&gt; split = someDataStream.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) &#123; output.add("even"); &#125; else &#123; output.add("odd"); &#125; return output; &#125;&#125;); Scala版本:val split = someDataStream.split( (num: Int) =&gt; (num % 2) match &#123; case 0 =&gt; List("even") case 1 =&gt; List("odd") &#125;) 1.18 SelectSplitStream → DataStream 从分流中选择一个或多个流。 Java版本：SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select("even");DataStream&lt;Integer&gt; odd = split.select("odd");DataStream&lt;Integer&gt; all = split.select("even","odd"); Scala版本:val even = split select "even"val odd = split select "odd"val all = split.select("even","odd") 1.19 Extract TimestampsDataStream → DataStream 从记录中提取时间戳，以便与使用事件时间语义的窗口一起工作。 Java版本:stream.assignTimestamps (new TimeStampExtractor() &#123;...&#125;); Scala版本:stream.assignTimestamps &#123; timestampExtractor &#125; 2. Physical partitioning通过以下功能，Flink 还可以在转换后的确切流分区上进行低层次的控制（如果需要）。 2.1 Custom partitioningDataStream → DataStream 使用用户自定义的分区器为每个元素选择指定的任务。 dataStream.partitionCustom(partitioner, "someKey");dataStream.partitionCustom(partitioner, 0); 2.2 Random partitioningDataStream → DataStream 根据均匀分布随机分配元素。 dataStream.shuffle(); 2.3 Rebalancing (Round-robin partitioning)DataStream → DataStream 对元素循环分区，为每个分区创建相同的负载。在在数据倾斜时用于性能优化。 dataStream.rebalance(); 2.4 RescalingDataStream → DataStream 为下游操作的子集循环分配元素。这非常有用，如果你想要在管道中使用，例如，从一个数据源的每个并行实例中输出到几个映射器的子集上来分配负载，但不希望发生 rebalance() 的完全重新平衡。这只需要本地数据传输，而不是通过网络传输数据，具体取决于其他配置值，例如 TaskManager 的插槽数。 上游操作向其发送元素的下游操作的子集取决于上游和下游操作的并行度。例如，如果上游操作并行度为2并且下游操作并行度为4，则一个上游操作将向两个下游操作分配元素，而另一个上游操作将分配给另外两个下游操作。另一方面，如果下游操作并行度为2而上游操作并行度为4，则两个上游操作将分配给一个下游操作，而另外两个上游操作将分配给另一个下游操作。 存在不同并行度不是成倍数关系，或者多个下游操作具有来自上游操作的不同数量的输入的情况。 这个图显示了在上面的例子中的连接模式： dataStream.rescale(); 2.5 BroadcastingDataStream → DataStream 将元素广播到每个分区。dataStream.broadcast() 3. 任务链 和 资源组链接两个连续的转换操作意味着将它们共同定位在同一个线程中以获得更好的性能。如果可能的话，Flink默认链接算子（例如，两个连续的 map 转换）。如果需要，API可以对链接进行精细控制。 如果要禁用整个作业中的链接，请使用 StreamExecutionEnvironment.disableOperatorChaining（）。对于更细粒度的控制，可用使用以下函数。请注意，这些函数只能在 DataStream 转换操作之后使用，因为它们引用上一个转换。例如，你可以使用 someStream.map（...）.startNewChain（），但不能使用 someStream.startNewChain（）。 资源组是 Flink 中的插槽，请参阅插槽。如果需要，你可以在不同的插槽中手动隔离算子。 3.1 开始一个新链从这个算子开始，开始一个新的链。将这两个 mapper 链接，并且 filter 不会链接到第一个 mapper。someStream.filter(...).map(...).startNewChain().map(...); 3.2 取消链不会将map算子链接到链上：someStream.map(...).disableChaining(); 3.3 设置插槽共享组设置操作的插槽共享组。Flink会将使用相同插槽共享组的操作放入同一插槽，同时保持在其他插槽中没有插槽共享组的操作。这可以用来隔离插槽。如果所有输入操作位于同一个插槽共享组中，则插槽共享组将继承自输入操作。缺省插槽共享组的名称为 default，可通过调用 slotSharingGroup（“default”）将操作显式放入此组。someStream.filter(...).slotSharingGroup("name"); 备注： Flink 版本： 1.4 原文： https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/index.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之提取器]]></title>
    <url>%2Fscala-notes-apply-update.html</url>
    <content type="text"><![CDATA[1. apply和update方法Scala允许你使用如下函数调用语法:f(arg1, arg2, ...) 扩展到可以应用于函数之外的值．如果 f 不是函数或方法，那么这个表达式就等同于调用:f.apply(arg1, arg2, ...) 如果它出现在赋值语句的等号左侧:f(arg1, arg2, ...) = value 则等同于调用:f.update(arg1, arg2, ..., value) 应用场景: (1) 常被用于数组和映射:val scores = new scala.collection.mutable.HashMap[String, Int]scores("Bob") = 100 // 调用scores.update("Bob", 100)val bobScores = scores("Bob") // 调用scores.apply("Bob") (2) 同样经常用在伴生对象中，用来构造对象而不用显示的使用new:class Fraction (n: Int, d: Int)&#123; ...&#125;object Fraction&#123; def apply(n: Int, d: Int) = new Fraction(n, d)&#125;// 使用val result = Fraction(3, 4) * Fraction(2, 5) 2. 提取器所谓提取器就是一个带有 unapply 方法的对象．可以把 unapply 方法理解为伴生对象中 apply 方法的反向操作. apply 方法接受构造参数，然后将他们变成对象．而 unapply 方法接受一个对象，然后从中提取值(通常这些值就是当初用来构造该对象的值)． 例如上面例子中的 Fraction 类， apply 方法从分子和分母创建出一个分数，而 unapply 方法则是去取出分子和分母: (1) 可以在变量定义时使用:// a b 分别被初始化成运算结果的分子和分母var Fraction(a, b) = Fraction(3, 4) * Fraction(2, 5) (2) 也可以用于模式匹配:// a 和 b 分别绑到分子和分母case Fraction(a, b) =&gt; ... 通常而言，模式匹配可能会失败，因此 unapply 方法返回的是一个Option．它包含一个元组，每个匹配到的变量各有一个值与之对应．下面中返回一个 Option[(Int, Int)]class Fraction (n: Int, d: Int)&#123; ...&#125;object Fraction&#123; def apply(n: Int, d: Int) = new Fraction(n, d) def unapply(input: Fraction) = if( input.den == 0 ) None else Some( (input.num, input.den) )&#125; 备注分母为0时返回None，表示无匹配 在上面例子中，apply 和 unapply 互为反向，但不一定总是互为反向．我们可以用提取器从任何类型的对象中提取信息．例如我们可以从字符串中提取名字和姓氏:// 提取器object Name&#123; def unapply(input: String) = &#123; val pos = input.indexOf(" ") if(pos == -1) Node else Some( (input.substring(0, pos), input.substring(pos + 1)) ) &#125;&#125;val author = "Lionel Messi"// 调用Name.unapply(author)val Name(first, last) = author// First Name is Lionel and last name is Messiprintln("First Name is " + first + " and last name is " + last) 3. 带单个参数或无参数的提取器在Scala中，并没有只带一个组件的元组．如果 unapply 方法要提取单值，则应该返回一个目标类型的 Option:object Number &#123; def unapply(input: String) : Option[Int] = &#123; try&#123; Some (Integer.parseInt(input.trim)) &#125; catch&#123; case ex: NumberFormatException =&gt; None &#125; &#125;&#125; 可以使用这个提取器，从字符串中提取数字:val Number(n) = "1990" 提取器也可以只是测试输入的数据而并不将其值提取出来，只需unapply方法返回Boolean:object IsContainZero&#123; def unapply(input: String) = input.contains("0")&#125; 4. unapplySeq方法如果要提取任意长度的值的序列，我们需要使用 unapplySeq 来命名我们的方法．它返回一个 Option[Seq[A]]，其中A是被提取的值的类型:object Names&#123; def unapplySeq(input: String): Option[Seq[String]] = &#123; if(input.trim == "") None else Some(input.trim.split("\\s+")) &#125;&#125;val namesStr = "Tom Lily Lucy"val Names(first, second, third) = namesStr// the first name is Tom and the second name is Lily and the third name is Lucyprintln(s"the first name is $first and the second name is $second and the third name is $third") 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之高阶函数]]></title>
    <url>%2Fscala-notes-higher-order-functions.html</url>
    <content type="text"><![CDATA[Scala混合了面向对象和函数式的特性．在函数式编程语言中，函数可以像任何其他数据类型一样被传递和操作．如果想要给算法传入明细动作时，只需要将明细动作包在函数当中作为参数传入即可． 1. 作为值的函数在Scala中，函数就和数字一样，可以在变量中存放:import scala.math._val num = 3.14 // num: Double = 3.14val fun = ceil _ // fun: Double =&gt; Double = &lt;function1&gt;println(num) // 3.14println(fun(num)) // 4.0 上述代码将num设置为3.14，将fun设置为 ceil 函数．num的类型为 Double，fun的类型为 (Double) =&gt; Double (即接受并返回Double的函数) 备注 ceil函数后的 _ 表示确实指的是ceil这个函数，而不是碰巧忘记了给它传递参数 可以对函数做如下两件事: 调用它 传递它 存放在变量中，或者作为参数传递给另一个函数 Example:// 调用fun(num) // 4.0// 传递Array(3.14, 2.14, 1.14).map(fun) // Array(4.0, 3.0, 2.0) 备注map方法接受一个函数参数，将它应用到数组中的所有值，然后返回结果的数组 2. 匿名函数在Scala中，不需要给每一个函数命名，就像不用给每个数字命名一样:(x: Double) =&gt; 3 * x 上述代码表示该函数将传递给它的参数乘以3． 对与上述匿名函数我们可以如下操作: (1) 可以将函数存放在变量中:val triple = (x: Double) =&gt; 3 * xtriple(2) // 6.0 上述代码等价于:def triple(x:Double) = 3 * x (2) 可以不用命名直接将函数传递给另一个函数:Array(3.14, 2.14, 1.14).map((x: Double) =&gt; 3 * x) 3. 带函数参数的函数下面是一个接受一个函数作为参数的函数:def valueAtOneQuarter(fun: (Double) =&gt; Double) = fun(0.25)valueAtOneQuarter(sqrt _) // 0.5 即 sqrt(0.25) 上述函数的参数类型为 (Double) =&gt; Double，即接受任何 Double 并返回 Double 的函数． 4. 参数类型推断当你将一个匿名函数传递给一个函数时，Scala会尽可能帮助你推断出类型信息．不需要将代码写成如下:valueAtOneQuarter( (x:Double) =&gt; 3 * x ) // 0.75 由于valueAtOneQuarter方法知道你会传入一个类型为(x:Double) =&gt; Double的函数，可以将上述代码改为:valueAtOneQuarter( (x) =&gt; 3 * x ) 或者valueAtOneQuarter( x =&gt; 3 * x ) // 只有一个参数的函数 可以省略括号 如果参数在=&gt;右侧只出现一次，可以使用 _ 替换，因此进一步改写:valueAtOneQuarter(3 * _) 5. 柯里化柯里化是指将原来接受两个参数的函数变成一个新的接受一个参数的函数的过程．新的函数返回一个以原有第二个参数作为参数的函数．def mul (x: Int, y: Int) = x * y 以下函数接受一个参数，生成另一个接受单个参数的函数:def mulOneAtATime(x: Int) = (y: Int) =&gt; x * y 要计算两个数的乘积，需要调用:mulOneAtATime(6)(7) 细分说明一下，mulOneAtATime(6)返回的是一个函数 (y: Int) =&gt; 6 * y．而这个函数又被应用到7，因此最终的结果为42． Scala支持如下简写来定义柯里化函数:def mulOneAtATime(x: Int) (y: Int) = x * y 我们可以看到多参数只是个虚饰，不是什么编程语言的特质． 6. 控制抽象在Scala中，我们可以将一系列语句组成不带参数也没有返回值的函数．如下函数在线程中执行某段代码:def runInThread(block: ()=&gt;Unit)&#123; new Thread&#123; override def run()&#123; block() &#125; &#125;.start()&#125; 上述函数为带函数参数的函数，函数参数类型为()=&gt;Unit(表示没有参数也没有返回值)．但是如此一来，当你调用该函数时，需要写不美观的()=&gt;:runInThread&#123; () =&gt; println("Hello");Thread.sleep(10000);println("Bye"); &#125; 要想在调用中省掉()=&gt;，可以使用换名调用表示法：在参数声明和调用该函数参数的地方略去()，但保留=:def runInThread(block: =&gt; Unit)&#123; // (1) new Thread&#123; override def run() &#123; block &#125; // (2) &#125;.start()&#125; 这样，我们如下调用:runInThread&#123; println("Hello");Thread.sleep(10000);println("Bye"); &#125; Scala程序员可以构建控制抽象:看上去像编程语言的关键字的函数．例如，下面我们定一个until语句，工作原理类似while，只不过把条件反过来用:def until (condition: =&gt; Boolean) (block: =&gt; Unit) &#123; if(!condition)&#123; block until (condition) (block) &#125;&#125; 使用:var x = 4until (x == 0) &#123; println(x) x = x -1&#125;// 4// 3// 2// 1var x = 0until (x == 0) &#123; println(x) x = x -1&#125;// 无输出 这样的函数参数有一个专业术语:换名调用参数．和常规的参数不同，函数被调用时，参数表达式不会被求值．毕竟，在调用until时，不希望x == 0被求值得到false．与之相反，表达式成为无参函数的函数体，而该函数被当做参数传递下去．仔细看一下until函数的定义．注意它是柯里化的:函数首先处理掉condition，然后把block当做完全独立的另一个参数．如果没有柯里化，调用就会变成如下:until (x == 0, &#123;...&#125;) 7. return表达式在Scala中，不需要使用return语句返回函数值．函数的返回值是函数体的值．不过，可以使用return来从一个匿名函数中返回值给包含这个匿名函数的带名函数．这对于抽象控制是很有用的:def indexOf(str: String, ch: Char) : Int = &#123; var i = 0; until (i == str.length)&#123; if(str(i) == ch) return i i += 1 &#125; return -1&#125; 在这里，匿名函数{ if(str(i) == ch) return i; i += 1 }被传递给until．当return表达式被执行时，包含它的带名函数indexOf终止并返回给定的值．如果要在带名函数中使用return的话，则需要给出其返回类型．例如上例中，编译器没法推断出它会返回Int，因此需要给出返回类型Int． 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之正则表达式]]></title>
    <url>%2Fscala-notes-regex.html</url>
    <content type="text"><![CDATA[1. Regex对象我们可以使用 scala.util.matching.Regex 类使用正则表达式．要构造一个 Regex 对象，使用 String 类的 r 方法即可:val numPattern = "[0-9]+".r 如果正则表达式包含反斜杠或引号的话，那么最好使用”原始”字符串语法 &quot;&quot;&quot;...&quot;&quot;&quot;:val positiveNumPattern = """^[1-9]\d*$""" 如果在Java中使用上述正则表达式，则应该使用下面方式(需要进行转义):val positiveNumPattern = "^[1-9]\\d*$" 相对于在Java中的使用方式，Scala这种写法可能更易读一些． 2. findAllInfindAllIn 方法返回遍历所有匹配项的迭代器．可以在 for 循环中使用它:val str = "a b 27 c 6 d 1"val numPattern = "[0-9]+".rfor(matchingStr &lt;- numPattern.findAllIn(str))&#123; println(matchingStr)&#125; 或者将迭代器转成数组:val str = "a b 27 c 6 d 1"val numPattern = "[0-9]+".rval matches = numPattern.findAllIn(str).toArray// Array(27,6,1) 3. findPrefixOf检查某个字符串的前缀是否能匹配，可以使用findPrefixOf方法:val str = "3 a b 27 c 6 d 1"val str2 = "a b 27 c 6 d 1"val numPattern = "[0-9]+".rval matches = numPattern.findPrefixOf(str)val matches2 = numPattern.findPrefixOf(str2)println(matches) // Some(3)println(matches2) // None 4. replaceFirstIn replaceAllIn可以使用如下命令替换第一个匹配项或者替换全部匹配项:val str = "3 a b 27 c 6 d 1"val numPattern = "[0-9]+".rval matches = numPattern.replaceFirstIn(str, "*")val matches2 = numPattern.replaceAllIn(str, "*")println(matches) // * a b 27 c 6 d 1println(matches2) // * a b * c * d * 5. 正则表达式组分组可以让我们方便的获取正则表达式的子表达式．在你想要提取的子表达式两侧加上圆括号:val str = "3 a"val numPattern = "([0-9]+) ([a-z]+)".rval numPattern(num, letter) = strprintln(num) // 3println(letter) // a 上述代码将num设置为3，letter设置为a 如果想从多个匹配项中提取分组内容，可以使用如下命令:val str = "3 a b c 4 f"val numPattern = "([0-9]+) ([a-z]+)".rfor(numPattern(num, letter) &lt;- numPattern.findAllIn(str))&#123; println(num + "---"+letter)&#125;// 3---a// 4---f 来源于: 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之Object]]></title>
    <url>%2Fscala-notes-object.html</url>
    <content type="text"><![CDATA[1. 单例对象Scala没有静态方法或静态字段，可以使用 object 来达到这个目的，对象定义了某个类的单个实例:object Account&#123; private var lastNumber = 0 def newUniqueNumber () = &#123;lastNumber += 1; lastNumber&#125;&#125; 当你在应用程序中需要一个新的唯一账号时，调用 Account.newUniqueNumber() 即可．对象的构造器在该对象第一次被使用时调用．在本例中，Account 的构造器在 Account.newUniqueNumber 的首次调用时执行．如果一个对象从未被使用，那么构造器也不会被执行． 对象本质上可以拥有类的所有特性，但是不能提供构造器参数． 在Scala中可以用对象来实现: 作为存放工具函数或常量的地方 高效的共享单个不可变实例 需要用单个实例来协调某个服务时(参考单例模式) 2. 伴生对象在Java中，通常会用到既有实例方法又有静态方法的类，在Scala中，可以通过类和类同名的 伴生对象 来达到同样的目的:class Account&#123; val id = Account.newUniqueNumber() private var balance = 0.0 def deposit(amount : Double) &#123; balance += amount &#125; ...&#125;// 伴生对象object Account&#123; private var lastNumber = 0 def newUniqueNumber () = &#123;lastNumber += 1; lastNumber&#125; &#125; 类和它的伴生对象可以相互访问私有特性．它们必须在同一个源文件中． 3. apply方法我们通常会定义和使用对象的 apply 方法．当遇到如下形式的表达式时，apply 方法就会被调用:Object(参数1，参数2，...，参数N) 通常，这样一个 apply 方法返回的是伴生类的对象．举例来说，Array对象定义了 apply 方法，让我们可以用下面这样的表达式来创建数组:Array("Mary", "had", "a", "little", "lamb") 不使用构造器，而是使用apply方法，对于使用嵌套表达式而言，省去new关键字会方便很多:Array(Array(1,7), Array(2,9)) 下面有一个定义apply方法的示例:class Account private (val id :Int, initialBalance: Double)&#123; private var balance = initialBalance ...&#125;// 伴生对象object Account&#123; def apply(initialBalance : Double)&#123; new Account(newUniqueNumber(), initialBalance) &#125; ...&#125; 这样我们就可以使用如下方式创建账号了:val acct = Account(1000.0) 4. 应用程序对象每个Scala程序都必须从一个对象的main方法开始，这个方法的类型为 Array[String]=&gt;Unit:object Hello&#123; def main(args: Array[String])&#123; println("Hello world!") &#125;&#125; 除了每次都提供自己main方法外，你可以扩展App特质，然后将程序代码放入构造器方法体内:object Hello extends App&#123; println("Hello world!")&#125; 如果需要命令行参数，则可以通过args属性得到:object Hello extends App&#123; if(args.length &gt; 0)&#123; println("Hello, " + args(0)) &#125; else&#123; println("Hello world!") &#125;&#125; 5. 枚举不同于Java，Scala中没有枚举类型，需要我们通过标准库类 Enumeration 来实现:object BusinessType extends Enumeration&#123; var FLIGHT, HOTEL, TRAIN, COACH = Value&#125; 继承 Enumeration 类，实现一个 BusinessType 对象，并以 Value 方法调用初始化枚举中的所有可选值．在这里我们定义了４个业务线类型，然后用Value调用它们初始化． 每次调用Value方法都返回内部类的新实例，该内部类也叫做Value．或者，可以向Value方法传入ID，名称:val FLIGHT = Value(0, "FLIGHT")val HOTEL = Value(10) // 名称为"HOTEL"val TRAIN = Value("TRAIN") // ID为11 如果不指定ID，ID为上一个枚举值上加一，如果不指定名称，名称默认为字段名．定义完成后，可以使用 BusinessType.FLIGHT，BusinessType.HOTEL，BusinessType.TRAIN 等来引用:def businessHandle(business: BusinessType.Value): Unit =&#123; if(business == BusinessType.FLIGHT)&#123; println("this is a flight behavior") &#125; else if(business == BusinessType.HOTEL)&#123; println("this ia a hotel behavior") &#125;&#125;def main(args: Array[String]): Unit = &#123; val business = BusinessType.FLIGHT businessHandle(business) // this is a flight behavior&#125; 如果觉的BusinessType.FLIGHT比较冗长繁琐，可以使用如下方式引入枚举值:import BusinessType._ 使用时直接使用枚举值名称即可:def businessHandle(business: BusinessType.Value): Unit =&#123; if(business == FLIGHT)&#123; println("this is a flight behavior") &#125; else if(business == HOTEL)&#123; println("this ia a hotel behavior") &#125;&#125; 记住枚举值的类型是BusinessType.Value而不是BusinessType，后者是拥有这些值的对象，可以增加一个类型别名:object BusinessType extends Enumeration&#123; type BusinessType = Value var FLIGHT, HOTEL, TRAIN, COACH = Value&#125; 如下使用:def businessHandle(business: BusinessType): Unit =&#123; if(business == FLIGHT)&#123; println("this is a flight behavior") &#125; else if(business == HOTEL)&#123; println("this ia a hotel behavior") &#125;&#125; 枚举值的ID可以通过id方法返回，名称通过toString方法返回:val business = FLIGHTprintln("ID:" + business.id + " name:" + business.toString) // ID:0 name:FLIGHT 可以通过如下方式输出所有的枚举值:for(business &lt;- BusinessType.values)&#123; println("ID:" + business.id + " name:" + business.toString)&#125;ID:0 name:FLIGHTID:1 name:HOTELID:2 name:TRAINID:3 name:COACH 你也可以通过枚举的ID或名称来进行查找定位:val FLIGHT1 = BusinessType(0)println("ID:" + FLIGHT1.id + " name:" + FLIGHT1.toString)val FLIGHT2 = BusinessType.withName("FLIGHT")println("ID:" + FLIGHT2.id + " name:" + FLIGHT2.toString)ID:0 name:FLIGHTID:0 name:FLIGHT 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之文件操作]]></title>
    <url>%2Fscala-notes-file.html</url>
    <content type="text"><![CDATA[1. 读取行读取文件，可以使用 scala.io.Source 对象的 fromFile 方法．如果读取所有行可以使用 getLines 方法:val source = Source.fromFile("/home/xiaosi/exception.txt", "UTF-8")val lineIterator = source.getLines()for(line &lt;- lineIterator)&#123; println(line)&#125;source.close() source.getLines 返回结果为一个迭代器，可以遍历迭代器逐条处理行． 如果想把整个文件当做一个字符串处理，可以调用mkString方法:val content = source.mkString 备注 在用完 Source 对象后，记得调用 close 方法进行关闭 2. 读取字符读取字符，可以直接把 Source 对象当做迭代器使用，因为 Source 类扩展了Iterator[Char]:val source = Source.fromFile("/home/xiaosi/exception.txt", "UTF-8")for(c &lt;- source)&#123; print(c + " ")&#125; 3. 从URL或其他源读取数据Source 对象有读取非文件源的方法:// 从URL中读取数据val sourceUrl = Source.fromURL("http://xxx", "UTF-8")// 从字符串中读取数据val sourceStr = Source.fromString("Hello World!")// 从标准输入读取数据val sourceStd = Source.stdin 4. 读取二进制文件Scala并没有提供读取二进制文件的方法．但是你可以使用Java类库来完成读取操作:val file = new File(fileName)val in = new FileInputStream(file)val bytes = new Array[byte](file.length.toInt)in.read(bytes)in.close() 5. 写入文本文件Scala并没有内置的对写入文件的支持．但是可以使用 java.io.PrintWriter 来完成:val out = new PrintWriter("/home/xiaosi/exception.txt")out.println("Hello World")out.println("Welcome")out.close() 6. 访问目录目前Scala并没有用来访问某个目录中的所有文件，或者递归的遍历所有目录的类，我们只能寻求一些替代方案. 利用如下代码可以实现递归遍历所有的子目录:// 递归遍历目录def subDirs(dir: File) : Iterator[File] = &#123; val children = dir.listFiles().filter(_.isDirectory) children.toIterator ++ children.toIterator.flatMap(subDirs _)&#125;val file = new File("/home/xiaosi/test")val iterator = subDirs(file)for(d &lt;- iterator)&#123; println(d)&#125; 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之类]]></title>
    <url>%2Fscala-notes-class.html</url>
    <content type="text"><![CDATA[1. 简单类与无参方法class Person &#123; var age = 0 // 必须初始化字段 def getAge() = age // 方法默认为公有的&#125; 备注 在Scala中，类并不声明为public． Scala源文件可以包含多个类，所有这些类都具有公有可见性．属性不声明默认为public． 使用类:val p = new Person // 或者new Person()p.age = 23println(p.getAge()) // 23 调用无参方法时，可以写上圆括号，也可以不写:p.getAge() // 23p.getAge // 23 2. 带getter和setter的属性2.1 Java getter和setter在Java类中，我们并不喜欢使用公有字段:public class Person&#123; public int age; // Java中不推荐使用这种方式&#125; 更倾向于使用getter和setter方法:public class Person&#123; private int age; public int getAge() &#123;return age;&#125; public void setAge(int age) &#123;this.age = age;&#125;&#125; 像这样的一对getter/setter通常被称为属性．我们会说Person类有一个age属性． 2.２ Scala getter和setter在Scala中对每个字段都提供了getter和setter方法:class Person&#123; var age = 0&#125; scala生成面向JVM的类，会生成一个私有的age字段以及相应的getter和setter方法．这两个方法都是公有的，因为我们没有将age声明为private．(对于私有字段而言,getter和setter方法也是私有的) 在scala中getter和setter方法分别叫age和age_=．使用的时候如下:val p = new Personp.age = 21 // 调用p.age_=(21)println(p.age) // 调用p.age()方法 备注 在scala中，getter和setter方法并非被命名为getXXX和setXXX，不过用意相同． 任何时候我们都可以自己重新定义getter和setter方法:class Person &#123; private var privateAge = 0 def age = privateAge def age_= (newAge : Int): Unit = &#123; if(newAge &gt; 150)&#123; privateAge = 150 &#125; else if(newAge &lt; 0)&#123; privateAge = 0 &#125; &#125;&#125; 使用:val p = new Personp.age = -1;println(p.age) // 0p.age = 189println(p.age) // 150 备注 Scala对每个字段生成getter和setter方法听上去有些恐怖，不过你可以控制这个过程: 如果字段是私有的，则getter和setter方法也是私有的 如果字段是val，则只有getter方法被生成 如果你不需要任何的getter和setter方法，可以将字段声明为private[this] 2.3 Example(1) 对于公有字段,getter和setter方法是公有的:class Student &#123; var age = 22&#125;val stu = new Studentstu.age = 23println(stu.age) // 23 (2) 对于私有字段,getter和setter方法是私有的:class Student &#123; private var age = 22&#125;val stu = new Student//stu.age = 23 // symbol age is inaccessible from this place//println(stu.age) // symbol age is inaccessible from this place (3) 如果字段是val，则只有getter方法被生成:class Student &#123; val age = 22&#125;val stu = new Student// stu.age = 23 // reassignment to valprintln(stu.age) // 22 3. 只带getter的属性如果只想需要一个只读的属性，有getter但没有setter，属性的值在对象构建完成之后就不再改变了，可以使用val字段:class Student &#123; val age = 22&#125; Scala会生成一个私有的final字段和一个getter方法，但没有setter方法 4. 对象私有字段在Scala中，方法可以访问该类的所有对象的私有字段:class Counter &#123; private var value = 0 def increment(): Unit = &#123; value += 1 &#125; // 对象可以访问另一个对象的私有字段 def isLess (other : Counter) = value &lt; other.value&#125; 之所以访问 other.value 是合法的，是因为 other 也是Counter对象，这与Java的private权限不一样. Scala允许我们定义更加严格的访问限制，通过private[this]这个修饰符来实现:private[this] var value = 0 这样 other.value 是不被允许访问的，这样以来Counter类只能访问当前对象的value字段，而不能访问同样是Counter类型的其他对象的字段． Scala允许你将访问权限赋予指定得类，private[类名]可以定义仅有指定类的方法可以访问给定的字段．这里的类名必须是当前定义的类，或者是包含该类的外部类． 备注 对于类私有的字段(private)，Scala会生成私有的getter和setter方法，但是对于对象私有的字段，不会生成getter和setter方法． 5. Bean属性Scala对于你定义的字段提供了getter和setter方法，但是并不是Java工具所期望的．JavaBeans规范把Java属性定义为一对getXXX/setXXX方法．很多Java工具都依赖这样的命令习惯． Scala给我们提供了@BeanProperty注解，这样就会字段生成我们期望的getXXX和setXXX方法:class Student &#123; @BeanProperty var age = 22&#125;val stu = new Studentstu.setAge(25)println(stu.getAge()) // 25 总结 scala字段 生成的方法 何时使用 val/var name 公有的name name_=(仅限var) 实现一个可以被公开访问并且背后是以字段形式保存的属性 @BeanProperty val/var name 公有的name getName() name_=(仅限var) setName() (仅限var) 与JavaBeans互操作 private val/var name 私有的name name_=(仅限var) 用于将字段访问限制在本类的方法．尽量使用private，除非真的需要一个公有属性 private[this] val/var name 无 用于将字段访问限制在同一个对象上调用的方法．不经常用 private[类名] val/var name 依赖于具体实现 将访问权限赋予外部类．不经常使用 6. 辅助构造器Scala可以有任意多的构造器，不过，Scala有一个构造器比其他所有构造器都重要，就是主构造器，除了主构造器之外，类还有任意多的辅助构造器．其同Java中的构造器十分相似，只有两处不同: 辅助构造器的名称为this 每一个辅助构造器都必须以一个先前已定义的其他辅助构造器或主构造器的调用开始 class Person &#123; private var name = "" private var age = 0 def this (name : String)&#123; this() // 调用主构造器 this.name = name &#125; def this (name : String, age : Int)&#123; this(name) // 调用前一个辅助构造器 this.age = age &#125;&#125; 可以使用如下三种方式构造对象:val p1 = new Person // 调用主构造器val p2 = new Person("Bob") // 调用第一个辅助构造器val p3 = new Person("Bob", 25) // 调用第二个辅助构造器 7. 主构造器在Scala中，每个类都有主构造器．主构造器并不以this方法定义，而是与类定义交织在一起． (1) 主构造器的参数直接放在类名之后class Person(val name:String) &#123; private var age = 0 def this (name : String, age : Int)&#123; this(name) // 调用主构造器 this.age = age &#125;&#125; 主构造器的参数被编译成字段，其值被初始化成构造时传入的参数．上述示例中name和age为Person类的字段． (2) 主构造器会执行类定义中的所有语句class Person(val name:String) &#123; println("constructed a person ...") private var age = 0 def this (name : String, age : Int)&#123; this(name) // 调用主构造器 this.age = age &#125;&#125; println语句是主构造器的一部分．每当有对象被构造出来时．上述代码就会被执行 (3) 通常可以在主构造器中使用默认参数来避免使用过多的辅助构造器class Person(val name:String = "", val age: Int = 0) &#123;&#125; 备注 如果类名之后没有参数，则该类具备一个无参主构造器. 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之Map与Tuple]]></title>
    <url>%2Fscala-notes-map-and-tuple.html</url>
    <content type="text"><![CDATA[1. 构造映射可以使用如下命令构造一个映射:scala&gt; val scores = Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 89, Bob -&gt; 98) 上面构造了一个不可变的Map[String, Int]，其值也不能被改变．如果想要一个可变映射，使用如下命令创建:scala&gt; val scores = scala.collection.mutable.Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.mutable.Map[String,Int] = Map(Bob -&gt; 98, Alice -&gt; 90, Kim -&gt; 89) 如果只想创建一个空的映射:scala&gt; val scores = new scala.collection.mutable.HashMap[String, Int]scores: scala.collection.mutable.HashMap[String,Int] = Map() 从上面我们可以知道使用-&gt;操作符来创建映射的键值对元素&quot;Alice&quot; -&gt; 90 我们也可以使用下面的方式定义映射:scala&gt; val scores = Map(("Alice",90), ("Kim",89), ("Bob",98))scores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 89, Bob -&gt; 98) 2. 获取映射中的值可以使用()来查找某个键对应的值:scala&gt; val bobscores = scores("Bob")bobscores: Int = 98 如果映射中并不包含对应键的值，则会抛出异常，这与Java返回null不同:scala&gt; val tomScores = scores("Tom")java.util.NoSuchElementException: key not found: Tom at scala.collection.MapLike$class.default(MapLike.scala:228) at scala.collection.AbstractMap.default(Map.scala:59) at scala.collection.MapLike$class.apply(MapLike.scala:141) at scala.collection.AbstractMap.apply(Map.scala:59) ... 32 elided 所以在获取某个键对应的值之前，要先检查映射中是否存在指定的键:scala&gt; val tomScores = if(scores.contains("Tom")) scores("Tom") else 0tomScores: Int = 0 以下是一个快捷写法:scala&gt; val tomScores = scores.getOrElse("Tom", 0)tomScores: Int = 0 3. 更新映射中的值在可变映射中，可以更新某个映射的值，也可以添加一个新的键值对:scala&gt; val scores = scala.collection.mutable.Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.mutable.Map[String,Int] = Map(Bob -&gt; 98, Alice -&gt; 90, Kim -&gt; 89)scala&gt; scores("Alice")=100 // 更新键值对scala&gt; scores("Tom")=67 // 添加键值对scala&gt; println(scores)Map(Bob -&gt; 98, Tom -&gt; 67, Alice -&gt; 100, Kim -&gt; 89) 还可以使用+=操作符来添加多个关系:scala&gt; scores += ("Bob" -&gt; 78, "Fred" -&gt; 89)res3: scores.type = Map(Bob -&gt; 78, Fred -&gt; 89, Tom -&gt; 67, Alice -&gt; 100, Kim -&gt; 89) 还可以使用-=操作符移除某个键对应的值:scala&gt; scores -= "Tom"res4: scores.type = Map(Bob -&gt; 78, Fred -&gt; 89, Alice -&gt; 100, Kim -&gt; 89) 虽然不可以更新一个不可变的映射，但是我们利用一些操作产生一个新的映射，并可以对原映射中的键值对进行修改或者添加新的键值对:scala&gt; val scores = Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 89, Bob -&gt; 98)scala&gt; val newScores = scores + ("Kim" -&gt; 78, "Tom" -&gt; 54)newScores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 78, Bob -&gt; 98, Tom -&gt; 54) 上例中scores是不可变映射，我们在它基础上对”Kim”进行了修改，添加了”Tom”，产生了一个新的映射newScores 4. 迭代映射可以使用如下命令迭代映射:scala&gt; for( key &lt;- scores.keySet ) println(key + "---" + scores(key))Alice---90Kim---89Bob---98 或者scala&gt; for( value &lt;- scores.values ) println(value)908998 5. 排序映射在操作映射时，我们需要选定一个映射(哈希表还是平衡树)．默认情况下，scala给的是哈希表．有时候我们想对键进行一个排序，顺序访问键，这就需要一个树形映射:scala&gt; val scores = scala.collection.immutable.SortedMap("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.immutable.SortedMap[String,Int] = Map(Alice -&gt; 90, Bob -&gt; 98, Kim -&gt; 89) 6. 与Java互操作如果你有一个Java映射，想要转换为Scala映射，以便便捷的使用Scala映射的方法，只需要增加如下语句:import scala.collection.JavaConversions.mapAsScalaMap 然后指定Scala映射类型来触发转换:scala&gt; val scores : scala.collection.mutable.Map[String,Int] = new java.util.TreeMap[String, Int]scores: scala.collection.mutable.Map[String,Int] = Map() 还可以将java.util.Properties到Map[String, String]的转换:scala&gt; import scala.collection.JavaConversions.propertiesAsScalaMapimport scala.collection.JavaConversions.propertiesAsScalaMapscala&gt; val props : scala.collection.Map[String, String] = System.getProperties()props: scala.collection.Map[String,String] =Map(env.emacs -&gt; "", java.runtime.name -&gt; Java(TM) SE Runtime Environment, sun.boot.library.path -&gt; /home/xiaosi/opt/jdk-1.8.0/jre/lib/amd64, java.vm.version -&gt; 25.91-b14, java.vm.vendor -&gt; Oracle Corporation, ... 相反，如果想要把Scal映射转换为Java映射，只需要提供相反的隐式转换即可:scala&gt; import scala.collection.JavaConversions.mapAsJavaMapimport scala.collection.JavaConversions.mapAsJavaMapscala&gt; import java.awt.font.TextAttribute._ // 引入下面的映射会用到的键import java.awt.font.TextAttribute._scala&gt; val attrs = Map(FAMILY -&gt; "Serif", SIZE -&gt; 12) // Scala映射attrs: scala.collection.immutable.Map[java.awt.font.TextAttribute,Any] = Map(java.awt.font.TextAttribute(family) -&gt; Serif, java.awt.font.TextAttribute(size) -&gt; 12)scala&gt; val font = new java.awt.Font(attrs) // Java映射font: java.awt.Font = java.awt.Font[family=Serif,name=Serif,style=plain,size=12] 7. 元组Tuple元组是不同类型的值的聚合，元组的值通过将单个的值包含在圆括号中构成的：scala&gt; val bobScore = (1, 98.5, "Bob")bobScore: (Int, Double, String) = (1,98.5,Bob) 可以使用方法_1，_2，_3访问其组员:scala&gt; val bobScore = (1, 98.5, "Bob")bobScore: (Int, Double, String) = (1,98.5,Bob)scala&gt; bobScore._1res10: Int = 1scala&gt; bobScore._3res11: String = Bob 通常，使用模式匹配的方式来获取元组的组元:scala&gt; val (id, score, name) = bobScore // 将变量id赋值为1，变量score赋值为98.5，变量name赋值为Bob val bobScore: (Int, Double, String)scala&gt; val (id, score, name) = bobScoreid: Int = 1score: Double = 98.5name: String = Bobscala&gt; println("name = " + name + ", score = " + score + ", name = " + name)name = Bob, score = 98.5, name = Bob 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之数组]]></title>
    <url>%2Fscala-notes-array.html</url>
    <content type="text"><![CDATA[1. 定长数组如果你需要一个长度不变的数组，可以使用Scala中的 Array．val nums = new Array[Int](10) // 10个整数的数组 所有元素初始化为0val strs = new Array[String](10) // 10个字符串的数组 所有元素初始化为nullval s = Array("Hello", "World") // 长度为2的Array[String] 类型是推断出来的 s(0) = "GoodBye" // Array("GoodBye"，"World") 备注 已提供初始值时不要使用new，例如上面的数组s 使用()而不是[]来访问元素 在JVM中，Scala的Array以Java数组方式实现． 2. 变长数组对于那种长度按需要变化的数组，Java有 ArrayList．Scala中等效数据结构为 ArrayBuffer．import scala.collection.mutable.ArrayBufferval b = ArrayBuffer[Int]() // 或者 new ArrayBuffer[Int]() 创建一个空的数组缓冲来存放整数b += 1 // ArrayBuffer(1) 用+=在尾端添加元素b += (1,2,3,5) // ArrayBuffer(1,1,2,3,5) 在尾端添加多个元素b ++= Array(8, 13, 21) // ArrayBuffer(1,1,2,3,5,8,13,21) 用++=操作追加任何集合b.trimEnd(5) // ArrayBuffer(1,1,2) 移除最后５个元素 可以在任意位置插入或移除元素，但这样的操作不如在尾端添加或移除元素操作那么高效:b.insert(2,6) // ArrayBuffer(1,1,6,2) 在下标2之前插入b.insert(2,7,8,9) // ArrayBuffer(1,1,7,8,9,6,2) 插入任意多的元素b.remove(2) // ArrayBuffer(1,1,8,9,6,2) 删除下标２的元素b.remove(2,3) // ArrayBuffer(1,1,2) 第二个参数的含义是要移除多少个元素 有时需要构建一个Array，但不知道最终需要装多少元素．这种情况下可以先构建一个数组缓冲，然后调用:b.toArray // Array(1,1,2) 3. 遍历数组和数组缓冲使用for循环遍历数组和数组缓冲:val b = Array(6,5,4,3,2,1)for(i &lt;- 0 until b.length)&#123; println(i + "-" + b(i))&#125; 输出结果:0-61-52-43-34-25-1 备注 until 是 RichInt 类的方法，返回所有小于(但不包括)上限的数字 如果想要每两个元素一跳，可以让i这样来进行遍历:val b = Array(6,5,4,3,2,1)for(i &lt;- 0 until (b.length, 2))&#123; println(i + "-" + b(i))&#125; 输出结果：0-62-44-2 如果要从数组的尾端开始:val b = Array(6,5,4,3,2,1)for(i &lt;- (0 until b.length).reverse)&#123; println(i + "-" + b(i))&#125; 如果在循环体中不需要用到数组下标，我们也可以直接访问数组元素:for(elem &lt;- b)&#123; println(elem)&#125; 4. 数组转换从一个数组(数组缓冲)出发，以某种方式对它进行转换是很简单的．这些转换操作不会修改原是数组，而是产生一个全新的数组:val a = Array(1,2,3,4)val result = for(elem &lt;- a) yield 2 * elem // result 是Array(2,4,6,8) for(...) yield循环创建了一个类型与原实际和相同的新集合．新元素为yield之后的表达式的值，每次迭代对应一个． 当你遍历一个集合时，如果只想处理满足特定条件的元素．可以通过for中的if来实现:val a = Array(1,2,3,4)val result = for(elem &lt;- a if elem % 2 == 0) yield 2 * elem 上面实例中对每个偶数元素翻倍，并丢掉奇数元素． 5. 常用操作5.1 sumval a = Array(6,1,7,4)a.sum // 18 要使用sum方法，元素类型必须是数值类型:整型，浮点数或者BigInteger/BigDecimal 5.2 min maxval a = Array(6,1,7,4)a.min // 1a.max // 7 min和max输出数组或数组缓冲中最小和最大的元素 5.3 sortedval a = Array(6,1,7,4)val asorted = a.sorted // Array(1, 4, 6, 7)val a = ArrayBuffer(6,1,7,4)val asorted = a.sortWith(_ &gt; _) // ArrayBuffer(7, 6, 4, 1) sorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，不会修改原始数组．可以使用sortWith方法提供一个比较函数． 5.4 mkStringval a = Array(6,1,7,4)a.mkString(" and ") // 6 and 1 and 7 and 4 如果想要显示数组或者数组缓冲的内容，可以使用mkString，允许指定元素之间的分隔符val a = Array(6,1,7,4)a.mkString("&lt;", ",", "&gt;") // &lt;6,1,7,4&gt; 该方法的另一个重载版本可以让你指定前缀和后缀 来源于: 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之基础语法]]></title>
    <url>%2Fscala-notes-basis.html</url>
    <content type="text"><![CDATA[1. 变量val定义的值实际上是一个常亮，无法改变其内容scala&gt; val num = 0num: Int = 0scala&gt; num = 2&lt;console&gt;:12: error: reassignment to val num = 2 ^ 如果要声明其值可变的变量，可以使用varscala&gt; var number = 0number: Int = 0scala&gt; number = 2number: Int = 2 在Scala中，建议使用val，除非你真的需要改变它的内容． 备注 不需要给出值或者变量的类型，可以从你用来初始化它的表达式推断出来．只声明值或者变量但不做初始化会报错：scala&gt; val str: String&lt;console&gt;:11: error: only classes can have declared but undefined members val str: String ^scala&gt; val str: String = "Hello"str: String = Hello 2. 常用类型常用类型： Byte Char Short Int Long Float Double Boolean 跟Java不同的是，这些类型是类．Scala并不刻意区分基本类型和引用类型．你可以对数字执行方法：scala&gt; 1.toString()res2: String = 1 3. 条件表达式Scala的 if/else 的语法结构和Java的一样．不过，在Scala中 if/else 表达式有值，这个值就是跟在 if 或 else 之后的表达式的值:if(x &gt; 0) 1 else -1 上述表达式的值是１或者-1，具体是哪一个取决于x的值．你可以将 if/else 表达式的值赋值给变量：val s = if(x &gt; 0) 1 else -1 等同于:if(x &gt; 0) s = 1 else s = -1 相对于第二种写法，第一种写法更好一些，因为它可以用来初始化一个val，而第二种写法当中，s必须是var． 备注 Scala中每个表达式都有一个类型scala&gt; val s = if(x &gt; 0) "positive" else -1;s: Any = positive 上述表达式的类型是两个分支类型的公共超类型．在这个例子中，其中一个分支是java.lang.String，而另一个分支是Int．它们的公共超类型是Any．if(x &gt; 0) 1 那么有可能if语句没有输出值．但是在Scala中，每个表达式都应该有某种值．这个问题的解决方案是引入一个 Unit 类，写作 ()．不带 else 的这个 if 语句等同于:if(x &gt; 0) 1 else () 4. 循环Scala拥有与Java和C++相同的while和do循环：while(n &gt; 2)&#123; println("num-&gt;" + n) n = n -1&#125; 但是Scala没有与for(初始化变量;检查变量是否满足某条件;更新变量)循环直接对应的结构．如果你需要这样的循环，有两个选择：一是选择while循环，二是使用如下for语句:for(i &lt;- 1 to n)&#123; println("num-&gt;" + i)&#125; 上述表达式的目标是让变量i遍历&lt;-右边的表达式的所有值．至于如何遍历，则取决于表达式的类型． 遍历字符串或者数组时，你通常需要使用从0到n-1的区间．这个时候你可以使用util方法而不是to方法．util方法返回一个并不包含上限的区间:val s = "Hello"for(i &lt;- 0 until s.length)&#123; println(i + " = " + s(i))&#125; 或者for(ch &lt;- "Hello")&#123; println(ch)&#125; 5. 函数要定义函数，需要给出函数的名称，参数和函数体:def abs (x: Double) = if (x &gt;= 0) x else -x 必须给出所有参数的类型，只要函数不是递归的，就可以不需要指定返回类型．Scala编译器可以通过=符号右侧的表达式的类型推断出返回类型．如果函数体需要多个表达式完成，可以使用代码块．块中最后一个表达式的值就是函数的返回值:def fac(n: Int) = &#123; var r = 1 for(i &lt;- 1 to n)&#123; r = r * i &#125; r&#125; 上例中函数返回值为r的值 备注 虽然在函数中使用 return 并没有什么不对，我们还是最好适应没有 return 的日子．之后，我们会使用大量的匿名函数，这些函数中 return 并不返回值给调用者．它跳出到包含它的函数中．我们可以把 return 当做是函数版的 break 语句，仅在需要时使用． 对于递归函数，我们必须指定返回类型：def fac(n: Int) : Int = if(n &lt; 0) 1 else n * fac(n-1) 6. 默认参数和带名参数我们在调用某些函数时并不显示的给出所有参数值，对于这些函数我们可以使用默认参数：def decorate (str : String, left : String = "[" , right : String = "]") &#123; left + str + right&#125; 这个函数带有两个参数，left 和 right，带有默认值 [ 和 ]:decorate("Hello") // [Hello]decorate("Hello", "&lt;", "&gt;") // &lt;Hello&gt; 你可以在提供参数值的时候指定参数名(带名参数)：decorate(left = "&lt;&lt;", str = "Hello", right = "&gt;&gt;") // &lt;&lt;Hello&gt;&gt; 你可以混用未命名参数和带名参数，只要那些未命名的参数是排在前面即可:decorate("Hello", right = "]###") // 实际调用 decorate("Hello", "[", "]###") 备注 带名参数并不需要跟参数列表的顺序完全一致 7. 变长参数可以实现一个接受可变长度参数列表的函数:def sum(args : Int *) = &#123; var result = 0 for(arg &lt;- args)&#123; result += arg &#125; result&#125; 可以使用任意多的参数来调用该函数:val result = sum(4, 5, 1) // 10 8. 过程Scala对于不返回值的函数有特殊的表示法．如果函数体包含在花括号当中但没有前面的=符号，那么返回类型就是Unit，这样的函数被称为过程:def welcome(str : String) &#123; println("welcome " + str)&#125; 或者显示声明Unit返回类型:def welcome(str : String) : Unit = &#123; println("welcome " + str)&#125; 9. 懒值当val被声明为lazy时，它的初始化将被推迟，直到我们首次对它取值:lazy val words = scala.io.Source.fromFile("/usr/share/dict/words").mkString 如果程序从不访问words，那么文件也不会被打开． 懒值对于开销较大的初始化语句而言十分有用． 备注 懒值并不是没有额外的开销．我们每次访问懒值，都会有一个方法被调用，而这个方法将会以线程安全的方式检查该值是否已被初始化． 10. 异常Scala的异常工作机制跟Java一样．当你抛出异常时:throw new IllegalArgumentException("x should not be negative") 当前的运算被终止，运行时系统查找可以接受 IllegalArgumentException 的异常处理器．控制权将在离抛出点最近的处理器中恢复．如果没有找到符合要求的异常处理器，则程序退出． 和Java一样，抛出的对象必须是 java.lang.Throwable 的子类．不过，与Java不同的是，Scala没有”受检”异常，你不需要声明函数或者方法可能会抛出某种异常． throw 表达式有特殊的类型Nothing．这在if/else表达式中很有用．如果一个分支的类型是Nothing，那么 if/else 表达式的类型就是另一个分支的类型:if (x &gt; 0) &#123; sqrt(x)&#125;else&#123; throw new IllegalArgumentException("x should not be negative")&#125; 第一个分支的类型是Double，第二个分支的类型是Nothing，因此if/else表达式的类型是Double 捕获异常的语法采用的是模式匹配的语法:try&#123; process(new URL("Http://hortsman.com/fred-tiny.gif"))&#125;catch &#123; case _: MalformedURLException =&gt; println ("Bad URL:" + url) case ex: IOException =&gt; ex.printStackTrace()&#125; 与Java一样，更通用的异常应该排在更具体的异常之后． try/finally 语句可以释放资源，不论有没有异常发生:var in = new URL("").openStream()try&#123; process (in)&#125;finally &#123; in.close()&#125; 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 处理背压]]></title>
    <url>%2Fhow-flink-handles-backpressure.html</url>
    <content type="text"><![CDATA[人们经常会问Flink是如何处理背压(backpressure)效应的。 答案很简单：Flink不使用任何复杂的机制，因为它不需要任何处理机制。它只凭借数据流引擎，就可以从容地应对背压。在这篇博文中，我们介绍一下背压。然后，我们深入了解 Flink 运行时如何在任务之间传送缓冲区中的数据，并展示流数传输自然双倍下降的背压机制(how streaming data shipping naturally doubles down as a backpressure mechanism)。 我们最终通过一个小实验展示了这一点。 1. 什么是背压像Flink这样的流处理系统需要能够从容地处理背压。背压是指系统在一个临时负载峰值期间接收数据的速率大于其处理速率的一种场景(备注:就是处理速度慢，接收速度快，系统处理不了接收的数据)。许多日常情况都会导致背压。例如，垃圾回收卡顿可能导致流入的数据堆积起来，或者数据源可能出现发送数据过快的峰值。如果处理不当，背压会导致资源耗尽，甚至导致数据丢失。 让我们看一个简单的例子。 假设一个数据流管道包含一个数据源，一个流作业和一个接收器sink，它稳定的以每秒500万个元素的速度处理数据，如下所示(一个黑条代表100万个元素，下图是系统某一秒的快照)： 在某些时候，流处理作业或sink有1秒的卡顿，导致500多万个元素的堆积。或者，数据源可能出现了一个峰值，在一秒内以双倍的速度产生数据。 我们如何处理这样的情况(如上数据源出现一个峰值，一秒内以双倍的速度产生数据)呢？ 当然，可以放弃这些元素(一秒内只能处理一半的数据)。但是，对于许多按Exactly One处理语义处理记录的流式应用程序来说，数据丢失是不可接受的。额外的数据可以缓存在某个地方。缓存也应该是可持久化的，因为在失败的情况下，这些数据需要被重新读取以防止数据丢失。理想情况下，这些数据应该被缓存在一个持久化的通道中(例如，如果数据源自己能保证持久性，Apache Kafka 就是这样的一种数据源)。理想状态下应对背压的措施是将整个管道从 sink 回压到数据源，并对源头进行限流，以将速度调整到管道最慢部分的速度，从而达到稳定状态: 2. Flink中的背压Flink运行时的构建组件是算子和流。每个算子消费中间数据流，并对其进行转换，并产生新的数据流。描述这种机制的最好比喻是Flink充分使用有界容量的分布式阻塞队列。与 Java 连接线程的常规阻塞队列一样，一旦队列的有效缓冲耗尽(有界容量)，较慢的接收者就会使发送者放慢发送速度。 以两个任务之间的简单流程为例，说明 Flink 如何实现背压： (1) 记录 A 进入Flink并由任务1处理。 (2) 记录被序列化在缓冲区， (3) 缓冲区输送到任务2中，然后任务2从缓冲区中读取记录。 为了使记录通过Flink进行处理，缓冲区必须是可用的。在Flink中，这些分布式队列被认为是逻辑数据流，通过生产流和消费流管理的缓冲池来实现有界容量。缓冲池是缓冲区的集合，它们在使用后会被回收。总体思路很简单：从缓冲池中取出一个缓冲区，填充数据，在数据消耗完后，将缓冲区放回缓冲池中，之后还可以再次使用它。 缓冲池的大小在运行时会动态变化。网络堆栈中的内存缓冲区的数量(=队列的容量)决定了系统在不同发送/接收速度可以进行的缓冲量。Flink保证始终有足够的缓冲区来进行进程处理(enough buffers to make some progress)，但是这个进程的速度取决于用户程序和可用内存的数量。更多的内存意味着系统可以轻松地缓冲一定的瞬时背压(短时间段，短 GC)。越少的内存意味着需要对背压进行直接响应(没有足够的缓冲区进行缓存，只能响应处理)。 从上面的简单例子可以看出：在任务1输出端和任务2输入端都有一个与其关联的缓冲池。如果有一个可用于序列化 A 的缓冲区，我们将其序列化并分配缓冲区。 我们在这里有必要看两个case： (1) 本地交换：如果任务1和任务2在同一个工作节点(TaskManager)上运行，缓冲区可以直接交给下一个任务。一旦任务2消费完，它就会被回收。如果任务2比任务1慢，则缓冲区将以低于任务1填充的速度进行回收，从而导致任务1速度变慢。 (2) 远程交换：如果任务1和任务2在不同的工作节点上运行，缓冲区一旦发送到线路中(TCP通道)就可以被回收。在接收端，数据从线路复制到输入缓冲池的缓冲区。如果没有缓冲区可用，从TCP连接读取操作将被中断。输出端通过一个简单的 watermark 机制保证不会在线上放置太多的数据。如果有足够的数据处在可发送状态，我们会一直复制更多的数据到线路中直到低于某个阈值时。这保证了没有太多的数据在传输途中。如果接收端没有消费新的数据(因为没有缓冲区可用)，这会减慢发送方的速度。 这种简单的在固定大小缓冲池之间的缓冲区流使Flink能够拥有一个强大的背压机制，在这种机制下，任务生产数据速度不会比消费的快。 我们描述的两个任务之间的数据传输的机制可以自然的推广到复杂管道上，保证背压在整个管道内传播。 让我们看看一个简单的实验，展示了Flink在背压情况下的行为。我们运行一个简单的生产者-消费者流式拓扑，其中任务在本地交换数据，在这里我们可以变换任务产生记录的速度。对于这个测试，我们使用比默认更少的内存，以使得背压效果更明显。我们使用每个任务有2个大小为4096字节的缓冲区。在通常的Flink部署中，任务将具有更大更多缓冲区，这会提高性能。这个测试在单个JVM中运行，但使用完整的Flink代码堆栈。 图中显示了生产者任务(黄色)和消费者任务(绿色)随着时间变化所达到的最大吞吐量(单个JVM中每秒达到800万个元素)的平均吞吐量占比(average throughput as a percentage of the maximum attained throughput of the producing (yellow) and consuming (green) tasks)。为了衡量平均吞吐量，我们每5秒测量一次任务处理的记录数量。 首先，我们以60％的速度运行生产任务(我们通过调用Thread.sleep()来模拟减速)。消费者以相同的速度处理数据，不会产生延迟。然后我们把消费者任务放慢到全速的30％。在这里，背压效果产生作用，因为我们看到生产者也自然放缓到全速的30％。然后，我们取消消费者任务的人为减速，并且这两项任务都达到最大吞吐量。我们再次把消费者任务放慢到全速的30％，管道立即响应，生产者任务也全速下降到30％。最后，我们再次停止减速，两项任务都以100％的速度持续下去。总而言之，我们看到生产者和消费者在管道上相互跟随彼此的吞吐量，这是我们在流水线中期望的行为。 3. 结论Flink与像Kafka这样的可持久化数据源，让你可以立即响应处理背压而不会丢失数据。Flink不需要专门的机制来处理背压，因为data shipping in Flink doubles as a backpressure mechanism。 因此，Flink实现了管道最慢部分允许的最大吞吐量。 原文:https://data-artisans.com/blog/how-flink-handles-backpressure]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 四种优化Flink应用程序的方法]]></title>
    <url>%2Ffour-ways-to-optimize-your-flink-applications.html</url>
    <content type="text"><![CDATA[Flink 是一个复杂的框架，并提供了许多方法来调整其执行。在本文中，我将展示四种不同的方法来提高 Flink 应用程序的性能。如果你不熟悉 Flink，你可以阅读其他介绍性的文章，比如这个，这个和这个。如果你已经熟悉 Apache Flink，本文将帮助你更快地创建应用程序。 1. 使用 Flink tuples当你使用像 groupBy，join 或 keyBy 这样的操作时， Flink 提供了多种方式在数据集中选择key。你可以使用 key 选择器函数：// Join movies and ratings datasetsmovies.join(ratings) // Use movie id as a key in both cases .where(new KeySelector&lt;Movie, String&gt;() &#123; @Override public String getKey(Movie m) throws Exception &#123; return m.getId(); &#125; &#125;) .equalTo(new KeySelector&lt;Rating, String&gt;() &#123; @Override public String getKey(Rating r) throws Exception &#123; return r.getMovieId(); &#125; &#125;) 或者你可以在 POJO 类型中指定一个字段名称：movies.join(ratings) // Use same fields as in the previous example .where("id") .equalTo("movieId") 但是，如果你正在使用 Flink tuple 类型，你可以简单地指定将要作为 key 的字段在元组中的位置：DataSet&lt;Tuple2&lt;String, String&gt;&gt; movies ...DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; ratings ...movies.join(ratings) // Specify fields positions in tuples .where(0) .equalTo(1) 最后一种方式会给你最好的性能，但可读性呢？ 这是否意味着你的代码现在看起来像这样：DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; result = movies.join(ratings) .where(0) .equalTo(0) .with(new JoinFunction&lt;Tuple2&lt;Integer,String&gt;, Tuple2&lt;Integer,Double&gt;, Tuple3&lt;Integer, String, Double&gt;&gt;() &#123; // What is happening here? @Override public Tuple3&lt;Integer, String, Double&gt; join(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second) throws Exception &#123; // Some tuples are joined with some other tuples and some fields are returned??? return new Tuple3&lt;&gt;(first.f0, first.f1, second.f1); &#125; &#125;); 在这种情况下，提高可读性的常见方法是创建一个继承自 TupleX 类的类，并为这些字段实现 getter 和 setter 方法。在这里，下面是 Flink Gelly 库的 Edge 类的大体实现，具有三个字段并继承了 Tuple3 类：public class Edge&lt;K, V&gt; extends Tuple3&lt;K, K, V&gt; &#123; public Edge(K source, K target, V value) &#123; this.f0 = source; this.f1 = target; this.f2 = value; &#125; // Getters and setters for readability public void setSource(K source) &#123; this.f0 = source; &#125; public K getSource() &#123; return this.f0; &#125; // Also has getters and setters for other fields ...&#125; 2. 重用 Flink对象另一个可以用来提高 Flink 应用程序性能的方法是当你从自定义函数中返回数据时使用可变对象。看看这个例子：stream .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123; @Override public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123; long changesCount // A new Tuple instance is created on every execution collector.collect(new Tuple2&lt;&gt;(userName, changesCount)); &#125; &#125; 正如你所看到的，在 apply 函数的每次执行中，我们都创建一个 Tuple2 类型的实例，这会给垃圾收集器造成很大压力。解决这个问题的一种方法是重复使用同一个实例：stream .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123; // Create an instance that we will reuse on every call private Tuple2&lt;String, Long&gt; result = new Tuple&lt;&gt;(); @Override public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123; long changesCount = ... // Set fields on an existing object instead of creating a new one result.f0 = userName; // Auto-boxing!! A new Long value may be created result.f1 = changesCount; // Reuse the same Tuple2 object collector.collect(result); &#125; &#125; 上述代码会更好些。虽然我们在每次调用的时候只创建了一个 Tuple2 实例，但是我们还是间接地创建了 Long 类型的实例。为了解决这个问题， Flink 提供了很多的值类（value classes），IntValue, LongValue, StringValue, FloatValue 等。这些类的目的是为内置类型提供可变版本，所以我们可以在用户自定义函数中重用这些类型，下面就是如何使用的例子：stream .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123; // Create a mutable count instance private LongValue count = new IntValue(); // Assign mutable count to the tuple private Tuple2&lt;String, LongValue&gt; result = new Tuple&lt;&gt;(&quot;&quot;, count); @Override // Notice that now we have a different return type public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, LongValue&gt;&gt; collector) throws Exception &#123; long changesCount = ... // Set fields on an existing object instead of creating a new one result.f0 = userName; // Update mutable count value count.setValue(changesCount); // Reuse the same tuple and the same LongValue instance collector.collect(result); &#125; &#125; 上面这些使用习惯在 Flink 类库中被普遍使用，比如 Flink Gelly。 3. 使用函数注解优化 Flink 应用程序的另一种方法是提供关于用户自定义函数对输入数据做什么的一些信息。由于 Flink 无法解析和理解代码，因此你可以提供关键信息，这将有助于构建更高效的执行计划。有三个注解我们可以使用： @ForwardedFields - 指定输入值中的哪些字段保持不变并在输出值中使用。 @NotForwardedFields - 指定在输出中同一位置不保留的字段。 @ReadFields - 指定用于计算结果值的字段。你只能指定那些在计算中使用的字段，而不是仅仅将数据拷贝到输出中的字段。 我们来看看如何使用 ForwardedFields 注解：// Specify that the first element is copied without any changes@ForwardedFields("0")class MyFunction implements MapFunction&lt;Tuple2&lt;Long, Double&gt;, Tuple2&lt;Long, Double&gt;&gt; &#123; @Override public Tuple2&lt;Long, Double&gt; map(Tuple2&lt;Long, Double&gt; value) &#123; // Copy first field without change return new Tuple2&lt;&gt;(value.f0, value.f1 + 123); &#125;&#125; 上述代码意味着输入元组的第一个元素将不会改变，并且在返回时也处于同一个位置（译者注：第一个位置）。 如果你不改变字段，只是简单地将它移到不同的位置上，你同样可以使用 ForwardedFields 注解来实现。下面例子中，我们简单地将输入元组的字段进行交换（译者注：第一个字段移到第二个位置，第二个字段移到第一个位置）：// 1st element goes into the 2nd position, and 2nd element goes into the 1st position@ForwardedFields("0-&gt;1; 1-&gt;0")class SwapArguments implements MapFunction&lt;Tuple2&lt;Long, Double&gt;, Tuple2&lt;Double, Long&gt;&gt; &#123; @Override public Tuple2&lt;Double, Long&gt; map(Tuple2&lt;Long, Double&gt; value) &#123; // Swap elements in a tuple return new Tuple2&lt;&gt;(value.f1, value.f0); &#125;&#125; 上面例子中提到的注解只能应用到只有一个输入参数的函数中，比如 map 或者 flatMap。如果你有两个输入参数的函数，你可以使用 ForwardedFieldsFirst 和 ForwardedFieldsSecond 注解分别为第一和第二个参数提供信息。 下面我们看一下如何在 JoinFunction 接口的实现中使用这些注解（译者注：第一个输入元组的两个字段拷贝到输出元组的第一个和第二个位置，第二个输入元组的第二个字段拷贝到输出元组的第三个位置）：// Two fields from the input tuple are copied to the first and second positions of the output tuple@ForwardedFieldsFirst("0; 1")// The third field from the input tuple is copied to the third position of the output tuple@ForwardedFieldsSecond("2")class MyJoin implements JoinFunction&lt;Tuple2&lt;Integer,String&gt;, Tuple2&lt;Integer,Double&gt;, Tuple3&lt;Integer, String, Double&gt;&gt;() &#123; @Override public Tuple3&lt;Integer, String, Double&gt; join(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second) throws Exception &#123; return new Tuple3&lt;&gt;(first.f0, first.f1, second.f1); &#125;&#125;) Flink 同样提供了 NotForwardedFieldsFirst, NotForwardedFieldsSecond, ReadFieldsFirst, 和 ReadFirldsSecond 注解来实现同样的功能。 4. 选择 join 类型如果你告诉 Flink 一些信息，可以加快 join 的速度，但在讨论它为什么会起作用之前，让我们先来谈谈 Flink 是如何执行 join的。 当 Flink 处理批量数据时，集群中的每台机器都存储了部分数据。要执行 join 操作，Flink 需要找到两个两个数据集中满足 join 条件的所有记录对（译者注：key 相同的数据）。 要做到这一点，Flink 首先必须将两个数据集中具有相同 key 的数据放在集群中的同一台机器上。有两种策略： Repartition-Repartition 策略：在这种场景下，根据它们的 key 对两个数据集进行重新分区，通过网络发送数据。这就意味着如果数据集非常大，这将花费大量的时间将数据在网络之间进行复制。 Broadcast-Forward 策略：在这种场景下，一个数据集保持不变，将第二个数据集拷贝到集群上第二个数据集拥有第一个数据集部分数据的所有机器上（译者注：将达尔戈数据集进行分发到对应机器上）。 如果用一个较大的数据集与一个小数据集进行 join，你可以使用 Broadcast-Forward 策略并避免对第一个数据集进行重分区的昂贵代价。这很容易做到：ds1.join(ds2, JoinHint.BROADCAST_HASH_FIRST) 这表示第一个数据集比第二个数据集小得多。 Flink 支持的其他 join 提示有以下几种： BROADCAST_HASH_SECOND – 表示第二个数据集比较小 REPARTITION_HASH_FIRST – 表示第一个数据集比较小 REPARTITION_HASH_SECOND – 表示第二个数据集有点小 REPARTITION_SORT_MERGE – 表示对两个数据集重新分区并使用排序和合并策略 OPTIMIZER_CHOOSES – Flink 优化器将决定如何连接数据集 原文： https://brewing.codes/2017/10/17/flink-optimize/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 垃圾收集器]]></title>
    <url>%2Fjvm-common-garbage-collector.html</url>
    <content type="text"><![CDATA[本文“垃圾收集器”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。Java 虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。下面讨论的是基于 JDK 1.7 Update 14 之后的 HotSpot 虚拟机。这个虚拟机包含的所有收集器如下图所示： 图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。下面会介绍这些收集器的特性，基本原理和使用场景。 1. Serial 收集器Serial 收集器是最基本，发展历史最悠久的收集器，曾经（JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，只会使用一个CPU 或一条收集线程去完成垃圾收集工作。重要的是，它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。在用户不可见的情况下把用户正常工作的线程全部停掉，这对应用程序来说都是很难接受的。但是虚拟机的设计者表示完全理解，却又表示很无奈。举个例子来说：你妈妈给你打扫房间的时候，肯定也会让你老老实实的在椅子或房间外待着，如果她一边打扫卫生，你一边乱扔垃圾，这房间还嫩打扫完吗？ Serial 收集器看起来是一个鸡肋收集器，但是到现在为止，它依然是虚拟机运行在 Client 模式下的默认新生代收集器。这是因为它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说， 该收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集的效率。 Serial 收集器在新生代收集时采用复制算法。 2. ParNew 收集器ParNew 收集器其实是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 Serial收集器可用的所有控制参数，收集算法，Stop The World，对象分配规则，回收策略等都与 Serial 收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。 ParNew 收集器除了多线程收集之外，其他与 Serial收集器相比并没有太多的创新之处，但它却是许多运行在 Server 模型下的虚拟机首选的新生代收集器。其中一个与性能无关但是很重要的原因是，除了 Serial收集器之外，目前只有它能与 CMS 收集器配合工作。在 JDK 1.5 HotSpot推出了一款强交互应用中划时代的垃圾收集器 - CMS 收集器 （Concurrent Mark Sweep）。这款收集器是 HotSpot 虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。不幸的是 CMS 收集器作为老年代的收集器，却无法与 JDK 1.4.0中已经存在的 Parallel Scavenge收集器配合工作，所以在 JDK 1.5 中使用 CMS 来收集老年代的时候，新生代智能选择 ParNew 或者 Serial收集器。 ParNew 收集器在单 CPU 的环境中绝对不会有比 Serial 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个 CPU 的环境中都不能百分百的保证可以超越 Serial 收集器。 随着可以使用的 CPU 的数量的增加，它对于 GC 时系统资源的有效利用还是很有好处的。 与 Serial 收集器一样，也是在新生代收集时采用复制算法。 3. Parallel Scavenge 收集器Parallel Scavenge 收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去跟 ParNew 收集器一样。 Parallel Scavenge 收集器的特点是它的关注点与其他的收集器不同， CMS 等收集器的关注点是尽可能的缩短垃圾收集时的用户线程的停顿时间， 而 Parallel Scavenge 收集器的目标则是达到一个可控制的吞吐量。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。由于与吞吐量关系密切，该收集器也经常成为 吞吐量优先 收集器。Parallel Scavenge 收集器可以通过设置 -XX:+UseAdaptivSizePolicy 参数开启 GC 自适应调节策略，不需要手工指定新生代的大小(-Xmn)，Eden与Survivor区的比例(-XX:SurvivorRatio)，晋升老年代对象年龄等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。自适应调节策略也是Parallel Scavenge 收集器与 ParNew收集器的一个重要区别。 备注:吞吐量 = 运行用户代码的时间 / （运行用户代码的时间 + 垃圾收集的时间）Example:虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，则吞吐量为99% 4. Serial Old 收集器Serial Old 收集器是 Serial 收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是在于 Client 模式下的虚拟机使用。如果在 Server 模式下，那么它主要有两大用途： 一种用途是在 JDK1.5 以及之前的版本中与 Parallel Scavenge 收集器搭配使用， 另一种用途是作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器Parallel Old 收集器是 Parallel Scavenge 收集器的老年代版本，使用多线程和 标记-整理算法。这个收集器是在 JDK 1.6 中才开始提供的。在此之前新生代 Parallel Scavenge 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 Parallel Scavenge 收集器，老年代除了 Serial Old收集器外别无选择。由于老年代 Serial Old 收集器在服务端应用性能上 拖累，使用了 Parallel Scavenge 收集器也未必能在整体应用上获得吞吐量最大化的效果。由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew 加 CMS 的组合给力。 直到 Parallel Old 收集器出现后。吞吐量优先 收集器终于有了名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器CMS (Concurrent Mark Sweep) 收集器是一种以获得最短回收停顿时间为目标的收集器。CMS 收集器非常符合重视响应时间速度以及希望系统停顿时间最短的应用。从名字上就可以看出，CMS 收集器是基于 标记-清除 算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些。整个过程可以分为4个步骤： 初始标记 并发标记 重新标记 并发清除 初始标记与重新标记这两个步骤仍然需要 Stop The World。 初始标记仅仅是标记一下 GC Roots 能直接关联到的对象，速度很快，并发标记是进行 GC Roots Tracing 的过程，而重新标记则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发比较的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS 收集器的内存回收过程是与用户线程一起并发执行的。 CMS 是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。但是 CMS 收集器还远达不到完美的程度，它有以下三个明显的缺点： (1) CMS 收集器对CPU资源非常敏感 其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS 默认启动的回收线程数是 （CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程多于 25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS 对用户程序的影响就可能变得很大。 (2) CMS 收集器无法处理浮动垃圾 CMS 收集器无法处理浮动垃圾，可能出现 Concurrent Mode Failure 失败而导致另一次 Full GC 的产生。由于 CMS 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS 无法在当次收集中处理掉它们，只好留待下一次 GC 时再清理掉。这一部分垃圾就称为 浮动垃圾。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此 CMS 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 JDK 1.5 默认设置下，CMS 收集器当老年代使用了 68% 的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高触发百发比。在 JDK1.6 中， CMS收集器的启动阈值已经提升至 92%。运行期间预留的内存无法满足程序需要，就会出现一次 Concurrent Mode Failure 失败，这时虚拟机将启动后备预案：临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说，如果启动阈值设置的太高很容易导致大量这样的失败，性能反而会降低。 (3) CMS 收集器会产生大量空间碎片 CMS 是一款基于 标记—清除 算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器G1（Garbage-First）是一款面向服务端应用的垃圾收集器。HotSpot 开发团队赋予它的使命是未来可以替换掉 JDK 1.5 中发布的 CMS 收集器。与其他 GC 收集器相比，G1 具备如下特点： (1) 并行与并发。G1 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短 Stop-The-World 停顿的时间，部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 Java 程序继续执行。 (2) 分代收集。与其他收集器一样，分代概念在 G1 中依然得以保留。虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 GC 的旧对象以获取更好的收集效果。 (3) 空间整合。与 CMS 的 标记—清理 算法不同，G1 从整体来看是基于 标记—整理 算法实现的收集器，从局部（两个 Region 之间）上来看是基于 复制 算法实现的，但无论如何，这两种算法都意味着 G1 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。 (4) 可预测的停顿。这是 G1 相对于 CMS 的另一大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 在 G1 之前的其他收集器进行收集的范围都是整个新生代或者老年代，而 G1 不再是这样。使用 G1 收集器时，Java 堆的内存布局就与其他收集器有很大差别，它将整个 Java 堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 Region（不需要连续）的集合。 G1 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 Java 堆中进行全区域的垃圾收集。G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region（这也就是 Garbage-First 名称的来由）。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率。 如果不计算维护 Remembered Set的操作， G1 收集器的运作大致可划分为以下几个步骤： (1) 初始标记。初始标记阶段仅仅只是标记一下 GC Roots 能直接关联到的对象，并且修改 TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 Region 中创建新对象，这阶段需要停顿线程，但耗时很短。 (2) 并发标记。并发标记阶段是从 GC Root 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 (3) 最终标记。最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中，这阶段需要停顿线程，但是可并行执行。 (4) 筛选回收。筛选回收阶段首先对各个 Region 的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 堆内内存与堆外内存]]></title>
    <url>%2Fjava-on-off-heap-memory.html</url>
    <content type="text"><![CDATA[一般情况下，Java 中分配的非空对象都是由 Java 虚拟机的垃圾收集器管理的，也称为堆内内存（on-heap memory）。虚拟机会定期对垃圾内存进行回收，在某些特定的时间点，它会进行一次彻底的回收（full gc）。彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对 Java 应用造成的影响，跟堆的大小是成正比的。过大的堆会影响 Java 应用的性能。 对于这个问题，一种解决方案就是使用堆外内存（off-heap memory）。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。 但是 Java 本身也在不断对堆内内存的实现方式做改进。两者各有什么优缺点？ Vanilla Java 博客作者 Peter Lawrey 撰写了一篇文章，在文中他对三种方式：用new来分配对象、对象池（object pool）和堆外内存，进行了详细的分析。 用new来分配对象内存是最基本的一种方式，Lawery提到： 在Java 5.0之前，分配对象的代价很大，以至于大家都使用内存池。但是从5.0开始，对象分配和垃圾回收变得快多了，研发人员发现了性能的提升，纷纷简化他们的代码，不再使用内存池，而直接用new来分配对象。从5.0开始，只有一些分配代价较大的对象，比如线程、套接字和数据库链接，用内存池才会有明显的性能提升。 对于内存池，Lawery认为它主要用于两类对象。第一类是生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能。第二种情况是加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。对此，Lawery还以 StringInterner 为例进行了说明。 最后Lawery分析了堆外内存，它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。堆外内存有以下特点： 对于大内存有良好的伸缩性 对垃圾回收停顿的改善可以明显感觉到 在进程间可以共享，减少虚拟机间的复制 Lawery还提到堆外内存最重要的还不是它能改进性能，而是它的确定性。 当然堆外内存也有它自己的问题，最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行序列化（serialization），而序列化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。 Lawery还介绍了OpenHFT公司提供三个开源库：Chronicle Queue、Chronicle Map和 Thread Affinity，这些库可以帮助开发人员使用堆外内存来保存数据。采用堆外内存有很多好处，同时也带来挑战，对堆外内存感兴趣的读者可以阅读Lawery的原文来了解更多信息。 转载于： http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 垃圾收集算法]]></title>
    <url>%2Fjvm-garbage-collection-algorithm.html</url>
    <content type="text"><![CDATA[本文“垃圾收集算法”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】 由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本节不打算过多地讨论算法的实现，只是介绍几种算法的思想及其发展过程。 1. 标记-清除算法最基础的收集算法是 标记-清除 （Mark-Sweep）算法，如它的名字一样，算法分为 标记 和 清除 两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个： 一个是效率问题，标记和清除过程的效率都不高； 另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-清除算法的执行过程如下图所示： 2. 复制算法为了解决效率问题，一种称为 复制 （Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。复制算法的执行过程如下图所示： 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 的专门研究表明，新生代中的对象 98% 是朝生夕死的，所以并不需要按照 1∶1 的比例来划分内存空间，而是将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中的一块 Survivor。当回收时，将 Eden 和 Survivor 中还存活着的对象一次性地拷贝到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才用过的 Survivor 的空间。HotSpot 虚拟机默认 Eden 和 Survivor 的大小比例是 8∶1，也就是每次新生代中可用内存空间为整个新生代容量的 90%（80%+10%），只有 10% 的内存是会被 浪费 的。当然，98% 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在 98% 的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块 Survivor 空间没有足够的空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。关于对新生代进行分配担保的内容，本章稍后在讲解垃圾收集器执行规则时还会再详细讲解。 3. 标记-整理算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 50% 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 100% 存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种 标记-整理 （Mark-Compact）算法，标记过程仍然与 标记-清除 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，标记-整理 算法的示意图如下图所示: 4. 分代收集算法当前商业虚拟机的垃圾收集都采用 分代收集 （Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 标记-清理 或 标记-整理 算法来进行回收。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache 缓存更新策略]]></title>
    <url>%2Fcache-update-policy.html</url>
    <content type="text"><![CDATA[看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是 错误 的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新策略。 这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 更新缓存的的策略有四种： Cache aside Read through Write through Write behind caching 我们下面一一来看一下这四种策略。 1. Cache aside这是最常用最常用的策略了。其具体逻辑如下： 失效：应用程序先从 cache 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从 cache 中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 这是标准的策略，包括Facebook的论文《Scaling Memcache at Facebook》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》，主要是怕两个并发的写操作导致脏数据。 那么，是不是 Cache Aside 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。 但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 2. Read/Write Through我们可以看到，在上面的 Cache Aside 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而 Read/Write Through 套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 Cache。 2.1 Read ThroughRead Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside 是由调用方负责把数据加载入缓存，而 Read Through 则用缓存服务自己来加载，从而对应用方是透明的。 2.2 Write ThroughWrite Through 套路和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 Cache 自己更新数据库（这是一个同步操作） 下图自来 Wikipedia 的 Cache) 词条。其中的 Memory 你可以理解为就是我们例子里的数据库。 3. Write Behind CachingWrite Behind 又叫 Write Back。一些了解 Linux 操作系统内核的同学对 write back 应该非常熟悉，这不就是 Linux 文件系统的 Page Cache 的算法吗？ 是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。 Write Back 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write back 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 Unix/Linux 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 Trade-Off。 另外，Write Back 实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的 write back 会在仅当这个 cache 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 lazy write。 在 wikipedia 上有一张 write back 的流程图，基本逻辑如下： 转载于： https://coolshell.cn/articles/17416.html]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>CaChe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 动态表的持续查询]]></title>
    <url>%2Fflink-sql-persistent-query-of-dynamic-table.html</url>
    <content type="text"><![CDATA[越来越多的公司采用流处理，并将现有的批处理应用迁移到流处理，或者对新的用例采用流处理实现的解决方案。其中许多应用集中在流数据分析上，分析的数据流来自各种源，例如数据库事务、点击、传感器测量或IoT 设备。 Apache Flink 非常适用于流分析应用程序，因为它支持事件时间语义，确保只处理一次，以及同时实现了高吞吐量和低延迟。因为这些特性，Flink 能够近实时对大量的输入数据计算出一个确定和精确的结果，并且在发生故障的时候提供一次性语义。 Flink 的核心流处理API，DataStream API，非常具有表现力，并且为许多常见操作提供了原语。在其他特性中，它提供了高度可定制的窗口逻辑，不同表现特征下的不同状态原语，注册和响应定时器的钩子，以及高效的异步请求外部系统的工具。另一方面，许多流分析应用遵循相似的模式，并不需要DataStream API 提供的表现力级别。他们可以使用领域特定的语言来使用更自然和简洁的方式表达。总所周知，SQL 是数据分析的事实标准。对于流分析，SQL 可以让更多的人在数据流的特定应用中花费更少的时间。然而，目前还没有开源的流处理器提供令人满意的SQL 支持。 1. 为什么流中的 SQL 很重要SQL 是数据分析使用最广泛的语言，有很多原因： SQL 是声明式的：你指定你想要的东西，而不是如何去计算； SQL 可以进行有效的优化：优化器计估算有效的计划来计算结果； SQL 可以进行有效的评估：处理引擎准确的知道计算内容，以及如何有效的执行； 最后，所有人都知道的，许多工具都理解SQL。 因此，使用SQL 处理和分析数据流，可以为更多人提供流处理技术。此外，因为SQL 的声明性质和潜在的自动优化，它可以大大减少定义高效流分析应用的时间和精力。 但是，SQL（以及关系数据模型和代数）并不是为流数据设计的。关系是（多）集合而不是无限序列的元组。当执行SQL 查询时，传统数据库系统和查询引擎读取和处理完整的可用数据集，并产生固定大小的结果。相比之下，数据流持续提供新的记录，使数据随着时间到达。因此，流查询需要不断的处理到达的数据，从来都不是“完整的”。 话虽如此，使用SQL 处理流并不是不可能的。一些关系型数据库系统维护了物化视图，类似于在流数据中评估SQL 查询。物化视图被定义为一个SQL 查询，就像常规（虚拟）视图一样。但是，查询的结果实际上被保存（或者是物化）在内存或硬盘中，这样视图在查询时不需要实时计算。为了防止物化视图的数据过时，数据库系统需要在其基础关系（定义的SQL 查询引用的表）被修改时更新更新视图。如果我们将视图的基础关系修改视作修改流（或者是更改日志流），物化视图的维护和流中的SQL 的关系就变得很明确了。 2. Flink 的关系API：Table API 和SQL从1.1.0版本（2016年8月发布）以来，Flink 提供了两个语义相当的关系API，语言内嵌的Table API（用于Java 和Scala）以及标准SQL。这两种API 被设计用于在线流和遗留的批处理数据API 的统一，这意味着无论输入是静态批处理数据还是流数据，查询产生完全相同的结果。 统一流和批处理的API 非常重要。首先，用户只需要学习一个API 来处理静态和流数据。此外，可以使用同样的查询来分析批处理和流数据，这样可以在同一个查询里面同时分析历史和在线数据。在目前的状况下，我们尚未完全实现批处理和流式语义的统一，但社区在这个目标上取得了很大的进展。 下面的代码片段展示了两个等效的Table API 和SQL 查询，用来在温度传感器测量数据流中计算一个简单的窗口聚合。SQL 查询的语法基于Apache Calcite 的分组窗口函数样式，并将在Flink 1.3.0版本中得到支持。 val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val tEnv = TableEnvironment.getTableEnvironment(env)// define a table source to read sensor data (sensorId, time, room, temp)val sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...// register the table sourcetEnv.registerTableSource(&quot;sensors&quot;, sensorTable)// Table APIval tapiResult: Table = tEnv.scan(&quot;sensors&quot;) // scan sensors table .window(Tumble over 1.hour on &apos;rowtime as &apos;w) // define 1-hour window .groupBy(&apos;w, &apos;room) // group by window and room .select(&apos;room, &apos;w.end, &apos;temp.avg as &apos;avgTemp) // compute average temperature// SQLval sqlResult: Table = tEnv.sql(&quot;&quot;&quot; |SELECT room, TUMBLE_END(rowtime, INTERVAL &apos;1&apos; HOUR), AVG(temp) AS avgTemp |FROM sensors |GROUP BY TUMBLE(rowtime, INTERVAL &apos;1&apos; HOUR), room |&quot;&quot;&quot;.stripMargin) 就像你看到的，两种API 以及Flink 主要的的DataStream 和DataSet API 是紧密结合的。Table 可以和DataSet 或DataStream 相互转换。因此，可以很简单的去扫描一个外部的表，例如数据库或者是Parquet 文件，使用Table API 查询做一些预处理，将结果转换为DataSet，并对其运行Gelly 图形算法。上述示例中定义的查询也可以通过更改执行环境来处理批量数据。 在内部，两种API 都被转换成相同的逻辑表示，由Apache Calcite 进行优化，并被编译成DataStream 或是DataSet 程序。实际上，优化和转换程序并不知道查询是通过Table API 还是SQL 来定义的。如果你对优化过程的细节感兴趣，可以看看我们去年发布的一篇博客文章。由于Table API 和SQL 在语义方面等同，只是在样式上有些区别，在这篇文章中当我们谈论SQL 时我们通常引用这两种API。 在当前的1.2.0版本中，Flink 的关系API 在数据流中，支持有限的关系操作，包括投影、过滤和窗口聚合。所有支持的操作有一个共同点，就是它们永远不会更新已经产生的结果记录。这对于时间记录操作，例如投影和过滤显然不是问题。但是，它会影响收集和处理多条记录的操作，例如窗口聚合。由于产生的结果不能被更新，在Flink 1.2.0中，输入的记录在产生结果之后不得不被丢弃。 当前版本的限制对于将产生的数据发往Kafka 主题、消息队列或者是文件这些存储系统的应用是可以被接受的，因为它们只支持追加操作，没有更新和删除。遵循这种模式的常见用例是持续的ETL 和流存档应用，将流进行持久化存档，或者是准备数据用于进一步的在线（流）或者是离线分析。由于不可能更新之前产生的结果，这一类应用必须确保产生的结果是正确的，并且将来不需要更正。下图说明了这样的应用。 虽然只支持追加查询对有些类型的应用和存储系统有用，但是还是有一些流分析的用例需要更新结果。这些流应用包括不能丢弃延迟到达的记录，需要早期的结果用于（长期运行）窗口聚合，或者是需要非窗口的聚合。在每种情况下，之前产生的结果记录都需要被更新。结果更新查询通常将其结果保存在外部数据库或者是键值存储，使其可以让外部应用访问或者是查询。实现这种模式的应用有仪表板、报告应用或者是其他的应用，它们需要及时的访问持续更新的结果。下图说明了这一类应用 3. 动态表的持续查询支持查询更新之前产生的结果是Flink 的关系API 的下一个重要步骤。这个功能非常重要，因为它大大增加了API 支持的用例的范围和种类。此外，一些新的用例可以采用DataStream API 来实现。 因此，当添加对结果更新查询的支持时，我们必须保留之前的流和批处理输入的语义。我们通过动态表的概念来实现。动态表是持续更新，并且能够像常规的静态表一样查询的表。但是，与批处理表查询终止后返回一个静态表作为结果不同的是，动态表中的查询会持续运行，并根据输入表的修改产生一个持续更新的表。因此，结果表也是动态的。这个概念非常类似我们之前讨论的物化视图的维护。 假设我们可以在动态表中运行查询并产生一个新的动态表，那会带来一个问题，流和动态表如何相互关联？答案是流和动态表可以相互转换。下图展示了在流中处理关系查询的概念模型。 首先，流被转换为动态表，动态表使用一个持续查询进行查询，产生一个新的动态表。最后，结果表被转换成流。要注意，这个只是逻辑模型，并不意味着查询是如何实际执行的。实际上，持续查询在内部被转换成传统的DataStream 程序。 随后，我们描述了这个模型的不同步骤： 在流中定义动态表 查询动态表 生成动态表 3.1 在流中定义动态表评估动态表上的SQL 查询的第一步是在流中定义一个动态表。这意味着我们必须指定流中的记录如何修改动态表。流携带的记录必须具有映射到表的关系模式的模式。在流中定义动态表有两种模式：附加模式和更新模式。 在附加模式中，流中的每条记录是对动态表的插入修改。因此，流中的所有记录都附加到动态表中，使得它的大小不断增长并且无限大。下图说明了附加模式。 在更新模式中，流中的记录可以作为动态表的插入、更新或者删除修改（附加模式实际上是一种特殊的更新模式）。当在流中通过更新模式定义一个动态表时，我们可以在表中指定一个唯一的键属性。在这种情况下，更新和删除操作会带着键属性一起执行。更新模式如下图所示。 3.2 查询动态表一旦我们定义了动态表，我们可以在上面运行查询。由于动态表随着时间进行改变，我们必须定义查询动态表的意义。假定我们有一个特定时间的动态表的快照，这个快照可以作为一个标准的静态批处理表。我们将动态表A 在点t 的快照表示为A[t]，可以使用人意的SQL 查询来查询快照，该查询产生了一个标准的静态表作为结果，我们把在时间t 对动态表A 做的查询q 的结果表示为q(A[t])。如果我们反复在动态表的快照上计算查询结果，以获取进度时间点，我们将获得许多静态结果表，它们随着时间的推移而改变，并且有效的构成一个动态表。我们在动态表的查询中定义如下语义。 查询q 在动态表A 上产生了一个动态表R，它在每个时间点t 等价于在A[t]上执行q 的结果，即R[t]=q(A[t])。该定义意味着在批处理表和流表上执行相同的查询q 会产生相同的结果。在下面的例子中，我们给出了两个例子来说明动态表查询的语义。 在下图中，我们看到左侧的动态输入表A，定义成追加模式。在时间t=8时，A 由6行（标记成蓝色）组成。在时间t=9 和t=12 时，有一行追加到A（分别用绿色和橙色标记）。我们在表A 上运行一个如图中间所示的简单查询，这个查询根据属性k 分组，并统计每组的记录数。在右侧我们看到了t=8（蓝色），t=9（绿色）和t=12（橙色）时查询q 的结果。在每个时间点t，结果表等价于在时间t 时再动态表A 上执行批查询。 这个例子中的查询是一个简单的分组（但是没有窗口）聚合查询。因此，结果表的大小依赖于输入表的分组键的数量。此外，值得注意的是，这个查询会持续更新之前产生的结果行，而不只是添加新行。 第二个例子展示了一个类似的查询，但是有一个很重要的差异。除了对属性k 分组以外，查询还将记录每5秒钟分组为一个滚动窗口，这意味着它每5秒钟计算一次k 的总数。再一次的，我们使用Calcite 的分组窗口函数来指定这个查询。在图的左侧，我们看到输入表A ，以及它在附加模式下随着时间而改变。在右侧，我们看到结果表，以及它随着时间演变。 与第一个例子的结果不同的是，这个结果表随着时间增长，例如每5秒钟计算出新的结果行（考虑到输入表在过去5秒收到更多的记录）。虽然非窗口查询（主要是）更新结果表的行，但是窗口聚合查询只追加新行到结果表中。 虽然这篇博客专注于动态表的SQL 查询的语义，而不是如何有效的处理这样的查询，但是我们要指出的是，无论输入表什么时候更新，都不可能计算查询的完整结果。相反，查询编译成流应用，根据输入的变化持续更新它的结果。这意味着不是所有的有效SQL 都支持，只有那些持续性的、递增的和高效计算的被支持。我们计划在后续的博客文章中讨论关于评估动态表的SQL 查询的详细内容。 3.3 生成动态表查询动态表生成的动态表，其相当于查询结果。根据查询和它的输入表，结果表会通过插入、更新和删除持续更改，就像普通的数据表一样。它可能是一个不断被更新的单行表，一个只插入不更新的表，或者介于两者之间。 传统的数据库系统在故障和复制的时候，通过日志重建表。有一些不同的日志技术，比如UNDO、REDO和UNDO/REDO日志。简而言之，UNDO 日志记录被修改元素之前的值来回滚不完整的事务，REDO 日志记录元素修改的新值来重做已完成事务丢失的改变，UNDO/REDO 日志同时记录了被修改元素的旧值和新值来撤销未完成的事务，并重做已完成事务丢失的改变。基于这些日志技术的原理，动态表可以转换成两类更改日志流：REDO 流和REDO+UNDO 流。 通过将表中的修改转换为流消息，动态表被转换为redo+undo 流。插入修改生成一条新行的插入消息，删除修改生成一条旧行的删除消息，更新修改生成一条旧行的删除消息以及一条新行的插入消息。行为如下图所示。 左侧显示了一个维护在附加模式下的动态表，作为中间查询的输入。查询的结果转换为显示在底部的redo+undo 流。输入表的第一条记录(1,A)作为结果表的一条新纪录，因此插入了一条消息+(A,1)到流中。第二条输入记录k=‘A’(4,A)导致了结果表中 (A,1)记录的更新，从而产生了一条删除消息-(A,1)和一条插入消息+(A,2)。所有的下游操作或数据汇总都需要能够正确处理这两种类型的消息。 在两种情况下，动态表会转换成redo 流：要么它只是一个附加表（即只有插入修改），要么它有一个唯一的键属性。动态表上的每一个插入修改会产生一条新行的插入消息到redo 流。由于redo 流的限制，只有带有唯一键的表能够进行更新和删除修改。如果一个键从动态表中删除，要么是因为行被删除，要么是因为行的键属性值被修改了，所以一条带有被移除键的删除消息发送到redo 流。更新修改生成带有更新的更新消息，比如新行。由于删除和更新修改根据唯一键来定义，下游操作需要能够根据键来访问之前的值。下图展示了如何将上述相同查询的结果表转换为redo 流。 插入到动态表的(1,A)产生了+(A,1)插入消息。产生更新的(4,A)生成了*(A,2)的更新消息。 Redo 流的通常做法是将查询结果写到仅附加的存储系统，比如滚动文件或者Kafka 主题，或者是基于键访问的数据存储，比如Cassandra、关系型DBMS以及压缩的Kafka 主题。还可以实现将动态表作为流应用的关键的内嵌部分，来评价持续查询和对外部系统的查询能力，例如一个仪表盘应用。 3.4 切换到动态表发生的改变在1.2版本中，Flink 关系API 的所有流操作，例如过滤和分组窗口聚合，只会产生新行，并且不能更新先前发布的结果。 相比之下，动态表能够处理更新和删除修改。 现在你可能会问自己，当前版本的处理模式如何与新的动态表模型相关？ API 的语义会完全改变，我们需要从头开始重新实现API，以达到所需的语义？ 所有这些问题的答案很简单。当前的处理模型是动态表模型的一个子集。 使用我们在这篇文章中介绍的术语，当前的模型通过附加模式将流转换为动态表，即一个无限增长的表。 由于所有操作仅接受插入更改并在其结果表上生成插入更改（即，产生新行），因此所有在动态附加表上已经支持的查询，将使用重做模型转换回DataStreams，仅用于附加表。 因此，当前模型的语义被新的动态表模型完全覆盖和保留。 4. 结论与展望Flink 的关系API 在任何时候都非常适合用于流分析应用，并在不同的生产环境中使用。在这篇博文中，我们讨论了Table API 和SQL 的未来。 这一努力将使Flink 和流处理更易于访问。 此外，用于查询历史和实时数据的统一语义以及查询和维护动态表的概念，将能够显着简化许多令人兴奋的用例和应用程序的实现。 由于这篇文章专注于流和动态表的关系查询的语义，我们没有讨论查询执行的细节，包括内部执行撤销，处理后期事件，支持结果预览，以及边界空间要求。 我们计划在稍后的时间点发布有关此主题的后续博客文章。 近几个月来，Flink 社区的许多成员一直在讨论和贡献关系API。 到目前为止，我们取得了很大的进步。 虽然大多数工作都专注于以附加模式处理流，但是日程上的下一步是处理动态表以支持更新其结果的查询。 如果您对使用SQL处理流程的想法感到兴奋，并希望为此做出贡献，请提供反馈，加入邮件列表中的讨论或获取JIRA 问题。 译文: http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table 原文: http://flink.apache.org/news/2017/04/04/dynamic-tables.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 脱离JVM？ Hadoop生态圈的挣扎与演化]]></title>
    <url>%2Fhadoop-ecosystem-break-away-jvm.html</url>
    <content type="text"><![CDATA[新世纪以来，互联网及个人终端的普及，传统行业的信息化及物联网的发展等产业变化产生了大量的数据，远远超出了单台机器能够处理的范围，分布式存储与处理成为唯一的选项。从2005年开始，Hadoop从最初Nutch项目的一部分，逐步发展成为目前最流行的大数据处理平台。Hadoop生态圈的各个项目，围绕着大数据的存储，计算，分析，展示，安全等各个方面，构建了一个完整的大数据生态系统，并有Cloudera，HortonWorks，MapR等数十家公司基于开源的Hadoop平台构建自己的商业模式，可以认为是最近十年来最成功的开源社区。 Hadoop 的成功固然是由于其顺应了新世纪以来互联网技术的发展趋势，同时其基于JVM的平台开发也为Hadoop的快速发展起到了促进作用。Hadoop生态圈的项目大都基于Java，Scala，Clojure等JVM语言开发，这些语言良好的语法规范，丰富的第三方类库以及完善的工具支持，为Hadoop这样的超大型项目提供了基础支撑。同时，作为在程序员中普及率最高的语言之一，它也降低了更多程序员使用，或是参与开发Hadoop项目的门槛。同时，基于Scala开发的Spark，甚至因为项目的火热反过来极大的促进了Scala语言的推广。但是随着Hadoop平台的逐步发展，Hadoop生态圈的项目之间的竞争加剧，越来越多的Hadoop项目注意到了这些JVM语言的一些不足之处，希望通过更有效率的处理方式，提升分布式系统的执行效率与健壮性。本文主要以Spark和Flink项目为例，介绍Hadoop社区观察到的一些因为JVM语言的不足导致的问题，以及相应的解决方案与未来可能的发展方向。 –more– 注：本文假设读者对Java和Hadoop系统有基本了解。 1. 背景目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，分布式计算引擎项目之间的竞争也相当激烈。MapReduce作为Hadoop平台的第一个分布式计算引擎，具有非常良好的可扩展性，Yahoo曾成功的搭建了上万台节点的MapReduce系统。但是MapReduce只支持Map和Reduce编程范式，使得复杂数据计算逻辑需要分割为多个Hadoop Job，而每个Hadoop Job都需要从HDFS读取数据，并将Job执行结果写回HDFS，所以会产生大量额外的IO开销，目前MapReduce正在逐渐被其他三个分布式计算引擎替代。Tez,Spark和Flink都支持图结构的分布式计算流，可在同一Job内支持任意复杂逻辑的计算流。Tez的抽象层次较低，用户不易直接使用，Spark与Flink都提供了抽象的分布式数据集以及可在数据集上使用的操作符，用户可以像操作Scala数据集合类似的方式在Spark/FLink中的操作分布式数据集，非常的容易上手，同时，Spark与Flink都在分布式计算引擎之上，提供了针对SQL，流处理，机器学习和图计算等特定数据处理领域的库。 随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了JVM本身，希望通过解决JVM本身带来的一些问题，提高分布式系统的性能或是健壮性，从而增强自身的竞争力。 JVM本身作为一个各种类型应用执行的平台，其对Java对象的管理也是基于通用的处理策略，其垃圾回收器通过估算Java对象的生命周期对Java对象进行有效率的管理。针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的JVM参数更有效率的管理Java对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身以及JVM的各参数有深入的了解，极大的提高了分布式计算平台的调优门槛（例如这篇文章中对Spark的调优 Tuning Java Garbage Collection for Spark Applications）。然而类似Spark或是Flink的分布式计算框架，框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的Java对象生命周期，从而为更有效率的管理Java对象提供了可能。 2. JVM存在的问题2.1. Java对象开销相对于c/c++等更加接近底层的语言，Java对象的存储密度相对偏低，例如【1】，“abcd”这样简单的字符串在UTF-8编码中需要4个字节存储，但Java采用UTF-16编码存储字符串，需要8个字节存储“abcd”，同时Java对象还对象header等其他额外信息，一个4字节字符串对象，在Java中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率的内存存储，则意味着CPU数据访问吞吐量更高，以及更少的磁盘落地可能。 2.2. 对象存储结构引发的cache miss为了缓解CPU处理速度与内存访问速度的差距【2】，现代CPU数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以cache line为单位加载数据，所以当CPU访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果CPU要访问的数据不在当前缓存所有的cache line中，则需要从内存中加载对应的数据，这被称为一次cache miss。当cache miss非常高的时候，CPU大部分的时间都在等待数据加载，而不是真正的处理数据。Java对象并不是连续的存储在内存上，同时很多的Java数据结构的数据聚集性也不好，在Spark的性能调优中，经常能够观测到大量的cache miss。Java社区有个项目叫做Project Valhalla，可能会部分的解决这个问题，有兴趣的可以看看这儿 OpenJDK: Valhalla。 2.3. 大数据的垃圾回收Java的垃圾回收机制，一直让Java开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面，垃圾回收也是Java应用的一颗不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大的影响了Java应用的性能和可用性。在当前的数据中心中，大容量的内存得到了广泛的应用，甚至出现了单台机器配置TB内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换，清洗，处理等步骤。在这个过程中，会产生海量的Java对象，JVM的垃圾回收执行效率对性能有很大影响。通过JVM参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及JVM的各参数有深入的了解，而且有时候这也远远不够。 2.4. OOM问题OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会出现OutOfMemoryError错误，JVM崩溃，分布式框架的健壮性和性能都会受到影响。通过JVM管理内存，同时试图解决OOM问题的应用，通常都需要检查Java对象的大小，并在某些存储Java对象特别多的数据结构中设置阈值进行控制。但是JVM并没有提供官方的检查Java对象大小的工具，第三方的工具类库可能无法准确通用的确定Java对象的大小【6】。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外的业务逻辑无关的代码。 3. 解决方案为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术： (1) 定制的序列化工具。显式内存管理的前提步骤就是序列化，将Java对象序列化成二进制数据存储在内存上（on heap或是off-heap）。通用的序列化框架，如Java默认的java.io.Serializable将Java对象以及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于Spark或是Flink这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如Hadoop的org.apache.hadoop.io.Writable，需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。 (2) 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有的内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收的压力。 (3) 缓存友好的数据结构和算法。只将操作相关的数据连续存储，可以最大化的利用L1/L2/L3缓存，减少Cache miss的概率，提升CPU计算的吞吐量。以排序为例，由于排序的主要操作是对Key进行对比，如果将所有排序数据的Key与Value分开，对Key连续存储，则访问Key时的Cache命中率会大大提高。 3.1 定制的序列化工具分布式计算框架可以使用定制序列化工具的前提是要处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。Spark与Flink数据集都支持任意Java或是Scala类型，通过自动生成定制序列化工具，Spark与Flink既保证了API接口对用户的友好度（不用像Hadoop那样数据类型需要继承实现org.apache.hadoop.io.Writable接口），同时也达到了和Hadoop类似的序列化效率。 3.1.1 Spark的序列化框架Spark 支持通用的计算框架，如 Java Serialization和 Kryo。其缺点之前也略有论述，总结如下： 占用较多内存。Kryo相对于Java Serialization更高，它支持一种类型到Integer的映射机制，序列化时用Integer代替类型信息，但还不及定制的序列化工具效率。 反序列化时，必须反序列化整个Java对象。 无法直接操作序列化后的二进制数据。 Project Tungsten 提供了一种更好的解决方式，针对于DataFrame API（Spark针对结构化数据的类SQL分析API，参考 Spark DataFrame Blog），由于其数据集是有固定Schema的Tuple（可大概类比为数据库中的行），序列化是针对每个Tuple存储其类型信息以及其成员的类型信息是非常浪费内存的，对于Spark来说，Tuple类型信息是全局可知的，所以其定制的序列化工具只存储Tuple的数据，如下图所示 对于固定大小的成员，如int，long等，其按照偏移量直接内联存储。对于变长的成员，如String，其存储一个指针，指向真正的数据存储位置，并在数据存储开始处存储其长度。通过这种存储方式，保证了在反序列化时，当只需访问某一个成员时，只需根据偏移量反序列化这个成员，并不需要反序列化整个Tuple。 Project Tungsten 的定制序列化工具应用在 Sort，HashTable，Shuffle等很多对Spark性能影响最大的地方。比如在Shuffle阶段，定制序列化工具不仅提升了序列化的性能，而且减少了网络传输的数据量，根据DataBricks的Blog介绍，相对于Kryo，Shuffle800万复杂Tuple数据时，其性能至少提高2倍以上。此外，Project Tungsten也计划通过Code generation技术，自动生成序列化代码，将定制序列化工具推广到Spark Core层，从而使得更多的Spark应用受惠于此优化。 3.1.2 Flink的序列化框架Flink在系统设计之初，就借鉴了很多传统 RDBMS 的设计，其中之一就是对数据集的类型信息进行分析，对于特定 Schema 的数据集的处理过程，进行类似RDBMS执行计划优化的优化。同时，数据集的类型信息也可以用来设计定制的序列化工具。和Spark类似，Flink支持任意的Java或是Scala类型，Flink通过Java Reflection框架分析基于Java的Flink程序UDF(User Define Function)的返回类型的类型信息，通过Scala Compiler分析基于Scala的Flink程序UDF的返回类型的类型信息。类型信息由TypeInformation类表示，这个类有诸多具体实现类，例如（更多详情参考Flink官方博客 Apache Flink: Juggling with Bits and Bytes）： BasicTypeInfo: 任意Java基本类型（装包或未装包）和String类型。 BasicArrayTypeInfo: 任意Java基本类型数组（装包或未装包）和String数组。 WritableTypeInfo: 任意Hadoop’s Writable接口的实现类. TupleTypeInfo: 任意的Flink tuple类型(支持Tuple1 to Tuple25). Flink tuples是固定长度固定类型的Java Tuple实现。 CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples). PojoTypeInfo: 任意的POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是public修饰符定义，要么有getter/setter方法。 GenericTypeInfo: 任意无法匹配之前几种类型的类。） 前6种类型数据集几乎覆盖了绝大部分的Flink程序，针对前6种类型数据集，Flink皆可以自动生成对应的TypeSerializer定制序列化工具，非常有效率的对数据集进行序列化和反序列化。对于第7中类型，Flink使用Kryo进行序列化和反序列化。此外，对于可被用作Key的类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据直接进行compare，hash等之类的操作。对于Tuple，CaseClass，Pojo等组合类型，Flink自动生成的TypeSerializer，TypeComparator同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的TypeSerializer，TypeComparator，如下图所示： 此外，如有需要，用户可通过集成TypeInformation接口，定制实现自己的序列化工具。 3.2 显式的内存管理垃圾回收的JVM内存管理回避不了的问题，JDK8的G1算法改善了JVM垃圾回收的效率和可用范围，但对于大数据处理的实际环境中，还是远远不够。这也和现在分布式框架的发展趋势有冲突，越来越多的分布式计算框架希望尽可能多的将待处理的数据集放在内存中，而对于JVM垃圾回收来说，内存中Java对象越少，存活时间越短，其效率越高。通过JVM进行内存管理的话，OutOfMemoryError也是一个很难解决的问题。同时，在JVM内存管理中，Java对象有潜在的碎片化存储问题（Java对象所有信息可能不是在内存中连续存储），也有可能在所有Java对象大小没有超过JVM分配内存时，出现OutOfMemoryError问题。 3.2.1 Flink的内存管理Flink将内存分为三个部分，每个部分都有不同的用途： Network buffers: 一些以32KB Byte数组为单位的buffer，主要被网络模块用于数据的网络传输。 Memory Manager pool: 大量以32KB Byte数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。 Remaining (Free) Heap: 主要留给UDF中用户自己创建的Java对象，由JVM管理。 Network buffers在Flink中主要基于Netty的网络传输，无需多讲。Remaining Heap用于UDF中用户自己创建的Java对象，在UDF中，用户通常是流式的处理数据，并不需要很多内存，同时Flink也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool（以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。 在Flink中，内存池由多个MemorySegment组成，每个MemorySegment代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment提供了根据偏移量访问数据的各种方法，如get/put int，long，float，double等，MemorySegment之间数据拷贝等方法，和java.nio.ByteBuffer类似。对于Flink的数据结构，通常包括多个向内存池申请的MemeorySegment，所有要存入的对象，通过TypeSerializer序列化之后，将二进制数据存储在MemorySegment中，在取出时，通过TypeSerializer反序列化。数据结构通过MemorySegment提供的set/get方法访问具体的二进制数据。 Flink这种看起来比较复杂的内存管理方式带来的好处主要有： 二进制的数据存储大大提高了数据存储密度，节省了存储空间。 所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。而对于大部分的分布式计算框架来说，这部分由于要缓存大量数据，是最有可能导致OOM的地方。 内存池虽然占据了大部分内存，但其中的MemorySegment容量较大(默认32KB)，所以内存池中的Java对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了JVM垃圾回收的压力。 Remaining Heap的内存虽然由JVM管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的Minor GC就会全部回收掉，一般不会触发Full GC。 Flink当前的内存管理在最底层是基于byte[]，所以数据最终还是on-heap，最近Flink增加了off-heap的内存管理支持，将会在下一个release中正式出现。Flink off-heap的内存管理相对于on-heap的优点主要在于（更多细节，请参考 Apache Flink: Off-heap Memory in Apache Flink and the curious JIT compiler）： 启动分配了大内存(例如100G)的JVM很耗费时间，垃圾回收也很慢。如果采用off-heap，剩下的Network buffer和Remaining heap都会很小，垃圾回收也不用考虑MemorySegment中的Java对象了。 更有效率的IO操作。在off-heap下，将MemorySegment写到磁盘或是网络，可以支持zeor-copy技术，而on-heap的话，则至少需要一次内存拷贝。 off-heap可用于错误恢复，比如JVM崩溃，在on-heap时，数据也随之丢失，但在off-heap下，off-heap的数据可能还在。此外，off-heap上的数据还可以和其他程序共享。 3.2.2 Spark的内存管理Spark的off-heap内存管理与Flink off-heap模式比较相似，也是通过Java UnSafe API直接访问off-heap内存，通过定制的序列化工具将序列化后的二进制数据存储与off-heap上，Spark的数据结构和算法直接访问和操作在off-heap上的二进制数据。Project Tungsten是一个正在进行中的项目，想了解具体进展可以访问：SPARK-7075 Project Tungsten (Spark 1.5 Phase 1)， SPARK-9697 Project Tungsten (Spark 1.6)。 3.3 缓存友好的计算磁盘IO和网络IO之前一直被认为是Hadoop系统的瓶颈，但是随着Spark，Flink等新一代的分布式计算框架的发展，越来越多的趋势使得CPU/Memory逐渐成为瓶颈，这些趋势包括： 更先进的IO硬件逐渐普及。10GB网络和SSD硬盘等已经被越来越多的数据中心使用。 更高效的存储格式。Parquet，ORC等列式存储被越来越多的Hadoop项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。 更高效的执行计划。例如Spark DataFrame的执行计划优化器的Fliter-Push-Down优化会将过滤条件尽可能的提前，甚至提前到Parquet的数据访问层，使得在很多实际的工作负载中，并不需要很多的磁盘IO。 由于CPU处理速度和内存访问速度的差距，提升CPU的处理效率的关键在于最大化的利用L1/L2/L3/Memory，减少任何不必要的Cache miss。定制的序列化工具给Spark和Flink提供了可能，通过定制的序列化工具，Spark和Flink访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时，尽量连续存储，减少内存碎片化对Cache命中率的影响，甚至更进一步，Spark与Flink可以将需要操作的部分数据（如排序时的Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能的提升Cache命中的概率。 3.3.1 Flink中的数据结构以Flink中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink通过特殊设计的排序算法，获得了非常好了性能，其排序算法的实现如下： 将待排序的数据经过序列化后存储在两个不同的MemorySegment集中。数据全部的序列化值存放于其中一个MemorySegment集中。数据序列化后的Key和指向第一个MemorySegment集中其值的指针存放于第二个MemorySegment集中。 对第二个MemorySegment集中的Key进行排序，如需交换Key位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。 排序完成后，访问数据时，按照第二个MemorySegment集中Key的顺序访问，并通过Pinter值找到数据在第一个MemorySegment集中的位置，通过TypeSerializer反序列化成Java对象返回。 这样实现的好处有： 通过Key和Full data分离存储的方式，尽量将被操作的数据最小化，提高Cache命中的概率，从而提高CPU的吞吐量。 移动数据时，只需移动Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。 TypeComparator直接基于二进制数据进行操作，节省了反序列化的时间。 3.3.2 Spark的数据结构Spark中基于off-heap的排序与Flink几乎一模一样，在这里就不多做介绍了，感兴趣的话，请参考：Project Tungsten: Bringing Apache Spark Closer to Bare Metal。 4. 总结本文主要介绍了Hadoop生态圈的一些项目遇到的一些因为JVM内存管理导致的问题，以及社区是如何应对的。基本上，以内存为中心的分布式计算框架，大都开始了部分脱离JVM，走上了自己管理内存的路线，Project Tungsten甚至更进一步，提出了通过LLVM，将部分逻辑编译成本地代码，从而更加深入的挖掘SIMD等CPU潜力。此外，除了Spark，Flink这样的分布式计算框架，HBase（HBASE-11425），HDFS（HDFS-7844）等项目也在部分性能相关的模块通过自己管理内存来规避JVM的一些缺陷，同时提升性能。 参考： project tungsten The “Memory Wall”: Modern Microprocessors flink memory management: Apache Flink: Juggling with Bits and Bytes java GC：Tuning Java Garbage Collection for Spark Applications Project Valhalla: OpenJDK: Valhalla java object size: dweiss/java-sizeof · GitHub Big Data Performance Engineering 原文: https://zhuanlan.zhihu.com/hadoop/20228397]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之资源调优]]></title>
    <url>%2Fspark-performance-resources-tuning.html</url>
    <content type="text"><![CDATA[1. 前言在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解资源调优。 2. 资源调优2.1 调优概述在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 2.2 Spark作业基本运行原理 详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 2.3 资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 (1) num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 (2) executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G-8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3-1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 (3) executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2-4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3-1/2左右比较合适，也是避免影响其他同学的作业运行。 (4) driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 (5) spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500-1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2-3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 (6) spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 (7) spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。 2.4 资源参数参考示例以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \ 原文: https://tech.meituan.com/spark-tuning-basic.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之开发调优]]></title>
    <url>%2Fspark-performance-optimization-basic.html</url>
    <content type="text"><![CDATA[1. 前言在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优。 2. 开发调优2.1 调优概述Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 2.2 原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 一个简单的例子:// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)rdd1.reduce(...) 2.3 原则二：尽可能复用同一个RDD除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 一个简单的例子:// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 2.4 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 对多次使用的RDD进行持久化的代码示例:// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 2.5 原则四：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 Broadcast与map进行join代码示例:// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 2.6 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 2.7 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 (1) 使用reduceByKey/aggregateByKey替代groupByKey 详情见“原则五：使用map-side预聚合的shuffle操作”。 (2)使用mapPartitions替代普通map mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ (3) 使用foreachPartitions替代foreach 原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 (4) 使用filter之后进行coalesce操作 通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 (5) 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 2.8 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 广播大变量的代码示例:// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 2.9 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 2.10 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 原文: https://tech.meituan.com/spark-tuning-basic.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 保存点之回溯时间]]></title>
    <url>%2Fflink-stream-turning-back-time-savepoints.html</url>
    <content type="text"><![CDATA[这篇文章是系列文章的第一篇，数据工匠团队会在这里为大家展示一些Apache Flink的核心功能。 流处理通常被大家与动态数据关联起来，相应的系统差不多会在数据被创造出来的那一刻就立刻对其进行处理或响应。像延迟、吞吐量、水印和处理迟到的数据等等都是大家讨论得最多的流处理话题，通常是关注现在，而不是过去。 可在实际项目中，却有许多种场景需要你的流处理程序把以前处理过的数据再重新处理一遍。这里有些例子： 为你的程序部署一个新版本，可能是有新功能、修复了问题、或者采用了更好的机器学习模型； 使用相同的源数据流对应用程序的不同版本进行A/B测试，两边都从同一个点开始测试，这样就不会牺牲之前的状态； 评估或开展将应用程序迁移到更新版本的处理框架上，或是一个不同的集群上； Apache Flink 的保存点（Savepoint）功能可以支持上面的所有场景，并且也是让 Flink 与其它分布式开源流处理器不同的一个显著区别点。 在本文中，我们会讲述如何使用保存点功能来重新处理数据，并一定程度地深入底层，讲述这个功能在Flink中是怎么实现的。 1. “重新处理”到底是什么意思？为了保证大家对重新处理数据的理解是一致的，我们先讨论一个你可能需要重新处理数据的业务例子。想像一个社交媒体公司，她除了基本的发贴功能之外，还发布了一种付费的、或者说是推广发贴的功能。 公司的用户可以访问一个简单的、基于 Flink 实现的仪表板，显示他们的所有文章（不管是普通的还是付费的）被大家查看、点击等等的次数。几个星期之后，从用户的反馈中就可以清晰地看到，这个仪表板如果能把普通的发贴数据和付费的发贴数据区别开来，那就会更好用。 要实现这个功能，就有必要返回到付费发贴功能最初发布的那个时刻，然后从那个时刻开始，把所有数据全都重新处理一遍。这一次要把付费贴和普通贴的展示和交互全都拆开来。如果要把从公司创立伊始产生的数据全都重新处理一遍，这就实在有点强人所难，所以能够从付费发贴的功能发布的时候开始重新处理，同时还保留之前的计算结果，这个功能就很有必要了。 所以当我们用到 重新处理 这个词时，我们的意思就是回到一个系统以前的、一致的状态（按开发者的定义，不一定非要是流的最早状态），然后从那个状态开始再处理一遍，可能也要在更改了你的 Flink 程序之后。 读者们可以看到的好消息就是： Flink 为大家免费提供了上述重新处理功能，相应的功能就叫保存点。我们说”免费”，意思是只要你的程序是容错的，并且可以从错误中恢复，那你就可以在 Flink 中创建一个保存点并重新处理数据，花费的额外准备工作量几乎为零。 2. 简单说说保存点到底是什么简而言之，一个 Flink 程序的保存点就是关于以下两点的全局一致的镜像： 所有数据源的位置； 所有并行算子的状态； “全局一致” 意味着所有并行算子的状态都在所有输入的相同的明确定义的位置处被记录下来了。 如果在过去的某个时刻，你为某个应用程序记下了保存点，那你就可以从那个保存点的位置开始启动一个新程序。新的程序将使用那个保存点位置保存下来的算子的状态进行初始化，并且会从记录的保存点里各个数据源的相应位置开始，重新处理全部数据。 因为 Flink 的保存点之间是相互完全独立的，所以对每个程序你都可以有多个保存点，这样你就可以根据这些不同的保存点的信息，回到不同的位置，启动多次、甚至不同的程序（如下图所示）。这个功能对于派生你的流处理程序，或者为它们打不同的版本，是非常有用的。 我们应该注意，在从某个保存点开始重新处理数据时，对事件的时间处理是非常重要的。重新处理基本上就意味着从过去到现在进行快速回放，也就是说，是全速地从某些存储系统中读出数据，直到赶上了当前的状态，然后再继续实时地处理新到达的数据。 因为程序对于时间的处理或者插入时间都是要依赖当前的本地时间的，那么如果在根据保存点启动程序时不使用事件的时间，而使用别的时间，对程序的逻辑而言就很可能导致错误的结果。 3. 听起来不错，那我该做什么？不用做很多！事实上，所有支持故障恢复的程序都是自动支持保存点的。因此，大多数进行有状态计算的程序已经满足了需要的条件。如果没有，可以对它们进行快速更新，让它们具备： 启用检查点功能：在每种情况下，我们都推荐在构建 Flink 程序的同时，把检查点功能打开，事实上在你的 Flink 程序中加上检查点只是需要增加几行代码而已。 可以重置的数据源（即Apache Kafka、Amazon Kinesis，或者文件系统等）：数据源必须能按照你想要重新处理的点开始，重放数据。 所有的状态都通过 Flink 的管理状态接口保存：所有具体的算子的状态都必须保存在 Flink 的容错状态数据结构中，这让它可以按照某个之前的保存点位置被重置。 配置一个合适的状态后台：Flink 提供了不同的状态后台来将检查点和保存点持久化。默认地，保存点都保存在 JobManager 中，但你要为你的程序配置一个适当的后台状态程序，比如 RocksDB 等。 如果你已经在运行一个容错的程序了，那就创建一个保存点，然后从保存点的位置开始重新启动程序，这只需要在 Flink 命令行里敲几个命令就可以了。咱们接下来挨个看看。 (1) 第一步：创建一个保存点 首先，获得所有运行中的 Flink 任务的列表：user$ flink list------------Running/Restarting Jobs------------10.10.2016 16:20:33 : job_id : Sample Job (RUNNING) （运行上面的命令时，你的真实任务ID会是一个包括字母和数字的字符串。） 然后，用相应的任务ID创建一个保存点：user$ flink savepoint job_id 现在你的保存点就已经可用了。 如果你准备马上根据你的保存点来重新启动任务，你通常会想要把现在正在运行的任务先停掉。你已经有了相应任务的ID，那把它停掉只要几秒钟就够了：user$ flink cancel job_id (2) 第二步：从一个保存点开始启动任务 当你更新完程序之后，就可以从你的保存点开始启动任务了。user$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar 如果你想在一个示例程序中自己重做这些步骤，我们推荐你看看一篇之前的博客文章，我们在那里讲了怎么做这件事。 4. 如果我想升级我的程序，该怎样做？如果你想从一个保存点开始启动一个修改过的程序，有几件事是要考虑的。我们可以区别下面这两种情况： 改变一个用户定义的函数的逻辑，比如MapFunction； 改变一个程序的架构，也就是增加或减少算子等； 第一种情况很简单，不需要什么特别的准备。你可以按你的需要去修改函数代码。不过，如果你用一个修改了的架构从保存点开始启动程序，那么为了能够恢复算子的状态，Flink 必须能够将保存点程序的算子与使用了新架构的新程序的算子对应起来。 在这种情况下，你就要手动地将算子ID分配给最初的和更新了的程序。因为如果没有算子ID的话，是没办法修改程序的架构的。所以最佳实践经验就要求一定要分配算子ID。 下面的代码段显示了如何为算子分配ID：DataStream stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(“source-id”) .shuffle() // The stateful mapper with ID .map(new StatefulMapper()) .uid(“mapper-id”)// Stateless sink (no specific ID required)stream.print() 请查阅文档，了解更多关于升级程序和保存点的细节。 5. 关于保存点的最佳实践要更好的利用上文中描述的 Flink 的重新处理功能，你应该经常触发，生成新的保存点。我们建议要根据某些时刻表（比如每天一次，每周一次，等等）自动地生成保存点，而且每当你关闭某个任务或发布程序的新版本时，也最好先生成保存点。 依据你想用 Flink 做的事件不同，生成保存点的最佳方法也会不同，但总的来说，在构建你的程序时你应该花些时间考虑如何使用这些保存点。 6. 这些东西是怎么工作的呢？保存点事实上只是检查点的一个延伸，这就是 Flink 的容错机制。如果开启了检查点功能，Flink 就会周期性地为所有的算子状态生成一个一致的检查点。在文档中详细的描述了检查点的细节，如果你是个 Flink 新手，花些时间去读读是非常值得的。 你可能会以为要生成一个一致的检查点，就得暂停数据处理，因为 Flink 必须要等着，直到所有没处理完的记录全被处理掉了，然后做个镜像，镜像生成之后再回去继续处理数据。事实并非如此！ Flink 是持续处理数据的，即使在生成检查点的时候也是这样。文档中的Barriers一节讲了实现这个功能的原理。 两者之间的关键区别：检查点是基于某些规定的时间间隔自动生成的，而保存点是由用户显式地触发生成的，而且不会象检查点那样过了一定的时间之后就会被删掉。 7. 总结我们讨论了 Apache Flink 的保存点和数据重处理功能，因为我们相信这就是 Flink 与开源世界中其它流处理器之间的重要区别之一。而且最重要的，在容错的 Flink 程序中获得重处理功能几乎是不需要任何代价的，只需要很少的改动。 Flink 社区现在还在积极地工作着，要把保存点功能做得更好，包括在改变并发度的情况下保存状态的解决方案等。有些相应的功能（比如Flink-3755）已经发布到主分支上了，而且会被包含到下一个小版本Flink 1.2.0中。 所以，当你需要把程序多部署一份，或者上个新版本，或者要做A/B测试，或者要让多个程序从同一个点开始处理数据时，你可以这么做了，而且不会丢失那些宝贵的状态数据。 当有真实的需求时，流处理基于实时的特性不应该阻挡你把时间调回过去的动作。 有兴趣了解关于 Apache FLink 的保存点的更多内容吗？数据工匠CTO Stephan Ewen做了一个关于这个话题的七分钟白板演练，你可以在MapR博客上看到相关内容。 原文: https://data-artisans.com/blog/turning-back-time-savepoints 译文: http://www.infoq.com/cn/articles/turning-back-time-savepoints]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 命令行界面]]></title>
    <url>%2Fflink-basic-command-line-interface.html</url>
    <content type="text"><![CDATA[1. 概述Flink 提供了一个命令行接口（CLI）用来运行打成JAR包的程序，并且可以控制程序的运行。命令行接口在 Flink 安装完之后即可拥有，本地单节点或是分布式部署安装都会有命令行接口。命令行接口启动脚本是 $FLINK_HOME/bin 目录下的 flink 脚本， 默认情况下会连接运行中的 Flink master(JobManager)， JobManager 的启动脚本与 CLI 在同一安装目录下。 使用命令行接口的前提条件是 JobManager 已经被启动(通过$FLINK_HOME/bin/start-local.sh 或是 $FLINK_HOME/bin/start-cluster.sh)或是 Flink YARN 环境可用。 JobManager 可以通过如下命令启动:$FLINK_HOME/bin/start-local.sh或$FLINK_HOME/bin/start-cluster.sh 2. Example(1) 运行示例程序，不传参数：./bin/flink run ./examples/batch/WordCount.jar (2) 运行示例程序，带输入和输出文件参数：./bin/flink run ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (3) 运行示例程序，带输入和输出文件参数,并设置16个并发度：./bin/flink run -p 16 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (4) 运行示例程序，并禁止 Flink 输出日志./bin/flink run -q ./examples/batch/WordCount.jar (5) 以独立(detached)模式运行示例程序./bin/flink run -d ./examples/batch/WordCount.jar (6) 在指定 JobManager 上运行示例程序./bin/flink run -m myJMHost:6123 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (7) 运行示例程序，指定程序入口类(Main方法所在类)：./bin/flink run -c org.apache.flink.examples.java.wordcount.WordCount ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (8) 运行示例程序，使用带有2个 TaskManager 的per-job YARN 集群./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar --input hdfs:///xiaosi/a.txt --output hdfs:///xiaosi/result.txt (9) 以JSON格式输出 WordCount 示例程序优化执行计划：./bin/flink info ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (10) 列出已经调度的和正在运行的Job(包含Job ID信息)./bin/flink list (11) 列出已经调度的Job(包含Job ID信息)./bin/flink list -s (13) 列出正在运行的Job(包含Job ID信息)./bin/flink list -r (14) 列出在Flink YARN中运行Job./bin/flink list -m yarn-cluster -yid &lt;yarnApplicationID&gt; -r (15) 取消一个Job./bin/flink cancel &lt;jobID&gt; (16) 取消一个带有保存点(savepoint)的Job./bin/flink cancel -s [targetDirectory] &lt;jobID&gt; (17) 停止一个Job(只适用于流计算Job)./bin/flink stop &lt;jobID&gt; 取消和停止一个作业的区别如下： 调用取消作业时，作业中的算子立即收到一个调用cancel()方法的指令以尽快取消它们。如果算子在调用取消操作后没有停止，Flink 将定期开启中断线程来取消作业直到作业停止。 停止作业是一种停止正在运行的流作业的更加优雅的方法。停止仅适用于使用实现StoppableFunction接口的数据源的那些作业。当用户请求停止作业时，所有数据源将收到调用stop()方法指令。但是作业还是会继续运行，直到所有数据源正确关闭。这允许作业处理完所有正在传输的数据(inflight data)。 3. 保存点保存点通过命令行客户端进行控制： 3.1 触发保存点./bin/flink savepoint &lt;jobID&gt; [savepointDirectory] 这会触发作业ID为jobId的保存点，并返回创建的保存点的路径。你需要此路径来还原和处理保存点。 此外，你可以选择指定一个目标文件系统目录来存储保存点。目录可以被 JobManager 访问。 如果你不指定目标目录，则需要配置默认目录（请参阅保存点）。 否则，触发保存点将失败。 3.2 使用YARN触发保存点./bin/flink savepoint &lt;jobId&gt; [savepointDirectory] -yid &lt;yarnAppId&gt; 这将触发作业ID为 jobId 以及 YARN 应用程序ID为 yarnAppId 的保存点，并返回创建的保存点的路径。 其他一切与上面的触发保存点中描述的相同。 3.3 根据保存点取消Job你可以自动触发一个保存点并取消作业:./bin/flink cancel -s [savepointDirectory] &lt;jobID&gt; 如果没有配置保存点目录，则需要为 Flink 安装配置默认的保存点目录(请参阅保存点）。 只有保存点触发成功，作业才被取消 3.4 恢复保存点./bin/flink run -s &lt;savepointPath&gt; ... 这个run命令提交作业时带有一个保存点标记，这使得程序可以从保存点中恢复状态。保存点路径是通过保存点触发命令得到的。 默认情况下，我们尝试将所有的保存点状态与正在提交的作业进行匹配。如果你想允许跳过无法使用新作业恢复的保存点状态，则可以设置allowNonRestoredState标志。当保存点触发时，如果想从程序中删除一个算子（作为程序的一部分），并且仍然想要使用这个保存点，则需要允许这一点。 ./bin/flink run -s &lt;savepointPath&gt; -n ... 如果想从程序中删除算子(作为保存点一部分的)，这时会非常有用。 3.5 销毁保存点./bin/flink savepoint -d &lt;savepointPath&gt; 销毁一个保存点同样需要一个路径。这个保存点路径是通过保存点触发命令得到的。 如果使用自定义状态实例（例如自定义 reducing 状态或 RocksDB 状态），则必须指定程序JAR的路径以及被触发的保存点，以便使用用户代码类加载器来销毁保存点：./bin/flink savepoint -d &lt;savepointPath&gt; -j &lt;jarFile&gt; 否则，你将遇到 ClassNotFoundException。 4. 用法下面是Flink命令行接口的用法:xiaosi@yoona:~/qunar/company/opt/flink-1.3.2$ ./bin/flink./flink &lt;ACTION&gt; [OPTIONS] [ARGUMENTS]The following actions are available:Action &quot;run&quot; compiles and runs a program. Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;run&quot; action options: -c,--class &lt;classname&gt; Class with the program entry point (&quot;main&quot; method or &quot;getPlan()&quot; method. Only needed if the JAR file does not specify the class in its manifest. -C,--classpath &lt;url&gt; Adds a URL to each user code classloader on all nodes in the cluster. The paths must specify a protocol (e.g. file://) and be accessible on all nodes (e.g. by means of a NFS share). You can use this option multiple times for specifying more than one URL. The protocol must be supported by the &#123;@link java.net.URLClassLoader&#125;. -d,--detached If present, runs the job in detached mode -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -n,--allowNonRestoredState Allow to skip savepoint state that cannot be restored. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered. -p,--parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. -q,--sysoutLogging If present, suppress logging output to standard out. -s,--fromSavepoint &lt;savepointPath&gt; Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537 ). -z,--zookeeperNamespace &lt;zookeeperNamespace&gt; Namespace to create the Zookeeper sub-paths for high availability mode Options for yarn-cluster mode: -yD &lt;arg&gt; Dynamic properties -yd,--yarndetached Start detached -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session -yj,--yarnjar &lt;arg&gt; Path to Flink jar file -yjm,--yarnjobManagerMemory &lt;arg&gt; Memory for JobManager Container [in MB] -yn,--yarncontainer &lt;arg&gt; Number of YARN container to allocate (=Number of Task Managers) -ynm,--yarnname &lt;arg&gt; Set a custom name for the application on YARN -yq,--yarnquery Display available YARN resources (memory, cores) -yqu,--yarnqueue &lt;arg&gt; Specify YARN queue. -ys,--yarnslots &lt;arg&gt; Number of slots per TaskManager -yst,--yarnstreaming Start Flink in streaming mode -yt,--yarnship &lt;arg&gt; Ship files in the specified directory (t for transfer) -ytm,--yarntaskManagerMemory &lt;arg&gt; Memory per TaskManager Container [in MB] -yz,--yarnzookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode Options for yarn mode: -ya,--yarnattached Start attached -yD &lt;arg&gt; Dynamic properties -yj,--yarnjar &lt;arg&gt; Path to Flink jar file -yjm,--yarnjobManagerMemory &lt;arg&gt; Memory for JobManager Container [in MB] -yqu,--yarnqueue &lt;arg&gt; Specify YARN queue. -yt,--yarnship &lt;arg&gt; Ship files in the specified directory (t for transfer) -yz,--yarnzookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability modeAction &quot;info&quot; shows the optimized execution plan of the program (JSON). Syntax: info [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;info&quot; action options: -c,--class &lt;classname&gt; Class with the program entry point (&quot;main&quot; method or &quot;getPlan()&quot; method. Only needed if the JAR file does not specify the class in its manifest. -p,--parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;list&quot; lists running and scheduled programs. Syntax: list [OPTIONS] &quot;list&quot; action options: -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -r,--running Show only running programs and their JobIDs -s,--scheduled Show only scheduled programs and their JobIDs Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;stop&quot; stops a running program (streaming jobs only). Syntax: stop [OPTIONS] &lt;Job ID&gt; &quot;stop&quot; action options: -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;cancel&quot; cancels a running program. Syntax: cancel [OPTIONS] &lt;Job ID&gt; &quot;cancel&quot; action options: -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -s,--withSavepoint &lt;targetDirectory&gt; Trigger savepoint and cancel job. The target directory is optional. If no directory is specified, the configured default directory (state.savepoints.dir) is used. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;savepoint&quot; triggers savepoints for a running job or disposes existing ones. Syntax: savepoint [OPTIONS] &lt;Job ID&gt; [&lt;target directory&gt;] &quot;savepoint&quot; action options: -d,--dispose &lt;arg&gt; Path of savepoint to dispose. -j,--jarfile &lt;jarfile&gt; Flink program JAR file. -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode: Please specify an action. 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 外部检查点]]></title>
    <url>%2Fflink-stream-deployment-externalized-checkpoints.html</url>
    <content type="text"><![CDATA[1. 概述检查点通过恢复状态和对应流位置来实现 Flink 状态容错，从而为应用程序提供与无故障执行相同的语义。 请参阅检查点以了解如何为你的应用程序启用和配置检查点。 2. 外部检查点 Externalized Checkpoints默认情况下检查点不会持久化存储在外部系统中，只是用来从故障中恢复作业。当一个程序被取消时它们会被删除。但是，你可以配置检查点定期持久化存储在外部系统中，类似于保存点(savepoints)。这些外部持久化的检查点将其元数据写入持久性存储中，即使在作业失败时也不会自动清除。这样，如果你的作业失败时，你会有一个检查点用于恢复作业。 CheckpointConfig config = env.getCheckpointConfig();config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); ExternalizedCheckpointCleanup模式配置当你取消作业时外部检查点如何操作： (1) ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：作业取消时保留外部检查点。请注意，在这种情况下，你必须手动清除取消后的检查点状态。 (2) ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION: 作业取消时删除外部检查点。检查点状态只有在作业失败时才可用。 2.1 目录结构与保存点类似，外部检查点由元数据文件组成，一些其他数据文件（取决于状态后端）。外部检查点元数据的目标目录是由配置属性state.checkpoints.dir确定的，目前它只能通过配置文件来设置。 state.checkpoints.dir: hdfs:///checkpoints/ 该目录包含恢复检查点所需的检查点元数据。对于MemoryStateBackend，这个元数据文件是独立的(self-contained)，不需要其他文件。 FsStateBackend 和 RocksDBStateBackend 需要写到不同的数据文件中，只需将这些文件的路径写入元数据文件。这些数据文件存储在状态后端指定的路径上。 env.setStateBackend(new RocksDBStateBackend(&quot;hdfs:///checkpoints-data/&quot;); 2.2 与保存点的区别外部检查点与保存点有一些差异。他们 使用状态后端指定的（低层次）数据格式 可能是增量存储的 不支持 Flink 部分功能（如重新调整）。 2.3 从外部检查点恢复作业可以通过使用检查点的元数据文件从外部检查点中恢复，就像从保存点恢复一样（请参阅保存点恢复）。请注意，如果元数据文件不是独立的，jobmanager 需要访问它所引用的数据文件（参见上面的目录结构）。 $ bin/flink run -s :checkpointMetaDataPath [:runArgs] 备注:Flink版本:1.4 术语翻译: 术语 翻译 Checkpoints 检查点 Externalized Checkpoints 外部检查点 savepoints 保存点 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之作业与调度]]></title>
    <url>%2Fflink-internals-job-scheduling.html</url>
    <content type="text"><![CDATA[1. 调度Flink中的执行资源是通过任务槽定义。每个TaskManager都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例。请注意，Flink经常同时执行连续的任务：对于流式处理程序时刻发生，但是对于批处理程序来说却是经常发生。 下图证明了这一点。考虑一个带有数据源，一个MapFunction 和 一个ReduceFunction 的程序。数据源和 MapFunction 以并行度4运行， ReduceFunction以并行度3运行。流水线由 Source-Map-Reduce 序列组成。在具有2个TaskManager（每个有3个插槽）的集群上，程序将按照下面的描述执行: 在内部，Flink通过SlotSharingGroup和 CoLocationGroup定义哪些任务可以共享一个槽（允许），哪些任务必须严格放置在同一个槽中。 2. JobManager 数据结构在作业执行期间，JobManager 追踪分布式任务，决定何时调度下一个任务（或任务集合），并对完成的任务或执行失败的任务进行相应的处理。 JobManager 接收 JobGraph，JobGraph表示由算子（JobVertex）和中间结果（IntermediateDataSet）组成的数据流。每个算子都具有属性，如并行度和执行的代码等。另外，JobGraph还有一组附加的库，运行算子代码必需使用这些库。 JobManager 将 JobGraph 转换成 ExecutionGraph。 ExecutionGraph 是 JobGraph 的并行版本：对于每个 JobVertex，对于每个并行子任务它都包含一个 ExecutionVertex。例如并行度为100的算子会有一个 JobVertex 以及 100个 ExecutionVertices。 ExecutionVertex跟踪特定子任务的执行状态。JobVertex 中所有的 ExecutionVertices 都保存在一个 ExecutionJobVertex 中，该 ExecutionJobVertex 跟踪整个算子的状态。除了顶点之外， ExecutionGraph 还包含 IntermediateResult 和 IntermediateResultPartition。前者跟踪 IntermediateDataSet 的状态，后者追踪每个分区的状态。 每个 ExecutionGraph 都有一个与之相关的作业状态。作业状态表示作业执行的当前状态。 Flink 作业首先处于 ctreated 状态，然后切换到 running 状态，一旦所有工作完成后切换到 finished 状态。在出现故障的情况下，作业首先切换到 failing 状态，取消所有正在运行任务的地方。如果所有作业顶点已达到最终状态，并且作业不可重新启动，那么作业转换 failed 状态。如果作业可以重新启动，那么它将进入 restarting 状态。一旦作业重新启动完成后，将进入 ctreated 状态。 在用户取消作业的情况下，将进入 cancelling 状态。这也需要取消所有正在运行的任务。一旦所有正在运行的任务都达到最终状态，作业将转换到 cancelled 状态。 不同于表示全局终端状态以及触发清理工作的 finished， canceled 和 failed 状态，suspended 状态只是本地终端。本地终端的意思是作业的执行已在相应的 JobManager 上终止，但 Flink 集群的另一个 JobManager 可从持久性 HA 存储中检索作业并重新启动作业。因此，进入 suspended 状态的作业将不会完全清理。 在 ExecutionGraph 的执行过程中，每个并行任务都经历了从 ctreated 到 finished 或 failed 的多个阶段。下图说明了它们之间的状态和可能的转换。任务可以执行多次（例如在故障恢复过程中）。出于这个原因， ExecutionVertex 执行跟踪信息保存在 Execution 中。 每个 ExecutionVertex 都有一个当前的Execution，以及之前的Executions。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Shell中判断HDFS文件是否存在]]></title>
    <url>%2Fhadoop-how-to-determine-the-hdfs-file-exists.html</url>
    <content type="text"><![CDATA[1. 用法Hadoop提供了-test命令可以验证文件目录是否存在。我们首先看一下-test命令的使用用法:hadoop fs -help-test -[defsz] &lt;path&gt;: Answer various questions about &lt;path&gt;, with result via exit status. -d return 0 if &lt;path&gt; is a directory. -e return 0 if &lt;path&gt; exists. -f return 0 if &lt;path&gt; is a file. -s return 0 if file &lt;path&gt; is greater than zero bytes in size. -z return 0 if file &lt;path&gt; is zero bytes in size. else, return 1. 命令参数 描述 -d 如果指定路径是一个目录返回0否则返回1 -e 如果指定路径存在返回0否则返回1 -f 如果指定路径是一个文件返回0否则返回1 -s 如果指定路径文件大小大于0返回0否则返回1 -z 如果指定指定文件大小等于0返回0否则返回1 2. Example:[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -ls test/advFound 1 itemsdrwxr-xr-x - xiaosi xiaosi 0 2018-01-25 15:39 test/adv/day=20180123[xiaosi@ying:~$]$[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123[xiaosi@ying:~$]$ echo $?0[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180124[xiaosi@ying:~$]$ echo $?1 3. Shell中判断sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123if [ $? -eq 0 ] ;then echo &apos;[info]目录已存在不需要创建&apos;else sudo -uxiaosi hadoop fs -mkdir -p test/adv/day=20180123fi]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之数据流容错]]></title>
    <url>%2Fflink-data-streaming-fault-tolerance.html</url>
    <content type="text"><![CDATA[1. 概述Apache Flink提供了一个容错机制来持续恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态也将最终反映每条记录来自数据流严格一次exactly once。 请注意，有一个开关可以降级为保证至少一次(least once)（如下所述）。 容错机制连续生成分布式流数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁生成，而不会对性能造成太大影响。流应用程序的状态存储在可配置的位置（例如主节点或HDFS）。 如果应用程序发生故障（由于机器，网络或软件故障），Flink会停止分布式流式数据流。然后系统重新启动算子并将其重置为最新的成功检查点。输入流被重置为状态快照的时间点。作为重新启动的并行数据流处理的任何记录都保证不属于先前检查点状态的一部分。 注意:默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参阅检查点。 为了实现这个机制的保证，数据流源（如消息队列或代理）需要能够将流重放到定义的最近时间点。Apache Kafka有这个能力，而Flink的Kafka连接器就是利用这个能力。有关Flink连接器提供的保证的更多信息，请参阅数据源和接收器的容错保证。 因为Flink的检查点是通过分布式快照实现的，所以我们交替使用快照和检查点两个概念。 2. CheckpointingFlink的容错机制的核心部分是生成分布式数据流和算子状态的一致性快照。这些快照作为一个一致性检查点，在系统发生故障时可以回溯。Flink的生成这些快照的机制在分布式数据流的轻量级异步快照中进行详细的描述。它受分布式快照Chandy-Lamport算法的启发，并且专门针对Flink的执行模型量身定制。 2.1 BarriersFlink分布式快照的一个核心元素是数据流Barriers。这些Barriers被放入数据流中，并作为数据流的一部分与记录一起流动。Barriers永远不会超越记录，严格按照相对顺序流动。Barriers将数据流中的记录分成进入当前快照的记录集合和进入下一个快照的记录集合。每个Barriers都携带前面快照的ID。Barriers不会中断流的流动，因此非常轻。来自不同快照的多个Barriers可以同时在流中，这意味着不同快照可以同时发生。 Barriers在数据流源处被放入的并行数据流。快照n放入Barriers的位置（我们称之为Sn）是快照覆盖数据的源流中的位置。例如，在Apache Kafka中，这个位置是分区中最后一个记录的偏移量。该位置Sn会报告给检查点协调员（Flink的JobManager）。 Barriers向下游流动。当中间算子从其所有输入流中接收到快照n的Barriers时，它会将快照n的Barriers发送到其所有输出流中。一旦Sink算子（流式DAG的末尾）从其所有输入流中接收到Barriers n，就向检查点协调器确认快照n。在所有Sink确认了快照之后，才被确认已经完成。 一旦快照n完成，作业将不会再向数据源询问Sn之前的记录，因为那时这些记录（以及它们的后代记录）已经通过了整个数据流拓扑。 接收多个输入流的算子需要根据快照Barriers对其输入流。上图说明了这一点： 当算子从一个输入流接收到Barriers n时，先不处理来自该数据流的记录，而是先进行缓存，等从其他所有输入流中都接收到Barriers n时，才开始处理缓存的数据(译者注：根据 Barriers n 对齐所有的输入流)。否则，就会把属于快照n和快照n + 1的记录混合在一起。 Barriers n的数据流暂时搁置。从这些数据流接收到的记录不会被处理，而是放入输入缓冲区中，等待其他输入数据流进行对齐(例如上图中的aligning部分)。 一旦接收到最后一个流的Barriers n时（译者注：这是触发Checkpoint），算子才发送所有缓存的记录，然后发送快照Barriers n(例如上图中的checkpoint部分)。 之后，恢复处理所有输入流中的记录，在处理来自数据流的记录之前优先处理来自输入缓冲区中的记录(例如上图中的continue部分)。 2.2 State当算子包含任何形式的状态时，这个状态也必须是快照的一部分。算子状态有不同的形式： 用户自定义状态：这是由转换函数（如map（）或filter（））直接创建和修改的状态。有关详细信息，请参阅状态概述 系统状态：这种状态指的是作为算子计算一部分的数据缓冲区。这种状态的一个典型例子是窗口缓冲区，在窗口缓冲区中，系统为窗口收集（以及聚合）记录，直到窗口被计算和删除。 在算子收到所有输入流中的Barriers以及在barriers发送到输出流之前，算子对其状态进行快照。这时，Barriers之前的记录都更新到状态中，Barriers之后的记录不会进行更新。由于快照的状态可能较大，因此需要存储在可配置的状态后端state backend中。默认情况下，会存储在JobManager的内存中，但是在生产环境下，应该配置为分布式可靠存储系统（如HDFS）。在状态被存储之后，算子确认检查点，将快照barriers发送到输出流，然后继续进行。 生成的快照包含： 对于每个并行流数据源，快照启动时在数据流中的偏移量/位置 对于每个算子，指向的状态（作为快照中一部分）的指针 2.3 Exactly Once vs. At Least Once对齐步骤可能会给流处理程序造成延迟。这个额外的延迟通常大约在几毫秒的数量级，但是我们已经看到一些因为异常值造成的延迟明显增加的情况。对于需要连续较低延迟（几毫秒）的应用程序而言，Flink有一个开关可以在检查点期间跳过流对齐。一旦算子看到每个输入的检查点Barriers，就会生成检查点快照。 当跳过对齐步骤时，当检查点n的某些barriers到达时，算子就会处理输入数据（译者注：不需要缓存输入数据来等待最后一个 Barriers的到来）。这样的话，在为检查点n生成状态快照之前也会处理到属于检查点n+1的元素。在恢复时，这些记录将会重复出现，因为它们既包含在检查点n的状态快照中，也会在检查点n之后作为数据的一部分进行重放。 对齐仅发生在当算子具有多个输入（例如join）或者具有多个输出（在流repartitioning/shuffle之后）的情况。正因为如此，只有高度并行流操作（map（），flatMap（），filter（）…）的数据流即使在 At-Least-Once 模式下也只能提供Exactly-Once语义。 2.4 异步状态快照请注意，上述机制意味着当算子在状态后端存储状态快照时会停止处理输入记录。这种同步状态快照在每次生成快照时都会造成延迟。 可以让算子在存储其状态快照的同时继续处理输入记录，有效地让状态快照在后台异步发生。要做到这一点，算子必须能够产生一个状态对象，以某种方式进行存储以便对算子状态进行修改后不会影响该状态对象。例如，copy-on-write数据结构（如RocksDB中使用的数据结构）具有这种功能。 在接收到输入端的Barriers后，算子启动其状态的异步快照复制。Barriers立即发送到输出流中，并继续进行正常的流处理。一旦后台复制过程完成，它就会向检查点协调器（JobManager）确认检查点。只有在所有sink接收到Barriers并且所有有状态的算子已经确认完成备份（可能在Barriers到达sink之后）时检查点才算完成。 有关状态快照的详细信息，请参阅状状态后端。 3. 恢复在这种机制下恢复很简单：一旦失败，Flink选择最近完成的检查点k。然后系统重新部署整个分布式数据流，并为每个算子提供作状态。数据源被设置为从位置Sk读取数据流。例如在Apache Kafka中，这意味着告诉消费者从偏移量Sk处开始提取数据。 如果增量对状态进行快照，算子将从最新且完整的快照状态开始，然后对该状态应用一系列增量快照更新。 请参阅重启策略了解更多信息。 4. 实现算子快照对算子进行快照，有两部分：同步部分和异步部分。 算子和状态后端将其快照作为Java FutureTask。该任务包含的状态同步部分已经完成异步部分挂起。然后异步部分由该检查点的后台线程执行。 算子检查点只是同步返回一个已经完成的FutureTask。如果需要执行异步操作，则在FutureTask的run（）方法中执行。 任务是可取消的，所以消耗句柄的数据流和其他资源是可以被释放。 Flink版本：1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 检查点启用与配置]]></title>
    <url>%2Fflink-stream-development-checkpointing-enable-config.html</url>
    <content type="text"><![CDATA[Flink 中的每个函数和操作符都可以是有状态的（请参阅使用状态了解详细信息）。有状态函数在处理单个元素/事件时存储数据。 为了能够状态容错，Flink 需要对状态进行 checkpoint。检查点允许 Flink 在流中恢复状态和位置，为应用程序提供与无故障执行相同的语义。 关于 Flink 流式容错机制背后的技术请参阅流式容错的详细文档。 1. 前提条件Flink 的检查点机制与流和状态的持久存储进行交互。一般来说，它要求： 一个可持久化（或保存很长时间）的数据源，可以重放特定时间段的记录。持久消息队列是这种数据源的一个例子（例如 Apache Kafka，RabbitMQ，Amazon Kinesis，Google PubSub）或 文件系统（例如 HDFS， S3， GFS， NFS， Ceph 等）。 状态的持久化存储，通常是分布式文件系统（例如 HDFS， S3， GFS， NFS， Ceph 等） 2. 启用和配置检查点默认情况下，检查点被禁用。要启用检查点，要在 StreamExecutionEnvironment 上调用 enableCheckpointing（n），其中n是检查点时间间隔（以毫秒为单位）。 检查点的其他参数包括： (1) exactly-once 与 at-least-once：你可以选择性的将模式传递给 enableCheckpointing（n） 方法来在两个保证级别之间进行选择。对于大多数应用来说，一般都选择 exactly-once。at-least-once可能与某些超低延迟（持续几毫秒）的应用程序有关。 (2) 检查点超时：如果在规定时间之前没有完成检查点，正在进行的检查点就会被终止。 (3) 检查点之间的最小时间：为了确保流式应用程序在检查点之间有一定的进展，可以定义检查点之间的时间间隔。例如，如果此值设置为5000，不论检查点持续时间和检查点间隔是多少，下一个检查点将在上一个检查点完成之后的5秒内启动。请注意，这意味着检查点间隔 checkpoint interval 永远不会小于此参数。 通过定义 检查点之间的时间差 (time between checkpoints)而不是检查点间隔(checkpoint interval)来配置应用程序通常更容易，因为 检查点之间的时间差 不会受到检查点有时花费比平均时间更长时间的影响（例如，如果目标存储系统暂时比较慢）。 请注意，这个值也意味着并发检查点的数量为1。 (4) 并发检查点的数量：默认情况下，当一个检查点正在运行时，系统不会触发另一个检查点。这确保了拓扑结构不会在检查点上花费太多时间，并且不会在处理流时有进展(not make progress with processing the streams)。可以允许多个重叠的检查点，这对于具有一定处理延迟（例如，因为函数调用外部服务需要等待一些时间响应），但是仍然想要做非常频繁的 checkpoints（100毫秒 ）重新处理很少见的失败情况具有一定意义。 定义检查点之间的最短时间时，不能使用此选项。 (5) 外部检查点externalized checkpoints：可以配置定期检查点持久化到从外部存储中。外部检查点将其元数据写入持久性存储，作业失败时也不会自动清理。这样，如果你的作业失败，你将会有一个检查点用来恢复。有关外部检查点的部署说明中有更多详细信息。 Java版本:StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()// start a checkpoint every 1000 msenv.enableCheckpointing(1000)// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig.setCheckpointTimeout(60000)// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig.setMaxConcurrentCheckpoints(1) 3. 相关配置选项其他参数和默认值也可以通过conf/flink-conf.yaml配置文件进行设置（请参阅完整指南的配置）： (1) state.backend：如果启用了检查点，用来存储算子状态检查点的终端。支持的终端： jobmanager：内存状态，备份到 JobManager/ZooKeeper 的内存中。应在较小状态（Kafka偏移量）或测试和本地调试时使用。 文件系统：状态存储在 TaskManager 的内存中，状态快照存储在文件系统中。Flink支持所有文件系统，例如 HDFS，S3，… (2) state.backend.fs.checkpointdir：用于在 Flink 支持的文件系统中存储检查点的目录。注意：JobManager 必须可以访问状态终端，本地安装时可以使用file：//。 (3) state.backend.rocksdb.checkpointdir: 用于存储 RocksDB 文件的本地目录，或由系统目录分隔符（例如Linux/Unix上的’：’（冒号））分隔的目录列表。（默认值是taskmanager.tmp.dirs） (4) state.checkpoints.dir: 外部检查点元数据的目标目录。 (5) state.checkpoints.num-retained: 已完成的检查点实例的数量。如果最新的检查点已损坏，必须使用多个实例才可以恢复回退到较早的检查点。（默认值：1） 4. 选择状态终端Flink 的检查点机制存储定时器中所有状态和有状态算子的一致性快照，包括连接器，窗口以及任何用户自定义的状态。检查点存储的位置（例如，JobManager 的内存，文件系统，数据库）取决于状态终端的配置。 默认情况下，状态保存在 TaskManager 的内存中，检查点存储在 JobManager 的内存中。为了适当地存储较大的状态，Flink 也支持多种方法在其他状态终端存储状态以及对状态进行检查点操作。状态终端的选择可以通过 StreamExecutionEnvironment.setStateBackend（...） 来配置。 有关可用状态终端以及作业范围和群集范围内配置选项的的详细信息，请参阅状态终端。 5. 迭代作业中的状态检查点目前 Flink 只为无迭代作业提供处理保证。在迭代作业上启用检查点会导致异常。为了在迭代程序上强制进行检查点操作，用户需要在启用检查点时设置特殊标志：env.enableCheckpointing（interval，force = true）。 6. 重启策略Flink支持不同的重启策略，控制在失败情况下重启的方式。有关更多信息，请参阅重启策略。 备注: Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 状态终端]]></title>
    <url>%2Fflink-stream-state-backends.html</url>
    <content type="text"><![CDATA[1. 概述Flink 提供了不同的状态终端，可以指定状态的存储方式和位置。 状态可以存储在Java的堆内或堆外。根据你的状态终端，Flink 也可以管理应用程序的状态，这意味着 Flink 可以处理内存管理（可能会溢出到磁盘，如果有必要），以允许应用程序存储非常大的状态。默认情况下，配置文件 flink-conf.yaml 为所有Flink作业决定其状态终端。 但是，默认的状态终端配置也可以被每个作业的配置覆盖，如下所示。 Java版本:StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(...); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()env.setStateBackend(...) 2. 可用的状态终端开箱即用，Flink 内置了如下状态终端： MemoryStateBackend FsStateBackend RocksDBStateBackend 如果没有配置，系统默认使用MemoryStateBackend。 2.1 MemoryStateBackendMemoryStateBackend 将数据以对象的形式保存在 Java 堆上。键值对状态和窗口算子拥有保存值，触发器等的哈希表。 在进行检查点操作时，状态终端对状态进行快照，并将其作为检查点确认消息的一部分发送给 JobManager（master），并将存储在其堆上。 MemoryStateBackend 可以配置为使用异步快照。尽管我们强烈建议使用异步快照来避免阻塞管道，但请注意，这是一项新功能，目前默认情况下不会启用。要启用此功能，用户可以在实例化 MemoryStateBackend的构造函数中设置相应的布尔值 true，例如：new MemoryStateBackend(MAX_MEM_STATE_SIZE, true); MemoryStateBackend 的使用限制： 每个单独状态的大小默认限制为5 MB。这个值可以在 MemoryStateBackend 的构造函数中增加。 不考虑配置的最大状态大小，状态不能大于akka frame大小。 聚合状态必须能够放进 JobManager 内存中。 MemoryStateBackend 适用场景： 本地开发和调试 只存储较小状态的作业，例如只包含 record-at-a-time 函数的作业（Map，FlatMap，Filter，…）。 Kafka消费者只需要很少的状态。 2.2 FsStateBackendFsStateBackend 使用文件系统URL（类型，地址，路径）进行配置，如 hdfs://namenode:40010/flink/checkpoints 或 file:///data/flink/checkpoints。 FsStateBackend 将正在使用的数据保存在 TaskManager 的内存中。在进行检查点操作时，将状态快照写入配置的文件系统文件和目录中。较小的元数据存储在 JobManager 的内存中（或者在高可用性模式下，存储在元数据检查点中）。 FsStateBackend 默认使用异步快照，以避免在写入状态检查点时阻塞处理管道。如果要禁用此功能，用户可以在实例化 FsStateBackend 的构造函数中将对应的布尔值设置为 false，例如：new FsStateBackend（path，false）; FsStateBackend 适用场景： 具有大状态，长窗口，大的键/值状态的作业。 所有高可用配置。 2.3 RocksDBStateBackendRocksDBStateBackend 使用文件系统URL（类型，地址，路径）进行配置，例如 hdfs://namenode:40010/flink/checkpoints 或 file:///data/flink/checkpoints。 RocksDBStateBackend 将 正在使用的数据保存在 RocksDB 数据库中，其位于 TaskManager 数据目录下（默认情况下）。进行检查点操作时，整个 RocksDB 数据库进行检查点操作存储到配置的文件系统和目录中。较小的元数据存储在 JobManager 的内存中（或者在高可用性模式下，存储在元数据检查点中）。 RocksDBStateBackend 总是执行异步快照。 RocksDBStateBackend 使用限制： 由于 RocksDB 的JNI桥接API基于 byte []，每个键和每个值支持的最大大小为 2^31 个字节。重要的是在 RocksDB 中使用合并操作的状态（例如ListState）可以累积超过2^31字节，然后在下一次检索时会失败。目前这是 RocksDB JNI 的限制。 RocksDBStateBackend 适用场景： 具有非常大的状态，长时间窗口，大键/值状态的作业。 所有高可用配置。 请注意，你可以保存的状态数量仅受可用磁盘空间的限制。与保存状态到内存的 FsStateBackend 相比，这可以保存非常大的状态。但是，这也意味着在这个状态终端下可以达到的最大吞吐量将会降低。 RocksDBStateBackend 是目前唯一个提供增量检查点的终端（见这里）。 3. 配置状态终端如果你不指定，默认的状态终端是 jobmanager。如果你希望为集群中的所有作业建立不同的默认值，可以在 flink-conf.yaml 中定义一个新的默认状态终端来完成。默认的状态终端可以被每个作业的配置覆盖，如下所示。 3.1 设置每个作业的状态终端作业状态终端在作业的 StreamExecutionEnvironment 上设置，如下例所示： Java版本:StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints")); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()env.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints")) 3.2 设置默认状态终端可以使用配置键 state.backend 在 flink-conf.yaml 配置文件中配置默认状态终端。 配置的值可以是 jobmanager（MemoryStateBackend），filesystem（FsStateBackend），rocksdb（RocksDBStateBackend），或实现状态终端工厂 FsStateBackendFactory 类的全限定类名，例如 RocksDBStateBackend 的 org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory。 如果默认状态终端设置为 filesystem，state.backend.fs.checkpointdir 定义了检查点数据存储目录。 配置文件中的示例部分可能如下所示：# The backend that will be used to store operator state checkpointsstate.backend: filesystem# Directory for storing checkpointsstate.backend.fs.checkpointdir: hdfs://namenode:40010/flink/checkpoints 备注: Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/state_backends.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 如何使用状态]]></title>
    <url>%2Fflink-stream-working-with-state.html</url>
    <content type="text"><![CDATA[1. Keyed State 与 Operator StateFlink有两种基本的状态：Keyed State和Operator State。 1.1 Keyed StateKeyed State总是与key相关，只能在与KeyedStream相关的函数和算子中使用。 KeyedStream 继承 DataStream，表示根据指定的key进行分组的数据流。使用DataStream提供的KeySelector根据key对其上的算子State进行分区。DataStream支持的典型操作也可以在KeyedStream上进行，除了诸如shuffle，forward和keyBy之类的分区方法之外。 KeyedStream可以通过调用DataStream.keyBy()来获得。而在KeyedStream上进行任何transformation都将转变回DataStream。 你可以将 Keyed State 视为已经分区或分片的Operator State，每个 key 对应一个状态分区。每个Keyed State在逻辑上只对应一个 &lt;并行算子实例，key&gt;，并且由于每个 key “只属于” 一个Keyed Operator的一个并行实例，我们可以简单地认为成 &lt;operator，key&gt;。 Keyed State 被进一步组织成所谓的 Key Group。Key Group 是 Flink 可以分配 Keyed State 的最小原子单位；Key Group的数量与最大并行度一样多。在执行期间，Keyed Operator的每个并行实例都与一个或多个Key Group的key一起工作。 1.2 Operator State使用Operator State (或非Keyed State)，每个算子状态都绑定到一个并行算子实例。Kafka Connector 是在Flink中使用算子状态的一个很好的例子。Kafka消费者的每个并行实例都要维护一个topic分区和偏移量的map作为其Operator State。 在并行度发生变化时，Operator State接口支持在并行算子实例之间进行重新分配状态。可以有不同的方案来处理这个重新分配。 2. Raw State 与 Managed StateKeyed State和Operator State以两种形式存在：托管状态Managed State和原生状态Raw State。 Managed State由Flink RunTime控制的数据结构表示，如内部哈希表或RocksDB。例如ValueState，ListState等。Flink RunTime对状态进行编码并将它们写入检查点。 Raw State是指算子保留在它们自己数据结构中的状态。当 Checkpoint 时，他们只写入一个字节序列到检查点中。Flink对状态的数据结构一无所知，只能看到原始字节。 所有数据流函数都可以使用Managed State，但Raw State接口只能在实现算子时使用。建议使用Managed State（而不是Raw State），因为在Managed State下，Flink可以在并行度发生变化时自动重新分配状态，并且还可以更好地进行内存管理。 如果你的Managed State需要自定义序列化逻辑，请参阅相应的指南以确保将来的兼容性。Flink的默认序列化器不需要特殊处理。 3. 使用Managed Keyed StateManaged Keyed State接口提供了对不同类型状态的访问，这些状态的作用域为当前输入元素的key。这意味着这种类型的状态只能用于KeyedStream，可以通过stream.keyBy（...）创建。 现在，我们先看看可用状态的不同类型，然后我们会看到如何在程序中使用。可用状态有： ValueState &lt;T&gt;：保存了一个可以更新和检索的值（如上所述，作用域为输入元素的key，所以每个key可能对应一个值）。该值可以使用update（T）来更新，使用T value（）来检索。 ListState &lt;T&gt;：保存了一个元素列表。可以追加元素并检索当前存储的所有元素的Iterable。使用add（T）添加元素，可以使用Iterable &lt;T&gt; get（）来检索Iterable。 ReducingState &lt;T&gt;：保存一个单一的值，表示添加到状态所有值的聚合。接口与ListState相同，但使用add（T）添加的元素时需要指定ReduceFunction。 AggregatingState &lt;IN，OUT&gt;：保存一个单一的值，表示添加到状态所有值的聚合。与ReducingState不同，聚合后的类型可能与添加到状态的元素类型不同。接口与ListState相同，但使用add（IN）添加到状态的元素使用指定的AggregateFunction进行聚合。 FoldingState &lt;T，ACC&gt;：保存一个单一的值，表示添加到状态所有值的聚合。与ReducingState不同，聚合后类型可能与添加到状态的元素类型不同。接口与ListState相同，但使用add（T）添加到状态的元素使用指定FoldFunction。 MapState ：保存了一个映射列表。可以将键值对放入状态，并检索当前存储的所有映射的Iterable。使用put（UK，UV）或putAll（Map &lt;UK，UV&gt;）添加映射。与用户key相关的值可以使用get（UK）来检索。映射，键和值的迭代视图可分别使用entries（），keys（）和values（）来检索。 所有类型的状态都有一个clear（）方法，它清除了当前活跃key的状态，即输入元素的key。 FoldingState和FoldingStateDescriptor已经在Flink 1.4中被弃用，将来会被彻底删除。请改用AggregatingState和AggregatingStateDescriptor。 请记住，这些状态对象仅能用于状态接口。状态没有必要一定存储在内存中，也可以保存在磁盘或其他地方。第二件要记住的是，你从状态获取的值取决于输入元素的key。因此，如果所使用的key不同，那你在一次用户函数调用中获得的值可能与另一次调用的不同。 为了得到一个状态句柄，你必须创建一个StateDescriptor。它包含了状态的名字（我们将在后面看到，你可以创建多个状态，必须有唯一的名称，以便引用它们），状态值的类型，以及用户自定义函数，如ReduceFunction。根据要检索的状态类型，你可以创建一个ValueStateDescriptor，ListStateDescriptor，ReducingStateDescriptor，FoldingStateDescriptor或MapStateDescriptor。 使用RuntimeContext来访问状态，所以只能在Rich函数中使用。请参阅这里了解有关信息，我们会很快看到一个例子。 在RichFunction中可用的RuntimeContext具有下面访问状态的方法： ValueState getState(ValueStateDescriptor) ReducingState getReducingState(ReducingStateDescriptor) ListState getListState(ListStateDescriptor) AggregatingState getAggregatingState(AggregatingState) FoldingState getFoldingState(FoldingStateDescriptor) MapState getMapState(MapStateDescriptor) 下面是FlatMapFunction的一个例子： Java版本:public class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; &#123; /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; @Override public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; // access the state value Tuple2&lt;Long, Long&gt; currentSum = sum.value(); // update the count 个数 currentSum.f0 += 1; // add the second field of the input value 总和 currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 &gt;= 2) &#123; out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); &#125; &#125; @Override public void open(Configuration config) &#123; ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( "average", // the state name TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set sum = getRuntimeContext().getState(descriptor); &#125;&#125;// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(0) .flatMap(new CountWindowAverage()) .print();// the printed output will be (1,4) and (1,5) Scala版本:class CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] &#123; private var sum: ValueState[(Long, Long)] = _ override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = &#123; // access the state value val tmpCurrentSum = sum.value // If it hasn&apos;t been used before, it will be null val currentSum = if (tmpCurrentSum != null) &#123; tmpCurrentSum &#125; else &#123; (0L, 0L) &#125; // update the count val newSum = (currentSum._1 + 1, currentSum._2 + input._2) // update the state sum.update(newSum) // if the count reaches 2, emit the average and clear the state if (newSum._1 &gt;= 2) &#123; out.collect((input._1, newSum._2 / newSum._1)) sum.clear() &#125; &#125; override def open(parameters: Configuration): Unit = &#123; sum = getRuntimeContext.getState( new ValueStateDescriptor[(Long, Long)](&quot;average&quot;, createTypeInformation[(Long, Long)]) ) &#125;&#125;object ExampleCountWindowAverage extends App &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment env.fromCollection(List( (1L, 3L), (1L, 5L), (1L, 7L), (1L, 4L), (1L, 2L) )).keyBy(_._1) .flatMap(new CountWindowAverage()) .print() // the printed output will be (1,4) and (1,5) env.execute(&quot;ExampleManagedState&quot;)&#125; 这个例子实现了一个穷人的计数窗口。我们通过第一个字段键入元组（在这个例子中都有相同的key为1）。该函数将计数和总和存储在ValueState中。一旦计数达到2，就输出平均值并清除状态，以便我们从0开始。注意，如果我们元组第一个字段具有不同值，那将为每个不同的输入key保持不同的状态值。 3.1 Scala DataStream API中的状态除了上面介绍的接口之外，Scala API还具有在KeyedStream上使用单个ValueState的有状态map（）或flatMap（）函数的快捷方式。用户函数可以在Option获取ValueState的当前值，并且必须返回将用于更新状态的更新值。val stream: DataStream[(String, Int)] = ...val counts: DataStream[(String, Int)] = stream .keyBy(_._1) .mapWithState((in: (String, Int), count: Option[Int]) =&gt; count match &#123; case Some(c) =&gt; ( (in._1, c), Some(c + in._2) ) case None =&gt; ( (in._1, 0), Some(in._2) ) &#125;) 4. 使用Managed Operator State要使用Managed Operator State，有状态函数可以实现更通用的CheckpointedFunction接口或ListCheckpointed &lt;T extends Serializable&gt;接口。 4.1 CheckpointedFunctionCheckpointedFunction接口提供了对有不同的重分配方案的非Keyed State的访问。它需要实现一下两种方法：void snapshotState(FunctionSnapshotContext context) throws Exception;void initializeState(FunctionInitializationContext context) throws Exception; 每当执行 Checkpoint 时，会调用snapshotState（）方法。每当用户自定义函数被初始化时，或当函数第一次初始化时，或者当函数从之前的检查点恢复时，initializeState（）方法被调用。鉴于此，initializeState（）不仅是初始化不同类型的状态的地方，而且还包括状态恢复逻辑的地方。 目前支持列表式的Managed Operator State。状态应该是一个可序列化的对象列表，相互间彼此独立，因此可以在扩展时重新分配。换句话说，这些对象可以在非Keyed State中重新分配比较细的粒度。根据状态访问方法，定义了以下重新分配方案： Even-split redistribution: 每个算子都返回一个状态元素的列表。状态是逻辑上所有列表的连接。在恢复/重新分配时，列表被均分成与并行算子一样多的子列表。每个算子都可以得到一个可能为空或者包含一个或多个元素的子列表。例如，如果并行度为1，一个算子的检查点状态包含元素element1和element2，将并行度增加到2时，element1在算子实例0上运行，而element2将转至算子实例1。 Union redistribution: 每个算子都返回一个状态元素列表。状态是逻辑上所有列表的连接。在恢复/重新分配时，每个算子都可以获得全部的状态元素列表。 下面是一个有状态的SinkFunction的例子，它使用CheckpointedFunction在将元素输出到外部之前进行缓冲元素。它演示了基本的均分再分配列表状态： Java版本:public class BufferingSink implements SinkFunction&lt;Tuple2&lt;String, Integer&gt;&gt;, CheckpointedFunction, CheckpointedRestoring&lt;ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; &#123; private final int threshold; private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState; private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements; public BufferingSink(int threshold) &#123; this.threshold = threshold; this.bufferedElements = new ArrayList&lt;&gt;(); &#125; @Override public void invoke(Tuple2&lt;String, Integer&gt; value) throws Exception &#123; bufferedElements.add(value); if (bufferedElements.size() == threshold) &#123; for (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123; // send it to the sink &#125; bufferedElements.clear(); &#125; &#125; @Override public void snapshotState(FunctionSnapshotContext context) throws Exception &#123; checkpointedState.clear(); for (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123; checkpointedState.add(element); &#125; &#125; @Override public void initializeState(FunctionInitializationContext context) throws Exception &#123; ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor = new ListStateDescriptor&lt;&gt;( "buffered-elements", TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;)); checkpointedState = context.getOperatorStateStore().getListState(descriptor); if (context.isRestored()) &#123; for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123; bufferedElements.add(element); &#125; &#125; &#125; @Override public void restoreState(ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; state) throws Exception &#123; // this is from the CheckpointedRestoring interface. this.bufferedElements.addAll(state); &#125;&#125; Scala版本:class BufferingSink(threshold: Int = 0) extends SinkFunction[(String, Int)] with CheckpointedFunction with CheckpointedRestoring[List[(String, Int)]] &#123; @transient private var checkpointedState: ListState[(String, Int)] = _ private val bufferedElements = ListBuffer[(String, Int)]() override def invoke(value: (String, Int)): Unit = &#123; bufferedElements += value if (bufferedElements.size == threshold) &#123; for (element &lt;- bufferedElements) &#123; // send it to the sink &#125; bufferedElements.clear() &#125; &#125; override def snapshotState(context: FunctionSnapshotContext): Unit = &#123; checkpointedState.clear() for (element &lt;- bufferedElements) &#123; checkpointedState.add(element) &#125; &#125; override def initializeState(context: FunctionInitializationContext): Unit = &#123; val descriptor = new ListStateDescriptor[(String, Int)]( &quot;buffered-elements&quot;, TypeInformation.of(new TypeHint[(String, Int)]() &#123;&#125;) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) if(context.isRestored) &#123; for(element &lt;- checkpointedState.get()) &#123; bufferedElements += element &#125; &#125; &#125; override def restoreState(state: List[(String, Int)]): Unit = &#123; bufferedElements ++= state &#125;&#125; initializeState方法有一个FunctionInitializationContext参数。这用来初始化非keyed state“容器”。这是一个ListState类型的容器，非keyed state对象将在检查点时存储。 注意一下状态是如何被初始化，类似于keyed state状态，使用包含状态名称和状态值类型相关信息的StateDescriptor： Java版本:ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor = new ListStateDescriptor&lt;&gt;( "buffered-elements", TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));checkpointedState = context.getOperatorStateStore().getListState(descriptor); Scala版本:val descriptor = new ListStateDescriptor[(String, Long)]( &quot;buffered-elements&quot;, TypeInformation.of(new TypeHint[(String, Long)]() &#123;&#125;))checkpointedState = context.getOperatorStateStore.getListState(descriptor) 状态访问方法的命名约定包含其重新分配模式及其状态结构。 例如，要使用带有联合重新分配方案的列表状态进行恢复，请使用getUnionListState（descriptor）访问状态。如果方法名称不包含重新分配模式，例如 getListState（descriptor），这表示使用基本的均分重分配方案。 在初始化容器之后，我们使用上下文的isRestored（）方法来检查失败后是否正在恢复。如果是，即我们正在恢复，将会应用恢复逻辑。 如修改后的BufferingSink的代码所示，在状态初始化期间恢复的这个ListState被保存在类变量中，以备将来在snapshotState（）中使用。 在那里ListState清除了前一个检查点包含的所有对象，然后用我们想要进行检查点的新对象填充。 Keyed State也可以在initializeState（）方法中初始化。这可以使用提供的FunctionInitializationContext完成。 4.2 ListCheckpointedListCheckpointed接口是CheckpointedFunction进行限制的一种变体，它只支持在恢复时使用均分再分配方案的列表样式状态。还需要实现以下两种方法： List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;void restoreState(List&lt;T&gt; state) throws Exception; snapshotState()方法应该返回一个对象列表来进行checkpoint，而restoreState()方法在恢复时必须处理这样一个列表。如果状态是不可重分区的，则可以在snapshotState()中返回一个Collections.singletonList(MY_STATE)。 4.2.1 Stateful Source Functions与其他算子相比，有状态的数据源需要得到更多的关注。为了能更新状态以及输出集合的原子性（在失败/恢复时需要一次性语义），用户需要从数据源的上下文中获取锁。 Java版本:public static class CounterSource extends RichParallelSourceFunction&lt;Long&gt; implements ListCheckpointed&lt;Long&gt; &#123; /** current offset for exactly once semantics */ private Long offset; /** flag for job cancellation */ private volatile boolean isRunning = true; @Override public void run(SourceContext&lt;Long&gt; ctx) &#123; final Object lock = ctx.getCheckpointLock(); while (isRunning) &#123; // output and state update are atomic synchronized (lock) &#123; ctx.collect(offset); offset += 1; &#125; &#125; &#125; @Override public void cancel() &#123; isRunning = false; &#125; @Override public List&lt;Long&gt; snapshotState(long checkpointId, long checkpointTimestamp) &#123; return Collections.singletonList(offset); &#125; @Override public void restoreState(List&lt;Long&gt; state) &#123; for (Long s : state) offset = s; &#125;&#125; Scala版本:class CounterSource extends RichParallelSourceFunction[Long] with ListCheckpointed[Long] &#123; @volatile private var isRunning = true private var offset = 0L override def run(ctx: SourceFunction.SourceContext[Long]): Unit = &#123; val lock = ctx.getCheckpointLock while (isRunning) &#123; // output and state update are atomic lock.synchronized(&#123; ctx.collect(offset) offset += 1 &#125;) &#125; &#125; override def cancel(): Unit = isRunning = false override def restoreState(state: util.List[Long]): Unit = for (s &lt;- state) &#123; offset = s &#125; override def snapshotState(checkpointId: Long, timestamp: Long): util.List[Long] = Collections.singletonList(offset)&#125; 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 状态概述]]></title>
    <url>%2Fflink-stream-state-overview.html</url>
    <content type="text"><![CDATA[有状态的函数和算子在处理单个元素/事件时存储数据，使得状态state成为任何精细操作的关键构件。 例如： 当应用程序搜索某些特定模式事件时，状态将存储迄今为止遇到的事件序列。 当按每分钟/小时/天聚合事件时，状态保存待处理的聚合事件。 在数据流上训练机器学习模型时，状态保存当前版本的模型参数。 当需要管理历史数据时，状态允许访问过去发生的事件。 Flink 需要了解状态，以便使用检查点进行状态容错，并允许流应用程序使用保存点。 对状态进行了解有助于你对 Flink 应用程序进行扩展，这意味着 Flink 负责在并行实例之间进行重新分配状态。 Flink 的可查询状态queryable state功能允许你在 Flink 运行时在外部访问状态。 在使用状态时，阅读有关Flink的 State Backends 应该对你很有帮助。Flink 提供不同的 State Backends，并指定状态的存储方式和位置。状态可以位于Java的堆内或堆外。根据你的 State Backends，Flink也可以管理应用程序的状态，这意味着Flink进行内存管理(可能会溢写到磁盘，如果有必要)，以允许应用程序保持非常大的状态。State Backends可以在不更改应用程序逻辑的情况下进行配置。 下一步 使用状态：显示如何在Flink应用程序中使用状态，并解释不同类型的状态。 检查点：描述如何启用和配置容错检查点。 可查询状态：解释如何在Flink运行时从外部访问状态。 为Managed State自定义序列化：讨论为状态自定义序列化逻辑及其升级。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 内置的时间戳提取器和Watermark生成器]]></title>
    <url>%2Fflink-stream-event-timestamp-and-extractors.html</url>
    <content type="text"><![CDATA[如Flink1.4 生成时间戳与Watermarks所介绍的，Flink提供了一个抽象类，允许程序员可以分配自己的时间戳并发送Watermark。更具体地说，可以通过AssignerWithPeriodicWatermarks或AssignerWithPunctuatedWatermarks接口来实现，具体实现取决于用户具体情况。第一个接口将周期性的发送Watermark，第二个则基于传入记录的某些属性发送Watermark，例如，当在流中遇到特殊元素时。 为了进一步缓解这些任务的编程工作，Flink带有一些内置的时间戳分配器。除了开箱即用的功能外，它们的实现也可以作为自定义实现的一个例子。 1. 递增时间戳分配器周期性生成Watermark最简单的例子是给定数据源任务中的时间戳会递增顺序出现。在这种情况下，由于没有时间戳比当前时间戳还早到达的，所以当前时间戳可以始终充当Watermark。 请注意，每个并行数据源任务的时间戳必须是升序的。例如，如果在特定设置中，一个并行数据源实例读取一个Kafka分区，那么只需要确保在每个Kafka分区内时间戳是升序的即可。每当并行数据流被shuffle，union，连接或合并时，Flink的Watermark合并机制能够产生正确的watermarks。 Java版本:DataStream&lt;MyEvent&gt; stream = ...DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;MyEvent&gt;() &#123; @Override public long extractAscendingTimestamp(MyEvent element) &#123; return element.getCreationTime(); &#125;&#125;); Scala版本:val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime ) 2. 允许固定数量延迟的分配器周期性生成Watermark的另一个例子是当Watermark落后于数据流中看到的最大时间戳(事件时间)一固定数量时间(a fixed amount of time)。这种情况涵盖了事先知道流中可能遇到的最大延迟的场景，例如，当创建一个测试用的自定义数据源时，其上每个元素的时间戳分布在一个固定时间段内。对于这些情况，Flink提供了BoundedOutOfOrdernessTimestampExtractor，带有一个maxOutOfOrderness参数，即在计算给定窗口最终结果一个元素在被忽略之前允许延迟的最大时间。延迟对应于t-t_w的结果，其中t是元素的(事件时间)时间戳，t_w是前一个Watermark时间戳。如果延迟大于0，则该元素被认为是迟到的，并且在计算其相应窗口的作业结果时默认为忽略该元素。 Java版本:DataStream&lt;MyEvent&gt; stream = ...DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;MyEvent&gt;(Time.seconds(10)) &#123; @Override public long extractTimestamp(MyEvent element) &#123; return element.getCreationTime(); &#125;&#125;); Scala版本:val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime )) 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 图解Watermark]]></title>
    <url>%2Fflink-stream-graphic-watermark.html</url>
    <content type="text"><![CDATA[如果你正在构建实时流处理应用程序，那么事件时间处理是你迟早必须使用的功能之一。因为在现实世界的大多数用例中，消息到达都是无序的，应该有一些方法，通过你建立的系统知道消息可能延迟到达，并且有相应的处理方案。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中使用它。 EventTime是事件在现实世界中发生的时间，ProcessingTime是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于处理时间的系统，看看它的缺点。 我们创建一个大小为10秒的滑动窗口，每5秒滑动一次，在窗口结束时，系统将发送在此期间收到的消息数。 一旦了解了EventTime处理在滑动窗口如何工作，那么了解其在滚动窗口中如何工作也就不是难事。所以让我们开始吧。 1. 基于处理时间的系统在这个例子中，我们期望消息具有一定格式的值，时间戳就是消息的那个值，同时时间戳是在源产生此消息的时间。由于我们正在构建基于处理时间的系统，因此以下代码忽略了时间戳部分。 我们需要知道消息中应包含消息产生时间是很重要的。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己生成这个产生时间。稍后我们将看到，事件时间处理提取此时间戳信息来处理延迟消息。 val text = senv.socketTextStream(&quot;localhost&quot;, 9999)val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125; .keyBy(0) .timeWindow(Time.seconds(10), Time.seconds(5)) .sum(1)counts.printsenv.execute(&quot;ProcessingTime processing example&quot;) 1.1 消息无延迟到达假设源分别在第13秒产生两个类型a的消息以及在第16秒产生一个消息。(小时和分钟不重要，因为窗口大小只有10秒)。 这些消息将落入如下所示窗口中。前两个在第13秒产生的消息将落入窗口1[5s-15s]和窗口2[10s-20s]中，第三个在第16秒产生的消息将落入窗口2[10s-20s]和窗口3[15s-25s]中。每个窗口得到的最终计数分别为(a，2)，(a，3)和(a，1)。 该输出跟预期的输出是一样的。现在我们看看当一个消息延迟到达系统时会发生什么。 1.2 消息延迟到达现在假设其中一条消息(在第13秒产生)可能由于网络拥塞延迟6秒(第19秒到达)。你能猜测出这个消息会落入哪个窗口？ 延迟的消息落入窗口2和窗口3中，因为19在10-20和15-25之间。窗口2的计算没有任何问题(因为消息本应该落入这个窗口)，但是它影响了窗口1和窗口3的计算结果。现在我们将尝试使用基于EventTime处理来解决这个问题。 2. 基于EventTime的系统要使用基于EventTime处理，我们需要一个时间戳提取器，从消息中提取事件时间信息。请记住，消息是有格式值，时间戳。 extractTimestamp方法获取时间戳并将其作为Long类型返回。现在忽略getCurrentWatermark方法，我们稍后会介绍： class TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable &#123; override def extractTimestamp(e: String, prevElementTimestamp: Long) = &#123; e.split(&quot;,&quot;)(1).toLong &#125; override def getCurrentWatermark(): Watermark = &#123; new Watermark(System.currentTimeMillis) &#125;&#125; 现在我们需要设置这个时间戳提取器，并将TimeCharactersistic设置为EventTime。其余的代码与ProcessingTime的情况保持一致： senv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val text = senv.socketTextStream(&quot;localhost&quot;, 9999) .assignTimestampsAndWatermarks(new TimestampExtractor)val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125; .keyBy(0) .timeWindow(Time.seconds(10), Time.seconds(5)) .sum(1)counts.printsenv.execute(&quot;EventTime processing example&quot;) 运行上述代码的结果如下图所示： 结果看起来更好一些，窗口2和3现在是正确的结果，但是窗口1仍然是有问题的。Flink没有将延迟的消息分配给窗口3，是因为在当前检查消息的事件时间，知道它不应该出现在窗口3中。但是为什么没有将消息分配给窗口1？原因是当延迟的信息到达系统时(第19秒)，窗口1的评估(evaluation)已经完成了(第15秒)。现在让我们尝试通过使用Watermark来解决这个问题。 3. WatermarkWatermark是一个非常重要概念，我将尽力给你一个简短的概述。如果你有兴趣了解更多信息，你可以从Google中观看这个演讲，还可以从dataArtisans那里阅读此博客。 Watermark本质上是一个时间戳。当Flink中的算子(operator)接收到Watermark时，它明白它不会再看到比该时间戳更早的消息。因此Watermark也可以被认为是告诉Flink在EventTime中多远的一种方式。 在这个例子的目的，就是把Watermark看作是告诉Flink一个消息可能延迟多少的方式。在上一次尝试中，我们将Watermark设置为当前系统时间。因此，期望消息没有任何的延迟。现在我们将Watermark设置为当前时间减去5秒，这就告诉Flink我们期望消息最多延迟5秒钟，这是因为每个窗口仅在Watermark通过时被评估。由于我们的Watermark是当前时间减去5秒，所以第一个窗口[5s-15s]将会在第20秒被评估。类似地，窗口[10s-20s]将会在第25秒进行评估，依此类推(译者注:窗口延迟评估)。 override def getCurrentWatermark(): Watermark = &#123; new Watermark(System.currentTimeMillis - 5000)&#125; 这里我们假定事件时间比当前系统时间晚5秒，但事实并非总是如此(有可能6秒，7秒等等)。在许多情况下，最好保留迄今为止收到的最大时间戳(从消息中提取)。使用迄今为止收到的最大时间戳减去预期的延迟时间来代替用当前系统时间减去预期的延迟时间。 进行上述更改后运行代码的结果是： 最后我们得到了正确的结果，所有窗口都按照预期输出计数，(a，2)，(a，3)和(a，1)。 4. Allowed Lateness我们也可以使用AllowedLateness功能设置消息的最大允许延迟时间来解决这个问题。 在我们之前使用Watermark - delay的方法中，只有当Watermark超过window_length + delay时，窗口才会被触发计算。如果你想要适应延迟事件，并希望窗口按时触发，则可以使用Allowed Lateness。 如果设置了允许延迟，Flink不会丢弃消息，除非它超过了window_end_time + delay的延迟时间。一旦收到一个延迟消息，Flink会提取它的时间戳并检查是否在允许的延迟时间内，然后检查是否触发窗口(按照触发器设置)。 因此，请注意，在这种方法中可能会多次触发窗口，如果你仅需要一次处理，你需要使你的sink具有幂等性。 5. 结论实时流处理系统的重要性日益增长，延迟消息的处理是你构建任何此类系统的一部分。在这篇博文中，我们看到延迟到达的消息会影响系统的结果，以及如何使用ApacheFlink的事件时间功能来解决它们。 原文:http://vishnuviswanath.com/flink_eventtime.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 生成时间戳与Watermarks]]></title>
    <url>%2Fflink-stream-event-timestamps-and-watermark.html</url>
    <content type="text"><![CDATA[本节适用于在事件时间上运行的程序。有关事件时间，处理时间和提取时间的介绍，请参阅Flink1.4 事件时间与处理时间。 为了处理事件时间，流处理程序需要相应地设置TimeCharacteristic。 Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Scala版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 1. 分配时间戳为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配事件时间戳。这通常通过访问/提取元素中某个字段的时间戳来完成。时间戳分配与生成watermarks相结合，告诉系统有关事件时间的进度progress。分配时间戳和生成watermarks有两种方法： 直接在数据流源中分配与生成 通过时间戳分配器/watermark生成器：在Flink时间戳分配器中也会定义要发送的watermarks 备注:时间戳和watermarks都是从Java历元1970-01-01T00：00：00Z以来的毫秒数。 1.1 带有时间戳和watermarks的数据源函数流数据源还可以直接为它们产生的元素分配时间戳，并且也可以发送watermarks。如果数据源分配了时间戳，那么就不需要时间戳分配器。 备注:如果继续使用时间戳分配器，将会覆盖数据源提供的时间戳和watermarks。 如果直接向数据源中的元素分配时间戳，数据源必须使用SourceContext上的collectWithTimestamp()方法。如果要生成watermarks，数据源必须调用emitWatermark（Watermark）函数。 以下是分配时间戳并生成watermarks的源(non-checkpointed)的简单示例： Java版本:@Overridepublic void run(SourceContext&lt;MyType&gt; ctx) throws Exception &#123; while (/* condition */) &#123; MyType next = getNext(); ctx.collectWithTimestamp(next, next.getEventTimestamp()); if (next.hasWatermarkTime()) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime())); &#125; &#125;&#125; Scala版本:override def run(ctx: SourceContext[MyType]): Unit = &#123; while (/* condition */) &#123; val next: MyType = getNext() ctx.collectWithTimestamp(next, next.eventTimestamp) if (next.hasWatermarkTime) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime)) &#125; &#125;&#125; 1.2 时间戳分配器/Watermark生成器时间戳分配器接收数据流并产生一个新的数据流，包含带有时间戳的元素和Watermark。如果原始流已经拥有时间戳或watermarks，那么如果使用时间戳分配器将会覆盖它们。 时间戳分配器通常在数据源之后立马指定，但也不是严格遵循这样的原则。例如，一个常见的模式是在时间戳分配器之前需要进行解析(MapFunction)和过滤(FilterFunction)。无论如何，时间戳分配器都需要在第一个基于事件时间的操作(例如第一个窗口操作)之前被指定。但也有特殊情况，当使用Kafka作为流作业的数据源时，Flink允许在数据源(消费者)内部定义时间戳分配器/watermarks生成器。有关如何执行此操作的更多信息，请参见Kafka Connector文档。 备注:本节的其余部分介绍了程序员为了创建自己的时间戳提取器/watermarks生成器而必须实现的主要接口。如果要查看Flink内置的执行器，请参阅[Pre-defined Timestamp Extractors / Watermark Emitters](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html) Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);DataStream&lt;MyEvent&gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo);DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream .filter( event -&gt; event.severity() == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks());withTimestampsAndWatermarks .keyBy( (event) -&gt; event.getGroup() ) .timeWindow(Time.seconds(10)) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter());val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())withTimestampsAndWatermarks .keyBy( _.getGroup ) .timeWindow(Time.seconds(10)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...) 1.2.1 Periodic Watermarks 分配器AssignerWithPeriodicWatermarks分配时间戳并定期生成Watermarks(可能取决于流元素，或纯粹基于处理时间)。 通过ExecutionConfig.setAutoWatermarkInterval()定义Watermarks的时间间隔(每n毫秒)。每次调用分配器的getCurrentWatermark()方法，如果返回的Watermark非null，并且大于先前的Watermark，则会发送(emitted)这个新的Watermarks。 以下是带有周期性Watermark的时间戳分配器的两个简单示例: Java版本:/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */public class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks&lt;MyEvent&gt; &#123; private final long maxOutOfOrderness = 3500; // 3.5 seconds private long currentMaxTimestamp; @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; long timestamp = element.getCreationTime(); currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp); return timestamp; &#125; @Override public Watermark getCurrentWatermark() &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound return new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125;&#125;/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */public class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks&lt;MyEvent&gt; &#123; private final long maxTimeLag = 5000; // 5 seconds @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; return element.getCreationTime(); &#125; @Override public Watermark getCurrentWatermark() &#123; // return the watermark as current time minus the maximum time lag return new Watermark(System.currentTimeMillis() - maxTimeLag); &#125;&#125; Scala版本:/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxOutOfOrderness = 3500L; // 3.5 seconds var currentMaxTimestamp: Long; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; val timestamp = element.getCreationTime() currentMaxTimestamp = max(timestamp, currentMaxTimestamp) timestamp; &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125;&#125;/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxTimeLag = 5000L; // 5 seconds override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current time minus the maximum time lag new Watermark(System.currentTimeMillis() - maxTimeLag) &#125;&#125; 1.2.2 Punctuated Watermarks 分配器每当某个事件表明一个新的Watermarks可能要生成时，需要调用AssignerWithPunctuatedWatermarks方法来生成Watermarks(To generate watermarks whenever a certain event indicates that a new watermark might be generated, use AssignerWithPunctuatedWatermarks)。对于这个类，Flink首先调用extractTimestamp()方法为元素分配时间戳，然后立即调用该元素上的checkAndGetNextWatermark()方法。 把在extractTimestamp()方法中分配的时间戳传递给checkAndGetNextWatermark()方法，并且可以决定是否要生成Watermarks。只要checkAndGetNextWatermark()方法返回非null的Watermark，并且该Watermark比以前最新的Watermark都大，则会发送这个新的Watermark。 Java版本:public class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks&lt;MyEvent&gt; &#123; @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; return element.getCreationTime(); &#125; @Override public Watermark checkAndGetNextWatermark(MyEvent lastElement, long extractedTimestamp) &#123; return lastElement.hasWatermarkMarker() ? new Watermark(extractedTimestamp) : null; &#125;&#125; Scala版本:class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123; if (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null &#125;&#125; 备注:可以在每个单独的事件上生成Watermark。但是，由于每个Watermark在下游引起一些计算，所以过多的Watermark会降低性能。 2. 每个Kafka分区一个时间戳当使用Apache Kafka作为数据源时，每个Kafka分区都可能有一个简单的事件时间模式(时间戳按升序递增或有界无序)。然而，当消费Kafka中的流时，多个分区通常并行消费，来自多个分区的事件会交叉在一起，破坏每个分区模式。 在这种情况下，你可以使用Flink的Kafka分区感知Watermark的生成(Kafka-partition-aware watermark generation)。使用该特性，在Kafka消费者中，每个Kafka分区都生成watermark，并且每个分区的watermark的合并方式与在数据流shuffle上合并方式相同(the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.)。 例如，如果在每个Kafka分区中的事件时间戳严格递增，则使用递增时间戳watermark生成器生成每个分区的watermark，在整体watermark上产生的结果也非常好。 下图显示了如何使用每个Kafka分区生成watermark，以及在这种情况下watermark如何通过流数据流进行传播: Java版本:FlinkKafkaConsumer09&lt;MyType&gt; kafkaSource = new FlinkKafkaConsumer09&lt;&gt;("myTopic", schema, props);kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;MyType&gt;() &#123; @Override public long extractAscendingTimestamp(MyType element) &#123; return element.eventTimestamp(); &#125;&#125;);DataStream&lt;MyType&gt; stream = env.addSource(kafkaSource); Scala版本:val kafkaSource = new FlinkKafkaConsumer09[MyType](&quot;myTopic&quot;, schema, props)kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123; def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp&#125;)val stream: DataStream[MyType] = env.addSource(kafkaSource) 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exactly once 未必严格一次]]></title>
    <url>%2Fexactly-once-is-not-necessarily-exactly-once.html</url>
    <content type="text"><![CDATA[分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中”严格一次（Exactly-once）” 是很多引擎追求的目标之一，很多 SPE 均宣称可提供”严格一次”的处理语义。 然而exactly-once具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用exactly-once来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在exactly-once处理语义方面的差异，以及为什么exactly-once更适合称之为有效一次(Effectively-once)。同时本文还将探讨在实现所谓exactly-once的语义过程中，各类常用技术之间需要进行的取舍。 1. 背景流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图，大部分情况（但也并非总是如此）下也可以看作有向无环图（Directed acyclic graph，DAG）。在这种图中，每个边（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边的数据或事件的算子（Operator）。有两种特殊类型的顶点，通常称之为 Source 和 Sink，Source 会消耗外部数据/事件并将其注入应用程序，而Sink通常负责收集应用程序生成的结果。图1展示了这样的一个流应用程序范例。 执行流/事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：最多一次（At-most-once）、最少一次（At-least-once），以及严格一次（Exactly-once）。这些不同处理语义可粗略理解如下： 2. 最多一次这其实是一种”尽力而为”的方法。数据或事件可以保证被应用程序中的所有算子最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图2列举了一个范例。 3. 最少一次数据或事件可保证被应用程序图中的所有算子最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为”最少一次”。图3展示了一个范例。在本例中，第一个算子最初处理事件时失败了，随后重试并成功，然后第二次重试并再次成功，然而第二次重试实际上是不必要的。 3. 严格一次事件可保证被流应用程序中的所有算子“严格一次”处理，哪怕遇到各种失败。为了实现exactly-once处理语义，通常主要会使用下列两种机制： 分布式快照/状态检查点 最少一次事件交付，外加消息去重 通过分布式快照/状态检查点方法实现的exactly-once是由 Chandy-Lamport 分布式快照算法启发而来的。在这种机制中，会定期为流应用程序中每个算子的所有状态创建检查点，一旦系统中任何位置出现失败，每个算子的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被回退到最新一致状态，并从该状态开始重新处理。图4展示了这种机制的一些基本概念。 在图4中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，算子在处理传入的数据时失败了。此时 S = 4 这个状态值已经被保存到持久存储中，而 S = 12 状态值正位于算子的内存中。为了解决这种差异，在 T3 时处理图将状态回退至 S = 4，并”重播”流中每个连续状态直到最新状态，并处理每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。 实现exactly-once的另一种方法是在实现至少一次事件交付的同时在每个算子一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个算子中移除重复的事件，随后才将结果事件发送给用户在算子中定义的逻辑。这种机制要求为每个算子保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel 以及 Apache Kafka Streams 等机制。图 5 展示了这种机制的概况。 4. 严格一次真的就一次吗？接着重新考虑一下exactly-once处理语义实际上能为最终用户提供怎样的保证。exactly-once这样的标签对于描述严格一次起到了一定的误导效果。 有些人可能认为exactly-once描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。但实际上没有一个 SPE 能完全保证只处理一次。面对各种可能的失败，根本不可能保证每个算子中由用户自定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。 假设这样一个场景：有个流处理算子需要执行 Map 操作输出传入事件的 ID，随后原样返回事件。例如这个操作可能使用了如下的虚构代码：Map (Event event) &#123; Print &quot;Event ID: &quot; + event.getId() Return event&#125; 每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑严格一次执行可以得到保证，那么事件 ID 只输出一次。然而这一点永远无法保证，因为用户自定义的逻辑执行过程中可能随时随地面临失败。SPE 无法自行判断用户自定义的逻辑到底执行到哪一步了。因此任何用户自定义逻辑都无法保证只执行一次。这也意味着用户自定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。 那么当 SPE 宣称提供exactly-once的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称exactly-once处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。 上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个算子的状态，并自动提交状态更新。对于机制1（分布式快照/状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个算子的状态检查点）；对于机制2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个算子的状态以及每个算子追踪已经被成功处理过事件的事务日志。 状态提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次(The committing of state or applying updates to the durable backend that is the source of truth can be described as occurring exactly-once)。然而在计算状态的更新/改动，例如处理在事件上执行用户自定义逻辑的事件，但是如果出现失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的效果只会在持久后端状态存储中体现一次。因此在这里我们认为”有效一次（Effectively-once）”术语可以更精确地描述这样的处理语义。 5. 分布式快照与至少一次事件交付外加去重机制的对比从语义的角度来看，分布式快照以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，有可能会产生明显的性能差异。 基于机制1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在流应用程序正常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率会更高一点，这会导致 SPE 需要暂停应用程序并回滚所有算子的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。 机制2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被算子的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个算子中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何算子遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个算子遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他算子的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格: 分布式快照/状态检查点 利 弊 性能和资源开销小 从失败中恢复时的性能影响大 随着拓扑规模的增大，对性能的潜在影响增高 至少一次交付外加去重 利 弊 失败对性能的影响更为局部 可能需要大量的存储与基础设施的支持 失败的影响未必会随着拓扑规模一起增加 每个算子处理每个事件均会产生性能开销 虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播/重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，算子实际上将具备幂等的特性。 6. 结论希望本文可以帮助大家意识到exactly-once这个术语极具误导性。提供exactly-once的处理语义实际上意味着在对流处理引擎所管理的算子的状态进行不同更新只会影响一次。exactly-once完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此这里我们更愿意使用有效一次这个术语来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只影响一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格/有效一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊。 原文:https://streaml.io/blog/exactly-once/ 译文:https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 对于流处理技术的谬见]]></title>
    <url>%2Fmisunderstanding-of-stream-processing.html</url>
    <content type="text"><![CDATA[我们在思考流处理问题上花了很多时间，更酷的是，我们也花了很多时间帮助其他人认识流处理，以及如何在他们的组织里应用流处理来解决数据问题。 我们首先要做的是纠正人们对流处理（作为一个快速变化的领域，这里有很多误见值得我们思考）的错误认识。 在这篇文章里，我们选出了其中的六个作为例子。因为我们对Apache Flink比较熟悉，所以我们会基于Flink来讲解这些例子。 谬见1：没有不使用批处理的流（Lambda架构） 谬见2：延迟和吞吐量：只能选择一个 谬见3：微批次意味着更好的吞吐量 谬见4：Exactly once？完全不可能 谬见5：流只能被应用在“实时”场景里 谬见6：不管怎么样，流仍然很复杂 1. 谬见1：没有不使用批处理的流（Lambda架构）Lambda架构在Apache Storm的早期阶段和其它流处理项目里是一个很有用的设计模式。这个架构包含了一个快速流层和一个批次层。 之所以使用两个单独的层，是因为Lambda架构里的流处理只能计算出大致的结果（也就是说，如果中间出现了错误，那么计算结果就不可信），而且只能处理相对少量的事件。 就算Storm的早期版本存在这样的问题，但现今的很多开源流处理框架都具有容错能力，它们可以在出现故障的前提下生成准确的计算结果，而且具有高吞吐的计算能力。所以没有必要再为了分别得到“快”和“准确”的结果而维护多层架构。现今的流处理器（比如Flink）可以同时帮你得到两种结果。 好在人们不再更多地讨论Lambda架构，说明流处理正在走向成熟。 2. 谬见2：延迟和吞吐量：只能选择一个早期的开源流处理框架要么是高吞吐的，要么是低延迟的，而海量且快速一直未能成为开源流处理框架的代名词。 不过Flink（可能还有其它的框架）就同时提供了高吞吐和低延迟。这里有一个基准测试结果的样例。 让我们从底层来剖析这个例子，特别是从硬件层，并结合具有网络瓶颈的流处理管道（很多使用Flink的管道都有这个瓶颈）。在硬件层不应该存在需要作出权衡的条件，所以网络才是影响吞吐量和延迟的主要因素。 一个设计良好的软件系统应该会充分利用网络的上限而不会引入瓶颈问题。不过对Flink来说，总是有可优化的空间，可以让它更接近硬件所能提供的效能。使用一个包含10个节点的集群，Flink现在每秒可以处理千万级别的事件量，如果扩展到1000个节点，它的延迟可以降低到几十毫秒。在我们看来，这种水平已经比很多现有的方案高出很多。 谬见3：微批次意味着更好的吞吐量我们可以从另一个角度来讨论性能，不过先让我们来澄清两个容易混淆的概念： 微批次:微批次建立在传统批次之上，是处理数据的一个执行或编程模型。通过这项技术，进程或任务可以把一个流当作一系列小型的批次或数据块(参阅:Apache Storm 微批次设计模式)。 缓冲:缓冲技术用于对网络、磁盘、缓存的访问进行优化。Wikipedia完美地把它定义为物理内存里的一块用于临时储存移动数据的区域。 那么第3个缪见就是说，使用微批次的数据处理框架能够比每次处理一个事件的框架达到更高的吞吐量，因为微批次在网络上传输的效率更高。这个缪见忽略了一个事实，流框架不会依赖任何编程模型层面的批次，它们只会在物理层面使用缓冲。Flink确实也会对数据进行缓冲，也就是说它会通过网络发送一组处理过的记录，而不是每次发送一条记录。从性能方面说，不对数据进行缓冲是不可取的，因为通过网络逐个发送记录不会带来任何性能上的好处。所以我们得承认在物理层面根本不存在类似一次一条记录这样的情况。 不过缓冲只能作为对性能的优化，所以缓冲： 对用户是不可见的 不应该对系统造成任何影响 不应该出现人为的边界 不应该限制系统功能 所以对Flink的用户来说，他们开发的程序能够单独地处理每个记录，那是因为Flink为了提升性能隐藏了使用缓冲的细节。 事实上，在任务调度里使用微批次会带来额外的开销，而如果这样做是为了降低延迟，那么这种开销会只增不减！流处理器知道该如何利用缓冲的优势而不会带来任务调度方面的开销。 4. 谬见4：Exactly once？完全不可能这个缪见包含了几个方面的内容： 从根本上说，Exactly once是不可能的 从端到端的Exactly once是不可能的 Exactly once从来都不是真实世界的需求 Exactly once以牺牲性能为代价 我们退一步讲，我们并不介意Exactly once这种观点的存在。Exactly once原先指的是一次性传递，而现在这个词被随意用在流处理里，让这个词变得令人困惑，失去了它原本的意义。不过相关的概念还是很重要的，我们不打算跳过去。 为了尽量准确，我们把一次性状态和一次性传递视为两种不同的概念。因为之前人们对这两个词的使用方式导致了它们的混淆。Apache Storm使用at least once来描述传递(Storm不支持状态)，而Apache Samza使用at least once来描述应用状态。 (1) 一次性状态是指应用程序在经历了故障以后恍如没有发生过故障一样。例如，假设我们在维护一个计数器应用程序，在发生了一次故障之后，它既不能多计数也不能少计数。在这里使用Exactly once这个词是因为应用程序状态认为每个消息只被处理了一次。 (2) 一次性传递是指接收端(应用程序之外的系统)在故障发生后会收到处理过的事件，恍如没有发生过故障一样。 流处理框架在任何情况下都不保证一次性传递，但可以做到一次性状态。Flink可以做到一次性状态，而且不会对性能造成显著影响。Flink还能在与Flink检查点相关的数据槽上做到一次性传递。 Flink检查点就是应用程序状态的快照，Flink会为应用程序定时异步地生成快照。这就是Flink在发生故障时仍然能保证一次性状态的原因：Flink定时记录（快照）输入流的读取位置和每个操作数的相关状态。如果发生故障，Flink会回滚到之前的状态，并重新开始计算。所以说，尽管记录被重新处理，但从结果来看，记录好像只被处理过一次。 那么端到端的一次性处理呢？通过恰当的方式让检查点兼具事务协调机制是可能的，换句话说，就是让源操作和目标操作参与到检查点里来。在框架内部，结果是一次性的，从端到端来看，也是一次性的，或者说接近一次性。例如，在使用Flink和Kafka作为数据源并发生数据槽（HDFS）滚动时，从Kafka到HDFS就是端到端的一次性处理。类似地，在把Kafka作为Flink的源并且把Cassandra作为Flink的槽时，如果针对Cassandra的更新是幂等时，那么就可以实现端到端的一次性处理。 值得一提的是，利用Flink的保存点，检查点可以兼具状态版本机制。使用保存点，在保持状态一致性的同时还可以“随着时间移动”。这样可以让代码的更新、维护、迁移、调试和各种模拟测试变得简单。 5. 谬见5：流只能被应用在“实时”场景里这个谬见包括几点内容： 我没有低延迟的应用，所以我不需要流处理器 流处理只跟那些持久化之前的过渡数据有关系 我们需要批处理器来完成笨重的离线计算 现在是时候思考一下数据集的类型和处理模型之间的关系了。 (1) 首先，有两种数据集： 没有边界的：从非预定义的端点持续产生的数据 有边界的：有限且完整的数据 很多真实的数据集是没有边界的，不管这些数据时存储在文件里，还是在HDFS的目录里，还是在像Kafka这样的系统里。举一些例子： 移动设备或网站用户的交互信息 物理传感器提供的度量指标 金融市场数据 机器日志数据 实际上，在现实世界中很难找到有边界的数据集，不过一个公司所有大楼的位置信息倒是有边界的（不过它也会随着公司业务的增长而变化）。 (2) 其次，有两种处理模型： 流：只要有数据生成就会一直处理 批次：在有限的时间内结束处理，并释放资源 让我们再深入一点，来区分两种没有边界的数据集：连续性流和间歇性流。 使用任意一种模型来处理任意一种数据集是完全可能的，虽然这不是最优的做法。例如，批次处理模型被长时间地应用在无边界的数据集上，特别是间歇性的无边界数据集。现实情况是，大多数批处理任务是通过调度来执行的，每次只处理无边界数据集的一小部分。这意味着流的无边界特质会给某些人带来麻烦（那些工作在流入管道上的人）。 批处理是无状态的，输出只取决于输入。现实情况是，批处理任务会在内部保留状态（比如reducer经常会保留状态），但这些状态只限在批次的边界内，而且它们不会在批次间流窜。当有人尝试实现类似带有”事件时间戳”的时间窗，那么”批次的边界内状态”就会变得很有用，这在处理无边界数据集时是个很常用的手段。处理无边界数据集的批处理器将不可避免地遇到延迟事件(因为上游的延迟)，批次内的数据有可能因此变得不完整。要注意，这里假设我们是基于事件时间戳来移动时间窗的，因为事件时间戳是现实当中最为准确的模型。在执行批处理的时候，迟到的数据会成为问题，即使通过简单的时间窗修复(比如翻转或滑动时间窗)也解决不了这个问题，特别是如果使用会话时间窗，就更难以处理了。因为完成一个计算所需要的数据不会都在一个批次里，所以在使用批次处理无边界数据集时，很难保证结果的正确性。最起码，它需要额外的开销来处理迟到的数据，还要维护批次之间的状态(要等到所有数据达到后才开始处理，或者重新处理批次)。 Flink内建了处理迟到数据的机制，迟到数据被视为真实世界无边界数据的正常现象，所以Flink设计了一个流处理器专门处理迟到数据。有状态的流处理器更适合用来处理无边界数据集，不管数据集是持续生成的还是间歇生成的。使用流处理器只是个锦上添花的事情。 6. 缪见6：不管怎么样，流仍然很复杂这是最后一个缪见。你也许会想：”理论虽好，但我仍然不会采用流技术，因为……”： 流框架难以掌握 流难以解决时间窗、事件时间戳、触发器的问题 流需要结合批次，而我已经知道如何使用批次，那为什么还要使用流？ 我们从来没有打算怂恿你使用流，虽然我们觉得流是个很酷的东西。我们相信，是否使用流完全取决于数据和代码的特点。在做决定之前问问自己：”我正在跟什么样类型的数据集打交道？” 无边界的（用户活动数据、日志、传感器数据） 有边界的 然后再问另一个问题：”哪部分变化最频繁？” 代码比数据变化更频繁 数据比代码变化更频繁 对于数据比代码变化更频繁的情况，例如在经常变化的数据集上执行一个相对固定的查询操作，这样会出现流方面的问题。所以，在认定流是一个”复杂”的东西之前，你可能在不知不觉中已经解决过流方面的问题！你可能使用过基于小时的批次任务调度，团队里的其他人可以创建和管理这些批次（在这种情况下，你得到的结果可能是不准确的，而你意识不到这样的结果是批次的时间问题和之前提过的状态问题造成的）。 为了能够提供一组封装了这些时间和状态复杂性的API，Flink社区为此工作了很长时间。在Flink里可以很简单地处理事件时间戳，只要定义一个时间窗口和一个能够抽取时间戳和水印的函数(只在每个流上调用一次)。处理状态也很简单，类似于定义Java变量，再把这些变量注册到Flink。使用Flink的StreamSQL可以在源源不断的流上面运行SQL查询。 最后一点：对代码比数据变化更频繁的情况该怎么办？对于这种情况，我们认为你遇到了探索性问题。使用笔记本或其它类似的工具进行迭代可能适合用来解决探索性问题。在代码稳定了之后，你仍然会碰到流方面的问题。我们建议从一开始就使用长远的方案来解决流方面的问题。 7. 流处理的未来随着流处理的日渐成熟和这些缪见的逐步淡去，我们发现流正朝着除分析应用之外的领域发展。正如我们所讨论的那样，真实世界正连续不断地生成数据。 原文:http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 主流流处理框架比较(2)]]></title>
    <url>%2Fmain-stream-processing-framework-comparison-part-two.html</url>
    <content type="text"><![CDATA[在上篇文章中，我们过了下基本的理论，也介绍了主流的流处理框架：Storm，Trident，Spark Streaming，Samza和Flink。今天咱们来点有深度的主题，比如，容错，状态管理或者性能。除此之外，我们也将讨论开发分布式流处理应用的指南，并给出推荐的流处理框架。 1. 容错性流处理系统的容错性与生俱来的比批处理系统难实现。当批处理系统中出现错误时，我们只需要把失败的部分简单重启即可；但对于流处理系统，出现错误就很难恢复。因为线上许多作业都是7 x 24小时运行，不断有输入的数据。流处理系统面临的另外一个挑战是状态一致性，因为重启后会出现重复数据，并且不是所有的状态操作是幂等的。容错性这么难实现，那下面我们看看各大主流流处理框架是如何处理这一问题。 1.1 Apache StormStorm使用上游数据备份和消息确认的机制来保障消息在失败之后会重新处理。消息确认原理：每个操作都会把前一次的操作处理消息的确认信息返回。Topology的数据源备份它生成的所有数据记录。当所有数据记录的处理确认信息收到，备份即会被安全拆除。失败后，如果不是所有的消息处理确认信息收到，那数据记录会被数据源数据替换。这保障了没有数据丢失，但数据结果会有重复，这就是at-least once传输机制。 Storm采用取巧的办法完成了容错性，对每个源数据记录仅仅要求几个字节存储空间来跟踪确认消息。纯数据记录消息确认架构，尽管性能不错，但不能保证exactly once消息传输机制，所有应用开发者需要处理重复数据。Storm存在低吞吐量和流控问题，因为消息确认机制在反压下经常误认为失败。 1.2 Spark StreamingSpark Streaming实现微批处理，容错机制的实现跟Storm不一样。微批处理的想法相当简单。Spark在集群各worker节点上处理micro-batches。每个micro-batches一旦失败，重新计算就行。因为micro-batches本身的不可变性，并且每个micro-batches也会持久化，所以exactly once传输机制很容易实现。 1.3 SamzaSamza的实现方法跟前面两种流处理框架完全不一样。Samza利用消息系统Kafka的持久化和偏移量。Samza监控任务的偏移量，当任务处理完消息，相应的偏移量被移除。消息的偏移量会被checkpoint到持久化存储中，并在失败时恢复。但是问题在于：从上次checkpoint中修复偏移量时并不知道上游消息已经被处理过，这就会造成重复。这就是at least once传输机制。 1.4 Apache FlinkFlink的容错机制是基于分布式快照实现的，这些快照会保存流处理作业的状态(本文对Flink的检查点和快照不进行区分，因为两者实际是同一个事物的两种不同叫法。Flink构建这些快照的机制可以被描述成分布式数据流的轻量级异步快照，它采用Chandy-Lamport算法实现。)。如果发生失败的情况，系统可以从这些检查点进行恢复。Flink发送checkpoint的栅栏（barrier）到数据流中（栅栏是Flink的分布式快照机制中一个核心的元素），当checkpoint的栅栏到达其中一个operator，operator会接所有收输入流中对应的栅栏（比如，图中checkpoint n对应栅栏n到n-1的所有输入流，其仅仅是整个输入流的一部分）。所以相对于Storm，Flink的容错机制更高效，因为Flink的操作是对小批量数据而不是每条数据记录。但也不要让自己糊涂了，Flink仍然是原生流处理框架，它与Spark Streaming在概念上就完全不同。Flink也提供exactly once消息传输机制。 2. 状态管理大部分大型流处理应用都涉及到状态。相对于无状态的操作(其只有一个输入数据，处理过程和输出结果)，有状态的应用会有一个输入数据和一个状态信息，然后处理过程，接着输出结果和修改状态信息。因此，我们不得不管理状态信息，并持久化。我们期望一旦因某种原因失败，状态能够修复。状态修复有可能会出现小问题，它并不总是保证exactly once，有时也会出现消费多次，但这并不是我们想要的。 2.1 Apache Storm我们知道，Storm提供at-least once的消息传输保障。那我们又该如何使用Trident做到exactly once的语义。概念上貌似挺简单，你只需要提交每条数据记录，但这显然不是那么高效。所以你会想到小批量的数据记录一起提交会优化。Trident定义了几个抽象来达到exactly once的语义，见下图，其中也会有些局限。 2.2 Spark StreamingSpark Streaming是微批处理系统，它把状态信息也看做是一种微批量数据流。在处理每个微批量数据时，Spark加载当前的状态信息，接着通过函数操作获得处理后的微批量数据结果并修改加载过的状态信息。 2.3 SamzaSamza实现状态管理是通过Kafka来处理的。Samza有真实的状态操作，所以其任务会持有一个状态信息，并把状态改变的日志推送到Kafka。如果需要状态重建，可以很容易的从Kafka的topic重建。为了达到更快的状态管理，Samza也支持把状态信息放入本地key-value存储中，所以状态信息不必一直在Kafka中管理，见下图。不幸的是，Samza只提供at-least once语义，exactly once的支持也在计划中。 2.4 Apache FlinkFlink提供状态操作，和Samza类似。Flink提供两种类型的状态：一种是用户自定义状态；另外一种是窗口状态。如图，第一个状态是自定义状态，它和其它的的状态不相互作用。这些状态可以分区或者使用嵌入式Key-Value存储状态(参阅文容错和状态)。当然Flink提供exactly-once语义。下图展示Flink长期运行的三个状态。 3. 单词计数例子中的状态管理单词计数的详细代码见上篇文章，这里仅关注状态管理部分。 让我们先看Trident：public static StormTopology buildTopology(LocalDRPC drpc) &#123; FixedBatchSpout spout = ... TridentTopology topology = new TridentTopology(); TridentState wordCounts = topology.newStream(&quot;spout1&quot;, spout) .each(new Fields(&quot;sentence&quot;),new Split(), new Fields(&quot;word&quot;)) .groupBy(new Fields(&quot;word&quot;)) .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;)); ... &#125; 在第九行代码中，我们通过调用persistentAggregate创建一个状态。其中参数Count存储单词数，如果你想从状态中处理数据，你必须创建一个数据流。从代码中也可以看出实现起来不方便。 Spark Streaming声明式的方法稍微好点：// Initial RDD input to updateStateByKeyval initialRDD = ssc.sparkContext.parallelize(List.empty[(String, Int)])val lines = ...val words = lines.flatMap(_.split(&quot; &quot;))val wordDstream = words.map(x =&gt; (x, 1))val trackStateFunc = (batchTime: Time, word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) Some(output) &#125;val stateDstream = wordDstream.trackStateByKey( StateSpec.function(trackStateFunc).initialState(initialRDD)) 首先我们需要创建一个RDD来初始化状态（第二行代码），然后进行transformations（第五行和六行代码）。接着在第八行到十四行代码，我们定义函数来处理单词数状态。函数计算并更新状态，最后返回结果。第十六行和十七行代码，我们得到一个状态信息流，其中包含单词数。 接着我们看下Samza:class WordCountTask extends StreamTask with InitableTask &#123; private var store: CountStore = _ def init(config: Config, context: TaskContext) &#123; this.store = context.getStore(&quot;wordcount-store&quot;) .asInstanceOf[KeyValueStore[String, Integer]] &#125; override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector, coordinator: TaskCoordinator) &#123; val words = envelope.getMessage.asInstanceOf[String].split(&quot; &quot;) words.foreach &#123; key =&gt; val count: Integer = Option(store.get(key)).getOrElse(0) store.put(key, count + 1) collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;), (key, count))) &#125; &#125; 首先在第三行代码定义状态，进行Key-Value存储，在第五行到八行代码初始化状态。接着在计算中使用，上面的代码已经很直白。 最后，讲下Flink使用简洁的API实现状态管理：val env = ExecutionEnvironment.getExecutionEnvironmentval text = env.fromElements(...)val words = text.flatMap ( _.split(&quot; &quot;) )words.keyBy(x =&gt; x).mapWithState &#123; (word, count: Option[Int]) =&gt; &#123; val newCount = count.getOrElse(0) + 1 val output = (word, newCount) (output, Some(newCount)) &#125;&#125; 我们仅仅需要在第六行代码中调用mapwithstate函数，它有一个函数参数（函数有两个变量，第一个是单词，第二个是状态。然后返回处理的结果和新的状态）。 4. 流处理框架性能这里所讲的性能主要涉及到的是延迟性和吞吐量。 对于延迟性来说，微批处理一般在秒级别，大部分原生流处理在百毫秒以下，调优的情况下Storm可以很轻松的达到十毫秒。 同时也要记住，消息传输机制保障，容错性和状态恢复都会占用机器资源。例如，打开容错恢复可能会降低10％到15％的性能，Storm可能降低70%的吞吐量。总之，天下没有免费的午餐。对于有状态管理，Flink会降低25%的性能，Spark Streaming降低50%的性能。 也要记住，各大流处理框架的所有操作都是分布式的，通过网络发送数据是相当耗时的，所以要利用数据本地性，也尽量优化你的应用的序列化。 5. 项目成熟度当你为应用选型时一定会考虑项目的成熟度。下面来快速浏览一下：Storm是第一个主流的流处理框架，后期已经成为长期的工业级的标准，并在像Twitter，Yahoo，Spotify等大公司使用。Spark Streaming是最近最流行的Scala代码实现的流处理框架。现在Spark Streaming被公司（Netflix, Cisco, DataStax, Intel, IBM等）日渐接受。Samza主要在LinkedIn公司使用。Flink是一个新兴的项目，很有前景。 你可能对项目的贡献者数量也感兴趣。Storm和Trident大概有180个代码贡献者；整个Spark有720多个；根据github显示，Samza有40个；Flink有超过130个代码贡献者。 6. 小结在进行流处理框架推荐之前，先来整体看下总结表： 7. 流处理框架推荐应用选型是大家都会遇到的问题，一般是根据应用具体的场景来选择特定的流处理框架。下面给出几个作者认为优先考虑的点： High level API：具有high level API的流处理框架会更简洁和高效； 状态管理：大部分流处理应用都涉及到状态管理，因此你得把状态管理作为评价指标之一； exactly once语义：exactly once会使得应用开发变得简单，但也要看具体需求，可能at least once或者at most once语义就满足你得要求； 自动恢复：确保流处理系统能够快速恢复，你可以使用Chaos Monkey或者类似的工具进行测试。快速的恢复是流处理重要的部分。 Storm：Storm非常适合任务量小但速度要求高的应用。如果你主要在意流处理框架的延迟性，Storm将可能是你的首先。但同时也要记住，Storm的容错恢复或者Trident的状态管理都会降低整体的性能水平。也有一个潜在的Storm更新项目-Twitter的Heron，Heron设计的初衷是为了替代Storm，并在每个单任务上做了优化但同时保留了API。 Spark Streaming：如果你得基础架构中已经涉及到Spark，那Spark Streaming无疑是值得你尝试的。因为你可以很好的利用Spark各种library。如果你需要使用Lambda架构，Spark Streaming也是一个不错的选择。但你要时刻记住微批处理的局限性，以及它的延迟性问题。 Samza：如果你想使用Samza，那Kafka应该是你基础架构中的基石，好在现在Kafka已经成为家喻户晓的组件。像前面提到的，Samza一般会搭配强大的本地存储一起，这对管理大数据量的状态非常有益。它可以轻松处理上万千兆字节的状态信息，但要记住Samza只支持at least once语义。 Flink：Flink流处理系统的概念非常不错，并且满足绝大多数流处理场景，也经常提供前沿的功能函数，比如，高级窗口函数或者时间处理功能，这些在其它流处理框架中是没有的。同时Flink也有API提供给通用的批处理场景。但你需要足够的勇气去上线一个新兴的项目，并且你也不能忘了看下Flink的roadmap。 8. Dataflow和开源最后，我们来聊下Dataflow和它的开源。Dataflow是Google云平台的一部分，Google云平台包含很多组件：大数据存储，BigQuery，Cloud PubSub，数据分析工具和前面提到的Dataflow。 Dataflow是Google管理批处理和流处理的统一API。它是建立在MapReduce（批处理），FlumeJava（编程模型）和MillWheel（流处理）之上。Google最近决定开源Dataflow SDK，并完成Spark和Flink的runner。现在可以通过Dataflow的API来定义Google云平台作业、Flink作业或者Spark作业，后续会增加对其它引擎的支持。 Google为Dataflow提供Java、Python的API，社区已经完成Scalable的DSL支持。除此之外，Google及其合作者提交Apache Beam到Apache。 原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 主流流处理框架比较(1)]]></title>
    <url>%2Fmain-stream-processing-framework-comparison-part-one.html</url>
    <content type="text"><![CDATA[分布式流处理是对无边界数据集进行连续不断的处理、聚合和分析。它跟MapReduce一样是一种通用计算，但我们期望延迟在毫秒或者秒级别。这类系统一般采用有向无环图（DAG）。 DAG是任务链的图形化表示，我们用它来描述流处理作业的拓扑。如下图，数据从sources流经处理任务链到sinks。单机可以运行DAG，但本篇文章主要聚焦在多台机器上运行DAG的情况。 1. 关注点当选择不同的流处理系统时，有以下几点需要注意的： 运行时和编程模型：平台框架提供的编程模型决定了许多特色功能，编程模型要足够处理各种应用场景。这是一个相当重要的点，后续会继续。 函数式原语：流处理平台应该能提供丰富的功能函数，比如，map或者filter这类易扩展、处理单条信息的函数；处理多条信息的函数aggregation；跨数据流、不易扩展的操作join。 状态管理：大部分应用都需要保持状态处理的逻辑。流处理平台应该提供存储、访问和更新状态信息。 消息传输保障：消息传输保障一般有三种：at most once，at least once和exactly once。At most once的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失；At least once意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失；Exactly once的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复。 容错：流处理框架中的失败会发生在各个层次，比如，网络部分，磁盘崩溃或者节点宕机等。流处理框架应该具备从所有这种失败中恢复，并从上一个成功的状态（无脏数据）重新消费。 性能：延迟时间（Latency），吞吐量（Throughput）和扩展性（Scalability）是流处理应用中极其重要的指标。 平台的成熟度和接受度：成熟的流处理框架可以提供潜在的支持，可用的库，甚至开发问答帮助。选择正确的平台会在这方面提供很大的帮助。 2. 运行时和编程模型运行时和编程模型是一个系统最重要的特质，因为它们定义了表达方式、可能的操作和将来的局限性。因此，运行时和编程模型决定了系统的能力和适用场景。 实现流处理系统有两种完全不同的方式： (1) 一种是称作原生流处理，意味着所有输入的记录一旦到达即会一个接着一个进行处理。 (2) 第二种称为微批处理。把输入的数据按照某种预先定义的时间间隔(典型的是几秒钟)分成短小的批量数据，流经流处理系统。 两种方法都有其先天的优势和不足。首先以原生流处理开始，原生流处理的优势在于它的表达方式。数据一旦到达立即处理，这些系统的延迟性远比其它微批处理要好。除了延迟性外，原生流处理的状态操作也容易实现，后续将详细讲解。一般原生流处理系统为了达到低延迟和容错性会花费比较大的成本，因为它需要考虑每条记录。原生流处理的负载均衡也是个问题。比如，我们处理的数据按key分区，如果分区的某个key是资源密集型，那这个分区很容易成为作业的瓶颈。 接下来看下微批处理。将流式计算分解成一系列短小的批处理作业，也不可避免的减弱系统的表达力。像状态管理或者join等操作的实现会变的困难，因为微批处理系统必须操作整个批量数据。并且，batch interval会连接两个不易连接的事情：基础属性和业务逻辑。相反地，微批处理系统的容错性和负载均衡实现起来非常简单，因为微批处理系统仅发送每批数据到一个worker节点上，如果一些数据出错那就使用其它副本。微批处理系统很容易建立在原生流处理系统之上。 编程模型一般分为组合式和声明式。组合式编程提供基本的构建模块，它们必须紧密结合来创建拓扑。新的组件经常以接口的方式完成。相对应地，声明式API操作是定义的高阶函数。它允许我们用抽象类型和方法来写函数代码，并且系统创建拓扑和优化拓扑。声明式API经常也提供更多高级的操作（比如，窗口函数或者状态管理）。后面很快会给出样例代码。 3. 主流流处理系统有一系列各种实现的流处理框架，不能一一列举，这里仅选出主流的流处理解决方案，并且支持Scala API。因此，我们将详细介绍Apache Storm，Trident，Spark Streaming，Samza和Apache Flink。前面选择讲述的虽然都是流处理系统，但它们实现的方法包含了各种不同的挑战。这里暂时不讲商业的系统，比如Google MillWheel或者Amazon Kinesis，也不会涉及很少使用的Intel GearPump或者Apache Apex。 Apache Storm最开始是由Nathan Marz和他的团队于2010年在数据分析公司BackType开发的，后来BackType公司被Twitter收购，接着Twitter开源Storm并在2014年成为Apache顶级项目。毋庸置疑，Storm成为大规模流数据处理的先锋，并逐渐成为工业标准。Storm是原生的流处理系统，提供low-level的API。Storm使用Thrift来定义topology和支持多语言协议，使得我们可以使用大部分编程语言开发，Scala自然包括在内。 Trident是对Storm的一个更高层次的抽象，Trident最大的特点以batch的形式进行流处理。Trident简化topology构建过程，增加了窗口操作、聚合操作或者状态管理等高级操作，这些在Storm中并不支持。相对应于Storm的At most once流传输机制，Trident提供了Exactly once传输机制。Trident支持Java，Clojure和Scala。 当前Spark是非常受欢迎的批处理框架，包含Spark SQL，MLlib和Spark Streaming。Spark的运行时是建立在批处理之上，因此后续加入的Spark Streaming也依赖于批处理，实现了微批处理。接收器把输入数据流分成短小批处理，并以类似Spark作业的方式处理微批处理。Spark Streaming提供高级声明式API（支持Scala，Java和Python）。 Samza最开始是专为LinkedIn公司开发的流处理解决方案，并和LinkedIn的Kafka一起贡献给社区，现已成为基础设施的关键部分。Samza的构建严重依赖于基于log的Kafka，两者紧密耦合。Samza提供组合式API，当然也支持Scala。 最后来介绍Apache Flink。Flink是个相当早的项目，开始于2008年，但只在最近才得到注意。Flink是原生的流处理系统，提供high level的API。Flink也提供API来像Spark一样进行批处理，但两者处理的基础是完全不同的。Flink把批处理当作流处理中的一种特殊情况。在Flink中，所有的数据都看作流，是一种很好的抽象，因为这更接近于现实世界。 快速的介绍流处理系统之后，让我们以下面的表格来更好清晰的展示它们之间的不同： 4. Word CountWordcount之于流处理框架学习，就好比hello world之于编程语言学习。它能很好的展示各流处理框架的不同之处，让我们从Storm开始看看如何实现Wordcount：TopologyBuilder builder = new TopologyBuilder();builder.setSpout("spout", new RandomSentenceSpout(), 5);builder.setBolt("split", new Split(), 8).shuffleGrouping("spout");builder.setBolt("count", new WordCount(), 12).fieldsGrouping("split", new Fields("word")); ...Map&lt;String, Integer&gt; counts = new HashMap&lt;String, Integer&gt;();public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String word = tuple.getString(0); Integer count = counts.containsKey(word) ? counts.get(word) + 1 : 1; counts.put(word, count); collector.emit(new Values(word, count));&#125; 首先，定义topology。第二行代码定义一个spout，作为数据源。然后是一个处理组件bolt，分割文本为单词。接着，定义另一个bolt来计算单词数（第四行代码）。也可以看到魔数5，8和12，这些是并行度，定义集群每个组件执行的独立线程数。第八行到十五行是实际的WordCount bolt实现。因为Storm不支持内建的状态管理，所有这里定义了一个局部状态。 按之前描述，Trident是对Storm的一个更高层次的抽象，Trident最大的特点以batch的形式进行流处理。除了其它优势，Trident提供了状态管理，这对wordcount实现非常有用:public static StormTopology buildTopology(LocalDRPC drpc) &#123; FixedBatchSpout spout = ... TridentTopology topology = new TridentTopology(); TridentState wordCounts = topology.newStream("spout1", spout) .each(new Fields("sentence"),new Split(), new Fields("word")) .groupBy(new Fields("word")) .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count")); ... &#125; 如你所见，上面代码使用higher level操作，比如each（第七行代码）和groupby（第八行代码）。并且使用Trident管理状态来存储单词数（第九行代码）。 下面是时候祭出提供声明式API的Apache Spark。记住，相对于前面的例子，这些代码相当简单，几乎没有冗余代码。下面是简单的流式计算单词数：val conf = new SparkConf().setAppName("wordcount")val ssc = new StreamingContext(conf, Seconds(1))val text = ...val counts = text.flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _)counts.print()ssc.start()ssc.awaitTermination() 每个Spark Streaming的作业都要有StreamingContext，它是流式函数的入口。StreamingContext加载第一行代码定义的配置conf，但更重要地，第二行代码定义batch interval（这里设置为1秒）。第六行到八行代码是整个单词数计算。这些是标准的函数式代码，Spark定义topology并且分布式执行。第十二行代码是每个Spark Streaming作业最后的部分：启动计算。记住，Spark Streaming作业一旦启动即不可修改。 接下来看下Apache Samza，另外一个组合式API例子：class WordCountTask extends StreamTask &#123; override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector, coordinator: TaskCoordinator) &#123; val text = envelope.getMessage.asInstanceOf[String] val counts = text.split(&quot; &quot;).foldLeft(Map.empty[String, Int]) &#123; (count, word) =&gt; count + (word -&gt; (count.getOrElse(word, 0) + 1)) &#125; collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;), counts)) &#125; Samza的属性配置文件定义topology，为了简明这里并没把配置文件放上来。定义任务的输入和输出，并通过Kafka topic通信。在单词数计算整个topology是WordCountTask。在Samza中，实现特殊接口定义组件StreamTask，在第三行代码重写方法process。它的参数列表包含所有连接其它系统的需要。第八行到十行简单的Scala代码是计算本身。 Flink的API跟Spark Streaming是惊人的相似，但注意到代码里并未设置batch interval：val env = ExecutionEnvironment.getExecutionEnvironment val text = env.fromElements(...) val counts = text.flatMap ( _.split(&quot; &quot;) ) .map ( (_, 1) ) .groupBy(0) .sum(1) counts.print() env.execute(&quot;wordcount&quot;) 上面的代码是相当的直白，仅仅只是几个函数式调用，Flink支持分布式计算。 5. 结论上面给出了基本的理论和主流流处理框架介绍，下篇文章将会更深入的探讨其它关注点。希望你能对前面的文章感兴趣，如果有任何问题，请联系我讨论这些主题。 原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 事件时间与Watermarks]]></title>
    <url>%2Fflink-stream-event-time-and-watermark.html</url>
    <content type="text"><![CDATA[1. watermarkFlink实现了数据流模型(Dataflow Model)中许多技术。如果想对事件时间(event time)和watermarks更详细的了解，请参阅下面的文章: The world beyond batch: Streaming 101 The Dataflow Model 支持事件时间的流处理器需要一种方法来衡量事件时间的进度。例如，一个构建小时窗口的窗口算子(operator)，当事件时间超过一小时末尾时需要告知窗口算子，以便算子可以关闭正在进行的窗口。 事件时间可以独立于处理时间来运行。例如，在一个程序中，算子的当前事件时间可以略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度继续运行。另一方面，另一个流式处理程序处理几个星期的事件时间只需几秒钟就可以，通过快速浏览缓存在Kafka Topic中历史数据。 Flink中测量事件时间进度的机制是watermarks。watermarks会作为数据流的一部分进行流动，并带有一个时间戳t。Watermark(t)表示数据流中的事件时间已达到时间t，意思就是说数据流之后不再有时间戳t‘&lt;= t的元素(即带时间戳的事件老于或等于watermark)。 下图显示了具有时间戳(逻辑上)的事件流以及内嵌的watermark。在这个例子中，事件是有序的(相对于它们的时间戳)，这意味着watermark只是数据流中的周期性标记。 watermark对于乱序数据流至关重要，如下图所示，事件并未按照时间戳进行排序。通常，watermark表示在数据流中那个时刻小于时间戳的所有事件都已经到达。一旦watermark到达算子，算子就可以将其内部的事件时间提到watermark的那个值。 2. 数据流中的并行Watermarkswatermarks是直接通过数据源函数(source functions)生成的或在数据源函数之后生成的。源函数的每个并行子任务通常独立生成watermarks。这些watermarks在指定并行数据源上定义事件时间。 watermarks贯穿整个流处理程序，他们会在watermark到达的算子时将事件时间提前(advance)。每当算子提前事件时间时，它都会为下游的后续算子生成一个新的watermarks(Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.)。 一些算子消耗多个输入流；例如，union操作，或者算子后面跟着keyBy(...)函数或者partition(...)函数。这样的算子的当前事件时间是其输入流的所有事件时间中的最小值。随着输入流更新事件时间，算子也会更新事件。 下图显示了事件和watermarks流经并行流的的示例，以及跟踪事件时间的算子: 3. 延迟元素某些元素可能违反watermarks条件，这意味着即使出现watermarks(t)，但是还是会出现很多的时间戳t&#39;&lt;= t的元素。事实上，在现实世界中，某些元素可能被任意地延迟，因此指定一个时间，带有事件时间戳的所有事件在此之前出现是不可能的。此外，即使延迟时间是有限制的，也不希望延迟太多的watermarks，因为它会在事件时间窗口的评估中导致太多的延迟。 因此，流处理程序中可能会明确的知道会有延迟元素。延迟元素是那些系统事件时钟(由watermark所示)已经超过了延迟元素的时间戳的那些元素。有关如何处理事件时间窗口中的延迟元素的更多信息，请参阅Allowed Lateness。 4. 调试Watermarks请参阅调试Windows和事件时间部分，以便在运行时调试Watermarks。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 事件时间与处理时间]]></title>
    <url>%2Fflink-stream-event-time-and-processing-time.html</url>
    <content type="text"><![CDATA[Flink在数据流中支持几种不同概念的时间。 1. 处理时间Processing Time(处理时间)是指执行相应操作机器的系统时间(Processing time refers to the system time of the machine that is executing the respective operation.)。 当一个流程序以处理时间来运行时，所有基于时间的操作(如时间窗口)将使用运行算子(operator)所在机器的系统时间。例如:一个基于处理时间按每小时进行处理的时间窗口将包括以系统时间为标准在一个小时内到达指定算子的所有的记录(an hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour.)。 处理时间是最简单的一个时间概念，不需要在数据流和机器之间进行协调。它有最好的性能和最低的延迟。然而，在分布式或者异步环境中，处理时间具有不确定性，因为容易受到记录到达系统速度的影响(例如从消息队列到达的记录)，还会受到系统内记录流在不同算子之间的流动速度的影响(speed at which records arrive in the system, and to the speed at which the records flow between operators inside the system)。 2. 事件时间Event Time(事件时间)是每个独立事件在它生产设备上产生的时间。在进入Flink之前，事件时间通常要嵌入到记录中，并且事件时间也可以从记录中提取出来。一个基于事件时间按每小时进行处理的时间窗口将包含所有的记录，其事件时间都在这一小时之内，不管它们何时到达，以及它们以什么顺序到达。 事件时间即使在乱序事件，延迟事件以及从备份或持久化日志中的重复数据也能获得正确的结果。对于事件时间，时间的进度取决于数据，而不是任何时钟。事件时间程序必须指定如何生成事件时间的Watermarks，这是表示事件时间进度的机制。 按事件时间处理往往会导致一定的延迟，因为它要等待延迟事件和无序事件一段时间。因此，事件时间程序通常与处理时间操作相结合使用。 3. 摄入时间Ingestion Time(摄入时间)是事件进入Flink的时间。在source operator中，每个记录将源的当前时间记为时间戳，基于时间的操作(如时间窗口)会使用该时间戳。 摄入时间在概念上处于事件时间和处理时间之间。与处理时间相比，摄入时间的成本稍微更高一些，但是可以提供更可预测的结果。因为摄入时间的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，然而对于处理时间，每个窗口算子可能将记录分配给不同的窗口(基于本地系统时钟以及传输延迟)。 与事件时间相比，摄入时间程序无法处理任何无序事件或延迟事件，但程序不必指定如何生成watermarks。 在内部，摄入时间与事件时间非常相似，但事件时间会自动分配时间戳以及自动生成watermark(with automatic timestamp assignment and automatic watermark generation)。 4. 选择时间特性Flink DataStream程序的第一部分通常设置基本的时间特性(base time characteristic)。该设置定义数据流源的行为方式(例如，它们是否产生时间戳)，以及窗口操作如KeyedStream.timeWindow(Time.seconds(30))应使用哪一类型时间，是事件时间还是处理时间等。 以下示例展示了一个聚合每小时时间窗口内的事件的Flink程序。窗口的行为会与时间特性相匹配。 Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);// alternatively:// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);DataStream&lt;MyEvent&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));stream .keyBy( (event) -&gt; event.getUser() ) .timeWindow(Time.hours(1)) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)// alternatively:// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))stream .keyBy( _.getUser ) .timeWindow(Time.hours(1)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...) 备注:为了以事件时间运行此示例，程序需要使用定义了事件时间并自动产生watermarks的源，或者程序必须在源之后设置时间戳分配器和watermarks生成器。上述函数描述了如何获取事件时间戳，以及展现事件流的无序程度。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 重启策略]]></title>
    <url>%2Fflink-restart-strategy.html</url>
    <content type="text"><![CDATA[Flink支持不同的重启策略，重启策略控制在作业失败后如何重启。可以使用默认的重启策略启动集群，这个默认策略在作业没有特别指定重启策略时使用。如果在提交作业时指定了重启策略，那么此策略将覆盖集群的默认配置策略。 1. 概述默认的重启策略通过Flink的配置文件flink-conf.yaml进行设置。配置参数restart-strategy定义了采取哪种策略。如果未启用检查点，那么将使用不重启策略。如果启用检查点且重启策略尚未配置，则固定延迟重启策略与Integer.MAX_VALUE一起使用进行尝试重启。请参阅下面可用的重启策略列表以了解支持哪些值。 每个重启策略都有自己的一套控制其行为的参数。这些值也在配置文件中配置。每个重启策略的描述都包含有关各个配置值的更多信息。 重启策略 值 固定延迟重启策略 fixed-delay 失败率重启策略 failure-rate 不重启策略 none 除了定义一个默认的重启策略之外，还可以为每个Flink作业定义一个指定的重启策略。此重启策略通过调用ExecutionEnvironment上的setRestartStrategy方法以编程的方式进行设置。请注意，这也适用于StreamExecutionEnvironment。 以下示例显示了如何为作业设置固定延迟重启策略。如果发生故障，系统将尝试每10s重新启动一次作业，最多重启3次。 Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)) 2. 重启策略下面介绍几种重启策略的配置选项。 2.1 固定延迟重启策略固定延迟重启策略尝试一定次数来重新启动作业。如果超过最大尝试次数，那么作业最终将失败。在两次连续的尝试重启之间，重启策略会等待一段固定的时间(译者注:连续重启时间间隔)。 通过在flink-conf.yaml中设置以下配置参数，可以将此策略默认启用：restart-strategy: fixed-delay 配置参数 描述 默认值 restart-strategy.fixed-delay.attempts 在声明作业失败之前，Flink重试执行的次数 1或者如果启用检查点，则为Integer.MAX_VALUE restart-strategy.fixed-delay.delay 延迟重试意味着在执行失败后，重新执行不会立即开始，而只会在某个延迟之后开始。当程序与外部系统进行交互时，延迟重试会很有帮助 akka.ask.timeout，或10s(如果通过检查点激活) Example:restart-strategy.fixed-delay.attempts: 3restart-strategy.fixed-delay.delay: 10 s 固定延迟重启策略也可以通过编程来设置： Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)) 2.2 失败率重启策略失败率重启策略在失败后重新启动作业，但当超过失败率(每个时间间隔的失败)时，作业最终会失败。在两次连续的重启尝试之间，重启策略会等待一段固定的时间。 通过在flink-conf.yaml中设置以下配置参数，可以将此策略默认启用: 配置参数 描述 默认值 restart-strategy.failure-rate.max-failures-per-interval 在一个作业声明失败之前，在给定时间间隔内最大的重启次数 1 restart-strategy.failure-rate.failure-rate-interval 计算失败率的时间间隔 1分钟 restart-strategy.failure-rate.delay 两次连续重启尝试之间的时间间隔 akka.ask.timeout Example:restart-strategy.failure-rate.max-failures-per-interval: 3restart-strategy.failure-rate.failure-rate-interval: 5 minrestart-strategy.failure-rate.delay: 10 s 失败率重新启动策略也可以通过编程来设置： Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per interval Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay)); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per unit Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay)) 2.3 不重启策略作业直接失败，不会尝试重新启动:restart-strategy: none 不重启策略也可以通过编程来设置： Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart()); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.noRestart()) 2.4 回退重启策略使用集群定义的重启策略(The cluster defined restart strategy is used. )。这有助于启用检查点的流式传输程序。默认情况下，如果没有定义其他重启策略，则选择固定延时重启策略。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 数据流类型与转换关系]]></title>
    <url>%2Fflink-stream-dataflow-type-and-transformation.html</url>
    <content type="text"><![CDATA[Flink 为流处理和批处理分别提供了 DataStream API 和 DataSet API。正是这种高层的抽象和 flunent API 极大地便利了用户编写大数据应用。不过很多初学者在看到官方文档中那一大坨的转换时，常常会蒙了圈，文档中那些只言片语也很难讲清它们之间的关系。所以本文将介绍几种关键的数据流类型，它们之间是如何通过转换关联起来的。下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。 1. DataStreamDataStream 是 Flink 流处理 API 中最核心的数据结构。它代表了一个运行在多个分区上的并行流。一个 DataStream 可以从 StreamExecutionEnvironment 通过 env.addSource(SourceFunction) 获得。 DataStream 上的转换操作都是逐条的，比如 map()，flatMap()，filter()。DataStream 也可以执行 rebalance（再平衡，用来减轻数据倾斜）和 broadcaseted（广播）等分区转换。 val stream: DataStream[MyType] = env.addSource(new FlinkKafkaConsumer08[String](...))val str1: DataStream[(String, MyType)] = stream.flatMap &#123; ... &#125;val str2: DataStream[(String, MyType)] = stream.rebalance()val str3: DataStream[AnotherType] = stream.map &#123; ... &#125; 上述 DataStream 上的转换在运行时会转换成如下的执行图： 如上图的执行图所示，DataStream 各个算子会并行运行，算子之间是数据流分区。如 Source 的第一个并行实例（S1）和 flatMap() 的第一个并行实例（m1）之间就是一个数据流分区。而在 flatMap() 和 map() 之间由于加了 rebalance()，它们之间的数据流分区就有3个子分区（m1的数据流向3个map()实例）。这与 Apache Kafka 是很类似的，把流想象成 Kafka Topic，而一个流分区就表示一个 Topic Partition，流的目标并行算子实例就是 Kafka Consumers。 2. KeyedStreamKeyedStream 用来表示根据指定的 key 进行分组的数据流。一个 KeyedStream可以通过调用 DataStream.keyBy() 来获得。而在 KeyedStream 上进行任何 transformation 都将转变回 DataStream。在实现中，KeyedStream 是把 key 的信息写入到了 transformation 中。每条记录只能访问所属 key 的状态，其上的聚合函数可以方便地操作和保存对应 key 的状态。 3. WindowedStream &amp; AllWindowedStreamWindowedStream代表了根据 key 分组，并且基于 WindowAssigner 切分窗口的数据流。所以 WindowedStream 都是从 KeyedStream 衍生而来的。而在 WindowedStream 上进行任何 transformation 也都将转变回 DataStream。 val stream: DataStream[MyType] = ...val windowed: WindowedDataStream[MyType] = stream .keyBy(&quot;userId&quot;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of dataval result: DataStream[ResultType] = windowed.reduce(myReducer) 上述 WindowedStream 的样例代码在运行时会转换成如下的执行图： Flink 的窗口实现中会将到达的数据缓存在对应的窗口buffer中（一个数据可能会对应多个窗口）。当到达窗口发送的条件时（由Trigger控制），Flink 会对整个窗口中的数据进行处理。Flink 在聚合类窗口有一定的优化，即不会保存窗口中的所有值，而是每到一个元素执行一次聚合函数，最终只保存一份数据即可。 在key分组的流上进行窗口切分是比较常用的场景，也能够很好地并行化（不同的 key 上的窗口聚合可以分配到不同的 task 去处理）。不过有时候我们也需要在普通流上进行窗口的操作，这就是 AllWindowedStream。AllWindowedStream 是直接在 DataStream 上进行 windowAll(...) 操作。AllWindowedStream 的实现是基于 WindowedStream 的（Flink 1.1.x 开始）。Flink 不推荐使用 AllWindowedStream，因为在普通流上进行窗口操作，就势必需要将所有分区的流都汇集到单个的 Task 中，而这个单个的 Task 很显然就会成为整个Job的瓶颈。 4. JoinedStreams &amp; CoGroupedStreams双流 Join 也是一个非常常见的应用场景。深入源码你可以发现，JoinedStreams 和 CoGroupedStreams 的代码实现有80%是一模一样的，JoinedStreams 在底层又调用了 CoGroupedStreams 来实现 Join 功能。除了名字不一样，一开始很难将它们区分开来，而且为什么要提供两个功能类似的接口呢？ 实际上这两者还是很点区别的。首先 co-group 侧重的是 group，是对同一个 key 上的两组集合进行操作，而 join 侧重的是 pair，是对同一个 key 上的每对元素进行操作。co-group 比 join 更通用一些，因为 join 只是 co-group 的一个特例，所以 join 是可以基于 co-group 来实现的（当然有优化的空间）。而在 co-group 之外又提供了 join 接口是因为用户更熟悉 join（源于数据库吧），而且能够跟 DataSet API 保持一致，降低用户的学习成本。 JoinedStreams 和 CoGroupedStreams 是基于 Window 上实现的，所以 CoGroupedStreams 最终又调用了 WindowedStream 来实现。 val firstInput: DataStream[MyType] = ...val secondInput: DataStream[AnotherType] = ...val result: DataStream[(MyType, AnotherType)] = firstInput.join(secondInput) .where(&quot;userId&quot;).equalTo(&quot;id&quot;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new JoinFunction () &#123;...&#125;) 上述 JoinedStreams 的样例代码在运行时会转换成如下的执行图： 双流上的数据在同一个 key 的会被分别分配到同一个 window 窗口的左右两个篮子里，当 window 结束的时候，会对左右篮子进行笛卡尔积从而得到每一对 pair，对每一对 pair 应用 JoinFunction。不过目前（Flink 1.1.x） JoinedStreams 只是简单地实现了流上的 join 操作而已，距离真正的生产使用还是有些距离。因为目前 join 窗口的双流数据都是被缓存在内存中的，也就是说如果某个 key 上的窗口数据太多就会导致 JVM OOM（然而数据倾斜是常态）。双流 join 的难点也正是在这里，这也是社区后面对 join 操作的优化方向，例如可以借鉴 Flink 在批处理 join 中的优化方案，也可以用 ManagedMemory 来管理窗口中的数据，并当数据超过阈值时能spill到硬盘。 5. ConnectedStreams在 DataStream 上有一个 union 的转换 dataStream.union(otherStream1, otherStream2, ...)，用来合并多个流，新的流会包含所有流中的数据。union 有一个限制，就是所有合并的流的类型必须是一致的。ConnectedStreams 提供了和 union 类似的功能，用来连接两个流，但是与 union 转换有以下几个区别： ConnectedStreams 只能连接两个流，而 union 可以连接多于两个流。 ConnectedStreams 连接的两个流类型可以不一致，而 union 连接的流的类型必须一致。 ConnectedStreams 会对两个流的数据应用不同的处理方法，并且双流之间可以共享状态。这在第一个流的输入会影响第二个流时, 会非常有用。 如下 ConnectedStreams 的样例，连接 input 和 other 流，并在 input 流上应用 map1 方法，在 other 上应用 map2 方法，双流可以共享状态（比如计数）。 val input: DataStream[MyType] = ...val other: DataStream[AnotherType] = ...val connected: ConnectedStreams[MyType, AnotherType] = input.connect(other)val result: DataStream[ResultType] = connected.map(new CoMapFunction[MyType, AnotherType, ResultType]() &#123; override def map1(value: MyType): ResultType = &#123; ... &#125; override def map2(value: AnotherType): ResultType = &#123; ... &#125; &#125;) 当并行度为2时，其执行图如下所示： 6. 总结本文介绍通过不同数据流类型的转换图来解释每一种数据流的含义、转换关系。后面的文章会深入讲解 Window 机制的实现，双流 Join 的实现等。 原文:http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 执行计划]]></title>
    <url>%2Fflink-execution-plans.html</url>
    <content type="text"><![CDATA[根据各种参数(如数据大小或集群中的机器数量)，Flink的优化器自动会为你的程序选择一个执行策略。很多情况下，准确的知道Flink如何执行你的程序是很有帮助的。 1. 计划可视化工具Flink内置一个执行计划的可视化工具。包含可视化工具的HTML文档位于tools/planVisualizer.html下。用JSON表示作业执行计划，并将其可视化为具有执行策略完整注释的图(visualizes it as a graph with complete annotations of execution strategies)。 备注:打开可视化工具的方式有所改变:由本地文件 tools/planVisualizer.html 改为 url http://flink.apache.org/visualizer/index.html 以下代码显示了如何从程序中打印执行计划的JSON： Java版本:final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();...System.out.println(env.getExecutionPlan()); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment...println(env.getExecutionPlan()) 要可视化执行计划，请执行以下操作： (1) 使用浏览器打开planVisualizer.html(或者直接在浏览器中输入http://flink.apache.org/visualizer/index.html 网址) (2) 将JSON字符串粘贴到文本框中 (3) 点击Draw按钮 完成上面这些步骤后，将会显示详细的执行计划。 2. Web界面Flink提供了一个用于提交和执行作业的Web界面。这个界面是JobManager Web监控界面的一部分，默认情况下在端口8081上运行。通过这个界面提交作业需要你在flink-conf.yaml中设置jobmanager.web.submit.enable：true。 你可以在作业执行之前指定程序参数。执行计划可视化器使你能够在执行Flink作业之前查看执行计划。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 并发执行]]></title>
    <url>%2Fflink-parallel-execute.html</url>
    <content type="text"><![CDATA[本节介绍如何在Flink中配置程序的并行执行。一个Flink程序由多个任务(transformations/operators，data sources和sinks)组成。一个任务被分成多个并发实例来执行，每个并发实例只处理任务输入数据的一个子集。一个任务的并发实例的个数称为并发度(parallelism)。 如果你想使用保存点，也应该考虑设置最大并发度。从保存点恢复时，可以更改特定算子或整个程序的并发度，并且此配置指定了并发的上限。 1. 设置并发度一个任务的并发度可以在Flink中指定不同级别。 1.1 算子级别单个算子，数据源，sink可以通过调用setParallelism()方法来定义并发度。例如，像这样： Java版本:DataStream&lt;String&gt; text = [...]DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text .flatMap(new LineSplitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1).setParallelism(5);wordCounts.print();env.execute("Word Count Example"); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentval text = [...]val wordCounts = text .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1).setParallelism(5)wordCounts.print()env.execute(&quot;Word Count Example&quot;) 1.2 执行环境级别如这所述，Flink程序是在执行环境的上下文中执行的。执行环境为它执行的所有算子，数据源和数据sink提供了默认的并发度。执行环境的并发度可以通过显式配置一个算子的并发度来覆盖。 执行环境的默认并发度可以通过调用setParallelism()方法来指定。要为执行的所有算子，数据源和sink设置并发度为3，请按如下方式设置执行环境的默认并发度： Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(3);DataStream&lt;String&gt; text = [...]DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = [...]wordCounts.print();env.execute("Word Count Example"); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(3)val text = [...]val wordCounts = text .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1)wordCounts.print()env.execute(&quot;Word Count Example&quot;) 1.3 客户端级别在向Flink提交作业时，可以在客户端设置并发度。客户端可以是Java或Scala程序。Flink的命令行接口(CLI)就是一种客户端。 对于CLI客户端，可以使用-p指定并发度参数。 例如：./bin/flink run -p 10 ../examples/*WordCount-java*.jar 在Java/Scala程序中，并发度设置如下： Java版本:try &#123; PackagedProgram program = new PackagedProgram(file, args); InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport("localhost:6123"); Configuration config = new Configuration(); Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader()); // set the parallelism to 10 here client.run(program, 10, true);&#125; catch (ProgramInvocationException e) &#123; e.printStackTrace();&#125; Scala版本:try &#123; PackagedProgram program = new PackagedProgram(file, args) InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(&quot;localhost:6123&quot;) Configuration config = new Configuration() Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader()) // set the parallelism to 10 here client.run(program, 10, true)&#125; catch &#123; case e: Exception =&gt; e.printStackTrace&#125; 1.4 系统级别可以通过在./conf/flink-conf.yaml中设置parallelism.default属性来为所有执行环境定义全系统默认并发度。详细信息请参阅配置文档。 2. 设置最大并发度最大并发度可以在可以设置并发度的地方设置(客户端级别和系统级别除外)。你可以调用setMaxParallelism()取代setParallelism()方法来设置最大并发度。 最大并发度的默认设置大致为operatorParallelism +（operatorParallelism / 2），下限为127，上限为32768。 备注:将最大并发度设置为非常大的数值可能会对性能造成不利影响，因为一些后端状态必须保持在内部数据结构，而这些内部数据结构随key-groups(这是可扩展状态的内部实现机制)的数量进行扩展。(some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state).) 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 累加器与计数器]]></title>
    <url>%2Fflink-accumulators-counters.html</url>
    <content type="text"><![CDATA[1. 概述累加器(Accumulators)是一个简单的构造器，具有加法操作和获取最终累加结果操作，在作业结束后可以使用。 最直接的累加器是一个计数器(counter)：你可以使用Accumulator.add()方法对其进行累加。在作业结束时，Flink将合并所有部分结果并将最终结果发送给客户端。在调试过程中，或者你快速想要了解有关数据的更多信息，累加器很有用。 目前Flink拥有以下内置累加器。它们中的每一个都实现了累加器接口： (1) IntCounter, LongCounter 以及 DoubleCounter: 参阅下面示例中使用的计数器。 (2) Histogram：为离散数据的直方图(A histogram implementation for a discrete number of bins.)。内部它只是一个整数到整数的映射。你可以用它来计算值的分布，例如 单词计数程序的每行单词分配。 2. 如何使用首先，你必须在你要使用的用户自定义转换函数中创建一个累加器(accumulator)对象(这里是一个计数器):private IntCounter numLines = new IntCounter(); 其次，你必须注册累加器(accumulator)对象，通常在rich函数的open()方法中注册。在这里你也可以自定义累加器的名字:getRuntimeContext().addAccumulator("num-lines", this.numLines); 现在你就可以在算子函数中的任何位置使用累加器，包括在open()和close()方法中:this.numLines.add(1); 最后结果将存储在JobExecutionResult对象中，该对象从执行环境的execute()方法返回(当前仅当执行等待作业完成时才起作用):JobExecutionResult result = env.execute();long lineCounter = result.getAccumulatorResult("num-lines");System.out.println(lineCounter); 每个作业的所有累加器共享一个命名空间。因此，你可以在作业的不同算子函数中使用同一个累加器。Flink在内部合并所有具有相同名称的累加器。 备注:目前累加器的结果只有在整个工作结束之后才可以使用。我们还计划在下一次迭代中可以使用前一次迭代的结果。你可以使用聚合器来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。 3. Exampleimport com.google.gson.Gson;import com.google.gson.GsonBuilder;import com.qunar.innovation.data.bean.AdsPushBehavior;import com.qunar.innovation.data.utils.ConstantUtil;import org.apache.flink.api.common.accumulators.LongCounter;import org.apache.flink.api.common.functions.RichMapFunction;import org.apache.flink.configuration.Configuration;public class AdsPushParseMap extends RichMapFunction&lt;String, AdsPushBehavior&gt; &#123; private static Gson gson = new GsonBuilder().setDateFormat("yyyy-MM-dd HH:mm:ss").create(); private final LongCounter behaviorCounter = new LongCounter(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); getRuntimeContext().addAccumulator(ConstantUtil.ADS_PUSH_APP_CODE, behaviorCounter); &#125; @Override public AdsPushBehavior map(String content) throws Exception &#123; try&#123; // 解析 AdsPushBehavior adsPushBehavior = gson.fromJson(content, AdsPushBehavior.class); this.behaviorCounter.add(1); return adsPushBehavior; &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125;&#125; import com.qunar.innovation.data.TestFlink;import com.qunar.innovation.data.functions.*;import com.qunar.innovation.data.utils.ConstantUtil;import org.apache.flink.api.common.JobExecutionResult;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.core.fs.FileSystem;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class AdsPushLocalStream &#123; private final static Logger LOGGER = LoggerFactory.getLogger(TestFlink.class); public static void main(String[] args) &#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;String&gt; dataSet = env.readTextFile("file:///home/xiaosi/input.txt"); // 处理数据 DataSet&lt;String&gt; adsPushDataSet = dataSet.map(new ContentMap()).name("contentMap").setParallelism(1). map(new AdsPushParseMap()).name("behaviorMap").setParallelism(1) .map(new AdsPushFeatureMap()).name("featureMap").setParallelism(1) .filter(new AdsPushFeatureFilter()).name("featureFilter").setParallelism(1); adsPushDataSet.writeAsText("file:///home/xiaosi/output", FileSystem.WriteMode.OVERWRITE); try &#123; JobExecutionResult result = env.execute(); long behaviorCounter = result.getAccumulatorResult(ConstantUtil.ADS_PUSH_APP_CODE); System.out.println(behaviorCounter); &#125; catch (Exception e) &#123; LOGGER.error(e.getMessage(), e); &#125; &#125;&#125; 3. 自定义累加器为了实现你自己的累加器，你只需要编写你的Accumulator接口的实现。如果你认为你的自定义累加器应与Flink一起传输，请随意创建一个拉取请求(Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.)。 你可以选择实现Accumulator或SimpleAccumulator。 Accumulator&lt;V，R&gt;非常灵活：它为要添加的值定义一个类型V，并为最终结果定义一个结果类型R。例如，对于直方图，V是数字，R是直方图。SimpleAccumulator适用于两种类型相同的情况，例如，计数器。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 定义keys的几种方法]]></title>
    <url>%2Fflink-how-to-specifying-keys.html</url>
    <content type="text"><![CDATA[一些转换(例如，join，coGroup，keyBy，groupBy)要求在一组元素上定义一个key。其他转换(Reduce，GroupReduce，Aggregate，Windows)允许在使用这些函数之前根据key对数据进行分组。 一个DataSet进行分组如下:DataSet&lt;...&gt; input = // [...]DataSet&lt;...&gt; reduced = input.groupBy(/*define key here*/).reduceGroup(/*do something*/); DataStream也可以指定一个key:DataStream&lt;...&gt; input = // [...]DataStream&lt;...&gt; windowed = input.keyBy(/*define key here*/).window(/*window specification*/); Flink的数据模型不是基于键值对。因此，没有必要将数据集类型打包成keys和values。keys是”虚拟”：它们只是被定义在实际数据之上的函数，以指导分组算子使用。 备注:在下面的讨论中，我们将使用DataStream API和keyBy。对于DataSet API，你只需要替换为DataSet和groupBy即可。 下面介绍几种Flink定义keys方法。 1. 为Tuples类型定义keys最简单的情况就是在元组的一个或多个字段上对元组进行分组。下面是在元组的第一个字段(整数类型)上进行分组： Java版本:DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = // [...]KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(0) Scala版本:val input: DataStream[(Int, String, Long)] = // [...]val keyed = input.keyBy(0) 下面，我们将在复合key上对元组进行分组，复合key包含元组的第一个和第二个字段: Java版本:DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = // [...]KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(0,1) Scala版本:val input: DataSet[(Int, String, Long)] = // [...]val grouped = input.groupBy(0,1) 如果你有一个包含嵌套元组的DataStream，例如：DataStream&lt;Tuple3&lt;Tuple2&lt;Integer, Float&gt;,String,Long&gt;&gt; ds; 如果指定keyBy(0)，则使用整个Tuple2作为key(以Integer和Float为key)。如果要使用嵌套中Tuple2的某个字段，则必须使用下面介绍的字段表达式指定keys。 2. 使用字段表达式定义keys你可以使用基于字符串的字段表达式来引用嵌套字段以及定义keys来进行分组，排序，连接或coGrouping。字段表达式可以非常容易地选择(嵌套)复合类型(如Tuple和POJO类型)中的字段。 在下面的例子中，我们有一个WC POJO，它有两个字段word和count。如果想通过word字段分组，我们只需将word传递给keyBy()函数即可。 // some ordinary POJO (Plain old Java Object)public class WC &#123; public String word; public int count;&#125;DataStream&lt;WC&gt; words = // [...]DataStream&lt;WC&gt; wordCounts = words.keyBy("word").window(/*window specification*/); 字段表达式语法: (1) 按其字段名称选择POJO字段。例如，user是指向POJO类型的user字段。 (2) 通过字段名称或0到offset的数值字段索引来选择元组字段(field name or 0-offset field index)。例如，f0和5分别指向Java元组类型的第一和第六字段。 (3) 你可以在POJO和元组中选择嵌套字段。例如，user.zip是指POJO类型user字段中的zip字段。支持POJO和Tuples的任意嵌套和组合，如f1.user.zip或user.f3.1.zip。 (4) 你可以使用*通配符表达式选择所有类型。这也适用于不是元组或POJO类型的类型。 Example:public static class WC &#123; public ComplexNestedClass complex; //nested POJO private int count; // getter / setter for private field (count) public int getCount() &#123; return count; &#125; public void setCount(int c) &#123; this.count = c; &#125;&#125;public static class ComplexNestedClass &#123; public Integer someNumber; public float someFloat; public Tuple3&lt;Long, Long, String&gt; word; public IntWritable hadoopCitizen;&#125; 下面是上述示例代码的有效字段表达式：count：WC类中的count字段。complex：递归地选择复合字段POJO类型ComplexNestedClass的所有字段。complex.word.f2：选择嵌套字段Tuple3的最后一个字段。complex.hadoopCitizen：选择Hadoop IntWritable类型。 3. 使用key Selector 函数定义keys定义key的另一种方法是key选择器函数。key选择器函数将单个元素作为输入，并返回元素的key。key可以是任何类型的。 以下示例显示了一个key选择器函数，它只返回一个对象的字段： Java版本:public class WC &#123; public String word; public int count;&#125;DataStream&lt;WC&gt; words = // [...]KeyedStream&lt;WC&gt; kyed = words.keyBy(new KeySelector&lt;WC, String&gt;() &#123; public String getKey(WC wc) &#123; return wc.word; &#125;&#125;); Scala版本:case class WC(word: String, count: Int)val words: DataStream[WC] = // [...]val keyed = words.keyBy( _.word ) 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 Flink程序剖析]]></title>
    <url>%2Fflink-anatomy-of-a-flink-program.html</url>
    <content type="text"><![CDATA[Flink程序程序看起来像转换数据集合的普通程序。每个程序都由相同的基本部分组成： 获得一个执行环境 加载/创建初始数据 指定在这些数据上的转换操作 指定计算结果存放位置 触发程序执行 现在我们将对每一步进行一个简要的概述。请注意，Java DataSet API的所有核心类都可以在org.apache.flink.api.java包中找到，而Java DataStream API的类可以在org.apache.flink.streaming.api中找到。Scala DataSet API的所有核心类都可以在org.apache.flink.api.scala包中找到，而Scala DataStream API的类可以在org.apache.flink.streaming.api.scala中找到。 StreamExecutionEnvironment是所有Flink程序的基础。你可以使用StreamExecutionEnvironment上的如下静态方法获取：Java版本:getExecutionEnvironment()createLocalEnvironment()createRemoteEnvironment(String host, int port, String... jarFiles) Scala版本:getExecutionEnvironment()createLocalEnvironment()createRemoteEnvironment(host: String, port: Int, jarFiles: String*) 通常情况下，我们只需要使用getExecutionEnvironment()即可，因为这会根据上下文做正确的选择：如果你在IDE内执行程序或作为常规的Java程序，将创建一个本地环境，在你的本地机器上执行你的程序。如果使用程序创建JAR文件并通过命令行调用它，那么Flink集群管理器将执行你的main方法，并且getExecutionEnvironment()返回一个用于在集群上执行你程序的执行环境。 对于指定数据源，执行环境有多种方法可以从文件中读取数据：可以逐行读取，以CSV格式文件读取或使用完全自定义的数据输入格式。只要将文本文件作为一系列行读取，就可以使用： Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;String&gt; text = env.readTextFile("file:///path/to/file"); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()val text: DataStream[String] = env.readTextFile(&quot;file:///path/to/file&quot;) 这将为你提供一个DataStream，然后就可以应用转换函数来创建新的派生DataStream。 通过调用DataStream上的转换函数来应用转换操作。例如，一个map转换函数看起来像这样： Java版本:DataStream&lt;String&gt; input = ...;DataStream&lt;Integer&gt; parsed = input.map(new MapFunction&lt;String, Integer&gt;() &#123; @Override public Integer map(String value) &#123; return Integer.parseInt(value); &#125;&#125;); Scala版本:val input: DataSet[String] = ...val mapped = input.map &#123; x =&gt; x.toInt &#125; 这将通过将原始集合中的每个String转换为Integer来创建一个新的DataStream。 一旦获得了包含最终结果的DataStream，就可以通过创建接收器(sink)将其写入外部系统中。下面是创建接收器的一些示例方法： Java版本:writeAsText(String path)print() Scala版本:writeAsText(path: String)print() 一旦你指定的完整程序需要触发程序执行，可以通过调用StreamExecutionEnvironment的execute()方法来触发程序的执行。根据执行环境的类型，执行将在你的本地机器上触发，或提交程序在集群上执行。 execute()方法返回一个JobExecutionResult，它包含执行时间和累加器结果。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 安装与启动]]></title>
    <url>%2Fflink-how-to-install-and-run.html</url>
    <content type="text"><![CDATA[1. 下载Flink 可以运行在 Linux, Mac OS X和Windows上。为了运行Flink, 唯一的要求是必须在Java 7.x (或者更高版本)上安装。Windows 用户, 请查看 Flink在Windows上的安装指南。 你可以使用以下命令检查Java当前运行的版本：java -version 如果你安装的是Java 8，输出结果类似于如下:java version &quot;1.8.0_91&quot;Java(TM) SE Runtime Environment (build 1.8.0_91-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode) 从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合方式。如果你只是打算使用本地文件系统，那么可以使用任何版本的Hadoop。进入下载目录，解压下载的压缩包:xiaosi@yoona:~$ tar -zxvf flink-1.3.2-bin-hadoop27-scala_2.11.tgz -C opt/flink-1.3.2/flink-1.3.2/opt/flink-1.3.2/opt/flink-cep_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-datadog-1.3.2.jarflink-1.3.2/opt/flink-metrics-statsd-1.3.2.jarflink-1.3.2/opt/flink-gelly_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-dropwizard-1.3.2.jarflink-1.3.2/opt/flink-gelly-scala_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-ganglia-1.3.2.jarflink-1.3.2/opt/flink-cep-scala_2.11-1.3.2.jarflink-1.3.2/opt/flink-table_2.11-1.3.2.jarflink-1.3.2/opt/flink-ml_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-graphite-1.3.2.jarflink-1.3.2/lib/... 2. 启动本地集群使用如下命令启动Flink：xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/start-local.shStarting jobmanager daemon on host yoona. 通过访问 http://localhost:8081 检查JobManager网页,确保所有组件都启动并已运行。网页会显示一个有效的TaskManager实例。 你也可以通过检查日志目录里的日志文件来验证系统是否已经运行:xiaosi@yoona:~/opt/flink-1.3.2/log$ cat flink-xiaosi-jobmanager-0-yoona.log | less2017-10-16 14:42:10,972 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager (Version: 1.3.2, Rev:0399bee, Date:03.08.2017 @ 10:23:11 UTC)...2017-10-16 14:42:11,109 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager without high-availability2017-10-16 14:42:11,111 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager on localhost:6123 with execution mode LOCAL...2017-10-16 14:42:11,915 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager web frontend...2017-10-16 14:42:13,941 INFO org.apache.flink.runtime.instance.InstanceManager - Registered TaskManager at localhost (akka://flink/user/taskmanager) as 0df4d4ebd25ffec4878906726c29f88c. Current number of registered hosts is 1. Current number of alive task slots is 1.... 3. Example Code你可以在GitHub上找到SocketWindowWordCount例子的完整代码，有Java和Scala两个版本。 Scala:package org.apache.flink.streaming.scala.examples.socketimport org.apache.flink.api.java.utils.ParameterToolimport org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Time/** * Implements a streaming windowed version of the "WordCount" program. * * This program connects to a server socket and reads strings from the socket. * The easiest way to try this out is to open a text sever (at port 12345) * using the ''netcat'' tool via * &#123;&#123;&#123; * nc -l 12345 * &#125;&#125;&#125; * and run this example with the hostname and the port as arguments.. */object SocketWindowWordCount &#123; /** Main program method */ def main(args: Array[String]) : Unit = &#123; // the host and the port to connect to var hostname: String = "localhost" var port: Int = 0 try &#123; val params = ParameterTool.fromArgs(args) hostname = if (params.has("hostname")) params.get("hostname") else "localhost" port = params.getInt("port") &#125; catch &#123; case e: Exception =&gt; &#123; System.err.println("No port specified. Please run 'SocketWindowWordCount " + "--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) and port " + "is the address of the text server") System.err.println("To start a simple text server, run 'netcat -l &lt;port&gt;' " + "and type the input text into the command line") return &#125; &#125; // get the execution environment val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // get input data by connecting to the socket val text: DataStream[String] = env.socketTextStream(hostname, port, '\n') // parse the data, group it, window it, and aggregate the counts val windowCounts = text .flatMap &#123; w =&gt; w.split("\\s") &#125; .map &#123; w =&gt; WordWithCount(w, 1) &#125; .keyBy("word") .timeWindow(Time.seconds(5)) .sum("count") // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1) env.execute("Socket Window WordCount") &#125; /** Data type for words with count */ case class WordWithCount(word: String, count: Long)&#125; Java版本:package org.apache.flink.streaming.examples.socket;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.ReduceFunction;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;/** * Implements a streaming windowed version of the "WordCount" program. * * &lt;p&gt;This program connects to a server socket and reads strings from the socket. * The easiest way to try this out is to open a text server (at port 12345) * using the &lt;i&gt;netcat&lt;/i&gt; tool via * &lt;pre&gt; * nc -l 12345 * &lt;/pre&gt; * and run this example with the hostname and the port as arguments. */@SuppressWarnings("serial")public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the host and the port to connect to final String hostname; final int port; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); hostname = params.has("hostname") ? params.get("hostname") : "localhost"; port = params.getInt("port"); &#125; catch (Exception e) &#123; System.err.println("No port specified. Please run 'SocketWindowWordCount " + "--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) " + "and port is the address of the text server"); System.err.println("To start a simple text server, run 'netcat -l &lt;port&gt;' and " + "type the input text into the command line"); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostname, port, "\n"); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split("\\s")) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy("word") .timeWindow(Time.seconds(5)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute("Socket Window WordCount"); &#125; // ------------------------------------------------------------------------ /** * Data type for words with count. */ public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + " : " + count; &#125; &#125;&#125; 4. 运行Example现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读取一段文本，并且每隔5秒打印之前5秒内每个单词出现的个数。例如：a tumbling window of processing time, as long as words are floating in. (1) 首先,我们可以通过netcat命令来启动本地服务:nc -l 9000 (2) 提交Flink程序:xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000Cluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programSubmitting job with JobID: a963626a1e09f7aeb0dc34412adfb801. Waiting for job completion.Connected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#941160871] with leader session id 00000000-0000-0000-0000-000000000000.10/16/2017 15:12:26 Job execution switched to status RUNNING.10/16/2017 15:12:26 Source: Socket Stream -&gt; Flat Map(1/1) switched to SCHEDULED10/16/2017 15:12:26 TriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to SCHEDULED10/16/2017 15:12:26 Source: Socket Stream -&gt; Flat Map(1/1) switched to DEPLOYING10/16/2017 15:12:26 TriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to DEPLOYING10/16/2017 15:12:26 Source: Socket Stream -&gt; Flat Map(1/1) switched to RUNNING10/16/2017 15:12:26 TriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to RUNNING 应用程序连接socket并等待输入，你可以通过web界面来验证任务期望的运行结果： 单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) :xiaosi@yoona:~/opt/flink-1.3.2$ nc -l 9000lorem ipsumipsum ipsum ipsumbye .out文件将在每个时间窗口截止之际打印每个单词的个数：xiaosi@yoona:~/opt/flink-1.3.2$ tail -f log/flink-*-jobmanager-*.outlorem : 1bye : 1ipsum : 4 使用以下命令来停止Flink:./bin/stop-local.sh 阅读更多的例子来熟悉Flink的编程API。 当你完成这些，可以继续阅读streaming指南。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之分布式运行环境]]></title>
    <url>%2Fflink-distributed-runtime.html</url>
    <content type="text"><![CDATA[1. 任务链与算子链在分布式运行中，Flink将算子(operator) SubTask 连接成 Task。每个 Task 都只由一个线程执行。将算子链接到 Task 是一个很有用处的优化：它降低了线程间切换和缓冲的开销，并增加了整体吞吐量，同时降低了延迟。链接行为可以在API中配置。 下图中的示例数据流由五个子任务执行，因此具有五个并行线程。 2. 作业管理器, 任务管理器, 客户端Flink运行时(runtime)由两种类型的进程组成： (1) 作业管理器JobManagers(也称为masters)协调分布式运行。主要功能是调度任务，协调检查点，协调故障恢复等。 至少有一个JobManager。高可用配置下将有多个JobManagers，其中一个始终是领导者，其他都是备份。 (2) 任务管理器TaskManagers(也称为workers)执行数据流中的任务(更具体地说是子任务)，并对数据流进行缓冲和交换。 跟JobManager一样，也是至少有一个TaskManager。 JobManagers和TaskManagers可以以不同方式启动：直接在机器上，在容器中，或者由像YARN这样的资源框架来管理。TaskManagers与JobManagers进行连接，来报告自己可用，并分配工作。 客户端不是运行时和程序执行的一部分，而是用来准备数据流并将其发送到JobManager。之后，客户端可以断开连接或保持连接来接收进度报告。客户端作为触发执行的Java/Scala程序的一部分运行，或者在命令行中运行./bin/flink命令来运行…. 3. 任务槽与资源每个worker(TaskManager)都是一个JVM进程，可以在不同的线程中执行一个或多个子任务(译者注:一个任务有一个线程执行)。worker使用任务槽(至少一个)来控制worker能接受多少任务。 每个任务槽代表TaskManager的一个固定资源子集。例如，一个拥有三个任务槽的TaskManager将为每个任务槽分配1/3的内存。资源任务槽化意味着子任务不会与其他作业中的子任务争夺内存，而是任务具有一定数量的保留托管内存。请注意，这里不会对CPU进行隔离。目前任务槽只分离任务的托管内存。 通过调整任务槽的数量，用户可以定义子任务与其他子任务进行隔离。如果每个TaskManager只拥有一个任务槽意味着每个任务组都会在独立的JVM中运行(例如，可以在单独的容器中启动)。如果拥有多个任务槽意味着多个子任务共享同一个JVM。同一JVM中的任务共享TCP连接(通过多路复用)和心跳消息，他们也可以共享数据集和数据结构，从而降低单个任务的开销。 默认情况下，Flink允许子任务共享任务槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个任务槽可能会是一个完整的作业管道。允许任务槽共享有两个主要好处： (1) Flink集群所需的任务槽数与作业中使用的最高并行度数保持一致。不需要计算一个程序总共包含多少个任务(不同任务具有不同的并行度)。 (2) 提高资源利用率。如果没有使用任务槽共享机制，那么非密集的sour/map()子任务就会与资源密集型window子任务阻塞一样多的资源。在我们的示例中，通过任务槽共享，将基本并行度从两个增加到六个，可以充分利用已分配的资源，同时确保繁重的子任务在TaskManager之间公平分配。 这些API还包括一个资源组机制，可以避免不合理的任务槽共享。 根据经验来说，默认任务槽数应该设置为CPU核的数量。如果使用超线程技术，每个任务槽需要2个或更多的硬件线程上下文(With hyper-threading, each slot then takes 2 or more hardware thread contexts)。 4. 后端状态键/值索引存储的确切数据结构取决于所选的后端状态。一个后端状态将数据存储在内存中hash map中，另一个后端状态使用RocksDB存储键/值。除了定义保存状态的数据结构之外，后端状态还实现了获取键/值状态的时间点快照逻辑并将该快照存储为检查点的一部分。 5. 保存点用Data Stream API编写的程序可以从保存点恢复执行。保存点允许更新你的程序和你的Flink集群，而不会丢失任何状态。 保存点是手动触发的检查点，它会捕获程序的快照并将其写入后端状态。他们依赖于常规检查点机制。在执行期间的程序定期在工作节点上生成快照并生成检查点。为了恢复，只需要最后完成的检查点，一旦新的检查点完成，可以安全地丢弃较旧的检查点。 保存点与这些定期检查点类似，只不过它们是由用户触发的，不会在新检查点完成时自动失效。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 工作过程]]></title>
    <url>%2Fhadoop-mapreduce-working-process.html</url>
    <content type="text"><![CDATA[1. 从输入到输出一个MapReducer作业经过了input，map，combine，reduce，output五个阶段，其中combine阶段并不一定发生，map输出的中间结果被分到reduce的过程成为shuffle（数据清洗）。在shuffle阶段还会发生copy（复制）和sort（排序）。 在MapReduce的过程中，一个作业被分成Map和Reducer两个计算阶段，它们由一个或者多个Map任务和Reduce任务组成。如下图所示，一个MapReduce作业从数据的流向可以分为Map任务和Reduce任务。当用户向Hadoop提交一个MapReduce作业时，JobTracker则会根据各个TaskTracker周期性发送过来的心跳信息综合考虑TaskTracker的资源剩余量，作业优先级，作业提交时间等因素，为TaskTracker分配合适的任务。Reduce任务默认会在Map任务数量完成5%后才开始启动。 Map任务的执行过程可以概括为：首先通过用户指定的InputFormat类中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。然后map函数经过处理之后将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。最后map函数将中间结果保存到本地。 Reduce任务的执行过程可以概括为：首先需要将已经完成Map任务的中间结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。 2. input如果使用HDFS上的文件作为MapReduce的输入，MapReduce计算框架首先会用 org.apache.hadoop.mapreduce.InputFomat 类的子类FileInputFormat类将作为输入HDFS上的文件切分形成输入分片(InputSplit)，每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MaoReduce作业的性能有非常大的影响。 InputSplit 只是逻辑上对输入数据进行分片，并不会将文件在磁盘上分成分片进行存储。InputSplit 只是记录了分片的元数据节点信息，例如起始位置，长度以及所在的节点列表等。数据切分的算法需要确定 InputSplit 的个数，对于 HDFS 上的文件，FileInputFormat 类使用 computeSplitSize 方法计算出 InputSplit 的大小，代码如下：protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123; return Math.max(minSize, Math.min(maxSize, blockSize));&#125; 其中 minSize 由 mapred-site.xml 文件中的配置项 mapred.min.split.size 决定，默认为1；maxSize 由 mapred-site.xml 文件中的配置项 mapred.max.split.size 决定，默认为9223 372 036 854 775 807；而 blockSize 是由 hdfs-site.xml 文件中的配置项 dfs.block.size 决定，默认为67 108 864字节（64M）。所以InputSplit的大小确定公式为：max(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size)); 一般来说，dfs.block.size 的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size 和 mapred.max.split.size 的大小即可。InputSplit的数量为文件大小除以InputSplitSize。InputSplit的原数据信息会通过一下代码取得：splits.add(new FileSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts())); 从上面的代码可以发现，元数据的信息由四部分组成：文件路径，文件开始位置，文件结束位置，数据块所在的host。 对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念，InputSplit所包含的数据是仍然存储在HDFS的块里面，它们之间的关系如下图所示： 当输入文件切分为InputSplit后，由FileInputFormat的子类（如TextInputFormat）的createRecordReader方法将InputSplit解析为键值对，代码如下：public RecordReader&lt;LongWritable, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) &#123; String delimiter = context.getConfiguration().get( "textinputformat.record.delimiter"); byte[] recordDelimiterBytes = null; if (null != delimiter) recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8); return new LineRecordReader(recordDelimiterBytes);&#125; 此处默认是将行号作为键。解析出来的键值对将被用来作为map函数的输入。至此input阶段结束。 3. map及中间结果的输出InputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和 sort（排序）的操作。 map函数产生输出时，并不是简单的刷写磁盘。为了保证I/O效率，采取了先写到内存的环形内存缓冲区，并做一次预排序，如下图所示： 每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲区大小是100M，该值可以通过mapred-site.xml文件中的 io.sort.mb 的配置项配置。一旦缓冲区内容达到阈值（由mapred-site.xml文件的 io.sort.spill.percent 的值决定，默认为0.80 或者 80%），一个后台线程便会将缓冲区的内容溢写到磁盘中。再写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到 mapred.local.dir（mapred-site.xml文件的配置项）配置的作业特定目录下。 在写磁盘之前，线程会根据数据最终要传入到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程按照建进行内排序，此时如果有一个Combiner，它会在排序后的输出上运行。 一旦内存缓冲区达到溢出的阈值，就会新建一个溢出写文件，因此在Map任务完成最后一个输出记录之后，会有若干个溢出写文件。在Map任务完成之前，溢出写文件被合并成一个已分区且已排序的输出文件作为map输出的中间结果，这也是Map任务的输出结果。 如果已经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner的意义在于使map输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。 为了提高磁盘IO性能，可以考虑压缩map的输出，这样会写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将 mapred-site.xml文件的配置项 mapred.compress.map.output 设为true即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项 mapred.map.output.compression.codec。 指定，如下列出了Hadoop支持的常见压缩格式： map输出的中间结果存储的格式为IFile，IFile是一种支持航压缩的存储格式，支持上述压缩算法。 Reducer通过Http方式得到输出文件的分区。将map输出的中间结果发送到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threds配置项决定，此配置针对每个节点，而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。 4. shuffleshuffle，也叫数据清洗。在某些语境下，代表map函数产生输出到reduce的消化输入的整个过程。 4.1 copy阶段Map任务输出的结果位于Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是，Reduce任务可能需要多个Map任务的输出作为其特殊的分区文件。每个Map任务的完成时间可能不同，当只要有一个任务完成，Reduce任务就开始复制其输出。这就是shuffle的copy阶段。如下图所示，Reduce任务有少量复制线程，可以并行取得Map任务的输出，默认值为5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies的配置项来改变。 如果map输出相当小，则会被复制到Reduce所在TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffle.input.buffer.percent配置项指定。否则，map输出将会被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapred-site.xml文件mapred.job.shuffle.merge.percent配置项决定）或缓冲区的文件数达到阈值大小（由mapred-site.xml文件mapred.inmem.merge.threshold配置项决定），则合并后溢写到磁盘中。 4.2 sort阶段随着溢写到磁盘的文件增多，shuffle进行sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的 io.sort.factor 配置项决定，默认为10）为10，合并操作将进行5次，每次将10个文件合并成一个文件，最后有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合并，将会直接把5个文件交给Reduce函数处理。到此shuffle阶段完成。 从shuffle的过程可以看出，Map任务处理的是一个InputSplit，而Reduce任务处理的是所有Map任务同一个分区的中间结果。 5. reduce及最后结果的输出reduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。 与map的中间结果不同的是，reduce的输出一般为HDFS。 6. sort排序贯穿于Map任务和Reduce任务，排序操作属于MapReduce计算框架的默认行为，不管流程是否需要，都会进行排序。在MapReduce计算框架中，主要用到了两种排序算法：快速排序和归并排序。 在Map任务和Reduce任务的过程中，一共发生了3次排序操作。 （1）当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阈值，在刷写磁盘之前，后台线程会将缓冲区的数据划分相应的分区。在每个分区中，后台线程按键进行内排序。如下图所示。 （2）在Map任务完成之前，磁盘上存在多个已经分好区，并排好序，大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。如下图所示。 （3）在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需在做一次排序就可以使输出文件整体有序: 在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序；第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。 7. 作业的进度组成一个MapReduce作业在Hadoop上运行时，客户端的屏幕通常会打印作业日志，如下： 对于一个大型的MapReduce作业来说，执行时间可能会比较比较长，通过日志了解作业的运行状态和作业进度是非常重要的。对于Map来说，进度代表实际处理输入所占比例，例如 map 60% reduce 0% 表示Map任务已经处理了作业输入文件的60%，而Reduce任务还没有开始。而对于Reduce的进度来说，情况比较复杂，从前面得知，reduce阶段分为copy，sort 和 reduce，这三个步骤共同组成了reduce的进度，各占1/3。如果reduce已经处理了2/3的输入，那么整个reduce的进度应该为 1/3 + 1/3 + 1/3 * (2/3) = 5/9 ，因为reduce开始处理时，copy和sort已经完成。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 配置远程登录]]></title>
    <url>%2FMySQL%2F%5BMySQL%5DMySQL%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95.html</url>
    <content type="text"><![CDATA[1. 修改配置修改/etc/mysql/mysql.conf.d目录下的mysqld.cnf配置文件:# Instead of skip-networking the default is now to listen only on# localhost which is more compatible and is not less secure.#bind-address = 127.0.0.1 在bind-address前面加个#进行注释，允许任意IP访问。或者指定自己需要远程访问的IP地址。然后重启mysql:ubuntu@VM-0-7-ubuntu:/etc/mysql/mysql.conf.d$ sudo /etc/init.d/mysql restartRestarting mysql (via systemctl): mysql.service. 2. 授权用户我们先看一下当前能登录到我们数据的用户以及允许连接的IP:mysql&gt; USE mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select User,Host from user;+------------------+-----------+| User | Host |+------------------+-----------+| debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost || root | localhost |+------------------+-----------+4 rows in set (0.00 sec) 我们可以看到只有一个默认的root用户，且只允许使用localhost连接。下面我们另外添加一个新的root用户在指定IP下使用指定密码来访问数据库:mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;Query OK, 0 rows affected, 1 warning (0.00 sec) *.*中第一个*代表数据库名称，第二个*代表表名。在这里我们设置的是所有数据库里的所有表都授权给用户，如果只想授权某数据库或某些数据库下某些表，可以把*替换成你所需的数据库名和表明即可:mysql&gt; GRANT ALL PRIVILEGES ON test_db.user TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;dev&apos; WITH GRANT OPTION; 上述表示是把test_db数据下的user数据表授权给用户。 root表示授予root用户可以登录数据库。%表示授权的用户使用哪些IP可以登录，这里表示可以使用用户root在任意IP地址来访问数据库。dev表示分配root用户对应的密码。 当然我们也可以直接用UPDATE更新root用户Host, 但不推荐：UPDATE user SET Host=&apos;%&apos; WHERE User=&apos;root&apos; AND Host=&apos;localhost&apos;; 授权用户之后，执行如下命令刷新一下权限:mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 至此我们已经完成了配置远程访问数据的所有操作，我们在看一下当前能访问我们数据库的用户:mysql&gt; select User,Host from user;+------------------+-----------+| User | Host |+------------------+-----------+| root | % || debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost || root | localhost |+------------------+-----------+5 rows in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Airflow使用指南一 安装与启动]]></title>
    <url>%2FAirflow%2F%5BAirFlow%5DAirFlow%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%E4%B8%80%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8.html</url>
    <content type="text"><![CDATA[1. 安装通过pip安装:xiaosi@yoona:~$ pip install airflow 如果速度比较慢，可以使用下面提供的源进行安装:xiaosi@yoona:~$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple airflow 如果出现下面提示，表示你的airflow安装成功了:Successfully installed airflow alembic croniter dill flask flask-admin flask-cache flask-login flask-swagger flask-wtf funcsigs future gitpython gunicorn jinja2 lxml markdown pandas psutil pygments python-daemon python-dateutil python-nvd3 requests setproctitle sqlalchemy tabulate thrift zope.deprecation Mako python-editor click itsdangerous Werkzeug wtforms PyYAML ordereddict gitdb2 MarkupSafe pytz numpy docutils setuptools lockfile six python-slugify idna urllib3 certifi chardet smmap2 UnidecodeCleaning up... 安装完成之后我的默认安装在~/.local/bin目录下 2. 配置如果不修改路径，默认的配置为~/airflow 永久修改环境变量echo &quot;export AIRFLOW_HOME=/home/xiaosi/opt/airflow&quot; &gt;&gt; /etc/profilesource /etc/profile 为了便于操作方便，进行如下配置:echo &quot;export PATH=/home/xiaosi/.local/bin:$PATH&quot; &gt;&gt; /etc/profilesource /etc/profile 3. 初始化初始化数据库:xiaosi@yoona:~$ airflow initdb[2017-08-02 16:39:22,319] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 16:39:22,432] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 16:39:22,451] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txtDB: sqlite:////home/xiaosi/opt/airflow/airflow.db[2017-08-02 16:39:22,708] &#123;db.py:287&#125; INFO - Creating tablesINFO [alembic.runtime.migration] Context impl SQLiteImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; e3a246e0dc1, current schemaINFO [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encrypted/home/xiaosi/.local/lib/python2.7/site-packages/alembic/util/messaging.py:69: UserWarning: Skipping unsupported ALTER for creation of implicit constraint warnings.warn(msg)INFO [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrationsINFO [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntanceINFO [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indicesINFO [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to LogINFO [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrunINFO [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_durationINFO [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_configINFO [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to userINFO [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start endINFO [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_missINFO [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connectionINFO [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable tableINFO [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user tableINFO [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state indexINFO [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal tableINFO [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats tableINFO [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tablesINFO [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indicesINFO [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstanceINFO [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run tableDone. 运行上述命令之后，会在$AIRFLOW_HOME目录下生成如下文件:xiaosi@yoona:~/opt/airflow$ ll总用量 88drwxrwxr-x 2 xiaosi xiaosi 4096 8月 2 16:39 ./drwxrwxr-x 26 xiaosi xiaosi 4096 7月 31 13:56 ../-rw-rw-r-- 1 xiaosi xiaosi 11424 8月 2 16:38 airflow.cfg-rw-r--r-- 1 xiaosi xiaosi 58368 8月 2 16:39 airflow.db-rw-rw-r-- 1 xiaosi xiaosi 1554 8月 2 16:38 unittests.cfg 4. 修改默认数据库找到$AIRFLOW_HOME/airflow.cfg配置文件，进行如下修改:sql_alchemy_conn = mysql://root:root@localhost:3306/airflow 备注 数据库用户名与密码均为root，airflow使用的数据库为airflow．使用如下命令创建对应的数据库:mysql&gt; create database airflow;Query OK, 1 row affected (0.00 sec) 重新初始化服务器数据库:xiaosi@yoona:~$ airflow initdb 出现了如下错误:xiaosi@yoona:~$ airflow initdbTraceback (most recent call last): File &quot;/home/xiaosi/.local/bin/airflow&quot;, line 17, in &lt;module&gt; from airflow import configuration File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/__init__.py&quot;, line 30, in &lt;module&gt; from airflow import settings File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 159, in &lt;module&gt; configure_orm() File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 147, in configure_orm engine = create_engine(SQL_ALCHEMY_CONN, **engine_args) File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py&quot;, line 387, in create_engine return strategy.create(*args, **kwargs) File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py&quot;, line 80, in create dbapi = dialect_cls.dbapi(**dbapi_args) File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py&quot;, line 110, in dbapi return __import__(&apos;MySQLdb&apos;)ImportError: No module named MySQLdb 解决方案: MySQL是最流行的开源数据库之一，但在Python标准库中并没有集成MySQL接口程序，MySQLdb是一个第三方包，需独立下载并安装。sudo apt-get install python-mysqldb 再次初始化:xiaosi@yoona:~$ airflow initdb[2017-08-02 17:22:21,169] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 17:22:21,282] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 17:22:21,302] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txtDB: mysql://root:***@localhost:3306/airflow[2017-08-02 17:22:21,553] &#123;db.py:287&#125; INFO - Creating tablesINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; e3a246e0dc1, current schemaINFO [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encryptedINFO [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrationsINFO [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntanceINFO [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indicesINFO [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to LogINFO [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrunINFO [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_durationINFO [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_configINFO [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to userINFO [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start endINFO [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_missINFO [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connectionINFO [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable tableINFO [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user tableINFO [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state indexINFO [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal tableINFO [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats tableINFO [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tablesINFO [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indicesINFO [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstanceINFO [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run tableDone. 查看一下airflow数据库中做了哪些操作:mysql&gt; use airflow;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+-------------------+| Tables_in_airflow |+-------------------+| alembic_version || chart || connection || dag || dag_pickle || dag_run || dag_stats || import_error || job || known_event || known_event_type || log || sla_miss || slot_pool || task_fail || task_instance || users || variable || xcom |+-------------------+19 rows in set (0.00 sec) 5. 启动通过如下命令就可以启动后台管理界面，默认访问localhost:8080即可:xiaosi@yoona:~$ airflow webserver[2017-08-02 17:25:31,961] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 17:25:32,075] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 17:25:32,095] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ ______ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / /___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__//home/xiaosi/.local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead. .format(x=modname), ExtDeprecationWarning[2017-08-02 17:25:32,469] [9703] &#123;models.py:167&#125; INFO - Filling up the DagBag from /home/xiaosi/opt/airflow/dagsRunning the Gunicorn Server with:Workers: 4 syncHost: 0.0.0.0:8080Timeout: 120Logfiles: - -================================================================= [2017-08-02 17:25:33,052] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 17:25:33,156] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 17:25:33,179] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt[2017-08-02 17:25:33 +0000] [9706] [INFO] Starting gunicorn 19.3.0[2017-08-02 17:25:33 +0000] [9706] [INFO] Listening at: http://0.0.0.0:8080 (9706)[2017-08-02 17:25:33 +0000] [9706] [INFO] Using worker: sync... 可以访问 http://localhost:8080/admin/ , 呈现出的主界面如下:]]></content>
      <categories>
        <category>Airflow</category>
      </categories>
      <tags>
        <tag>Airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之编程模型]]></title>
    <url>%2Fflink-programming-model.html</url>
    <content type="text"><![CDATA[1. 抽象层次Flink提供不同级别的抽象层次来开发流处理和批处理应用程序。 (1) 最低级别的抽象只是提供有状态的数据流。通过Process Function集成到DataStream API中。它允许用户不受限制的处理来自一个或多个数据流的事件，并可以使用一致的容错状态(consistent fault tolerant state)。另外，用户可以注册事件时间和处理时间的回调函数，允许程序实现复杂的计算。 (2) 在实际中，大多数应用程序不需要上述描述的低级抽象，而是使用如DataStream API(有界/无界流)和DataSet API(有界数据集)的核心API进行编程。这些核心API提供了用于数据处理的通用构建模块，如用户指定的各种转换，连接，聚集，窗口，状态等。在这些API中处理的数据类型被表示为对应编程语言中的类。 低级别的Process Function与DataStream API集成在一起，使得可以对特定操作使用较低级别的抽象接口。DataSet API为有限数据集提供了额外的原语(primitives)，如循环/迭代。 (3) Table API是以表为核心的声明式DSL，可以动态地改变表(当表表示流数据时)。Table API遵循(扩展的)关系模型：每个表都有一个schema(类似于关系数据库中的表)，对应的API提供了类似的操作(offers comparable operations)，如select，project，join，group-by，aggregate等。Table API程序声明性地定义了如何在逻辑上实现操作，而不是明确指定操作实现的具体代码。尽管Table API可以通过各种类型的用户自定义函数进行扩展，它比核心API表达性要差一些，但使用上更简洁(编写代码更少)。另外，Table API程序也会通过一个优化器，在执行之前应用优化规则。 可以在表和DataStream/DataSet之间进行无缝转换，允许程序混合使用Table API和DataStream和DataSet API。 (4) Flink提供的最高级抽象是SQL。这种抽象在语法和表现力方面与Table API类似，但是是通过SQL查询表达式实现程序。SQL抽象与Table API紧密交互，SQL查询可以在Table API中定义的表上执行。 2. 程序与数据流Flink程序的基本构建块是流和转换操作。 备注:Flink的DataSet API中使用的数据集也是内部的流 - 稍后会介绍这一点。 从概念上讲，流是数据记录(可能是永无止境的)流，而转换是将一个或多个流作为输入，并产生一个或多个输出流。 执行时，Flink程序被映射到由流和转换算子组成的流式数据流(streaming dataflows)。每个数据流从一个或多个source开始，并在一个或多个sink中结束。数据流类似于有向无环图(DAG)。尽管通过迭代构造允许特殊形式的环，但是为了简单起见，大部分我们都会这样描述。 程序中的转换与数据流中的算子通常是一一对应的。然而，有时候，一个转换可能由多个转换算子组成。 3. 并行数据流图Flink中的程序本质上是分布式并发执行的。在执行过程中，一个流有一个或多个流分区，每个算子有一个或多个算子子任务。算子子任务之间相互独立，并且在不同的线程中执行，甚至有可能在不同的机器或容器上执行。 算子子任务的数量是该特定算子的并发数。流的并发数总是产生它的算子的并发数。同一程序的不同算子可能具有不同的并发级别。 在两个算子之间的流可以以一对一模式或重新分发模式传输数据: (1) 一对一流(例如上图中的Source和map()算子之间的流)保留了元素的分区和排序。这意味着将会在map()算子的子任务[1]中看到在Source算子的子任务[1]中产生的相同元素，并且具有相同的顺序。 (2) 重分发流(例如上图的的map()和keyBy()/window()/apply()之间，以及在keyBy()/window()/apply()和Sink之间的数据流)改变了流的分区。每个算子子任务根据所选的转换操作将数据发送到不同的目标子任务。比如keyBy()(根据key的哈希值重新分区)，broadcast()，或者rebalance()(随机重新分区)。在重新分配交换中，只会在每对发送与接受子任务(比如，map()的子任务[1]与keyBy()/window()/apply()的子任务[2])中保留元素间的顺序。在上图的例子中，尽管在子任务之间每个 key 的顺序都是确定的，但是由于程序的并发引入了不确定性，最终到达Sink的元素顺序就不能保证与一开始的元素顺序完全一致。 关于配置并发的更多信息可以参阅并发执行文档。 4. 窗口聚合事件(比如计数、求和)在流上的工作方式与批处理不同。比如，不可能对流中的所有元素进行计数，因为通常流是无限的(无界的)。相反，流上的聚合(计数，求和等)需要由窗口来划定范围，比如在最近5分钟内计算，或者对最近100个元素求和。 窗口可以是时间驱动的(比如：每30秒）或者数据驱动的(比如：每100个元素)。窗口通常被区分为不同的类型，比如滚动窗口(没有重叠)，滑动窗口(有重叠)，以及会话窗口(由不活动的间隙所打断) 更多的窗口示例可以在这篇博客中找到。更多详细信息在窗口文档。 5. 时间当提到流程序(例如定义窗口)中的时间时，你可以参考不同的时间概念： (1) 事件时间是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过时间戳分配器访问事件时间戳。 (2) 摄入时间是事件进入Flink数据流源(source)算子的时间。 (3) 处理事件是每一个执行基于时间操作算子的本地时间。 更多关于如何处理时间的详细信息可以查看事件时间文档. 6. 有状态操作尽管数据流中的很多操作一次只查看一个独立的事件(比如事件解析器)，但是有些操作会记录多个事件间的信息(比如窗口算子)。这些操作被称为有状态的 。 有状态操作的状态保存在一个可被视为嵌入式键值对存储中。状态与由有状态算子读取的流一起被严格地分区与分布(distributed)。因此，只有在应用keyBy()函数之后，才能访问keyed streams上的键/值对状态，并且仅限于与当前事件key相关联的值(access to the key/value state is only possible on keyed streams, after a keyBy() function, and is restricted to the values associated with the current event’s key. )。对齐流和状态的key(Aligning the keys of streams and state)确保了所有状态更新都是本地操作，保证一致性，而没有事务开销(guaranteeing consistency without transaction overhead)。这种对齐还使得Flink可以透明地重新分配状态与调整流的分区。 7. 容错性检查点Flink组合使用流重放与检查点实现了容错。检查点与每一个输入流以及每一个算子对应的状态所在的特定点相关联(A checkpoint is related to a specific point in each of the input streams along with the corresponding state for each of the operators.)。一个流数据流可以可以从一个检查点恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性(一次处理语义) 检查点时间间隔是在恢复时间(需要重放的事件数量)内消除执行过程中容错开销的一种手段。 更多关于检查点与容错的详细信息可以查看容错文档。 8. 批处理操作Flink将批处理程序作为流处理程序的一种特殊情况来执行，只是流是有界的(有限个元素)。在内部DataSet被视为数据流(A DataSet is treated internally as a stream of data)。因此上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外： (1) 批处理程序的容错不使用检查点。通过重放全部流来恢复。这是可能的，因为输入是有限的。这使恢复的成本更高(This pushes the cost more towards the recovery)，但是使常规处理更便宜，因为它避免了检查点。 (2) DataSet API中的有状态操作使用简化的in-memory/out-of-core数据结构，而不是键/值索引。 (3) DataSet API引入了特殊的同步(基于superstep的)迭代，而这种迭代仅仅能在有界流上执行。详细信息可以查看迭代文档。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 简单查询FetchTask]]></title>
    <url>%2Fhive-tuning-fetch-task-conversion.html</url>
    <content type="text"><![CDATA[1. 简介某些 SELECT 查询可以转换为一个 FETCH 任务，从而最大限度地可以减少交互的延迟。在目前情况下，查询只能是单一数据源，不能有任何的子查询，不能有任何的聚合，去重（导致RS - ReduceSinkOperator，会产生 MapReduce 任务），Lateral views 以及 Join。Fetch 任务是 Hive 中执行效率比较高的任务之一。直接遍历文件并输出结果，而不是启动 MapReduce 作业进行查询。对于简单的查询，如带有 LIMIT 语句的 SELECT * 查询，这会非常快(单位数秒级)。在这种情况下，Hive 可以通过执行 HDFS 操作来返回结果。 Example:hive&gt; SELECT * FROM tmp_client_behavior LIMIT 1;OK2017-08-16 22:24:54 ...Time taken: 0.924 seconds, Fetched: 1 row(s) 如果我们只想得到几列怎么办？hive&gt; select vid, gid, os from tmp_client_behavior;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID = wirelessdev_20170817203931_02392cd8-5df7-42e6-87ea-aaa1418e000cTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1472052053889_23316306, Tracking URL = xxxKill Command = /home/q/hadoop/hadoop-2.2.0/bin/hadoop job -kill job_1472052053889_23316306Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02017-08-17 20:39:39,886 Stage-1 map = 0%, reduce = 0%2017-08-17 20:39:46,000 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.32 secMapReduce Total cumulative CPU time: 1 seconds 320 msecEnded Job = job_1472052053889_23316306MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Cumulative CPU: 1.32 sec HDFS Read: 581021 HDFS Write: 757 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 320 msecOK... 从日志中我们可以看到会启动 MapReduce 任务，只有1个 Mapper 但没有 Reducer。有没有方法可以让我们避免启动上面的 MapReduce 作业？这就需要设置 hive.fetch.task.conversion 配置：&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;none|minimal|more&lt;/value&gt;&lt;/property&gt; Hive 已经做过优化了，从Hive 0.10.0 版本开始，对于简单的不需要聚合去重的查询语句，可以不需要运行 MapReduce 任务，直接通过查询 HDFS 获取数据:hive&gt; select vid, gid, os from tmp_client_behavior limit 10;OK60001 A34D4B08788A adr... 2. 配置2.1 hive.fetch.task.conversion&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;none|minimal|more&lt;/value&gt;&lt;/property&gt; 可支持的选项有 none,minimal 和 more，从Hive 0.10.0 版本到 Hive 0.13.1 版本起，默认值为 minimal，Hive 0.14.0版本以及更高版本默认值改为 more: none: 禁用 hive.fetch.task.conversion（在Hive 0.14.0版本中引入） minimal: 当使用 LIMIT 或在分区列上执行过滤（WHERE 和 HAVING子句），以及执行 SELECT * 时，可以转换为 Fetch 任务（SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only）。 more：当使用 SELECT，LIMIT 以及过滤时，more 选项下也可以转换为 Fetch 任务（SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)）。more 可以在 SELECT 子句中使用任何表达式，包括UDF。（UDTF和 Lateral views尚不支持）。 对具体使用条件有点疑问 2.2 hive.fetch.task.conversion.threshold&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion.threshold&lt;/name&gt; &lt;value&gt;1073741824&lt;/value&gt;&lt;/property&gt; 从 Hive 0.13.0 版本到 Hive 0.13.1 版本起，默认值为-1（表示没有任何的限制），Hive 0.14.0 版本以及更高版本默认值改为 1073741824(1G)。 使用 hive.fetch.task.conversion 的输入阈值（以字节为单位）。如果目标表在本机，则输入长度通过文件长度的总和来计算。如果不在本机，则表的存储处理程序可以选择实现 org.apache.hadoop.hive.ql.metadata.InputEstimator 接口。负阈值意味着使用 hive.fetch.task.conversion 没有任何的限制。 Example:hive&gt; set hive.fetch.task.conversion.threshold=100000000;hive&gt; select * from passwords limit 1;Total jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_201501081639_0046, Tracking URL = http://n1a.mycluster2.com:50030/jobdetails.jsp?jobid=job_201501081639_0046Kill Command = /opt/mapr/hadoop/hadoop-0.20.2/bin/../bin/hadoop job -kill job_201501081639_0046Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02015-01-15 12:19:06,474 Stage-1 map = 0%, reduce = 0%2015-01-15 12:19:11,496 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 0.85 secMapReduce Total cumulative CPU time: 850 msecEnded Job = job_201501081639_0046MapReduce Jobs Launched:Job 0: Map: 1 Cumulative CPU: 0.85 sec MAPRFS Read: 0 MAPRFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 850 msecOKroot x 0 0 root /root /bin/bashTime taken: 6.698 seconds, Fetched: 1 row(s) 调整阈值变大则会使用 Fetch 任务:hive&gt; set hive.fetch.task.conversion.threshold=600000000;hive&gt; select * from passwords limit 1;OKroot x 0 0 root /root /bin/bashTime taken: 0.325 seconds, Fetched: 1 row(s) 此参数根据表大小而不是结果集大小来计算或估计。 3. 设置Fetch任务(1) 直接在命令行中使用set命令进行设置:hive&gt; set hive.fetch.task.conversion=more; (2) 使用hiveconf进行设置bin/hive --hiveconf hive.fetch.task.conversion=more (3) 上面的两种方法都可以开启了Fetch Task，但是都是临时起作用的；如果你想一直启用这个功能，可以在${HIVE_HOME}/conf/hive-site.xml里面修改配置：&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;more&lt;/value&gt;&lt;/property&gt;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hive 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.x HDFS架构]]></title>
    <url>%2Fhadoop-2.x-hdfs-architecture.html</url>
    <content type="text"><![CDATA[1. 概述Hadoop分布式文件系统(HDFS)是一个分布式文件系统，设计初衷是可以在商用硬件上运行。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的也有显著的差异。HDFS具有高容错能力，可以部署在低成本的硬件上。HDFS提供对应用程序数据的高吞吐量访问，适用于具有大数据集的应用程序。HDFS放宽了一些POSIX要求，以便对文件系统数据进行流式访问。HDFS最初是作为Apache Nutch网络搜索引擎项目的基础架构构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL为: http://hadoop.apache.org/ 2. 设想与目标2.1 硬件故障 硬件故障很常见不要感到意外。HDFS实例可能由成百上千台服务器机器组成，每台机器存储部分文件系统的数据。事实上，有大量的组件，并且每个组件具有不一定的故障概率，这意味着可能HDFS的某些组件总是不起作用的。因此，故障检测和快速自动恢复是HDFS的核心架构。 2.2 流式数据访问运行在HDFS上的应用程序需要流式访问其数据集。HDFS不是运行在通用文件系统上通用应用程序。HDFS设计是为了更多的批量处理，而不是与用户进行交互。重点是数据访问的高吞吐量，而不是数据访问的低延迟。 2.3 大数据集运行在HDFS上的应用程序具有较大的数据集。HDFS中的文件大小一般为几GB或几TB。因此，HDFS需要支持大文件。它需要提供高数据聚合带宽并可以在单个集群中扩展到的数百个节点。它需要在一个实例中支持数千万个文件。 2.4 简单一致性模型HDFS数据访问模式为一次写入多次读取。文件一旦创建、写入和关闭后，除了追加和截断外，文件不能更改。可以支持将内容追加到文件末尾，但不能在随意位置更新文件内容。该假设简化了数据一致性问题，并实现了数据访问的高吞吐量。MapReduce应用程序或Web爬虫程序应用程序与此模型完美匹配。 2.5 ‘移动计算比移动数据便宜’如果应用程序能够在其操作的数据附近执行，那么应用程序所请求的计算效率会更高一些。当数据集很大时，这一点更能体现。这样可以最大限度地减少网络拥塞并提高系统的整体吞吐量。我们假设将计算迁移到更靠近数据的位置比将数据转移到应用程序运行的位置更好。HDFS为应用程序提供接口，使其更靠近数据所在的位置。 2.6 跨越异构硬件和软件平台的可移植性HDFS被设计为可以从一个平台轻松地移植到另一个平台。这有助于HDFS作为大型应用程序的首选平台。 3. NameNode and DataNodesHDFS是一个主/从结构。一个HDFS集群包含一个NameNode，管理文件系统命名空间以及管理客户端对文件访问的主服务。除此之外，还有一些DataNode，通常集群中的每个节点都有一个DataNode，用于管理它们所运行节点相关的存储。HDFS公开文件系统命名空间，并允许用户数据存储在文件中。在内部，一个文件被分成一个或多个数据块，这些数据块被存储在一组DataNode中。NameNode执行文件系统命名空间操作，例如打开，关闭和重命名文件和目录等。它也决定数据块到DataNode的映射。DataNode负责为文件系统客户端的读写请求提供服务。DataNode还根据来自NameNode的指令执行数据块的创建，删除和复制。 NameNode和DataNode是设计用于在商业机器上运行的软件。这些机器通常运行GNU/Linux操作系统(OS)。HDFS是使用Java语言构建的; 任何支持Java的机器都可以运行NameNode或DataNode。使用高可移植性的Java语言意味着HDFS可以部署在各种机器上。一个典型的部署是有一台专用机器来运行NameNode。集群中的其他机器运行DataNode实例。该体系结构并不排除在同一台计算机上运行多个DataNode，但在实际部署中很少出现这种情况。 集群中NameNode的存在大大简化了系统的体系结构。NameNode是所有HDFS元数据的决策者和存储仓库。系统的这种设计方式可以允许用户数据不会经过NameNode，直接与DataNode进行连接。 4. 文件系统命名空间HDFS支持传统的分层文件组织方式。用户或应用程序可以创建目录以及在这些目录内存储文件。文件系统命名空间层次结构与大多数其他文件系统类似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。HDFS支持用户配额和访问权限。HDFS不支持硬链接或软链接。但是，HDFS体系结构并不排除实现这些功能。 NameNode维护文件系统的命名空间。对文件系统命名空间或其属性的任何更改都会在NameNode中记录。应用程序可以指定HDFS应该维护的文件的副本数量。文件的副本数称为该文件的复制因子。这个信息由NameNode存储。 5. 数据复制HDFS旨在大型集群多台机器上可靠地存储非常大的文件。将每个文件存储为一系列的数据块。文件的数据块被复制多份以实现容错。数据块大小和副本因子是可以通过配置文件进行配置。 一个文件的数据块除最后一个块以外的所有其他块的大小都相同，在添加对可变长度块和hsync的支持后，用户可以不用填充最后一个块到配置大小而启动一个新块。 应用程序可以指定文件的副本数量。复制因子可以在文件创建时指定，也可以在以后更改。HDFS中的文件是一次性编写的(追加和截断除外)，并且严格限定在任何时候都只能有一个编写者。 NameNode做出关于块复制的所有决定。它周期性的从集群中的每个DataNode接收Heartbeat和Blockreport。收到Heartbeat意味着DataNode运行正常。Blockreport包含DataNode上所有块的列表。 5.1 副本安置副本的放置对HDFS的可靠性和性能至关重要。优化副本放置能将HDFS与大多数其他分布式文件系统区分开来。这是一个需要大量调整和体验的功能。机架感知副本放置策略的目的是提高数据可靠性，可用性和网络带宽利用率。副本放置策略的目前实现是朝这个方向迈进的第一步。实施这一策略的短期目标是在生产环境上进行验证，更多地了解其行为，并为测试和研究更复杂的策略奠定基础。 大型HDFS实例运行在通常分布在多个机架上的一组计算机上。不同机架中的两个节点之间的通信必须经过交换机。在大多数情况下，同一机架中的机器之间的网络带宽大于不同机架中的机器之间的网络带宽。 NameNode通过Hadoop机架感知中概述的过程确定每个DataNode所属的机架Id。一个简单但不是最佳的策略是将副本放在不同的机架上。这可以防止整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在集群中均匀分配副本，以便轻松平衡组件故障的负载(This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure)。但是，此策略会增加写入成本，因为写入需要将数据块传输到多个机架。 正常情况下，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架的同一个节点上，另一个放在本地机架的不同节点上，最后放在另一个机架的不同节点上。这个政策降低了机架间写入流量，这通常会提高写入性能。机架故障的几率远远小于节点故障的几率;此策略不会影响数据可靠性和可用性的保证。但是，它降低了读取数据时使用的总体网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本在同一个节点上，三分之二的副本在同一个机架上，另外三分之一在其它机架上均匀分布。此策略可提高写入性能，而不会影响数据可靠性或读取性能。 这里描述的就是当前默认副本放置策略。 5.2 副本选择为了尽量减少全局带宽消耗和读取延迟，HDFS会尝试将读取请求发送到离读取者最近的副本上(HDFS tries to satisfy a read request from a replica that is closest to the reader.)。 如果在与读取者节点相同的机架上存在副本，则该副本优选满足读取请求。如果HDFS进群跨越多个数据中心，则保存在本地数据中心的副本优先于任何远程副本。 5.3 安全模式在启动时，NameNode进入一个称为Safemode(安全模式)的特殊状态。当NameNode处于安全模式状态时，不会发生数据块的复制。NameNode接收来自DataNode的Heartbeat和Blockreport消息。Blockreport包含DataNode托管的数据块列表。每个块都有指定的最小数量的副本。当该数据块的最小副本数与NameNode签入时，将认为该块被安全地复制。在安全复制数据块的可配置百分比检入NameNode（再加上30秒）之后，NameNode退出安全模式状态。然后确定仍然少于指定副本数量的数据块列表（如果有的话）。NameNode然后将这些块复制到其他DataNode。 6. 文件系统元数据持久化HDFS命名空间存储在NameNode中。NameNode使用称之为EditLog编辑日志的事务日志来持久化存储在文件系统元数据上发生的每一个变化。例如，在HDFS中创建一个新文件会导致NameNode向EditLog编辑日志中插入一条记录。同样，更改文件的复制因子也会导致将新记录插入到EditLog编辑日志中。NameNode使用其本地主机OS文件系统中的文件来存储EditLog编辑日志。整个文件系统命名空间，包括数据块到文件的映射以及文件系统属性，都存储在一个名为FsImage的文件中。FsImage作为文件存储在NameNode的本地文件系统中。 NameNode将整个文件系统命名空间和文件Blockmap的快照(image)保存在内存中。这个关键的元数据被设计得很紧凑，这样一个具有4GB内存的NameNode足以支持大量的文件和目录。当NameNode启动时，它会从磁盘中读取FsImage和EditLog编辑日志，将EditLog编辑日志中的所有事务应用到内存中的FsImage(applies all the transactions from the EditLog to the in-memory representation of the FsImage)，并将这个新版本刷新到磁盘上生成一个新FsImage。它可以截断旧的EditLog编辑日志，因为它的事务已经被应用到持久化的FsImage上。这个过程被称为检查点。在目前的实现中，只有在NameNode启动时才会出现检查点。在未来版本中正在进行工作的NameNode也会支持周期性的检查点。 DataNode将HDFS数据存储在本地文件系统的文件中。DataNode不了解HDFS文件(The DataNode has no knowledge about HDFS files)。它将每个HDFS数据块存储在本地文件系统中的单个文件中。DataNode不会在同一目录中创建所有文件。相反，它使用启发式来确定每个目录的最佳文件数量并适当地创建子目录。由于本地文件系统可能无法有效地支持单个目录中的大量文件，因此在同一目录中创建所有本地文件并不是最佳选择。当DataNode启动时，它会扫描其本地文件系统，生成一个包含所有HDFS数据块(与每个本地文件相对应)的列表，并将此报告发送给NameNode：这是Blockreport。 7. 通信协议所有的HDFS通信协议都是基于TCP/IP协议的。客户端建立到NameNode机器上的可配置TCP端口的连接。它使用ClientProtocol与NameNode交谈。DataNode使用DataNode协议与NameNode进行通信。远程过程调用(RPC)抽象包装客户端协议和数据节点协议。根据设计，NameNode永远不会启动任何RPC。而是只响应由DataNode或客户端发出的RPC请求。 8. 稳定性HDFS的主要目标是即使在出现故障时也能可靠地存储数据。三种常见的故障类型是NameNode故障，DataNode故障和网络分裂(network partitions)。 8.1 数据磁盘故障，心跳和重新复制每个DataNode定期向NameNode发送一个Heartbeat消息。网络分裂可能导致一组DataNode与NameNode失去联系。NameNode通过丢失Heartbeat消息来检测这种情况。NameNode将最近没有Heartbeats的DataNode标记为死亡，并且不会将任何新的IO请求转发给它们。任何注册在标记为死亡的DataNode中的数据不再可用。DataNode死亡可能导致某些块的复制因子降到其指定值以下。NameNode不断跟踪哪些块需要复制，并在需要时启动复制。重新复制可能由于许多原因而产生：DataNode可能变得不可用，副本可能被破坏，DataNode上的硬盘可能出现故障，或者文件的复制因子可能需要增加。 为了避免由于DataNode的状态震荡而导致的复制风暴，标记DataNode死亡的超时时间设置的比较保守(The time-out to mark DataNodes dead is conservatively long)(默认超过10分钟)。用户可以设置较短的时间间隔以将DataNode标记为陈旧，并避免陈旧节点在读取或按配置写入时性能出现负载(Users can set shorter interval to mark DataNodes as stale and avoid stale nodes on reading and/or writing by configuration for performance sensitive workloads)。 8.2 集群重新平衡HDFS体系结构与数据重新平衡方案兼容。如果某个DataNode上的可用空间低于某个阈值，那么会自动将数据从一个DataNode移动到另一个DataNode。对于特定文件突然高需求(sudden high demand)的情况下，可能会动态创建额外的副本并重新平衡集群中的其他数据。这些类型的数据重新平衡方案尚未实现。 8.3 数据完整性从DataNode上获取的数据块可能会损坏。发生损坏可能是由存储设备故障，网络故障或软件错误引起。HDFS客户端实现了对HDFS上文件内容进行校验和检查。当客户端创建一个HDFS文件时，它会计算每个文件的对应数据块的校验和，并将这些校验和存储在同一个HDFS命名空间中的单独隐藏文件中。当客户端检索文件内容时，它会验证从每个DataNode收到的数据是否与存储在相关校验和文件中的校验和相匹配。如果不匹配，那么客户端可以选择从另一个具有该数据块副本的DataNode中检索该数据块。 8.4 元数据磁盘故障FsImage和EditLog编辑日志是HDFS中的中心数据结构。这些文件的损坏可能会导致HDFS实例无法正常运行。为此，NameNode可以配置为支持维护FsImage和EditLog编辑日志的多个副本。任何对FsImage或EditLog编辑日志的更新都会引起每个FsImages和EditLogs编辑日志同步更新。同步更新FsImage和EditLog编辑日志的多个副本可能会降低NameNode支持的每秒的命名空间事务的速度(degrade the rate of namespace transactions per second)。但是，这种降低是可以接受的，因为尽管HDFS应用程序实质上是非常密集的数据，但是它们也不是元数据密集型的。当NameNode重新启动时，它会选择最新的一致的FsImage和EditLog编辑日志来使用。 另一个增强防御故障的方法是使用多个NameNode以启用高可用性，或者使用NFS上的共享存储或使用分布式编辑日志(称为Journal)。后者是推荐的方法。 8.5 快照快照支持在特定时刻存储数据副本。快照功能的一种用法是将损坏的HDFS实例回滚到先前已知的良好时间点。 9. 数据组织9.1 数据块HDFS为支持大文件而设计的。与HDFS兼容的应用程序是处理大型数据集的应用程序。这些应用程序只写入数据一次，但是读取一次或多次，并读取速度要求满足流式处理速度。HDFS支持在文件上一次写入多次读取语义。HDFS使用的一般块大小为128 MB。因此，一个HDFS文件被分成多个128MB的块，如果可能的话，每个块将保存在不同的DataNode上。 9.2 分阶段客户端创建文件的请求不会立即到达NameNode。事实上，最初HDFS客户端将文件数据缓存到本地缓冲区。应用程序写入重定向到本地缓冲区。当本地文件累积超过一个块大小的数据时，客户端才会联系NameNode。NameNode将文件名插入到文件系统层次结构中，并为其分配一个数据块。NameNode将DataNode和目标数据块的标识和返回给客户请求。然后，客户端将本地缓冲区中的数据块保存到指定的DataNode上。当文件关闭时，本地缓冲区中剩余的未保存数据也被传输到DataNode。客户端然后告诉NameNode该文件已关闭。此时，NameNode将文件创建操作提交到持久化存储中。如果NameNode在文件关闭之前崩溃，那么文件会丢失。 在仔细考虑在HDFS上运行的目标应用程序之后，采用了上述方法。这些应用程序需要流式写入文件。如果客户端直接写入远程文件目录而没有在客户端进行任何缓冲，那么网络速度和网络拥塞会大大影响吞吐量。这种方法并非没有先例。较早的分布式文件系统，例如AFS，已经使用客户端缓存来提高性能。POSIX的要求已经放宽，以实现更高的数据传输性能。 9.3 副本流水线当客户端将数据写入HDFS文件时，首先将数据写入本地缓冲区，如上一节所述。假设HDFS文件复制因子为3。当本地缓冲区累积了一个块的用户数据时，客户端从NameNode中检索DataNode列表。该列表包含保存数据的数据块副本的DataNode。客户端然后将数据块刷新到第一个DataNode。第一个DataNode开始接收一小部分数据，将这一小部分数据写入其本地存储库，然后传输到列表中的第二个DataNode。第二个DataNode依次接收数据块的每一部分数据，将其写入存储库，然后再将刷新到第三个DataNode。最后，第三个DataNode将数据写入其本地存储库。因此，DataNode可以以流水线的方式从前一个DataNode接收数据，同时将数据转发到流水线中的下一个DataNode。因此，数据从一个DataNode流到下一个。 10. 访问应用程序可以以多种不同的方式访问HDFS。HDFS为应用程序提供了一个FileSystem Java API。Java API和REST API的C语言包装器也可以使用。另外还有一个HTTP浏览器(HTTP browser)，也可以用来浏览HDFS实例的文件。通过使用NFS网关，可以将HDFS作为客户端本地文件系统的一部分。 10.1 FS ShellHDFS将用户数据以文件和目录的形式进行组织。它提供了一个名为FS shell的命令行接口，让用户可以与HDFS中的数据进行交互。这个命令集的语法类似于用户已经熟悉的其他shell(例如bash，csh)。 以下是一些示例操作/命令对： 操作 命令 创建/foodir目录 bin/hadoop dfs -mkdir /foodir 删除目录/foodir bin/hadoop fs -rm -R /foodir 查看/foodir/myfile.txt中内容 bin/hadoop dfs -cat /foodir/myfile.txt FS shell针对需要脚本语言与存储数据进行交互的应用程序。 10.2 DFSAdminDFSAdmin命令集用于管理HDFS集群。这些是仅能由HDFS管理员使用的命令。以下是一些示例操作/命令对： 操作 命令 使集群处于安全模式 bin/hdfs dfsadmin -safemode enter 生成DataNode列表 bin/hdfs dfsadmin -report 重新投放或停用DataNode(s) bin/hdfs dfsadmin -refreshNodes 10.3 浏览器接口一个典型的HDFS安装会配置一个Web服务器，通过一个可配置的TCP端口公开HDFS命名空间。这允许用户使用Web浏览器浏览HDFS命名空间并查看其文件的内容。 备注:Hadoop版本: 2.7.3 原文:http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 1.x Secondary NameNode]]></title>
    <url>%2Fhadoop-mapReduce1.x-secondary-nameNode.html</url>
    <content type="text"><![CDATA[Secondary NameNode是Hadoop中命名不当的其中一个组件。不当命名很容易造成歧义，通过Secondary NameNode这个名字，我们很容易理解为是一个备份NameNode，但实际上它不是。很多Hadoop的初学者对Secondary NameNode究竟做了什么以及为什么存在于HDFS中感到困惑。因此，在这篇博文中，我试图解释HDFS中Secondary NameNode的作用。 通过它的名字，你可能会认为它和NameNode有关，它确实是NameNode相关。所以在我们深入研究Secondary NameNode之前，让我们看看NameNode究竟做了什么。 1. NameNodeNameNode保存HDFS的元数据，如名称空间信息，块信息等。使用时，所有这些信息都存储在主存储器中。 但是这些信息也存储在磁盘中用于持久性存储。 上图显示了NameNode如何将信息存储在磁盘中。上图中两个不同的文件是： fsimage - 它是NameNode启动时文件系统元数据的快照 编辑日志 - 它是在NameNode启动之后对文件系统进行更改的序列 只有在重新启动NameNode时，编辑日志才会合并到fsimage以获取文件系统元数据的最新快照。但是在线上集群上，NameNode重启并不是很常见，这就意味对于NameNode长时间运行的集群来说编辑日志可能会变得非常大(可能会无限增长)。在这种情况下我们会遇到以下问题： 编辑日志变得非常大，对于管理来说是一个挑战 NameNode重启需要很长时间，因为很多更改需要合并(译者注:需要恢复编辑日志中的各项操作，导致NameNode重启会比较慢) 在崩溃的情况下，我们将丢失大量的元数据，因为fsimage是比较旧的(译者注:生成最新fsimage之后的各项操作都保存在编辑日志中，而不是fsimage，还未合并) 所以为了解决这个问题，我们需要一个机制来帮助我们减少编辑日志的大小，并且得到一个最新的fsimage，这样NameNode上的负载就会降低一些。这与Windows恢复点非常相似，它可以让我们获得操作系统的快照，以便在出现问题时退回到上一个恢复点。 现在我们理解了NameNode的功能，以及保持最新元数据的挑战。所以这一切都与Seconadary NameNode有关？ 2. Seconadary NameNode通过Secondary NameNode实现编辑日志与fsimage的合来解决上述问题。 上图显示了Secondary NameNode的工作原理： 它定期从NameNode获取编辑日志，并与fsimage合并成新的fsimage 一旦生成新的fsimage，就会复制回NameNode NameNode下次重新启动时将使用这个fsimage进行重新启动，从而减少启动时间 Secondary NameNode的整个目的就是在HDFS中提供一个检查点。它只是NameNode的一个帮助节点。这也是它在社区内被认为是检查点节点的原因。 所以我们现在明白所有的Secondary NameNode都会在文件系统中设置一个检查点，这将有助于NameNode更好地运行。它不可以替换NameNode或也不是NameNode的备份。所以从现在开始习惯把它叫做检查点节点。 原文:http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 图解HDFS工作原理]]></title>
    <url>%2Fhadoop-illustrate-how-hdfs-works.html</url>
    <content type="text"><![CDATA[结合Maneesh Varshney的漫画改编，为大家分析HDFS存储机制与运行原理。 1. HDFS角色如下图所示，HDFS存储相关角色与功能如下： Client：客户端，系统使用者，调用HDFS API操作文件；与NameNode交互获取文件元数据；与DataNode交互进行数据读写。 Namenode：元数据节点，是系统唯一的管理者。负责元数据的管理；与client交互进行提供元数据查询；分配数据存储节点等。 Datanode：数据存储节点，负责数据块的存储与冗余备份；执行数据块的读写操作等。 2. HDFS写文件2.1 发送写数据请求 HDFS中的存储单元是block。文件通常被分成64或128M一块的数据块进行存储。与普通文件系统不同的是，在HDFS中，如果一个文件大小小于一个数据块的大小，它是不需要占用整个数据块的存储空间的。 2.2 文件切分 2.3 DataNode分配 2.4 数据写入 2.5 完成写入 2.6 角色定位 2.7 写操作分析通过写过程，我们可以了解到： HDFS属于Master与Slave结构。一个集群中只有一个NameNode，可以有多个DataNode； HDFS存储机制保存了多个副本，当写入1T文件时，我们需要3T的存储，3T的网络流量带宽；系统提供容错机制，副本丢失或宕机可自动恢复，保证系统高可用性。 HDFS默认会将文件分割成block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，会导致内存的负担很重。 3. HDFS读文件3.1 用户需求 HDFS采用的是一次写入多次读取的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。 3.2 联系元数据节点 3.3 下载数据 前文提到在写数据过程中，数据存储已经按照客户端与DataNode节点之间的距离进行了排序，距客户端越近的DataNode节点被放在最前面，客户端会优先从本地读取该数据块。 3.4 思考 4. HDFS容错机制一4.1 三类故障4.1.1 节点失败 4.1.2 网络故障 4.1.3 数据损坏(脏数据) 4.2 故障监测机制4.2.1 节点失败监测机制 4.2.2 通信故障监测机制 4.2.3 数据错误监测机制 4.3 心跳信息与数据块报告 HDFS存储理念是以最少的钱买最烂的机器并实现最安全、难度高的分布式文件系统（高容错性低成本），从上可以看出，HDFS认为机器故障是种常态，所以在设计时充分考虑到单个机器故障，单个磁盘故障，单个文件丢失等情况。 5. HDFS容错机制二5.1 写容错 5.2 读容错 6. HDFS容错机制三6.1 数据节点(DN)失效 7. 备份规则 7.1 机架与数据节点 7.2 副本放置策略 数据块的第一个副本优先放在写入数据块的客户端所在的节点上，但是如果这个客户端上的数据节点空间不足或者是当前负载过重，则应该从该数据节点所在的机架中选择一个合适的数据节点作为本地节点。 如果客户端上没有一个数据节点的话，则从整个集群中随机选择一个合适的数据节点作为此时这个数据块的本地节点。 HDFS的存放策略是将一个副本存放在本地机架节点上，另外两个副本放在不同机架的不同节点上。 这样集群可在完全失去某一机架的情况下还能存活。同时，这种策略减少了机架间的数据传输，提高了写操作的效率，因为数据块只存放在两个不同的机架上，减少了读取数据时需要的网络传输总带宽。这样在一定程度上兼顾了数据安全和网络传输的开销。 来源于: 京东大数据专家公众号]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 2.x 工作原理]]></title>
    <url>%2Fhadoop-mapReduce2.x-working-principle.html</url>
    <content type="text"><![CDATA[1. 概述对于节点数超出4000的大型集群，MapReduce 1的系统开始面领着扩展性的瓶颈。在2010年雅虎的一个团队开始设计下一代MapReduce.由此，YARN(Yet Another Resource Negotiator的缩写或者为YARN Application Resource Neforiator的缩写)应运而生。 YARN 将 Jobtracker 的职能划分为多个独立的实体，从而改善了’经典的’MapReduce面临的扩展瓶颈问题。Jobtracker负责作业调度和任务进度监视、追踪任务、重启失败或过慢的任务和进行任务登记，例如维护计数器总数。 YARN将这两种角色划分为两个独立的守护进程：管理集群上资源使用的资源管理器(ResourceManager)和管理集群上运行任务生命周期的应用管理器(ApplicationMaster)。基本思路是：应用服务器与资源管理器协商集群的计算资源：容器(每个容器都有特定的内存上限)，在这些容器上运行特定应用程序的进程。容器由集群节点上运行的节点管理器(NodeManager)监视，以确保应用程序使用的资源不会超过分配给它的资源。 与jobtracker不同，应用的每个实例（这里指一个MapReduce作业）有一个专用的应用master(ApplicationMaster)，它运行在应用的运行期间。这种方式实际上和最初的Google的MapReduce论文里介绍的方法很相似，该论文描述了master进程如何协调在一组worker上运行的map任务和reduce任务。 如前所述，YARN比MapReduce更具一般性，实际上MapReduce只是YARN应用的一种形式。有很多其他的YARN应用(例如能够在集群中的一组节点上运行脚本的分布式shell)以及其他正在开发的程序。YARN设计的精妙之处在于不同的YARN应用可以在同一个集群上共存。例如，一个MapReduce应用可以同时作为MPI应用运行，这大大提高了可管理性和集群的利用率。 此外，用户甚至有可能在同一个YARN集群上运行多个不同版本的MapReduce，这使得MapReduce升级过程更容易管理。注意，MapReduce的某些部分(比如作业历史服务器和shuffle处理器)以及YARN本身仍然需要在整个集群上升级。 YARN上的MapReduce比经典的MapReduce包括更多的实体： 提交MapReduce作业的客户端 YARN资源管理器(ResourceManager)，负责协调集群上计算资源的分配 YARN节点管理器(NodeManager)，负责启动和监视集群中机器上的计算容器(container) MapReduce应用程序master(ApplicationMaster)，负责协调运行MapReduce作业的任务。它和MapReduce任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理 分布式文件系统（一般为HDFS），用来与其他实体见共享作业文件 作业运行过程如下图所示: 2. 作业提交MapReduce 2 中的作业提交是使用与MapReduce 1相同的用户API(参见上图步骤1)。MapReduce 2实现了ClientProtocol，当mapreduce.framework.name设置为yarn时启动。提交的过程与经典的非常相似。从资源管理器ResourceManager(而不是jobtracker)获取新的作业ID，在YARN命名法中它是一个应用程序ID(参见上图步骤2)。作业客户端检查作业的输出说明，计算输入分片(虽然有选项yarn.app.mapreduce.am.compute-splits-in-cluster在集群上来产生分片，这可以使具有多个分片的作业从中受益)并将作业资源(包括作业JAR、配置和分片信息)复制到HDFS(参见上图步骤3）。最后，通过调用资源管理器上的submitApplication()方法提交作业(参见上图步骤4)。 3. 作业初始化资源管理器收到调用它的submitApplication()消息后，便将请求传递给调度器(scheduler)。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动应用程序的master进程(ApplicationMaster)(参见上图步骤5a和5b)。 MapReduce作业的ApplicationMaster是一个Java应用程序,它的主类是MRAppMaster。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告(参见上图步骤6)。接下来，它接受来自共享文件系统的在客户端计算的输入分片(参见上图步骤7)。对每一个分片创建一个map任务对象以及由mapreduce.job.reduces属性确定的多个reduce任务对象。 接下来，ApplicationMaster决定如何运行构成MapReduce作业的各个任务。如果作业很小，就选择在与它同一个JVM上运行任务。 相对于在一个节点上顺序运行它们，判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这不同于MapReduce 1，MapReduce 1从不在单个tasktracker上运行小作业。这样的作业称为uberized，或者作为uber任务运行。 哪些任务是小任务呢？ 默认情况下，小任务就是小于10个mapper且只有1个reducer且输入大小小于一个HDFS块的任务。(通过设置mapreduce.job.ubertask.maxmaps、mapreduce.job.ubertask.maxreduces和mapreduce.job.ubertask.maxbytes可以改变一个作业的上述值。)将mapreduce.job.ubertask.enable设置为false也可以完全使uber任务不可用。 在任何任务运行之前，作业的setup方法为了设置作业的OutputCommitter被调用来建立作业的输出目录。在MapReduce 1中，它在一个由tasktracker运行的特殊任务中被调用，而在YARN执行框架中，该方法由应用程序master直接调用。 4. 任务分配如果作业不适合作为uber任务运行，那么ApplicationMaster就会为该作业中的所有map任务和reduce任务向资源管理器请求容器(参见上图步骤8)。附着心跳信息的请求包括每个map任务的数据本地化信息，特别是输入分片所在的主机和相应机架信息。调度器使用这些信息来做调度策略(像jobtracker的调度器一样)。理想情况下，它将任务分配到数据本地化的节点，但如果不可能这样做，调度器就会相对于非本地化的分配有限使用机架本地化的分配。 请求也为任务指定了内存需求。在默认情况下，map任务和reduce任务都分配到1024MB的内存，但这可以通过mapreduce.map.memory.mb和mapreduce.reduce.memory.mb来设置。 内存的分配方式不同于MapReduce 1，后者中tasktrackers有在集群配置时设置的固定数量的槽，每个任务在一个槽上运行。槽有最大内存分配限制，这对集群是固定的，导致当任务使用较少内存时无法充分利用内存(因为其他等待的任务不能使用这些未使用的内存)以及由于任务不能获取足够内存而导致作业失败。 在YARN中，资源划分更细的粒度，所以可以避免上述问题。具体而言，应用程序可以请求最小到最大限制范围内的任意最小值倍数的内存容量。默认的内存分配容量是调度器特定的，对于容量调度器，它的默认值最小值是1024MB(由 yarn.sheduler.capacity.minimum-allocation-mb设置)，默认的最大值是10240MB(由yarn.sheduler.capacity.maximum-allocation-mb设置)。因此，任务可以通过适当设置mapreduce.map.memory.mb和 mapreduce.reduce.memory.mb来请求1GB到10GB间的任务1GB倍数的内存容量(调度器在需要的时候使用最接近的倍数)。 5. 任务执行一旦资源管理器的调度器为任务分配了容器，ApplicationMaster就通过与节点管理器通信来启动容器(参见上图步骤9a和9b)。该任务由主类YarnChild的Java应用程序执行，在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、JAR文件和所有来自分布式缓存的文件(参见上图步骤10)。最后，运行map任务或reduce任务(参见上图步骤11)。 6. 进度和状态更新在YARN下运行时，任务每三秒钟通过umbilical接口向ApplicationMaster汇报进度和状态(包含计数器)，作为作业的汇聚视图(aggregate view)。相比之下，MapReduce 1通过tasktracker到jobtracker来实现进度更新。 客户端每秒钟(通过mapreduce.client.progressmonitor.pollinterval设置)查询一次ApplicationMaster以接收进度更新，通常都会向用户显示。 7. 作业完成除了向ApplicationMaster查询进度外，客户端每5秒钟通过调用Job的waitForCompletion()来检查作业是否完成。查询的间隔可以通过mapreduce.client.completion.pollinterval属性进行设置。 作业完成后，ApplicationMaster和任务容器清理其工作状态，OutputCommitter的作业清理方法会被调用。作业历史服务器保存作业的信息供用户需要时查询。 来源于: Hadoop 权威指南]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 1.x 工作原理]]></title>
    <url>%2Fhadoop-mapReduce1.x-working-principle.html</url>
    <content type="text"><![CDATA[下面解释一下作业在经典的MapReduce 1.0中运行的工作原理。最顶层包含4个独立的实体: 客户端，提交MapReduce作业。 JobTracker，协调作业的运行。JobTracker是一个Java应用程序，它的主类是JobTracker。 TaskTracker，运行作业划分后的任务。TaskTracker是一个Java应用程序，它的主类是TaskTracker。 分布式文件系统(一般为HDFS)，用来在其他实体间共享作业文件。 1. 作业提交Job的submit()方法创建一个内部的JobSunmmiter实例，并且调用其submitJobInternal()方法。提交作业后，waitForCompletion()每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功，就显示作业计数器。如果失败，导致作业失败的错误被记录到控制台。 JobSunmmiter所实现的作业提交过程如下: (1) 向jobtracker请求一个新的作业ID(通过调用JobTracker的getNewJobId()方法获取）。参见上图步骤2。 (2) 检查作业的输出说明。例如，如果没有指定输出目录或输出目录已经存在，作业就不提交，错误抛回给MapReduce程序。 (3) 计算作业的输入分片。如果分片无法计算，比如因为输入路径不存在，作业不提交，错误返回给MapReduce程序。 (4) 将运行作业所需要的资源(包括作业JAR文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的目录下jobtracker的文件系统中。作业JAR中副本较多(由mapred.submit.replication属性控制，默认值为10)，因此在运行作业的任务时，集群中有很多个副本可供tasktracker访问。参见上图步骤3. (5) 告知jobtracker作业准备执行(通过调用JobTracker的submitJob()方法实现)。参见上图步骤4. 2. 作业初始化当JobTracker接收到其submitJob()方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度，并对其进行初始化。初始化包括创建一个表示正在运行作业的对象，用于封装任务和记录信息，以便跟踪任务的状态和进程(参见上图步骤 5)。 为了创建任务运行列表，作业调度器首先从共享文件系统中获取JobClient已计算好的输入分片信息(参见上图步骤6)。然后为每个分片创建一个Map任务。创建的Reduce任务的数量由JobConf的mapred.reduce.task属性决定，它是用setNumReduceTasks()方法来设置的，然后调度器创建相应数量的要运行的reduce任务。任务在此时被指定ID。 除了Map任务和Reduce任务，还会创建两个任务:作业创建和作业清理。这两个任务在TaskTracker中执行，在Map任务之前运行代码来创建作业，并且在所有Reduce任务完成之后完成清理工作。配置项OutputCommitter属性能设置运行的代码。默认值是FileOutputCommitter。作业创建为作业创建输出路径和临时工作空间。作业清理清除作业运行过程中的临时目录。 3. 任务分配tasktracker运行一个简单的循环来定期发送心跳(heartbeat)给jobtracker。心跳向jobtracker表明tasktracker是否还存活，同时也充当两者之间的消息通道。作为心跳的一部分，tasktracker会指明它是否已经准备好运行新的任务，如果是，jobtracker会为它分配一个任务，并使用心跳的返回值与tasktracker进行通信(参见上图步骤7)。 在jobtracker为tasktracker选择任务之前，jobtracker必须先选定任务所在的作业。一旦选择好作业，jobtracker就可以为该作业选定一个任务。 对于Map任务和Reduce任务，tasktracker有固定数据的任务槽。例如，一个tasktracker可能可以同时运行两个Map任务和两个Reduce任务。准确数据由tasktracker核的数据和内存大小来决定。默认调度器在处理Reduce任务槽之前，会填满空闲的Map任务槽，因此，如果tasktracker至少有一个空闲的Map任务槽，jobtracker会为它选择一个Map任务，否则选择一个Reduce任务。 为了选择一个Reduce任务，jobtracker简单地从待运行的Reduce任务列表中选取下一个来执行，用不着考虑数据的本地化。然而，对于一个Map任务，jobtracker会考虑tasktracker的网络设置，并选取一个距离其输入分片最近的tasktracker。在最理想的情况下， 任务是数据本地化的(data-local)，也就是任务运行在输入分片所在的节点上。同样，任务也可能是机架本地化的(rack-local)：任务和输入分片在所同一机架，但不在同一节点上。一些任务即不是数据本地化的，也不是机架本地化的，而是从与它们自身运行的不同机架上检索数据。可以通过查看作业的计数器得知每类任务的比例。 4. 任务执行现在，tasktracker已经被分配了一个任务，下一步是运行该任务。第一步，通过从共享文件系统把作业的JAR文件复制到tasktracker所在的文件系统，从而实现作业的JAR文件本地化。同时，tasktracker将应用程序所需要的全部文件从分布式缓存复制到本地磁盘(参见上图步骤8)。第二步，tasktracker为任务新建一个本地工作目录，并把JAR文件中的内容解压到这个文件夹下。第三步，tasktracker新建一个TaskRunner实例来运行该任务。 TaskRunner启动一个新的JVM(参见上图步骤9)来运行每个任务(参见上图步骤10)，以便用户定义的map和reduce函数的任务软件问题都不会影响到tasktracker(例如导致崩溃或挂起等）。但在不同的任务之前重用JVM还是可能的。 5. 进度和状态的更新MapReduce作业是长时间运行的批处理作业，运行时间范围从数分钟到数小时。这是一个很长的时间段，所以对于用户而言，能够的制作也进展是很重要的。一个作业和它的每个任务都有一个状态(status)，包括:作业或任务的状态(比如，运行状态，成功完成，失败状态)，map和reduce的进度，作业计数器的值，状态消息或描述。 6. 作业完成当jobtracker收到作业最后一个任务已完成的通知后(这是一个特定的作业清理任务)，便把作业的状态设置为’成功’。然后，在Job查询状态时，便知道任务以成功完成，于是Job打印一条消息告知用户，然后从waitForCompletion()方法返回。Job的统计信息和计数值也在这是输出到控制台。 最后，jobtracker清空作业的工作状态，只是tasktracker也清空作业的工作状态(如删除中间输出)。 来源于: Hadoop 权威指南]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 内部表与外部表]]></title>
    <url>%2Fhive-managed-table-external-table.html</url>
    <content type="text"><![CDATA[托管表(内部表)和外部表是Hive中的两种不同类型的表，在这篇文章中，我们将讨论Hive中表的类型以及它们之间的差异以及如何创建这些表以及何时将这些表用于特定的数据集。 1. 内部表托管表(Managed TABLE)也称为内部表(Internal TABLE)。这是Hive中的默认表。当我们在Hive中创建一个表，没有指定为外部表时，默认情况下我们创建的是一个内部表。如果我们创建一个内部表，那么表将在HDFS中的特定位置创建。默认情况下，表数据将在HDFS的/usr/hive/warehouse目录中创建。如果我们删除了一个内部表，那么这个表的表数据和元数据都将从HDFS中删除。 1.1 创建表我们可以用下面的语句在Hive里面创建一个内部表：CREATE TABLE IF NOT EXISTS tb_station_coordinate( station string, lon string, lat string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;,&apos;; 我们已经成功创建了表并使用如下命令检查表的详细信息：hive&gt; describe formatted tb_station_coordinate;OK# col_name data_type comment station string lon string lat string # Detailed Table Information Database: default Owner: xiaosi CreateTime: Tue Dec 12 17:42:09 CST 2017 LastAccessTime: UNKNOWN Retention: 0 Location: hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE &#123;\&quot;BASIC_STATS\&quot;:\&quot;true\&quot;&#125; numFiles 0 numRows 0 rawDataSize 0 totalSize 0 transient_lastDdlTime 1513071729 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim , serialization.format , Time taken: 0.16 seconds, Fetched: 33 row(s) 从上面我们可以看到表的类型Table Type为MANAGED_TABLE，即我们创建了一个托管表(内部表)。 1.2 导入数据我们使用如下命令将一个样本数据集导入到表中：hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;Loading data to table default.tb_station_coordinateOKTime taken: 2.418 seconds 如果我们在HDFS的目录/user/hive/warehouse/tb_station_coordinate查看，我们可以得到表中的内容：xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinateFound 1 items-rwxr-xr-x 1 xiaosi supergroup 374 2017-12-12 17:50 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txtxiaosi@yoona:~$xiaosi@yoona:~$xiaosi@yoona:~$ hadoop fs -text /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt桂林北站,110.302159,25.329024杭州东站,120.213116,30.290998山海关站,119.767555,40.000793武昌站,114.317576,30.528401北京南站,116.378875,39.865052... &gt;/home/xiaosi/station_coordinate.txt是本地文件系统路径。从上面的输出我们可以看到数据是从本地的这个路径复制到HDFS上的/user/hive/warehouse/tb_station_coordinate/目录下。为什么会自动复制到HDFS这个目录下呢？这个是由Hive的配置文件设置的。在Hive的${HIVE_HOME}/conf/hive-site.xml配置文件中指定，hive.metastore.warehouse.dir属性指向的就是Hive表数据存放的路径(在这配置的是/user/hive/warehouse/)。Hive每创建一个表都会在hive.metastore.warehouse.dir指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面/user/hive/warehouse/tb_station_coordinate。 1.3 删除表现在让我们使用如下命令删除上面创建的表:hive&gt; drop table tb_station_coordinate;Moved: &apos;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&apos; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/CurrentOKTime taken: 1.327 seconds 从上面的输出我们可以得知，原来属于tb_station_coordinate表的数据被移到hdfs://localhost:9000/user/xiaosi/.Trash/Current文件夹中(如果你的Hadoop没有采用回收站机制，那么删除操作将会把属于该表的所有数据全部删除)(回收站机制请参阅:Hadoop Trash回收站使用指南)。 如果我们在HDFS的目录/user/hive/warehouse/tb_station_coordinate查看：xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinatels: `/user/hive/warehouse/tb_station_coordinate&apos;: No such file or directory 你可以看到输出为No such file or directory，因为表及其内容都从HDFS从删除了。 2. 外部表当数据在Hive之外使用时，创建外部表(EXTERNAL TABLE)来在外部使用。无论何时我们想要删除表的元数据，并且想保留表中的数据，我们使用外部表。外部表只删除表的schema。 2.1 外部普通表我们使用如下命令创建一个外部表：CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate( station string, lon string, lat string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;,&apos;; 我们现在已经成功创建了外部表。我们使用如下命令检查关于表的细节：hive&gt; describe formatted tb_station_coordinate;OK# col_name data_type comment station string lon string lat string # Detailed Table Information Database: default Owner: xiaosi CreateTime: Tue Dec 12 18:16:13 CST 2017 LastAccessTime: UNKNOWN Retention: 0 Location: hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate Table Type: EXTERNAL_TABLE Table Parameters: COLUMN_STATS_ACCURATE &#123;\&quot;BASIC_STATS\&quot;:\&quot;true\&quot;&#125; EXTERNAL TRUE numFiles 0 numRows 0 rawDataSize 0 totalSize 0 transient_lastDdlTime 1513073773 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim , serialization.format , Time taken: 0.132 seconds, Fetched: 34 row(s) 从上面我们可以看到表的类型Table Type为EXTERNAL_TABLE，即我们创建了一个外部表。 2.2 导入数据我们使用如下命令将一个样本数据集导入到表中：hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;Loading data to table default.tb_station_coordinateOKTime taken: 2.418 seconds 如果我们在HDFS的目录/user/hive/warehouse/tb_station_coordinate查看，我们可以得到表中的内容：xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinateFound 1 items-rwxr-xr-x 1 xiaosi supergroup 374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txtxiaosi@yoona:~$xiaosi@yoona:~$xiaosi@yoona:~$ hadoop fs -text /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt桂林北站,110.302159,25.329024杭州东站,120.213116,30.290998山海关站,119.767555,40.000793武昌站,114.317576,30.528401... 2.3 删除表现在让我们使用如下命令删除上面创建的表:hive&gt; drop table tb_station_coordinate;OKTime taken: 0.174 secondshive&gt; 我们的Hadoop已经开启了回收站机制，但是删除操作并没有将数据进行删除，不像删除内部表一样，输出Moved: &#39;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&#39; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current(回收站机制请参阅:Hadoop Trash回收站使用指南)。为了验证我们真的没有删除数据，我们在HDFS目录下查看数据:xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinateFound 1 items-rwxr-xr-x 1 xiaosi supergroup 374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txtxiaosi@yoona:~$xiaosi@yoona:~$ hadoop fs -text /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt桂林北站,110.302159,25.329024杭州东站,120.213116,30.290998山海关站,119.767555,40.000793武昌站,114.317576,30.528401北京南站,116.378875,39.865052... 你可以看到表中的数据仍然在HDFS中。所以我们得知如果我们创建一个外部表，在删除表之后，只有与表相关的元数据被删除，而不会删除表的内容。 2.4 创建表指定外部目录只有当你的数据在/user/hive/warehouse目录中时，上述方法才能有效。但是，如果你的数据在另一个位置，如果你删除该表，数据也将被删除。所以在这种情况下，你需要在创建表时设置数据的外部位置，如下所示：CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate( station string, lon string, lat string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;,&apos;LOCATION &apos;/user/xiaosi/test/coordinate/&apos;; 备注:你也可以通过在创建表时设置数据存储位置来创建一个内部表。但是，如果删除表，数据将被删除。 如果你想要创建外部表，需要在创建表的时候加上 EXTERNAL 关键字，同时指定外部表存放数据的路径(例如2.4所示)，也可以不指定外部表的存放路径(例如2.3所示)，这样Hive将在HDFS上的/user/hive/warehouse/目录下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里。 3. 使用场景3.1 内部表 数据是临时的 希望使用Hive来管理表和数据的生命周期 删除后不想要数据 3.2 外部表 这些数据也在Hive之外使用。 Hive不管理数据和权限设置以及目录等，需要你有另一个程序或过程来做这些事情 不是基于现有表(AS SELECT)来创建的表 可以创建表并使用相同的模式并指向数据的位置 参考:https://acadgild.com/blog/managed-and-external-tables-in-hive/ https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce2.0 架构详解]]></title>
    <url>%2Fhadoop-mapReduce2.0-architecture-detail.html</url>
    <content type="text"><![CDATA[1. MapReduce 2.0 概述Apache Hadoop 0.23比以前的版本有了很大的改进。以下是MapReduce的一些亮点；请注意，HDFS也有一些主要的改进，这些都不在本文的讨论范围之内。 MapReduce 2.0(又名MRv2或YARN)。新的架构将JobTracker的两个主要功能 - 资源管理和作业生命周期管理 - 分解成单独的组件： 管理计算资源到应用程序的全局分配的ResourceManager(RM)。 每个应用程序的ApplicationMaster(AM)，用于管理应用程序的生命周期。 在Hadoop 0.23中，MapReduce应用程序是由MapReduce ApplicationMaster执行的MapReduce传统意义上的单一作业。 每个节点上还有一个NodeManager(NM)，用于管理该节点上的用户进程。RM和NM构成集群的计算框架。该设计还允许在NM中运行长时间的辅助服务(The design also allows plugging long-running auxiliary services to the NM)；这些都是特定于应用程序的服务，作为配置的一部分进行指定，并在启动期间由NM加载。对于YARN上的MapReduce应用，shuffle是由NM加载的典型的辅助服务。请注意，在Hadoop 0.23版本之前，shuffle是TaskTracker的一部分。 每个应用程序的ApplicationMaster是一个特定框架库，负责与ResourceManager协商资源，并与NodeManager一起来执行和监视这些任务。在YARN设计中，MapReduce只是一个应用程序框架， 该设计也可以允许使用其他框架来构建和部署分布式应用程序。例如，Hadoop 0.23附带了一个分布式Shell应用程序，允许在YARN集群上的多个节点上运行一个shell脚本。 2. MapReduce 2.0 设计下图显示了一个YARN集群。只有一个资源管理器，它有两个主要的服务： 可插拔的调度器，用于管理和实施集群中的资源调度策略。请注意，在编写本博文时，Hadoop 0.23中支持两个调度器，默认的FIFO调度器和Capacity调度器; Fair调度器尚未支持(译者注:博文2012编写，现在已经支持)。 Applications Manager(AsM)，负责管理集群中运行的Application Masters，例如，负责启动Application Masters，以及在发生故障时在不同节点上监视和重新启动Application Masters。 上图还显示了在集群上的每个节点上都运行一个NM服务。该图还显示了有两个AM(AM1和AM2)。对于给定的任意YARN集群中，有多少个应用程序(作业)，就运行多少个Application Masters。每个AM管理应用程序的各个任务(启动，监视，以及在发生故障时重新启动任务)。该图还显示了AM1管理三个任务(容器1.1,1.2和1.3)，AM2管理四个任务(容器2.1,2.2,2.3和2.4)。每个任务运行在每个节点的Container中。在AM联系对应的NM来启动应用程序的各个任务之前，从RM的调度器中获取这些容器。这些容器可以大致与以前的Hadoop版本中的Map/Reduce插槽进行比较。然而，从集群利用角度来看，Hadoop-0.23中的资源分配模型更加优化。 3. MapReduce 2.0 资源分配模型在较早的Hadoop版本中，集群中的每个节点都静态分配运行预定义数量的Map插槽和预定义数量的Reduce插槽的资源容量。插槽无法在Map和Reduce之间共享。这种静态分配槽的方式并不是最佳选择，因为在MR作业生命周期中槽的需求是不同的(典型地，当作业开始时对Map槽有需求，相反，对Reduce槽的需求是在最后)。实际上，在一个真正的集群中，作业是随机提交的，每个集群都有自己的Map/Reduce槽需求，对集群充分利用不是不可能，是非常难。 Hadoop 0.23中的资源分配模型通过提供更灵活的资源建模来解决此类缺陷。以容器的形式请求资源，其中每个容器具有许多非静态属性。在写本博客时(2012年)，唯一支持的属性是内存(RAM)。然而，该模型是通用的，并且有意在将来的版本中添加更多的属性(例如CPU和网络带宽)。在这个新的资源管理模型中，每个属性只定义了最小值和最大值，并且AM可以请求具有这些最小值的倍数的属性值的容器。 4. MapReduce 2.0 主要组件我们将详细介绍MapReduce架构的主要组件，以了解这些组件的功能以及它们如何交互的。 2.1 Client – Resource Manager下展示了在YARN集群上运行应用程序的初始步骤。通常，客户端与RM(特别是与RM的Applications Manager组件)通信来开启此步骤。图中标记为(1)的第一步是让客户端告诉Applications Manager我们提交应用程序的意愿，这是通过创建应用程序请求(New Application Request)完成的。标记为(2)的RM响应通常包含一个新生成的唯一应用程序ID，以及有关客户端在请求资源以运行应用程序AM时所需要的集群资源容量的信息。 使用从RM接收到的响应信息，客户端可以构建并提交标记为(3)的应用程序提交上下文(Application Submission Context)，除了RM所需要来启动AM的信息之外，通常还包含诸如调度器队列，优先级和用户信息之类的信息。这些信息包含在容器启动上下文(Container Launch Context)中，还包含应用程序的jar，作业文件，安全令牌和任何需要的资源等。 在提交申请之后，客户端可以向RM查询应用程序报告，并接收返回的报告，并且如果需要，客户端也可以要求RM终止该应用程序。这三个步骤在下图中展示: 2.2 Resource Manager – Application Master当RM从客户端接收到应用程序提交上下文时，会找到一个满足运行AM资源需求的可用容器，并与该容器的NM联系，以便在该节点上启动AM进程。下图描述了AM和RM(特别是与RM的调度器)之间的通信步骤。图中标记为(1)的第一步是AM将自己注册到RM中。这一步由一个握手过程组成，同时还传递了AM将要监听的RPC端口，监视应用程序状态和进度的跟踪URL等信息。 标记为(2)的RM注册响应为AM传递一些基本信息，比如集群的最小和最大资源容量。AM将使用这些信息为各个任务的任何资源请求来计算和请求资源。标记为(3)的从AM到RM的资源分配请求主要包含所请求的容器列表，并且还可能包含该AM所释放的容器列表。心跳和进度信息也可以通过资源分配请求进行传达，如箭头(4)所示。 当RM的调度器接收到资源分配请求时，它基于调度策略计算满足该请求的容器列表，并且返回分配响应，标记为(5)，其中包含分配的资源列表。使用资源列表，AM开始联系相关联的NM(很快就会看到)，最后，如箭头(6)所示，当作业完成时，AM向RM发送应用完成的消息并退出。 2.3 Application Master – Container Manager下图描述了AM和Node Manager之间的通信。AM为每个容器请求NM来启动它，如图中箭头(1)所示。在容器运行时，AM可以分别请求并接收容器状态报告，如步骤(2)和(3)所示。 基于以上讨论，编写YARN应用程序的开发人员应主要关注以下接口： ClientRMProtocol：Client RM(图3)。这是客户端与RM进行通信以启动新的应用程序(即AM)，检查应用程序状态或终止应用程序的协议。 AMRMProtocol：AM RM(图4)。这是AM用来向RM注册或取消注册，以及从RM调度器请求资源来运行任务的协议。 ContainerManager：AM NM(图5)。这是AM用来与NM进行通信以启动或停止容器以及获取容器状态更新的协议。 原文:http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce新一代架构MRv2]]></title>
    <url>%2Fhadoop-mapReduce-new-generation-architecture-mrv2.html</url>
    <content type="text"><![CDATA[MapReduce在hadoop-0.23中经历了彻底的改变，现在我们称之为MapReduce 2.0(MRv2)或者YARN。 MRv2的基本思想是将JobTracker的两个主要功能，资源管理和作业调度/监视的功能拆分为独立的守护进程。设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager(RM)和每个应用程序特有的ApplicationMaster(AM)。每个应用程序要么是单个作业，要么是DAG作业。 1. ResourceManagerResourceManager(RM)和每个从节点以及NodeManager(NM)构成了数据计算框架。ResourceManager是系统中所有应用程序资源分配的最终决策者。 ResourceManager有两个主要组件:Scheduler(调度器) 和 ApplicationsManager。 1.1 SchedulerScheduler根据容量，队列等限制条件将资源分配给各种正在运行的应用程序。Scheduler是’纯调度器’，因为它负责监视或跟踪应用程序的状态。此外，它也不保证会重启由于应用程序错误或硬件故障原因导致失败的任务。Scheduler仅根据应用程序的资源请求来执行调度。它基于’资源容器’(Resource Container)这一抽象概念来实现的，资源容器包括如内存，cpu，磁盘，网络等。 Scheduler是一个可插拔的组件，它负责将集群资源分配给不同队列和应用程序。目前Scheduler支持诸如CapacityScheduler和FairScheduler。CapacityScheduler支持分层队列，以便更可预测地共享群集资源 1.2 ApplicationsManagerApplicationsManager(ASM)主要负责接受作业提交，协商获取第一个容器来执行应用程序的ApplicationMaster(negotiating the first container for executing the application specific ApplicationMaster)，并提供在故障时重新启动ApplicationMaster的服务。 2. NodeManagerNodeManager是每个节点上框架代理，主要负责启动应用所需要的容器，监视它们的资源使用情况(cpu，内存，磁盘，网络)，并将其报告给ResourceManager的Scheduler。 3. ApplicationMaster事实上，每一个应用程序的ApplicationMaster是一个框架库，负责与Scheduler协商合适的资源容器以及与NodeManager一起跟踪他们的状态并监视进度。 MRV2保持与以前稳定版本(hadoop-1.x)API的兼容性。这意味着所有的Map-Reduce作业仍然可以在MRv2上运行，只需重新编译即可。 原文:http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 推测执行]]></title>
    <url>%2Fspeculative-execution-in-hadoop-mapreduce.html</url>
    <content type="text"><![CDATA[1. 概述Hadoop不会去诊断或修复执行慢的任务，相反，它试图检测任务的运行速度是否比预期慢，并启动另一个等效任务作为备份(备份任务称为推测任务)。这个过程在Hadoop中被称为推测执行。 在这篇文章中，我们将讨论推测执行 - Hadoop中提高效率的一个重要功能，我们有必要去了解Hadoop中的推测执行是否总是有帮助的，或者我们需要关闭它时如何禁用。 2. 什么是推测执行在Hadoop中，MapReduce将作业分解为任务，并且这些任务并行而不是顺序地运行，从而缩短了总体执行时间。这种执行模式对缓慢的任务很敏感(即使他们的数量很少)，因为它们减慢了整个工作的执行速度。 任务执行缓慢的原因可能有各种，包括硬件退化或软件错误配置等，尽管花费的时间超过了预期的时间，但是由于任务仍然有可能成功完成，因此很难检测缓慢原因。Hadoop不会尝试诊断和修复运行缓慢的任务，而是尝试检测并为其运行一个备份任务。这在Hadoop中被称为推测执行。这些备份任务在Hadoop中被称为推测任务。 3. 推测执行如何工作现在让我们看看Hadoop的推测执行过程。 首先，在Hadoop MapReduce中启动所有任务。为那些已经运行了一段时间(至少一分钟)且比作业中其他任务平均进度慢的任务启动推测任务。如果原始任务在推测性任务之前完成，那么推测任务将被终止，相反，如果推测性任务在原始任务之前完成，那么原始任务被终止。一个任务成功完成之后，任何正在运行的重复任务都将被终止。 4. 推测执行的优势Hadoop MapReduce推测执行在某些情况下是很有帮助的，因为在具有100个节点的Hadoop集群中，硬件故障或网络拥塞等问题很常见，并行或重复运行任务会更好一些，因为我们不必等到有问题的任务执行之后。 但是如果两个重复的任务同时启动，就会造成集群资源的浪费。 5. 配置推测执行推测执行是Hadoop MapReduce作业中的一种优化技术，默认情况下启用的。你可以在mapred-site.xml中禁用mappers和reducer的推测执行，如下所示：&lt;property&gt; &lt;name&gt;mapred.map.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapred.reduce.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 6. 有没有必要关闭推测执行推测执行的主要目的是减少工作执行时间，但是，由于重复的任务，集群效率受到影响。由于在推测执行中正在执行冗余任务，因此这可能降低整体吞吐量。出于这个原因，一些集群管理员喜欢关闭Hadoop中的推测执行。 对于Reduce任务，关闭推测执行是有益的，因为任意重复的reduce任务都必须将取得map任务输出作为最先的任务，这可能会大幅度的增加集群上的网络传输。 关闭推测执行的另一种情况是考虑到非幂等任务。然而在很多情况下，将任务写成幂等的并使用OutputCommitter来提升任务成功时输出到最后位置的速度，这是可行的。 原文:https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Trash回收站使用指南]]></title>
    <url>%2Fhadoop-hdfs-trash.html</url>
    <content type="text"><![CDATA[我们在删除一个文件时，遇到如下问题，提示我们不能删除文件放回回收站:sudo -uxiaosi hadoop fs -rm -r tmp/data_group/test/employee/employee_salary.txt17/12/06 16:34:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.17/12/06 16:34:48 WARN fs.TrashPolicyDefault: Can&apos;t create trash directory: hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employeerm: Failed to move to trash: hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt. Consider using -skipTrash option 去回收站对应目录下观察一下，得出的结论是：无法创建目录employee，因为employee文件已经存在，自然导致employee_salary.txt文件不能放回收回站:-rw-r--r-- 3 xiaosi xiaosi 352 2017-12-06 16:18 hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee 跟如下是同样的道理:xiaosi@yoona:~$ ll employee-rw-rw-r-- 1 xiaosi xiaosi 0 12月 6 16:56 employeexiaosi@yoona:~$xiaosi@yoona:~$xiaosi@yoona:~$ mkdir employeemkdir: 无法创建目录&quot;employee&quot;: 文件已存在 借此机会，详细研究了一下HDFS的Trash回收站机制。 1. 配置HDFS的回收站就像Windows操作系统中的回收站一样。它的目的是防止你无意中删除某些东西。你可以通过设置如下属性来启用此功能(默认是不开启的)：&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;description&gt;Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt;Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.&lt;/description&gt; &lt;/property&gt; 属性 说明 fs.trash.interval 分钟数，当超过这个分钟数后检查点会被删除。如果为零，回收站功能将被禁用。 fs.trash.checkpoint.interval 检查点创建的时间间隔(单位为分钟)。其值应该小于或等于fs.trash.interval。如果为零，则将该值设置为fs.trash.interval的值。 2. Trash启用回收站功能后，使用rm命令从HDFS中删除某些内容时，文件或目录不会立即被清除，它们将被移动到回收站Current目录中(/user/${username}/.Trash/current)。如果检查点已经启用，会定期使用时间戳重命名Current目录。.Trash中的文件在用户可配置的时间延迟后被永久删除。回收站中的文件和目录可以简单地通过将它们移动到.Trash目录之外的位置来恢复:sudo -uxiaosi hadoop fs -rm tmp/data_group/test/employee/employee_salary.txt17/12/06 17:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.Moved: &apos;hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt&apos; to trash at: hdfs://cluster/user/xiaosi/.Trash/Current 说明: Deletion interval表示检查点删除时间间隔(单位为分钟)。这里是fs.trash.interval的值。NameNode运行一个线程来定期从文件系统中删除过期的检查点。 Emptier interval表示在运行线程来管理检查点之前，NameNode需要等待多长时间(以分钟为单位)，即检查点创建时间间隔。NameNode删除超过fs.trash.interval的检查点，并为/user/${username}/.Trash/Current创建一个新的检查点。该频率由fs.trash.checkpoint.interval的值确定，且不得大于Deletion interval。这确保了在emptier窗口内回收站中有一个或多个检查点。 例如，可以设置如下:fs.trash.interval = 360 (deletion interval = 6 hours)fs.trash.checkpoint.interval = 60 (emptier interval = 1 hour) 这导致NameNode为Current目录下的垃圾文件每小时创建一个新的检查点，并删除已经存在超过6个小时的检查点。 在回收站生命周期结束后，NameNode从HDFS命名空间中删除该文件。删除文件会导致与文件关联的块被释放。请注意，用户删除文件的时间与HDFS中相应增加可用空间的时间之间可能存在明显的时间延迟，即用户删除文件，HDFS可用空间不会立马增加，中间有一定的延迟。 3. 检查点检查点仅仅是用户回收站下的一个目录，用于存储在创建检查点之前删除的所有文件或目录。如果你想查看回收站目录，可以在/user/${username}/.Trash/{timestamp_of_checkpoint_creation}处看到:hadoop fs -ls hdfs://cluster/user/xiaosi/.Trash/Found 3 itemsdrwx------ - xiaosi xiaosi 0 2017-12-05 08:00 hdfs://cluster/user/xiaosi/.Trash/171205200038drwx------ - xiaosi xiaosi 0 2017-12-06 01:00 hdfs://cluster/user/xiaosi/.Trash/171206080038drwx------ - xiaosi xiaosi 0 2017-12-06 08:00 hdfs://cluster/user/xiaosi/.Trash/Current 最近删除的文件被移动到回收站Current目录，并且在可配置的时间间隔内，HDFS会为在Current回收站目录下的文件创建检查点/user/${username}/.Trash/&lt;日期&gt;，并在过期时删除旧的检查点。 4. 清空回收站首先想到的是只要删除整个回收站目录，将会清空回收站。诚然，这是一个选择。但是我们有更好的选择。HDFS提供了一个命令行工具来完成这个工作：hadoop fs -expunge 该命令使NameNode永久删除回收站中比阈值更早的文件，而不是等待下一个emptier窗口。它立即从文件系统中删除过期的检查点。 5. 注意点回收站功能默认是禁用的。对于生产环境，建议启用回收站功能以避免意外的删除操作。启用回收站提供了从用户操作删除或用户意外删除中恢复数据的机会。但是为fs.trash.interval和fs.trash.checkpoint.interval设置合适的值也是非常重要的，以使垃圾回收以你期望的方式运作。例如，如果你需要经常从HDFS上传和删除文件，则可能需要将fs.trash.interval设置为较小的值，否则检查点将占用太多空间。 当启用垃圾回收并删除一些文件时，HDFS容量不会增加，因为文件并未真正删除。HDFS不会回收空间，除非文件从回收站中删除，只有在检查点过期后才会发生。 回收站功能默认只适用于使用Hadoop shell删除的文件和目录。使用其他接口(例如WebHDFS或Java API)以编程的方式删除的文件或目录不会移动到回收站，即使已启用回收站，除非程序已经实现了对回收站功能的调用。 有时你可能想要在删除文件时临时禁用回收站，也就是删除的文件或目录不用放在回收站而直接删除，在这种情况下，可以使用-skipTrash选项运行rm命令。例如：sudo -uxiaosi hadoop fs -rm -skipTrash tmp/data_group/test/employee/employee_salary.txt 这会绕过垃圾回收站并立即从文件系统中删除文件。 资料: https://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/ https://my.oschina.net/cloudcoder/blog/179381 http://debugo.com/hdfs-trash/ http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce中的InputSplit]]></title>
    <url>%2Fhadoop-mapreduce-inputsplit.html</url>
    <content type="text"><![CDATA[Hadoop的初学者经常会有这样两个问题： Hadoop的一个Block默认是128M(或者64M)，那么对于一条记录来说，会不会造成一条记录被分到两个Block中？ 从Block中读取数据进行切分时，会不会造成一条记录被分到两个InputSplit中？ 对于上面的两个问题，首先要明确两个概念：Block和InputSplit。在Hadoop中，文件由一个一个的记录组成，最终由mapper任务一个一个的处理。例如，示例数据集包含有关1987至2008年间美国境内已完成航班的信息。如果要下载数据集可以打开如下网址： http://stat-computing.org/dataexpo/2009/the-data.html 。每一年都会生成一个大文件（例如：2008年文件大小为108M），在每个文件中每单独的一行都代表一次航班信息。换句话说，一行代表一个记录。HDFS以固定大小的Block为基本单位存储数据，而对于MapReduce而言，其处理单位是InputSplit。 1. Block块是以block size进行划分数据。因此，如果集群的block size为128MB，则数据集的每个块将为128MB，除非最后一个块小于block size（文件大小不能被 block size 完全整除）。例如下图中文件大小为513MB，513%128=1，最后一个块e小于block size，大小为1MB。因此，块是以block size进行切割，并且块甚至可以在到逻辑记录结束之前结束(blocks can end even before a logical record ends)。 假设我们的集群中block size是128MB，每个逻辑记录大约100MB（假设为巨大的记录）。所以第一个记录将完全在一个块中，因为记录大小为100MB小于块大小128 MB。但是，第二个记录不能完全在一个块中，第二条记录将出现在两个块中，从块1开始，溢出到块2中。 2.InputSplit但是如果每个Map任务都处理特定数据块中的所有记录，那怎么处理这种跨越块边界的记录呢？如果分配一个Mapper给块1，在这种情况下，Mapper不能处理第二条记录，因为块1中没有完整的第二条记录。因为HDFS对文件块内部并不清楚，它不知道一个记录会什么时候可能溢出到另一个块(because HDFS has no conception of what’s inside the file blocks, it can’t gauge when a record might spill over into another block)。InputSplit就是解决这种跨越块边界记录问题的，Hadoop使用逻辑表示存储在文件块中的数据，称为输入拆分InputSplit。InputSplit是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等。它的划分方法完全取决于用户自己。但是需要注意的是InputSplit的多少决定了MapTask的数目，因为每个InputSplit会交由一个MapTask处理。 当MapReduce作业客户端计算InputSplit时，它会计算出块中第一个记录的开始位置和最后一个记录的结束位置。在最后一个记录不完整的情况下，InputSplit包括下一个块的位置信息和完成该记录所需的数据的字节偏移（In cases where the last record in a block is incomplete, the input split includes location information for the next block and the byte offset of the data needed to complete the record）。下图显示了数据块和InputSplit之间的关系： 块是磁盘中的数据存储的物理块，其中InputSplit不是物理数据块。它只是一个逻辑概念，并没有对实际文件进行切分，指向块中的开始和结束位置。因此，当Mapper尝试读取数据时，它清楚地知道从何处开始读取以及在哪里停止读取。InputSplit的开始位置可以在一个块中开始，在另一个块中结束。InputSplit代表了逻辑记录边界，在MapReduce执行期间，Hadoop扫描块并创建InputSplits，并且每个InputSplit将被分配给一个Mapper进行处理。 原文：http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/http://hadoopinrealworld.com/inputsplit-vs-block/ http://hadoopinrealworld.com/inputsplit-vs-block/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Reducer总是能复用为Combiner？]]></title>
    <url>%2Fhadoop-can-reducer-always-be-reused-for-combiner.html</url>
    <content type="text"><![CDATA[Combiner函数是一个可选的中间函数，发生在Map阶段，Mapper执行完成后立即执行。使用Combiner有如下两个优势： Combiner可以用来减少发送到Reducer的数据量，从而提高网络效率。 Combiner可以用于减少发送到Reducer的数据量，这将提高Reduce端的效率，因为每个reduce函数将处理相比于未使用Combiner之前更少的记录。 Combiner与Reducer结构相同，因为Combiner和Reducer都对Mapper的输出进行处理。这给了我们一个复用Reducer作为Combiner的好机会。但问题是，复用Reducer作为Combiner总是可行的吗？ 1. Reducer作为Combiner的适用场景假设我们正在编写一个MapReduce程序来计算股票数据集中每个股票代码的最大收盘价。Mapper将数据集中每个股票记录的股票代码作为key和收盘价作为value。Reducer然后将循环遍历股票代码对应的所有收盘价，并从收盘价列表中计算最高收盘价。假设Mapper 1处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设Mapper 2处理股票代码为ABC的2个记录，收盘价分别为100和31。那么Reducer将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有收盘价，并将计算最高收盘价为111。 我们可以在每个Mapper之后使用相同的Reducer作为Combiner。Mapper 1上的Combiner将处理3个收盘价格–50，60和111，并且仅输出111，因为它是3个收盘价的最大值。Mapper 2上的Combiner将处理2个收盘价格–100和31，并且仅输出100，因为它是2个收盘价的最大值。现在使用Combiner之后，Reducer仅处理股票代码ABC的2个收盘价(原先需要处理5个收盘价)，即来自Mapper 1的111和来自Mapper 2的100，并且将从这两个值中计算出最大收盘价格为111。 正如我们看到的，使用Combiner情况下Reducer输出与没有使用Combiner的输出结果是相同的，因此在这种情况下复用Reducer作为Combiner是没有问题。 2. Reducer作为Combiner的不适用场景假设我们正在编写一个MapReduce程序来计算股票数据集中每个股票代码的平均交易量（average volume for each symbol）。Mapper将数据集中每个股票记录的股票代码作为key和交易量（volume）作为value。Reducer然后将循环遍历股票代码对应的所有交易量，并从交易量列表中计算出平均交易量（average volume from the list of volumes for that symbol）。假设Mapper 1处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设Mapper 2处理股票代码为ABC的2个记录，收盘价分别为100和31。那么Reducer将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有交易量，并将计算出平均交易量为70.4。permalink: hadoop-setup-and-start50 + 60 + 111 + 100 + 31 / 5 = 352 / 5 = 70.4 让我们看看如果我们在每个Mapper之后复用Reducer作为Combiner会发生什么。Mapper 1上的Combiner将处理3个交易量–50，60和111，并计算出三个交易量的平均交易量为73.66。Mapper 2上的Combiner将处理2个交易量–100和31，并计算出两个交易量的平均交易量为65.5。那么在复用Reducer作为Combiner的情况下，Reducer仅处理股票代码ABC的2个平均交易量，来自Mapper1的73.66和来自Mapper2的65.5，并计算股票代码ABC最终的平均交易量为69.58。73.66 + 65.5 /2 = 69.58 这与我们不复用Reducer作为Combiner得出的结果不一样，因此复用Reducer作为Combiner得出平均交易量是不正确的。 所以我们可以看到Reducer不能总是被用于Combiner。所以，当你决定复用Reducer作为Combiner的时候，你需要问自己这样一个问题:使用Combiner与不使用Combiner的输出结果是否一样？ 3. 区别Combiner需要实现Reducer接口。Combiner只能用于特定情况。 与Reducer不同，Combiner有一个约束，Combiner输入/输出键和值类型必须与Mapper的输出键和值类型相匹配。而Reducer只是输入键和值类型与Mapper的输出键和值类型相匹配。 Combiner只能用于满足交换律（a.b = b.a）和结合律（a.(b.c)= (a.b).c）的情况。这也意味着Combiner可能只能用于键和值的一个子集或者可能不能使用。 Reducer可以从多个Mapper获取数据。Combiner只能从一个Mapper获取其输入。 原文：http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Partitioner使用教程]]></title>
    <url>%2Fhadoop-mapreduce-partitioner-usage.html</url>
    <content type="text"><![CDATA[partitioner在处理输入数据集时就像条件表达式(condition)一样工作。分区阶段发生在Map阶段之后，Reduce阶段之前。partitioner的个数等于reducer的个数(The number of partitioners is equal to the number of reducers)。这就意味着一个partitioner将根据reducer的个数来划分数据(That means a partitioner will divide the data according to the number of reducers)。因此，从一个单独partitioner传递过来的数据将会交由一个单独的reducer处理(the data passed from a single partitioner is processed by a single Reducer)。 1. Partitionerpartitioner对Map中间输出结果的键值对进行分区。使用用户自定义的分区条件来对数据进行分区，它的工作方式类似于hash函数。partitioner的总个数与作业的reducer任务的个数相同。下面我们以一个例子来说明partitioner是如何工作的。 2. MapReduce的Partitioner实现为了方便，假设我们有一个Employee表，数据如下。我们使用下面样例数据作为输入数据集来验证partitioner是如何工作的。 Id Name Age Gender Salary 1201 gopal 45 Male 50,000 1202 manisha 40 Female 50,000 1203 khalil 34 Male 30,000 1204 prasanth 30 Male 30,000 1205 kiran 20 Male 40,000 1206 laxmi 25 Female 35,000 1207 bhavya 20 Female 15,000 1208 reshma 19 Female 15,000 1209 kranthi 22 Male 22,000 1210 Satish 24 Male 25,000 1211 Krishna 25 Male 25,000 1212 Arshad 28 Male 20,000 1213 lavanya 18 Female 8,000 我们写一个程序来处理输入数据集，对年龄进行分组(例如：小于20，21-30，大于30)，并找到每个分组中的最高工资的员工。 2.1 输入数据以上数据存储在/home/xiaosi/tmp/partitionerExample/input/目录中的input.txt文件中，数据存储格式如下：1201 gopal 45 Male 500001202 manisha 40 Female 510001203 khaleel 34 Male 300001204 prasanth 30 Male 310001205 kiran 20 Male 400001206 laxmi 25 Female 350001207 bhavya 20 Female 150001208 reshma 19 Female 140001209 kranthi 22 Male 220001210 Satish 24 Male 250001211 Krishna 25 Male 260001212 Arshad 28 Male 200001213 lavanya 18 Female 8000 基于以上输入数据，下面是具体的算法描述。 2.2 Map任务Map任务以键值对作为输入，我们存储文本数据在text文件中。Map任务输入数据如下： 2.2.1 Inputkey以特殊key+文件名+行号的模式表示(例如，key = @input1)，value为一行中的数据(例如，value = 1201\tgopal\t45\tMale\t50000)。 2.2.2 Method读取一行中数据，使用split方法以\t进行分割，取出性别存储在变量中String[] str = value.toString().split("\t", -3);String gender = str[3]; 以性别为key，行记录数据为value作为输出键值对，从Map任务传递到Partition任务：context.write(new Text(gender), new Text(value)); 对text文件中的所有记录重复以上所有步骤。 2.2.3 Output得到性别与记录数据组成的键值对 2.3 Partition任务Partition任务接受来自Map任务的键值对作为输入。Partition意味着将数据分成几个片段。根据给定分区条件规则，基于年龄标准将输入键值对数据划分为三部分。 2.3.1 Input键值对集合中的所有数据。key为记录中性别字段值，value为该性别对应的完整记录数据。 2.3.2 Method从键值对数据中读取年龄字段值String[] str = value.toString().split("\t");int age = Integer.parseInt(str[2]); 根据如下条件校验age值：// age 小于等于20if (age &lt;= 20) &#123; return 0;&#125;// age 大于20 小于等于30else if (age &gt; 20 &amp;&amp; age &lt;= 30) &#123; return 1 % numReduceTask;&#125;// age 大于30else &#123; return 2 % numReduceTask;&#125; 2.3.3 Output键值对所有数据被分割成三个键值对集合。Reducer会处理每一个集合。 2.4 Reduce任务partitioner任务的数量等于reducer任务的数量。这里我们有三个partitioner任务，因此我们有三个reducer任务要执行。 2.4.1 InputReducer将使用不同的键值对集合执行三次。key为记录中性别字段值，value为该性别对应的完整记录数据。 2.4.2 Method读取记录数据中的Salary字段值：String[] str = value.toString().split("\t", -3);int salary = Integer.parseInt(str[4]); 获取salary最大值:if (salary &gt; max) &#123; max = salary;&#125; 对于每个key集合（Male与Female为两个key集合）中的数据重复以上步骤。执行完这三个步骤之后，我们将会分别从女性集合中得到一个最高工资，从男性集合中得到一个最高工资。context.write(new Text(key), new IntWritable(max)); 2.4.3 Output最后，我们将在不同年龄段的三个集合中获得一组键值对数据。它分别包含每个年龄段的男性集合的最高工资和每个年龄段的女性集合的最高工资。 执行Map，Partition和Reduce任务后，键值对数据的三个集合存储在三个不同的文件中作为输出。 所有这三项任务都被视为MapReduce作业。这些作业的以下要求和规范应在配置中指定： 作业名称 keys和values的输入输出格式 Map，Reduce和Partitioner任务的类 Configuration conf = getConf();//Create JobJob job = new Job(conf, "topsal");job.setJarByClass(PartitionerExample.class);// File Input and Output pathsFileInputFormat.setInputPaths(job, new Path(arg[0]));FileOutputFormat.setOutputPath(job,new Path(arg[1]));//Set Mapper class and Output format for key-value pair.job.setMapperClass(MapClass.class);job.setMapOutputKeyClass(Text.class);job.setMapOutputValueClass(Text.class);//set partitioner statementjob.setPartitionerClass(CaderPartitioner.class);//Set Reducer class and Input/Output format for key-value pair.job.setReducerClass(ReduceClass.class);//Number of Reducer tasks.job.setNumReduceTasks(3);//Input and Output format for datajob.setInputFormatClass(TextInputFormat.class);job.setOutputFormatClass(TextOutputFormat.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(Text.class); 3. Examplepackage com.sjf.open.test;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapred.JobPriority;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import com.sjf.open.utils.FileSystemUtil;public class PartitionerExample extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int status = ToolRunner.run(new PartitionerExample(), args); System.exit(status); &#125; private static class mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; try &#123; String[] str = value.toString().split("\t", -3); String gender = str[3]; context.write(new Text(gender), new Text(value)); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125; &#125; private static class reducer extends Reducer&lt;Text, Text, Text, IntWritable&gt; &#123; private int max = Integer.MIN_VALUE; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; String[] str = value.toString().split("\t", -3); int salary = Integer.parseInt(str[4]); if (salary &gt; max) &#123; max = salary; &#125; &#125; context.write(new Text(key), new IntWritable(max)); &#125; &#125; private static class partitioner extends Partitioner&lt;Text, Text&gt; &#123; @Override public int getPartition(Text key, Text value, int numReduceTask) &#123; System.out.println(key.toString() + "------" + value.toString()); String[] str = value.toString().split("\t"); int age = Integer.parseInt(str[2]); if (numReduceTask == 0) &#123; return 0; &#125; if (age &lt;= 20) &#123; return 0; &#125; else if (age &gt; 20 &amp;&amp; age &lt;= 30) &#123; return 1 % numReduceTask; &#125; else &#123; return 2 % numReduceTask; &#125; &#125; &#125; @Override public int run(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("./run &lt;input&gt; &lt;output&gt;"); System.exit(1); &#125; String inputPath = args[0]; String outputPath = args[1]; int numReduceTasks = 3; Configuration conf = this.getConf(); conf.set("mapred.job.queue.name", "test"); conf.set("mapreduce.map.memory.mb", "1024"); conf.set("mapreduce.reduce.memory.mb", "1024"); conf.setBoolean("mapred.output.compress", true); conf.setClass("mapred.output.compression.codec", GzipCodec.class, CompressionCodec.class); Job job = Job.getInstance(conf); job.setJarByClass(PartitionerExample.class); job.setPartitionerClass(partitioner.class); job.setMapperClass(mapper.class); job.setReducerClass(reducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileSystem fileSystem = FileSystem.get(conf); fileSystem.delete(new Path(outputPath), true); FileSystemUtil.filterNoExistsFile(conf, job, inputPath); FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.setNumReduceTasks(numReduceTasks); boolean success = job.waitForCompletion(true); return success ? 0 : 1; &#125;&#125; 4. 集群上执行运行结果:17/01/03 20:22:02 INFO mapreduce.Job: Running job: job_1472052053889_705919817/01/03 20:22:21 INFO mapreduce.Job: Job job_1472052053889_7059198 running in uber mode : false17/01/03 20:22:21 INFO mapreduce.Job: map 0% reduce 0%17/01/03 20:22:37 INFO mapreduce.Job: map 100% reduce 0%17/01/03 20:22:55 INFO mapreduce.Job: map 100% reduce 100%17/01/03 20:22:55 INFO mapreduce.Job: Job job_1472052053889_7059198 completed successfully17/01/03 20:22:56 INFO mapreduce.Job: Counters: 43 File System Counters FILE: Number of bytes read=470 FILE: Number of bytes written=346003 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=485 HDFS: Number of bytes written=109 HDFS: Number of read operations=12 HDFS: Number of large read operations=0 HDFS: Number of write operations=6 Job Counters Launched map tasks=1 Launched reduce tasks=3 Rack-local map tasks=1 Total time spent by all maps in occupied slots (ms)=5559 Total time spent by all reduces in occupied slots (ms)=164768 Map-Reduce Framework Map input records=13 Map output records=13 Map output bytes=426 Map output materialized bytes=470 Input split bytes=134 Combine input records=0 Combine output records=0 Reduce input groups=6 Reduce shuffle bytes=470 Reduce input records=13 Reduce output records=6 Spilled Records=26 Shuffled Maps =3 Failed Shuffles=0 Merged Map outputs=3 GC time elapsed (ms)=31 CPU time spent (ms)=2740 Physical memory (bytes) snapshot=1349193728 Virtual memory (bytes) snapshot=29673148416 Total committed heap usage (bytes)=6888620032 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=351 File Output Format Counters Bytes Written=109 原文:https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫使用Requests获取网页文本内容中文乱码]]></title>
    <url>%2FPython%2F%5BPython%5D%20%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8Requests%E8%8E%B7%E5%8F%96%E7%BD%91%E9%A1%B5%E6%96%87%E6%9C%AC%E5%86%85%E5%AE%B9%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.html</url>
    <content type="text"><![CDATA[1. 问题使用Requests去获取网页文本内容时，输出的中文出现乱码。 2. 乱码原因爬取的网页编码与我们爬取编码方式不一致造成的。如果爬取的网页编码方式为utf8，而我们爬取后程序使用ISO-8859-1编码方式进行编码并输出，这会引起乱码。如果我们爬取后程序改用utf8编码方式，就不会造成乱码。 3. 乱码解决方案3.1 Content-Type我们首先确定爬取的网页编码方式，编码方式往往可以从HTTP头(header)的Content-Type得出。 Content-Type，内容类型，一般是指网页中存在的Content-Type，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件，这就是经常看到一些Asp网页点击的结果却是下载到的一个文件或一张图片的原因。如果未指定ContentType，默认为TEXT/HTML。charset决定了网页的编码方式，一般为gb2312、utf-8等 HTML语法格式:&lt;meta content=&quot;text/html; charset=utf-8&quot; http-equiv=&quot;Content-Type&quot;/&gt; station_request = requests.get("http://blog.csdn.net/sunnyyoona")content_type = station_request.headers['content-type']print content_type # text/html; charset=utf-8 3.2 chardet如果上述方式没有编码信息，一般可以采用chardet等第三方网页编码智能识别工具识别:pip install chardet 使用chardet可以很方便的实现文本内容的编码检测。虽然HTML页面有charset标签，但是有些时候并不准确，这时候我们可以使用chardet来进一步的判断:raw_data = urllib.urlopen('http://blog.csdn.net/sunnyyoona').read()print chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;raw_data = urllib.urlopen('http://www.jb51.net').read()print chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'GB2312'&#125; 函数返回值为字典，有2个元素，一个是检测的可信度，另外一个就是检测到的编码。 3.3 猜测编码当你收到一个响应时，Requests会猜测响应(response)的编码方式，用于在你调用Response.text方法时，对响应进行解码。Requests首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 charadet来尝试猜测编码方式。 只有当HTTP头部不存在明确指定的字符集，并且Content-Type头部字段包含text值之时，Requests才不去猜测编码方式。在这种情况下， RFC 2616指定默认字符集必须是ISO-8859-1。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置Response.encoding属性，或使用原始的Response.content。 # 一等火车站url = "https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99"headers = &#123;'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'&#125;r = requests.get(url, headers=headers)print r.headers['Content-Type'] # text/html# 猜测的编码方式print r.encoding # ISO-8859-1print r.text # 出现乱码raw_data = urllib.urlopen(url).read()print chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125; 如上所述，只有当HTTP头部不存在明确指定的字符集，并且Content-Type头部字段包含text值之时，Requests才不去猜测编码方式。直接使用ISO-8859-1编码方式。而使用chardet检测结果来看，网页编码方式与猜测的编码方式不一致，这就造成了结果输出的乱码。 3.4 解决你可以使用r.encoding = xxx来更改编码方式，这样Requests将在你调用r.text时使用r.encoding的新值，使用新的编码方式。下面示例使用chardet检测的编码方式解码网页:# 一等火车站url = "https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99"headers = &#123;'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'&#125;r = requests.get(url, headers=headers)# 检测编码raw_data = urllib.urlopen(url).read()charset = chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;encoding = charset['encoding']# 更改编码方式r.encoding = encodingprint r.text # 未出现乱码 参考:http://docs.python-requests.org/en/latest/user/quickstart/#response-contenthttp://blog.csdn.net/a491057947/article/details/47292923https://www.cnblogs.com/GUIDAO/p/6679574.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法一 快速排序]]></title>
    <url>%2Falgorithm-quick-sort.html</url>
    <content type="text"><![CDATA[1. 分析 2. 伪代码 3. 思路图 4. 运行过程 5. 代码5.1 C++版本/********************************** 日期：2014-04-01* 作者：SJF0115* 题目：快速排序**********************************/#include &lt;iostream&gt;#include &lt;stdio.h&gt;using namespace std;//对子数组array[p...r]就地重排int Partition(int array[],int p,int r)&#123; int j,temp; //定义哨兵 int x = array[r]; //i为小于哨兵元素的最后一个元素下标 int i = p - 1; //j为待排序元素的第一个元素 for(j = p;j &lt; r;j++)&#123; //跟哨兵比较 if(array[j] &lt; x)&#123; i++; //交换array[i] array[j] temp = array[j]; array[j] = array[i]; array[i] = temp; &#125; &#125; //交换array[i+1](大于哨兵元素的第一个元素) array[r] temp = array[i+1]; array[i+1] = array[r]; array[r] = temp; //返回分割下标 return i + 1;&#125;//快排void QuickSort(int array[],int p,int r)&#123; if(p &gt;= r || array == NULL)&#123; return; &#125; int index = Partition(array,p,r); QuickSort(array,p,index-1); QuickSort(array,index+1,r);&#125;int main()&#123; int array[] = &#123;2,8,7,1,3,5,6,4&#125;; QuickSort(array,0,7); for(int i = 0;i &lt;= 7;i++)&#123; printf("%d\n",array[i]); &#125;&#125; 5.2 Java版本package com.sjf.open;/** 快速排序 * @author sjf0115 * @Date Created in 下午5:24 18-3-27 */public class QuickSort &#123; /** * 分割点 * @param array * @param start * @param end * @return */ int partition(int array[], int start, int end)&#123; int x = array[end]; int i = start - 1; int tmp; for(int j = start;j &lt; end;j++)&#123; if(array[j] &lt; x)&#123; i++; tmp = array[j]; array[j] = array[i]; array[i] = tmp; &#125; &#125; tmp = array[i+1]; array[i+1] = array[end]; array[end] = tmp; return i+1; &#125; /** * 快速排序 * @param array * @param start * @param end */ void quickSort(int array[], int start, int end)&#123; if(start &gt; end || array == null)&#123; return; &#125; int index = partition(array, start, end); quickSort(array, start, index-1); quickSort(array, index+1, end); &#125; public static void main(String[] args) &#123; QuickSort quickSort = new QuickSort(); int array[] = &#123;4,1,6,3,9,0&#125;; quickSort.quickSort(array, 0, 5); for(int num : array)&#123; System.out.println(num); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github搭建博客更换皮肤]]></title>
    <url>%2FHexo%2FHexo%2BGithub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%8D%A2%E7%9A%AE%E8%82%A4.html</url>
    <content type="text"><![CDATA[1. 更换皮肤主题地址: https://github.com/haojen/hexo-theme-Anisina 2. 添加Disqus评论系统Anisina主题支持Disqus和多说评论，要想使用这两者，需要对其进行使用配置 (1) 首先，你需要注册其中任何一个评论系统的帐号，在这里我们使用Disqus评论系统:https://disqus.com (2) 在Disqus设置页面中点 Add Disqus to your site 添加你的网站地址, 和设置Choose your unique Disqus URL, 这一栏填写的就是hexo中所使用的short_name 3. Tagsxiaosi@yoona:~/qunar/study/hexo-blog$ hexo new page &quot;Tags&quot;INFO Created: /media/xiaosi/司吉峰/study/hexo-blog/source/Tags/index.md]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github为NexT主题添加文章阅读量统计功能]]></title>
    <url>%2FHexo%2F%5BHexo%5DHexo%2BGithub%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html</url>
    <content type="text"><![CDATA[在注册完成 LeanCloud 帐号并验证邮箱之后，我们就可以登录我们的 LeanCloud 帐号，进行一番配置之后拿到 AppID 以及 AppKey 这两个参数即可正常使用文章阅读量统计的功能了。 1. 创建应用(1) 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示： (2) 在出现的界面点击创建应用： (3) 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的: (4) 创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置： (5) 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建 Class。在弹出的选项中选择创建 Class 来新建 Class 用来专门保存我们博客的文章访问量等数据。 备注:点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter。 由于 LeanCloud 升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。 (6) 创建完成之后，左侧数据栏应该会多出一栏名为 Counter 的栏目，这个时候我们点击左侧的设置，切换到我们创建的应用 smartsi 应用的操作界面。 在弹出的界面中，选择左侧的 应用Key 选项，即可发现我们创建应用的 AppID 以及 AppKey，有了它，我们就有权限能够通过主题中配置好的 Javascript 代码与这个应用的 Counter表进行数据存取操作了: 复制 AppID 以及 AppKey 并在 NexT 主题的 _config.yml 文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:leancloud_visitors: enable: true app_id: 你的app_id app_key: 你的app_key 2. 后台管理当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们 LeanCloud 对应的应用的 Counter 表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的 Counter 表中： 我们可以修改其中的 time 字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。 url 字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。 title 字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。 其他字段皆为自动生成，具体作用请查阅 LeanCloud 官方文档，如果你不知道有什么作用请不要随意修改。 3. Web安全因为AppID以及AppKey是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。 选择应用的设置的安全中心选项卡,在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全: 如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出: 这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。 原文:https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github配置与主题]]></title>
    <url>%2FHexo%2F%5BHexo%5DHexo%2BGithub%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98.html</url>
    <content type="text"><![CDATA[1. 基本配置1.1 语言设置每个主题都会配置几种界面显示语言，修改语言只要编辑站点配置文件，找到 language 字段，并将其值更改为你所需要的语言(例如，简体中文)：language: zh-Hans 1.2 网站标题，作者打开站点配置文件，修改这些值：title: SmartSi #博客标题subtitle: #博客副标题description: #博客描述author: sjf0115 #博客作者 注意：配置文件要符合英文标点符号使用规范: 冒号后必须空格，否则会编译错误 1.3 域名与文章链接# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://sjf0115.github.io #你的博客网址root: / #博客跟目录，如果你的博客在网址的二级目录下，在这里填上permalink: :year/:month/:day/:title/ # 文章链接permalink_defaults: 2. 安装与启用主题最简单的安装方法是克隆整个仓库，在这里我们使用的是NexT主题：cd hexogit clone https://github.com/theme-next/hexo-theme-next themes/next 或者你可以看到其他详细的安装说明 安装后，我们要启用我们安装的主题，与所有Hexo主题启用的模式一样。 当克隆/下载完成后，打开站点配置文件， 找到 theme 字段，并将其值更改为 next 。theme: next 3. 主题风格NexT 主题目前提供了3中风格类似，但是又有点不同的主题风格，可以通过修改 主题配置文件 中的 Scheme 值来启用其中一种风格，例如我的博客用的是 Mist 风格，只要把另外两个用#注释掉即可:# Schemes#scheme: Musescheme: Mist#scheme: Pisces 4. 设置 RSSNexT 中 RSS 有三个设置选项，满足特定的使用场景。 更改 主题配置文件，设定 rss 字段的值： false：禁用 RSS，不在页面上显示 RSS 连接。 留空：使用 Hexo 生成的 Feed 链接。 你可以需要先安装 hexo-generator-feed 插件。 具体的链接地址：适用于已经烧制过 Feed 的情形。 5. 导航栏添加标签菜单新建标签页面，并在菜单中显示标签链接。标签页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。 (1) 在终端窗口下，定位到 Hexo 站点目录下。使用如下命令新建一名为 tags 页面:hexo new page &quot;tags&quot; (2) 编辑刚新建的页面，将页面的类型设置为 tags ，主题将自动为这个页面显示标签云。页面内容如下：title: 标签date: 2017-12-22 12:39:04type: &quot;tags&quot; (3) 在菜单中添加链接。编辑 主题配置文件 ，添加 tags 到 menu 中，如下:menu: home: / archives: /archives tags: /tags (4) 使用时在你的文章中添加如下代码：---title: title namedate: 2017-12-12-22 12:39:04tags: - first tag - second tag--- 6. 添加分类页面新建分类页面，并在菜单中显示分类链接。分类页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。 (1) 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 categories ：hexo new page categories (2) 编辑刚新建的页面，将页面的 type 设置为 categories ，主题将自动为这个页面显示分类。页面内容如下：---title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;--- (3) 在菜单中添加链接。编辑 主题配置文件 ， 添加 categories 到 menu 中，如下:menu: home: / archives: /archives categories: /categories (4) 使用时在你的文章中添加如下代码：---title: title namedate: 2017-12-12-22 12:39:04type: &quot;categories&quot;--- 7. 侧边栏社交链接侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件 中。 (1) 链接放置在 social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址 || 图标：social: GitHub: https://github.com/sjf0115 || github E-Mail: mailto:1203745031@qq.com || envelope CSDN: http://blog.csdn.net/sunnyyoona 备注:如果没有指定图标（带或不带分隔符），则会加载默认图标。 Example: (2) 设定链接的图标，对应的字段是 social_icons。其键值格式是: 匹配键: Font Awesome 图标名称， 匹配键与上一步所配置的链接的显示文本相同（大小写严格匹配），图标名称是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，你可以设置成 false 来去掉图标:# Social Iconssocial_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 微博: weibo 8. 友情链接编辑 主题配置文件 添加：友情链接配置示例# Blog rollslinks_icon: linklinks_title: Linkslinks_layout: block#links_layout: inlinelinks: CSDN: http://blog.csdn.net/sunnyyoona Example: 9. 站点建立时间这个时间将在站点的底部显示，例如 © 2017 - 2018。 编辑 主题配置文件，新增字段 since:配置示例since: 2017 10. 腾讯公益404页面腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 http://smartsi.club/404.html (1) 使用方法，新建 404.html 页面，放到主题的 source 目录下，内容如下：&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; (2) 开启404页面功能 在 menu 下添加 commonweal: /404/ || heartbeat：menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat 11. 开启打赏功能越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 主题配置文件 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能：打赏功能配置示例reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 12. 订阅微信公众号备注:此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。 在微信公众号平台下载您的二维码，并将它存放于博客source/uploads/目录下。 然后编辑 主题配置文件，如下：配置示例# Wechat Subscriberwechat_subscriber: enabled: true qcode: /uploads/wechat-qcode.jpg description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！ 13. 设置背景动画NexT 自带两种背景动画效果，编辑 主题配置文件， 搜索 canvas_nest 或 three_waves，根据你的需求设置值为 true 或者 false 即可： 备注:three_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。 canvas_nest 配置示例# canvas_nestcanvas_nest: true //开启动画canvas_nest: false //关闭动画 three_waves 配置示例# three_wavesthree_waves: true //开启动画three_waves: false //关闭动画 14. 设置阅读全文在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法：(1) 在文章中使用 &lt;!-- more --&gt; 手动进行截断，Hexo 提供的方式 推荐(2) 在文章的 front-matter 中添加 description，并提供文章摘录(3) 自动形成摘要，在 主题配置文件 中添加：auto_excerpt: enable: true length: 150 默认截取的长度为 150 字符，可以根据需要自行设定 建议使用 &lt;!-- more --&gt;（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 Hexo 中的插件更好的识别。 15. 站内搜索NexT 支持集成 Swiftype、 微搜索、Local Search 和 Algolia。在这里我使用的是Local Search，下面将介绍如何使用: (1) 添加百度/谷歌/本地 自定义站点内容搜索，安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：npm install hexo-generator-searchdb --save (2) 编辑 站点配置文件，新增以下内容到任意位置：search: path: search.xml field: post format: html limit: 10000 (3) 编辑 主题配置文件，启用本地搜索功能：# Local searchlocal_search: enable: true 其他搜索方式请查看搜索服务 16. 不蒜子统计备注：此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 (1) 全局配置。编辑 主题配置文件 中的 busuanzi_count 的配置项。当 enable: true 时，代表开启全局开关。若 site_uv 、site_pv 、 page_pv 的值均为 false 时，不蒜子仅作记录而不会在页面上显示。 (2) 站点UV配置。当 site_uv: true 时，代表在页面底部显示站点的UV值。site_uv_header 和 site_uv_footer 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为 [site_uv_header]UV值[site_uv_footer]。# 效果：本站访客数12345人次site_uv: truesite_uv_header: 本站访客数site_uv_footer: 人次 (3) 站点PV配置。当 site_pv: true 时，代表在页面底部显示站点的PV值。site_pv_header 和 site_pv_footer 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为 [site_pv_header]PV值[site_pv_footer]。# 效果：本站总访问量12345次site_pv: truesite_pv_header: 本站总访问量site_pv_footer: 次 (4) Example:busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 本站访客数 site_uv_footer: 次 # custom pv span for the whole site site_pv: true site_pv_header: 本站总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: false page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 17. 开启about自我介绍页面(1) 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 about ：cd your-hexo-sitehexo new page abhout (2) 编辑刚新建的页面，将页面的类型设置为 about。页面内容如下：---title: aboutdate: 2014-12-22 12:39:04type: &quot;about&quot;--- (3) 在菜单中添加链接。编辑 主题配置文件 ， 添加 about 到 menu 中，如下:menu: home: / || home about: /about/ || user 参考: http://theme-next.iissnan.com/theme-settings.html http://theme-next.iissnan.com/third-party-services.html]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 搭建静态博客]]></title>
    <url>%2Fhexo_generate_blog.html</url>
    <content type="text"><![CDATA[1.简介Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章（经常玩CSDN上的人都知道），在几秒内，即可利用靓丽的主题生成静态网页。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，在这里我是基于github搭建的个人博客站点。 2. 环境配置安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js Git 2.1 GitGit安装参考博文：http://blog.csdn.net/sunnyyoona/article/details/51453880 2.2 Node.js安装 Node.js 的最佳方式是使用 nvm:wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh 安装完成后，重启终端并执行下列命令即可安装 Node.js:nvm install 4 2.3 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。一般情况下我们机器上没有安装npm，首先要安装npm：sudo apt-get install npm 下面使用npm安装Hexo，安装过程中我们可能会遇到下面的问题: 我们需要运行下面的命令，才能安装成功： 再重新安装hexo: 3. 建站3.1 目录和文件安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。hexo init blog cd blognpm install 备注:在我这我初始化的目录名称为blog 新建完成后，指定目录blog文件如下：xiaosi@yoona:~/blog$ tree -L 2.├── _config.yml├── package.json├── scaffolds│ ├── draft.md│ ├── page.md│ └── post.md├── source│ └── _posts└── themes └── landscape5 directories, 5 files 文件 说明 scaffolds 脚手架，也就是一个工具模板 source 存放博客正文内容 _posts 文件箱 themes 存放皮肤的目录 themes/landscape 默认的皮肤 _config.yml 全局的配置文件 备注：我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。_posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。 3.2 全局配置_config.yml配置信息：（网站的配置信息，可以在此配置大部分的参数） 配置 说明 站点信息 定义标题，作者，语言 URL URL访问路径 文件目录 正文的存储目录 写博客配置 文章标题，文章类型，外部链接等 目录和标签 默认分类，分类图，标签图 归档设置 归档的类型 服务器设置 IP，访问端口，日志输出 时间和日期格式 时间显示格式，日期显示格式 分页设置 每页显示数量 评论 外挂的Disqus评论系统 插件和皮肤 换皮肤，安装插件 Markdown语言 markdown的标准 CSS的stylus格式 是否允许压缩 部署配置 github发布项目地址 配置_config.yml：# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# 站点信息title: Yoonasubtitle:description: Stay Hungry Stay Foolishauthor: sjf0115language:timezone:# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://sjf0115.club/root: /permalink: :year/:month/:day/:title/permalink_defaults:# Directory 文件目录source_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writing 写博客配置new_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Home page setting# path: Root path for your blogs index page. (default = &apos;&apos;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &apos;&apos; per_page: 10 order_by: -date# Category &amp; Tag 目录和标签default_category: uncategorizedcategory_map:tag_map:# Date / Time format 时间和日期格式## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination 分页设置## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions 插件与皮肤## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape# Deployment 部署配置## Docs: https://hexo.io/docs/deployment.htmldeploy: type:git repo:git@github.com:sjf0115/hexo-blog.git 3.3 创建新文章接下来，我们开始新博客了，创建第一博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。 下面我们创建一篇名为hexo的文章： 在_post目录下，就会生成文件：”hexo.md”:xiaosi@yoona:~/blog/source/_posts$ ll总用量 5drwxrwxrwx 1 xiaosi xiaosi 256 12月 1 10:17 ./drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 09:59 ../-rwxrwxrwx 1 xiaosi xiaosi 826 12月 1 09:59 hello-wor 然后，我们编辑文件：hexo.md，以markdown语法写文章，然后保存。在命令行，启动服务器进行保存:xiaosi@yoona:~/blog/source/posts$ hexo sNative thread-sleep not available.This will result in much slower performance, but it will still work.You should re-install spawn-sync or upgrade to the lastest version of node if possible.Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more detailsINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 通过浏览器打开， http://localhost:4000/ ，就出现了我们新写的文章。 4. 发布项目到github4.1 静态化处理写完文章之后，可以发布到github上面。hexo是一个静态博客框架。静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的：xiaosi@yoona:~/blog$ hexo generateNative thread-sleep not available.This will result in much slower performance, but it will still work.You should re-install spawn-sync or upgrade to the lastest version of node if possible.Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more detailsINFO Start processingINFO Files loaded in 143 msINFO Generated: index.htmlINFO Generated: archives/index.htmlINFO Generated: archives/2016/index.htmlINFO Generated: categories/diary/index.htmlINFO Generated: archives/2016/05/index.htmlINFO Generated: fancybox/blank.gifINFO Generated: archives/2017/12/index.htmlINFO Generated: fancybox/fancybox_loading.gifINFO Generated: fancybox/fancybox_overlay.pngINFO Generated: fancybox/fancybox_sprite@2x.pngINFO Generated: archives/2017/index.htmlINFO Generated: tags/hexo/index.htmlINFO Generated: fancybox/fancybox_sprite.pngINFO Generated: js/script.jsINFO Generated: fancybox/jquery.fancybox.cssINFO Generated: css/style.cssINFO Generated: fancybox/jquery.fancybox.pack.jsINFO Generated: fancybox/helpers/jquery.fancybox-buttons.jsINFO Generated: fancybox/helpers/jquery.fancybox-media.jsINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.cssINFO Generated: fancybox/helpers/jquery.fancybox-buttons.cssINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.jsINFO Generated: css/fonts/fontawesome-webfont.woffINFO Generated: css/fonts/fontawesome-webfont.eotINFO Generated: css/fonts/FontAwesome.otfINFO Generated: fancybox/helpers/fancybox_buttons.pngINFO Generated: fancybox/fancybox_loading@2x.gifINFO Generated: fancybox/jquery.fancybox.jsINFO Generated: 2017/12/01/hello-world/index.htmlINFO Generated: css/fonts/fontawesome-webfont.ttfINFO Generated: 2016/05/17/hexo/index.htmlINFO Generated: css/fonts/fontawesome-webfont.svgINFO Generated: css/images/banner.jpgINFO 33 files generated in 1.19 s 在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件：xiaosi@yoona:~/blog/public$ ll总用量 24drwxrwxrwx 1 xiaosi xiaosi 4096 12月 1 10:26 ./drwxrwxrwx 1 xiaosi xiaosi 4096 12月 1 10:24 ../drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 2017/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 archives/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 categories/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 css/drwxrwxrwx 1 xiaosi xiaosi 4096 12月 1 10:24 fancybox/-rwxrwxrwx 1 xiaosi xiaosi 9841 12月 1 10:24 index.html*drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 js/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 tags/ 4.2 发布到github接下来，我们把这个博客发布到github。 在github中创建一个项目hexo-blog，项目地址：https://github.com/sjf0115/hexo-blog 编辑全局配置文件：_config.yml，找到deploy的部分，设置github的项目地址：# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:sjf0115/hexo-blog.git 然后，通过如下命令进行部署: 出现上述问题，可以使用配置ssh秘钥解决。如果出现deployer找不到git: ERROR Deployer not found: git错误，使用下面方式解决： 再来一次hexo deploy： 到目前为止这个静态的web网站就被部署到了github，检查一下分支是gh-pages。gh-pages是github为了web项目特别设置的分支: 然后，点击”Settings”，找到GitHub Pages，提示“Your site is published at http://sjf0115.github.io/hexo-blog: 打开网页，就是我们刚刚发布站点： 可以看到网页样式出现问题，不用担心，我们设置域名之后就OK了。 4.3 设置域名 在dnspod控制台，设置主机记录@，类型A，到IP 23.235.37.133（github地址）: 对域名判断是否生效，对域名执行ping： 在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名sjf0115.club。通过浏览器，访问http://sjf0115.club ， 就打开了我们建好的博客站点:]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Map Join 原理]]></title>
    <url>%2Fhive-optimization-how-to-use-map-join.html</url>
    <content type="text"><![CDATA[1. Join如何运行首先，让我们讨论一下 Join 如何在Hive中运行。Common Join 操作如图1所示被编译为 MapReduce 任务。Common Join 任务涉及 Map 阶段和 Reduce 阶段。Mapper 从连接表中读取数据并将连接的 key 和连接的 value 键值对输出到中间文件中。Hadoop 在所谓的 shuffle 阶段对这些键值对进行排序和合并。Reducer 将排序结果作为输入，并进行实Join。Shuffle 阶段代价非常昂贵，因为它需要排序和合并。减少 Shuffle 和 Reduce 阶段的代价可以提高任务性能。 Map Join 的目的是减少 Shuffle 和 Reducer 阶段的代价，并仅在 Map 阶段进行 Join。通过这样做，当其中一个连接表足够小可以装进内存时，所有 Mapper 都可以将数据保存在内存中并完成 Join。因此，所有 Join 操作都可以在 Mapper 阶段完成。但是，这种类型的 Map Join 存在一些扩展问题。当成千上万个 Mapper 同时从 HDFS 将小的连接表读入内存时，连接表很容易成为性能瓶颈，导致 Mapper 在读取操作期间超时。 2. 使用分布式缓存Hive-1641 解决了这个扩展问题。优化的基本思想是在原始 Join 的 MapReduce 任务之前创建一个新的 MapReduce 本地任务。这个新任务是将小表数据从 HDFS 上读取到内存中的哈希表中。读完后，将内存中的哈希表序列化为哈希表文件。在下一阶段，当 MapReduce 任务启动时，会将这个哈希表文件上传到 Hadoop 分布式缓存中，该缓存会将这些文件发送到每个 Mapper 的本地磁盘上。因此，所有 Mapper 都可以将此持久化的哈希表文件加载回内存，并像之前一样进行 Join。优化的 Map Join 的执行流程如下图所示。优化后，小表只需要读取一次。此外，如果多个 Mapper 在同一台机器上运行，则分布式缓存只需将哈希表文件的一个副本发送到这台机器上。 由于 Map Join 比 Common Join 更快，因此最好尽可能运行 Map Join。以前，Hive用户需要在查询中给出提示来指定哪一个是小表。例如：SELECT /*+MAPJOIN（a）*/ FROM src1 x JOIN src2 y ON x.key = y.key;。 这种方式用户体验不好，因为有时用户可能会提供错误的提示或者根本不提供任何提示。在没有用户提示的情况下将 Common Join 转换为 Map Join 用户体验会更好。 3. 根据文件大小将Join转换为MapJoinHive-1642 通过自动将 Common Join 转换为 Map Join 来解决此问题。对于 Map Join，查询处理器应该知道哪个输入表是大表。其他输入表在执行阶段被识别为小表，并将这些表保存在内存中。然而，查询处理器在编译时不知道输入文件大小，因为一些表可能是从子查询生成的中间表。因此查询处理器只能在执行期间计算出输入文件的大小。 如上图所示，左侧流程显示了先前的 Common Join 执行流程，这非常简单。另一方面，右侧流程是新的 Common Join 执行流程。在编译期间，查询处理器生成一个包含任务列表的 Conditional Task。在执行期间运行其中一个任务。首先，应将原始的 Common Join 任务放入任务列表中。然后，查询处理器通过假设每个输入表可能是大表来生成一系列的 Map Join 任务。例如，select * from src1 x join src2 y on x.key=y.key。因为表 src2 和 src1 都可以是大表，所以处理器生成两个 Map Join 任务，其中一个假设 src1 是大表，另一个假设 src2 是大表。 在执行阶段，Conditional Task 知道每个输入表的确切文件大小，即使该表是中间表。如果所有表都太大而无法转换为 Map Join，那么只能像以前一样运行 Common Join 任务。如果其中一个表很大而其他表足够小可以运行 Map Join，则将 Conditional Task 选择相应 Map Join 本地任务来运行。通过这种机制，可以自动和动态地将 Common Join 转换为 Map Join。 目前，如果小表的总大小大于25MB，Conditional Task 会选择原始 Common Join 来运行。25MB是一个非常保守的数字，你可以使用 set hive.smalltable.filesize 来修改。 4. Example我们具体看一个 Map Join:SELECT active.md5KeyId, active.active_time, click.click_timeFROM( SELECT md5KeyId, active_time FROM adv_push_active WHERE month = '201711' AND substr(activeTime, 1, 10) = '2017-11-02') activeJOIN( SELECT md5KeyId, click_time FROM adv_push_click WHERE dt = '20171102') clickON upper(active.md5KeyId) = upper(click.md5KeyId); 输出信息如下：2017-11-02 17:32:55 Starting to launch local task to process map join; maximum memory = 5148508162017-11-02 17:33:01 Dump the side-table for tag: 0 with group count: 162 into file: file:/tmp/smartsi/xxx/hive_2017-11-02_17-32-16_498_230990327610539360-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile70--.hashtable2017-11-02 17:33:01 Uploaded 1 File to: file:/tmp/smartsi/xxx/hive_2017-11-02_17-32-16_498_230990327610539360-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile70--.hashtable (17191 bytes)2017-11-02 17:33:01 End of local task; Time Taken: 5.852 sec.Execution completed successfullyMapredLocal task succeededLaunching Job 1 out of 1Number of reduce tasks is set to 0 since there no reduce operator...Hadoop job information for Stage-3: number of mappers: 789; number of reducers: 02017-11-02 17:33:13,789 Stage-3 map = 0%, reduce = 0%2017-11-02 17:34:14,076 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 3229.1 sec 我们可以看到在原始 Join 的 MapReduce 任务之前创建了一个 MapReduce Local Task。这个新任务是将小表数据从 HDFS 上读取到内存中的哈希表中，并列化为哈希表文件。后面会将这个哈希表文件上传到 Hadoop 分布式缓存中。该缓存会将这些文件发送到每个 Mapper 的本地磁盘上。这些完成之后才会启动一个只有 Map Task 的 MapReduce 作业来完成 Join。 原文：https://www.facebook.com/note.php?note_id=470667928919]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hive 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 多文件输出MultipleOutputFormat]]></title>
    <url>%2Fhadoop-base-multiple-output-format.html</url>
    <content type="text"><![CDATA[FileOutputFormat 及其子类产生的文件放在输出目录下。每个 reducer 一个文件并且文件由分区号命名：part-r-00000，part-r-00001，等等。有时可能要对输出的文件名进行控制或让每个 reducer 输出多个文件。MapReduce 为此提供了 MultipleOutputFormat 类。 MultipleOutputFormat 类可以将数据写到多个文件，这些文件的名称源于输出的键和值或者任意字符串。这允许每个 reducer（或者只有 map 作业的 mapper）创建多个文件。采用 name-r-nnnnn 形式的文件名用于 reduce 输出，其中 name 是由程序设定的任意名字，nnnnn 是一个指名块号的整数（从0开始）。块号保证从不同块（mapper 或者 reducer）写的输出在相同名字情况下不会冲突。 1. 重定义输出文件名我们可以对输出的文件名进行控制。考虑这样一个需求：按男女性别来区分度假订单数据。这需要运行一个作业，作业的输出是男女各一个文件，此文件包含男女性别的所有数据记录。 这个需求可以使用 MultipleOutputs 来实现： package com.sjf.open.test;import java.io.IOException;import org.apache.commons.lang3.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapred.JobPriority;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import com.sjf.open.utils.ConfigUtil;/** * Created by xiaosi on 16-11-7. */public class VacationOrderBySex extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int status = ToolRunner.run(new VacationOrderBySex(), args); System.exit(status); &#125; public static class VacationOrderBySexMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; public String fInputPath = ""; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; super.setup(context); fInputPath = ((FileSplit) context.getInputSplit()).getPath().toString(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); if(fInputPath.contains("vacation_hot_order"))&#123; String[] params = line.split("\t"); String sex = params[2]; if(StringUtils.isBlank(sex))&#123; return; &#125; context.write(new Text(sex.toLowerCase()), value); &#125; &#125; &#125; public static class VacationOrderBySexReducer extends Reducer&lt;Text, Text, NullWritable, Text&gt; &#123; private MultipleOutputs&lt;NullWritable, Text&gt; multipleOutputs; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; multipleOutputs = new MultipleOutputs&lt;NullWritable, Text&gt;(context); &#125; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; multipleOutputs.write(NullWritable.get(), value, key.toString()); &#125; &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; multipleOutputs.close(); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("./run &lt;input&gt; &lt;output&gt;"); System.exit(1); &#125; String inputPath = args[0]; String outputPath = args[1]; int numReduceTasks = 16; Configuration conf = this.getConf(); conf.setBoolean("mapred.output.compress", true); conf.setClass("mapred.output.compression.codec", GzipCodec.class, CompressionCodec.class); Job job = Job.getInstance(conf); job.setJobName("vacation_order"); job.setJarByClass(VacationOrderBySex.class); job.setMapperClass(VacationOrderBySexMapper.class); job.setReducerClass(VacationOrderBySexReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, inputPath); FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.setNumReduceTasks(numReduceTasks); boolean success = job.waitForCompletion(true); return success ? 0 : 1; &#125;&#125; 在生成输出的 reduce 中，在 setup() 方法中构造一个 MultipleOutputs 的实例并将它赋予一个实例变量。在 reduce() 方法中使用 MultipleOutputs 实例来写输出，而不是 context。write() 方法作用于键，值和名字。这里使用的是性别作为名字，因此最后产生的输出名称的形式为 sex-r-nnnnn：-rw-r--r-- 3 xiaosi xiaosi 0 2016-12-06 10:41 tmp/order_by_sex/_SUCCESS-rw-r--r-- 3 xiaosi xiaosi 88574 2016-12-06 10:41 tmp/order_by_sex/f-r-00005.gz-rw-r--r-- 3 xiaosi xiaosi 60965 2016-12-06 10:41 tmp/order_by_sex/m-r-00012.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00000.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00001.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00002.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00003.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00004.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00005.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00006.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00007.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 10:41 tmp/order_by_sex/part-r-00008.gz 我们可以看到在输出文件中不仅有我们想要的输出文件类型，还有part-r-nnnnn形式的文件，但是文件内没有信息，这是程序默认的输出文件。所以我们在指定输出文件名称时（name-r-nnnnn），不要指定name为part，因为它已经被使用为默认值了。 2. 多目录输出在 MultipleOutputs 的 write() 方法中指定的基本路径相对于输出路径进行解释，因为它可以包含文件路径分隔符（/），创建任意深度的子目录。例如，我们改动上面的需求：按男女性别来区分度假订单数据，不同性别数据位于不同子目录（例如：sex=f/part-r-00000）。public static class VacationOrderBySexReducer extends Reducer&lt;Text, Text, NullWritable, Text&gt; &#123; private MultipleOutputs&lt;NullWritable, Text&gt; multipleOutputs; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; multipleOutputs = new MultipleOutputs&lt;NullWritable, Text&gt;(context); &#125; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; String basePath = String.format("sex=%s/part", key.toString()); multipleOutputs.write(NullWritable.get(), value, basePath); &#125; &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; multipleOutputs.close(); &#125;&#125; 后产生的输出名称的形式为 sex=f/part-r-nnnnn 或者 sex=m/part-r-nnnnn：-rw-r--r-- 3 xiaosi xiaosi 0 2016-12-06 12:26 tmp/order_by_sex/_SUCCESS-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00000.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00001.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00002.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00003.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00004.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00005.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00006.gz-rw-r--r-- 3 xiaosi xiaosi 20 2016-12-06 12:26 tmp/order_by_sex/part-r-00007.gzdrwxr-xr-x - xiaosi xiaosi 0 2016-12-06 12:26 tmp/order_by_sex/sex=fdrwxr-xr-x - xiaosi xiaosi 0 2016-12-06 12:26 tmp/order_by_sex/sex=m 3. 延迟输出FileOutputFormat 的子类会产生输出文件(part-r-nnnnn)，即使文件是空的，也会产生。我们有时候不想要这些空的文件，我们可以使用 LazyOutputFormat 进行处理。它是一个封装输出格式，可以指定分区第一条记录输出时才真正创建文件。要使用它，使用 JobConf 和相关输出格式作为参数来调用 setOutputFormatClass() 方法即可：Configuration conf = this.getConf();Job job = Job.getInstance(conf);LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class); 再次检查一下我们的输出文件（第一个例子）：sudo -uxiaosi hadoop fs -ls tmp/order_by_sex/Found 3 items-rw-r--r-- 3 xiaosi xiaosi 0 2016-12-06 13:36 tmp/order_by_sex/_SUCCESS-rw-r--r-- 3 xiaosi xiaosi 88574 2016-12-06 13:36 tmp/order_by_sex/f-r-00005.gz-rw-r--r-- 3 xiaosi xiaosi 60965 2016-12-06 13:36 tmp/order_by_sex/m-r-00012.gz]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop SSH免密码登录]]></title>
    <url>%2Fhadoop-ssh-password-free-login.html</url>
    <content type="text"><![CDATA[1. 创建ssh-key这里我们采用rsa方式，使用如下命令：xiaosi@xiaosi:~$ ssh-keygen -t rsa -f ~/.ssh/id_rsaGenerating public/private rsa key pair.Created directory &apos;/home/xiaosi/.ssh&apos;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/xiaosi/.ssh/id_rsa.Your public key has been saved in /home/xiaosi/.ssh/id_rsa.pub.The key fingerprint is:SHA256:n/sFaAT94A/xxxxxxxxxxxxxxxxxxxxxxx xiaosi@xiaosiThe key&apos;s randomart image is:+---[xxxxx]----+| o= .. .. || o.= .. .|| *.* o .|| +.4.=E+..|| .SBo=. h+ || ogo..oo. || or +j..|| ...+o=.|| ... o=+|+----[xxxxx]-----+ 备注：这里会提示输入pass phrase，不需要输入任何字符，回车即可。 2. 生成authorized_keys文件xiaosi@xiaosi:~$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 记得要把authorized_keys文件放到.ssh目录下，与rsa等文件放在一起，否则免登录失败，debug如下（ssh -vvv localhost进行调试，查找错误原因）：xiaosi@xiaosi:~$ ssh -vvv localhostOpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips 1 Mar 2016debug1: Reading configuration data /etc/ssh/ssh_configdebug1: /etc/ssh/ssh_config line 19: Applying options for *debug2: resolving &quot;localhost&quot; port 22debug2: ssh_connect_direct: needpriv 0debug1: Connecting to localhost [127.0.0.1] port 22.debug1: Connection established.debug1: identity file /home/xiaosi/.ssh/id_rsa type 1...debug2: we sent a publickey packet, wait for replydebug3: receive packet: type 51debug1: Authentications that can continue: publickey,passworddebug1: Trying private key: /home/xiaosi/.ssh/id_dsadebug3: no such identity: /home/xiaosi/.ssh/id_dsa: No such file or directorydebug1: Trying private key: /home/xiaosi/.ssh/id_ecdsadebug3: no such identity: /home/xiaosi/.ssh/id_ecdsa: No such file or directorydebug1: Trying private key: /home/xiaosi/.ssh/id_ed25519debug3: no such identity: /home/xiaosi/.ssh/id_ed25519: No such file or directorydebug2: we did not send a packet, disable methoddebug3: authmethod_lookup passworddebug3: remaining preferred: ,passworddebug3: authmethod_is_enabled passworddebug1: Next authentication method: passwordxiaosi@localhost&apos;s password: 3. 验证xiaosi@xiaosi:~$ ssh localhostThe authenticity of host &apos;localhost (127.0.0.1)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:378enl3ckhdpObP8fnsHr1EXz4d1q2Jde+jUplkub/Y.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;localhost&apos; (ECDSA) to the list of known hosts.sign_and_send_pubkey: signing failed: agent refused operationxiaosi@localhost&apos;s password: 4. authorized_keys权限我们可以看到还是让我输入密码，很大可能是authorized_keys文件权限的问题，我们给该文件赋予一定权限：xiaosi@xiaosi:~$ chmod 600 ~/.ssh/authorized_keys 再次验证：xiaosi@xiaosi:~$ ssh localhostWelcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-24-generic x86_64) * Documentation: https://help.ubuntu.com/0 个可升级软件包。0 个安全更新。Last login: Thu Jun 16 08:05:50 2016 from 127.0.0.1 到此表示OK了。 备注：第一次需要输入密码，以后再次登陆就不需要输入密码了。 有更明白的小伙伴可以指导一下。。。。。。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 安装与启动]]></title>
    <url>%2Fhadoop-setup-and-start.html</url>
    <content type="text"><![CDATA[1. SSH参考博文：[Hadoop]SSH免密码登录以及失败解决方案（http://blog.csdn.net/sunnyyoona/article/details/51689041#t1） 2. 下载(1) 直接从官网上下载 http://hadoop.apache.org/releases.html (2) 使用命令行下载：xiaosi@yoona:~$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz--2016-06-16 08:40:07-- http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz正在解析主机 mirrors.hust.edu.cn (mirrors.hust.edu.cn)... 202.114.18.160正在连接 mirrors.hust.edu.cn (mirrors.hust.edu.cn)|202.114.18.160|:80... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度： 196015975 (187M) [application/octet-stream]正在保存至: “hadoop-2.6.4.tar.gz” 3. 解压缩Hadoop包解压位于根目录/文件夹下的hadoop-2.7.3.tar.gz到~/opt文件夹下xiaosi@yoona:~$ tar -zxvf hadoop-2.7.3.tar.gz -C opt/ 4. 配置配置文件都位于安装目录下的/etc/hadoop文件夹下：xiaosi@yoona:~/opt/hadoop-2.7.3/etc/hadoop$ lscapacity-scheduler.xml hadoop-env.sh httpfs-log4j.properties log4j.properties mapred-site.xml.templateconfiguration.xsl hadoop-metrics2.properties httpfs-signature.secret log4j.properties slavescontainer-executor.cfg hadoop-metrics.properties httpfs-site.xml mapred-env.cmd ssl-client.xml.examplecore-site.xml hadoop-policy.xml kms-acls.xml mapred-env.sh ssl-server.xml.examplecore-site.xml hdfs-site.xml kms-env.sh mapred-queues.xml.template yarn-env.cmdhadoop-env.cmd hdfs-site.xml kms-log4j.properties mapred-site.xml yarn-env.shhadoop-env.sh httpfs-env.sh kms-site.xml mapred-site.xml yarn-site.xml Hadoop的各个组件均可利用XML文件进行配置。core-site.xml文件用于配置Common组件的属性，hdfs-site.xml文件用于配置HDFS属性，而mapred-site.xml文件则用于配置MapReduce属性。 备注：Hadoop早期版本采用一个配置文件hadoop-site.xml来配置Common，HDFS和MapReduce组件。从0.20.0版本开始该文件以分为三，各对应一个组件。 4.1 配置core-site.xmlcore-site.xml 配置如下：&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/$&#123;user.name&#125;/tmp/hadoop&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.xiaosi.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.xiaosi.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 4.2 配置hdfs-site.xmlhdfs-site.xml配置如下：&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/xiaosi/tmp/hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/xiaosi/tmp/hadoop/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.3 配置 mapred-site.xmlmapred-site.xml配置如下：&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行Hadoop的时候可能会找不到jdk，需要我们修改hadoop.env.sh脚本文件，唯一需要修改的环境变量就是JAVE_HOME，其他选项都是可选的：export JAVA_HOME=/home/xiaosi/opt/jdk-1.8.0 5. 运行5.1 初始化HDFS系统在配置完成后，运行hadoop前，要初始化HDFS系统，在bin/目录下执行如下命令：xiaosi@yoona:~/opt/hadoop-2.7.3$ ./bin/hdfs namenode -format 5.2 启动开启NameNode和DataNode守护进程：xiaosi@yoona:~/opt/hadoop-2.7.3$ ./sbin/start-dfs.shStarting namenodes on [localhost]localhost: starting namenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-namenode-yoona.outlocalhost: starting datanode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-datanode-yoona.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-secondarynamenode-yoona.out 通过jps命令查看namenode和datanode是否已经启动起来：xiaosi@yoona:~/opt/hadoop-2.7.3$ jps13400 SecondaryNameNode13035 NameNode13197 DataNode13535 Jps 从启动日志我们可以知道，日志信息存储在hadoop-2.7.3/logs/目录下，如果启动过程中有任何问题，可以通过查看日志来确认问题原因。 6. Yarn模式安装6.1 配置修改yarn-site.xml，添加如下配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce.shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml，做如下修改：&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 本地模式下是value值是local，yarn模式下value值是yarn。 6.2 启动yarn启动yarn：xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-resourcemanager-yoona.outlocalhost: starting nodemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-nodemanager-yoona.out 关闭yarn:xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/stop-yarn.shstopping yarn daemonsstopping resourcemanagerlocalhost: stopping nodemanager 6.3 检查是否运行成功打开浏览器，输入：http://localhost:8088/cluster]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 近实时搜索]]></title>
    <url>%2Felasticsearch-base-near-real-time-search.html</url>
    <content type="text"><![CDATA[1. 按段搜索随着 按段搜索 的发展，索引文档与文档可被搜索的延迟显着下降。新文档可以在数分钟内可被搜索，但仍然不够快。 在这里磁盘是瓶颈。提交一个新的段到磁盘需要 fsync 来确保段被物理性地写入磁盘，这样在断电的时候也不会丢失数据。但是 fsync 代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。 我们需要的是一个更轻量的方式来使文档可被搜索，这意味着要从整个过程中移除 fsync。 在 Elasticsearch 和磁盘之间的是文件系统缓存。如前所述，内存中索引缓冲区中的文档(如下第一图)被写入新的段(如下第二图)．但是新的段首先被写入到文件系统缓存中 - 成本较低 - 只是稍后会被刷到磁盘 - 成本较高。但一旦文件在缓存中，它就可以像任何其他文件一样打开和读取。 Lucene 允许新段被写入和打开，使其包含的文档在没有进行一次完整提交之前便对搜索可见。这是一种比提交更轻量级的过程，并在不影响性能的前提下可以被频繁地执行。 2. Refresh API在 ElasticSearch 中，这种轻量级写入和打开新片段的过程称为刷新refresh。默认情况下，每个分片每秒会自动刷新一次。这就是为什么我们说 Elasticsearch 是近实时搜索：文档更改不会立即对搜索可见，但会在1秒之内对搜索可见。 这可能会让新用户感到困惑：他们索引文档后并尝试搜索它，但是没有搜索到。这个问题的解决办法是使用 Refresh API 手动刷新一下：POST /_refreshPOST /blogs/_refresh 第一个语句刷新所有索引，第二个语句只是刷新blogs索引 虽然刷新比提交(一次完整提交会将段刷到磁盘)更轻量级，但是仍然具有性能成本。编写测试时手动刷新可能很有用，但在生产环境中不要每次索引文档就去手动刷新。它会增大性能开销。相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并做相应的补偿措施。 并非所有场景都需要每秒刷新一次。也许你正在使用 Elasticsearch 来索引数百万个日志文件，而你更希望优化索引速度，而不是近实时搜索。你可以通过设置 refresh_interval 来降低每个索引的刷新频率：PUT /my_logs&#123; "settings": &#123; "refresh_interval": "30s" &#125;&#125; 每30秒刷新一次my_logs索引。 refresh_interval 可以在现有索引上动态更新。你可以在构建大型新索引时关闭自动刷新，然后在生产环境中开始使用索引时将其重新打开：PUT /my_logs/_settings&#123; &quot;refresh_interval&quot;: -1 &#125;PUT /my_logs/_settings&#123; &quot;refresh_interval&quot;: &quot;1s&quot; &#125; 第一个语句禁用自动刷新,第二个语句每秒自动刷新一次; refresh_interval 需要一个持续时间值， 例如 1s （1 秒） 或 2m （2 分钟）。 一个绝对值 1 表示的是 1毫秒 –无疑会使你的集群陷入瘫痪(每一毫秒刷新一次)。 ElasticSearch版本：2.x 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/near-real-time.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 持久化变更]]></title>
    <url>%2Felasticsearch-base-making-changes-persistent.html</url>
    <content type="text"><![CDATA[1. 持久化变更如果没有使用 fsync 将文件系统缓存中的数据刷（flush）到磁盘上，我们无法保证数据在断电后甚至在正常退出应用程序后仍然存在。为了使 Elasticsearch 具有可靠性，我们需要确保将更改持久化到磁盘上。 在ElasticSearch 动态更新索引中，我们说过一次完整提交会将段刷到磁盘，并写入到一个包含所有段列表的提交点 commit point。Elasticsearch 在启动或重新打开索引时使用此提交点来确定哪些段属于当前分片。 当我们每秒刷新（refresh）一次即可实现近实时搜索，但是我们仍然需要定期进行全面的提交，以确保我们可以从故障中恢复。但发生在两次提交之间文件变化怎么办？ 我们也不想丢失。 Elasticsearch添加了一个 Translog 或者叫事务日志，它记录了 Elasticsearch 中的每个操作。使用 Translog，处理过程现在如下所示： (1) 索引文档时，将其添加到内存索引缓冲区中，并追加到 Translog 中，如下图所示: (2) 刷新refresh使分片处于下图描述的状态，分片每秒被刷新（refresh）一次： 内存缓冲区中的文档写入一个新的段中，而没有 fsync。 段被打开以使其可以搜索。 内存缓冲区被清除。 (3) 该过程继续，将更多的文档添加到内存缓冲区并追加到 Translog 中，如下图所示: (4) 每隔一段时间，例如 Translog 变得非常大，索引被刷新 flush 到磁盘，一个新的 Translog 被创建，并执行一个全量提交： 内存缓冲区中的任何文档都将写入新的段。 内存缓冲区被清除。 一个提交点被写入硬盘。 文件系统缓存通过 fsync 被刷新 flush 到磁盘。 老的 Translog 被删除。 Translog 提供所有尚未刷新 flush 到磁盘的操作的一个持久化记录。启动时，Elasticsearch 将使用最后一个提交点从磁盘中恢复已知的段，然后将重新执行 Translog 中的所有操作，以添加最后一次提交后发生的更改。 Translog也被用来提供实时CRUD。当你试着通过ID查询、更新、删除一个文档，在尝试从相应的段中检索文档之前，首先检查 Translog 来查看最近的变更。这意味着它总是能够实时地获取到文档的最新版本。 2. flush API在 Elasticsearch 中执行提交和截断 Translog 的操作被称作一次 flush。分片每30分钟或者当 Translog 变得太大时会自动 flush 一次。有关可用于控制这些阈值的设置，请参阅Translog文档。 flush API 可用于执行手动刷新：POST /blogs/_flushPOST /_flush?wait_for_ongoing 第一个语句刷新 blogs 索引；第二个语句刷新所有索引，等待所有刷新完成后返回。 你很少需要手动 flush；通常情况下，自动 flush 就足够了。这就是说，在重启节点或关闭索引之前执行 flush 有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引，它需要重新执行 Translog 中所有的操作，所以如果 Translog 中日志越短，恢复越快。 3. Translog有多安全？Translog 的目的是确保操作不会丢失。这就提出了一个问题：Translog的安全性如何？ 在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认情况下，Translog 每5秒进行一次 fsync 刷新到磁盘，或者在每次写请求(例如index, delete, update, bulk)完成之后执行。这个过程发生在主分片和副本分片上。最终，这意味着在整个请求被fsync 到主分片和副本分片上的 Translog 之前，你的客户端不会得到一个200 OK响应。 在每个请求之后执行 fsync 都会带来一些性能消耗，尽管实际上相对较小（特别是对于bulk导入，在单个请求中平摊了许多文档的开销）。 但是对于一些高容量的集群而言，丢失几秒钟的数据并不严重，因此使用异步的 fsync 还是比较有好处的。比如，写入的数据被缓存到内存后，再每5秒整体执行一次 fsync。 可以通过将 durability 参数设置为异步来启用此行为：PUT /my_index/_settings&#123; "index.translog.durability": "async", "index.translog.sync_interval": "5s"&#125; 可以为每个索引单独进行配置，并可以动态更新。如果你决定启用异步 Translog 行为，你需要确认如果发生崩溃，丢失掉 sync_interval 时间段的数据也没有关系。在决定使用这个参数前请注意这个特征！ 如果你不确定此操作的后果，最好使用默认值（&quot;index.translog.durability&quot;：&quot;request&quot;）以避免数据丢失。 ElasticSearch版本：2.x 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/translog.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 段合并]]></title>
    <url>%2Felasticsearch-base-sgement-merge.html</url>
    <content type="text"><![CDATA[由于自动刷新过程每秒会创建一个新的段，这样会导致短时间内段数量暴增。段数目太多会带来比较大的麻烦。每一个段都会消耗文件句柄、内存和 CPU 运行周期。更重要的是，每个搜索请求都必须按顺序检查每个段。所以段越多，搜索也就越慢。 Elasticsearch 通过在后台进行 段合并 来解决这个问题。小的段被合并成大的段，然后这些大的段会被合并成更大的段。 段合并的时候将那些被标记为删除的旧文档从文件系统中删除。被标记删除的文档或者更新文档的旧版本文档不会被拷贝到新的更大的段中。 段合并不需要你做什么，在索引和搜索时会自动发生。该过程的工作原理如下图所示，两个提交过的段和一个未提交的段被合并到更大的段中： 在索引时，刷新 refresh 进程会创建新的段并开放供搜索。 合并进程选择几个相似大小的段，在后台将它们合并到一个新的更大的段中。这不会中断索引和搜索。 下图阐述了合并的完成过程： 新的片段被刷新 flush 到磁盘。 写入一个新的提交点，其中包含新的段，并排除旧的较小段。 新的段开放供搜索。 旧段被删除。 合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。默认情况下，Elasticsearch 会调节合并进程，以使搜索仍具有足够的资源来执行。 关于为你的实例调整合并的建议，请参阅分段与合并 2. optimize APIoptimize API 可以看做是强制合并API。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。这样做的目的是减少段的数量（通常减少到一个），来提升搜索性能。 optimize API 不应该被用在一个动态索引上，一个正在被更新的索引。后台合并进程可以很好地完成这项工作。optimizing 会阻碍这个进程。不要干扰它！ 在某些具体情况下，optimize API 可能是有好处的。例如，日志记录，每天、每周、每月的日志被存储在一个索引中。较旧的索引基本上都是只读的，他们不太可能改变。 在这种情况下，将旧索引的每个分片优化为以个单独段是有用的，它会使用更少的资源，同时搜索会更快：POST /logstash-2014-10/_optimize?max_num_segments=1 上述代码将索引中的每个分片合并到一个段中。 请注意，使用 optimize API 触发段合并的操作不会受到任何的限制。这可能会消耗掉你节点上全部的I/O资源, 使其没有足够的资源来处理搜索请求，从而有可能使集群失去响应。如果你想要对索引执行 optimize，你需要先使用分片分配（查看 迁移旧索引）把索引移到一个安全的节点，再执行。 Elasticsearch版本：2.x 原文：https://www.elastic.co/guide/cn/elasticsearch/guide/2.x/merge-process.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 动态更新索引]]></title>
    <url>%2Felasticsearch-base-dynamically-updatable-indices.html</url>
    <content type="text"><![CDATA[1. 不变性倒排索引被写入磁盘后是 不可改变(immutable)：永远不会被修改。不变性有如下几个重要的优势： 不需要锁。如果你没有必要更新索引，你就没有必要担心多进程会同时修改数据。 一旦索引被读入内核的文件系统缓存中，由于其不会改变，便会留在那里。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 其它缓存(例如filter缓存)，在索引的生命周期内始终保持有效。因为数据不会改变，不需要在每次数据改变时被重建。 写入一个大的倒排索引中允许数据被压缩，减少磁盘 I/O 和 缓存索引所需的RAM量。 当然，一个不变的索引也有缺点。主要是它是不可变的! 你不能修改它。如果你需要让一个新的文档可被搜索，你需要重建整个索引。这对索引可以包含的数据量或可以更新索引的频率造成很大的限制。 2. 动态更新索引下一个需要解决的问题是如何更新倒排索引，而不会失去其不变性的好处？ 答案是：使用多个索引。 通过增加一个新的补充索引来反映最近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询–从最旧的开始–再对各个索引的查询结果进行合并。 Lucene 是 Elasticsearch 所基于的Java库，引入了 按段搜索 的概念。 每一个段本身就是一个倒排索引， 但 Lucene 中的 index 除表示段 segments 的集合外，还增加了提交点 commit point 的概念，一个列出了所有已知段的文件，如下图所示展示了带有一个提交点和三个分段的 Lucene 索引: 新文档首先被添加到内存中的索引缓冲区中，如下图所示展示了一个在内存缓存中包含新文档准备提交的Lucene索引: 然后写入到一个基于磁盘的段，如下图所示展示了在一次提交后一个新的段添加到提交点而且缓存被清空: 2.1 索引与分片一个 Lucene 索引就是我们 Elasticsearch 中的分片shard，而 Elasticsearch 中的一个索引是分片的集合。当 Elasticsearch 搜索索引时，它将查询发送到属于该索引的每个分片(Lucene索引)的副本(主分片，副本分片)上，然后将每个分片的结果聚合成全局结果集，如ElasticSearch 内部原理之分布式文档搜索中描述。 2.2 按段搜索过程(1) 新文档被收集到内存索引缓冲区中，如上第一图； (2) 每隔一段时间，缓冲区就被提交： 一个新的段(补充的倒排索引)被写入磁盘。 一个新的提交点commit point被写入磁盘，其中包括新的段的名称。 磁盘进行同步 — 所有在文件系统缓冲区中等待写入的都 flush 到磁盘，以确保它们被写入物理文件。 (3) 新分段被开启，使其包含的文档可以被搜索。 (4) 内存缓冲区被清除，并准备好接受新的文档。 当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。 3. 删除与更新段是不可变的，因此无法从旧的段中删除文档，也不能更新旧的段来反映文档的更新。相反，每个提交点 commit point 都包括一个 .del 文件，文件列出了哪个文档在哪个段中已经被删除了。 当文档被’删除’时，它实际上只是在 .del 文件中被标记为已删除。标记为已删除的文档仍然可以匹配查询，但在最终查询结果返回之前，它将从结果列表中删除。 文档更新也以类似的方式工作：当文档更新时，旧版本文档被标记为已删除，新版本文档被索引到新的段中。也许文档的两个版本都可以匹配查询，但是在查询结果返回之前旧的标记删除版本的文档会被移除。 在ElasticSearch 段合并中，我们将展示如何从文件系统中清除已删除的文档。 Elasticsearch版本：2.x 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/dynamic-indices.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 副本分片]]></title>
    <url>%2Felasticsearch-base-replica-shard.html</url>
    <content type="text"><![CDATA[1. 副本分片到目前为止，我们只讨论了主分片，但是我们还有另一个工具：副本分片。副本分片的主要目的是为了故障转移（failover），如深入集群生命周期所述：如果持有主分片的节点挂掉了，则一个副本分片会提升为主分片的角色。 在索引写入时，副本分片做着与主分片相同的工作。新文档首先被索引进主分片然后再同步到其它所有的副本分片。增加副本数并不会增加索引容量。 但是，副本分片可以为读取请求提供帮助。如果通常情况下，你的索引搜索占很大比重（偏向于查询使用），则可以通过增加副本数量来增加搜索性能，但这样你也会为此付出占用额外的硬件资源的代价。 让我们回到那个具有两个主分片的索引示例中。我们通过添加第二个节点来增加索引的容量。添加更多节点不会帮助我们提升索引写入能力，但是我们可以在搜索时通过增加副本分片的的个数来充分利用额外硬件资源：PUT /my_index/_settings&#123; "number_of_replicas": 1&#125; 拥有两个主分片，另外加上每个主分片的一个副本，我们总共拥有四个分片：每个节点一个，如下图所示： 2. 通过副本进行负载均衡搜索性能取决于最慢节点的响应时间，所以尝试均衡所有节点的负载是一个好想法。如果我们只是增加一个节点而不是两个，最终我们会有三个节点，其中两个节点只拥有一个分片，另一个节点拥有两个分片做着两倍的工作。 我们可以通过调整分片副本数量来平衡这些。通过分配两个副本，最终我们会拥有六个分片，刚好可以平均分给三个节点PUT /my_index/_settings&#123; "number_of_replicas": 2&#125; 作为奖励，我们同时提升了我们的可用性。我们可以容忍丢失两个节点而仍然保持一份完整数据的拷贝。 如下图所示： 事实上节点 3 拥有两个副本分片，没有主分片并不重要。副本分片与主分片做着相同的工作。它们只是扮演着略微不同的角色。没有必要确保主分片均匀地分布在所有节点中。 ElasticSearch版本：2.x 原文：Replica Shards]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch Scroll游标搜索]]></title>
    <url>%2Felasticsearch-base-scroll-search.html</url>
    <content type="text"><![CDATA[1. 深分页在ElasticSearch 分页搜索一文中，我们了解到分布式系统中深度分页。在这里我们再具体的了解一下深度分页，可能带来的问题，以及 ElasticSearch 给出的解决方案。 在 ElasticSearch 内部原理之分布式文档搜索 一文中我们了解到分布式搜索的工作原理，分布式搜索这种先查后取的过程支持用 from 和 size 参数分页，但是这是有限制的。请记住，每个分片必须构建一个长度为 from+size 的优先级队列，所有这些队列都需要传递回协调节点。协调节点需要对 number_of_shards *（from + size） 个文档进行排序，以便正确找到 size 个文档。 取决于你的文档的大小，分片的数量和你使用的硬件，给 10,000 到 50,000 的结果文档深分页（ 1,000 到 5,000 页）是完全可行的。但是使用足够大的 from 值，排序过程可能会变得非常沉重，使用大量的CPU、内存和带宽。因为这个原因，我们强烈建议你不要使用深分页。 实际上，’深分页’很少符合我们的行为。当2到3页过去以后，我们会停止翻页，并且改变搜索条件。不知疲倦地一页一页的获取网页直到你的服务崩溃的罪魁祸首一般是机器人或者网络爬虫。 如果你确实需要从集群里取回大量的文档，你可以通过使用scroll查询（禁用排序）来更有效率的取回文档，具体我们会在下面进行讨论。 2. 游标ScrollScroll 查询用于从 Elasticsearch 中有效地检索大量文档，而又不需付出深度分页那种代价。 Scroll 允许我们先进行初始化搜索，然后再不断地从 Elasticsearch 中取回批量结果，直到取回所有结果。这有点像传统数据库中的 cursor。 Scroll 会搜索在某个时间上生成快照。在搜索初始化完成后，搜索不会看到之后发生在索引上的更改。通过保留旧的数据文件来实现这一点，以便可以保留其在开始搜索时索引的视图。 深分页的代价主要花费在结果数据全局排序上，如果我们禁用排序，那么我们可以花费较少的代价就能返回所有的文档。为此，我们按 _doc 排序。这样 Elasticsearch 只是从仍然还有结果数据需要返回的每个分片返回下一批结果。 启用游标查询，我们执行一个搜索请求，并将 scroll 值设置为游标查询窗口打开的时间长度（即我们期望的游标查询的过期时间）。每次运行游标查询时都会刷新游标查询的过期时间，所以这个时间只需要足够处理当前批的结果就可以了，而不是处理所有与查询匹配的文档。超时设置是非常重要的，因为保持游标查询窗口打开需要消耗资源，我们希望在不再需要时释放它们。设置这个超时能够让 Elasticsearch 在稍后空闲的时候自动释放这部分资源。 GET /old_index/_search?scroll=1m&#123; "query": &#123; "match_all": &#123;&#125;&#125;, "sort" : ["_doc"], "size": 1000&#125; 上面语句保持游标查询窗口一分钟。并且根据_doc进行排序； 这个查询的返回结果包括一个 _scroll_id 字段，它是一个Base-64编码的长字符串。现在我们可以将 _scroll_id 传递给 _search/scroll 接口来检索下一批结果：GET /_search/scroll&#123; "scroll": "1m", "scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMDk5NDpkUmpiR2FjOFNhNnlCM1ZDMWpWYnRROzEwOTk1OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MTA5OTM6ZFJqYkdhYzhTYTZ5QjNWQzFqVmJ0UTsxMTE5MDpBVUtwN2lxc1FLZV8yRGVjWlI2QUVBOzEwOTk2OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MDs="&#125; 再次设置游标查询过期时间为一分钟。 这个游标查询返回的下一批结果。虽然我们指定了请求大小为 1000，但是我们可能会得到更多的文件。当查询的时候，size 作用于每个分片，所以每个批次实际返回的文档数量最大为 size * number_of_primary_shards 。 游标查询每次都返回一个新的 _scroll_id。每次我们进行下一个游标查询时，我们必须传递上一个游标查询返回的 _scroll_id。 当没有更多的命中返回时，我们已经处理了所有匹配的文档。 3. Java中使用游标import static org.elasticsearch.index.query.QueryBuilders.*;QueryBuilder qb = termQuery("multi", "test");SearchResponse scrollResp = client.prepareSearch(test) .addSort(FieldSortBuilder.DOC_FIELD_NAME, SortOrder.ASC) .setScroll(new TimeValue(60000)) .setQuery(qb) .setSize(100) .get();// 直到没有命中时返回do &#123; for (SearchHit hit : scrollResp.getHits().getHits()) &#123; // 处理hit... &#125; scrollResp = client.prepareSearchScroll(scrollResp.getScrollId()).setScroll(new TimeValue(60000)).execute().actionGet();&#125; while(scrollResp.getHits().getHits().length != 0); ElasticSearch版本：2.x 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/scroll.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 内部原理之分布式文档搜索]]></title>
    <url>%2Felasticsearch-internal-distributed-document-search.html</url>
    <content type="text"><![CDATA[这个要比基本的创建-读取-更新-删除（CRUD）请求要难一些。CRUD操作是处理的单个文档。这就意味着我们明确的知道集群中的哪个分片存储我们想要的文档。 一个 CRUD 操作只对单个文档进行处理，文档有唯一的组合，由 _index, _type, 和 路由值 （默认是该文档的 _id ）组成。这表示我们确切的知道此文档在集群中哪个分片中。 搜索请求是更复杂的执行模型，因为我们不知道哪些文档会与查询匹配，它们可能存在在集群中的任意一个分片中。搜索请求不得不搜索我们关注的一个或多个索引中的每个分片拷贝(主分片或者副本分片)，以查看分片中中是否有匹配的文档。 但找到所有匹配到文档只是完成了一半工作．在searchAPI返回一’页’结果之前，来自多个分片的结果必须聚合成一个排序的列表。 因此，搜索需要完成两个阶段，query 和 fetch。 1. Query阶段在初始化查询阶段（query phase），查询将广播到索引中的每个分片的拷贝上(主分片或者副本分片)。每个分片在本地执行搜索并建立了匹配文档的优先级队列。 1.1 优先级队列优先级队列只是一个存有前n个（top-n）匹配文档的有序列表。优先级队列的大小取决于 from 和 size 分页参数。例如，以下搜索请求将需要足够大的优先级队列来容纳100个文档：GET /_search&#123; "from": 90, "size": 10&#125; 1.2 QueryQuery阶段过程如下图所示： Query阶段包含如下步骤： 客户端发送一个Search请求给节点3，节点3创建了一个大小为 from+size 的空优先级队列。 节点3将搜索请求转发到索引中每个分片的主分片或副本分片上。每个分片在本地执行查询，并将结果添加到大小为from+size的本地排序的优先级队列中。 每个分片将其优先级队列中的所有文档的文档ID和排序值返回给协调节点节点3，节点3将这些值合并到其自己的优先级队列中，以生成全局排序的结果列表。 当一个搜索请求被发送到一个节点，这个节点就变成了协调节点。这个节点的工作是向所有相关的分片广播搜索请求并且把它们的响应整合成一个全局的有序结果集。将这个结果集返回给客户端。 第一步是将请求广播到索引里每个节点的分片拷贝上。就像document GET requests请求一样，搜索请求可以被任意主分片或者副本分片处理。这就是为什么说更多的副本能够提高搜索吞吐率的原因。协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。 每个分片在本地执行查询并建立一个长度为 from+size 的有序优先级队列，这个长度意味着它自己的结果数量就足够满足全局的请求要求。分片返回一个轻量级的结果列表给协调节点。只包含文档ID值和排序需要用到的值，例如 _score。 协调节点将这些分片结果合并到其自己的排序优先级队列中，表示全局排序的结果集。到此查询阶段结束。 一个索引可以由一个或多个主分片组成，因此针对单个索引的搜索请求需要能够组合来自多个分片的结果。搜索多个或所有索引的工作方式完全相同 - 只是会涉及更多的分片。 2. Fetch阶段查询阶段标示出哪些文档满足我们的搜索请求，我们只返回了文档ID以及对排序有用的值，并没有返回文档本身。我们仍然需要检索那些文档。这就是 fetch 阶段的工作，过程如下图所示： Fetch 阶段由以下步骤构成： 协调节点标示出哪些文档需要取回，并且向相关分片发出多个GET请求。 如果需要，每个分片加载并丰富文档，然后将文档返回协调节点。 一旦所有的文档都被取回，协调节点将结果返回给客户端。 协调节点首先决定哪些文档是实际需要取回的。例如，如果我们查询指定{ &quot;from&quot;: 90, &quot;size&quot;: 10 }，那么前90条结果将会被丢弃，只需要检索接下来的10个结果。这些文档可能来自与查询请求相关的一个、多个或者全部分片。 协调节点给拥有相关文档的每个分片创建一个 multi-get request，并发送请求给同样处理查询阶段的分片拷贝。 分片加载文档体– _source 字段–如果有需要，用metadata和search snippet highlighting丰富结果文档。一旦协调节点接收到所有的结果文档，它就组合这些结果为单个响应返回给客户端。 ElasticSearch 版本: 2.x 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/distributed-search.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 内部原理之分布式文档存储]]></title>
    <url>%2Felasticsearch-internal-distributed-document-store.html</url>
    <content type="text"><![CDATA[之前的文章中，我们已经知道如何存储数据到索引中以及如何检索它。但是我们掩盖了数据存储到集群中以及从集群中获取数据的具体实现的技术细节。 1. 路由文档到分片中当你索引一篇文档时，它会存储到一个主分片中。但是 ElasticSearch 如何知道文档是属于哪个分片呢？当我们创建一个新的文档，它是怎么知道它是应该存储到分片1上还是分片2上？ 数据存储到分片的过程是有一定规则的，并不是随机发生的，因为我们日后还需要从分片中检索出文档。数据存储过程取决于下面的公式：shard = hash(routing) % number_of_primary_shards Routing 值是一个任意字符串，默认为 文档的id，也可以设置为一个用户自定义的值。Routing 这个字符串通过一个 hash 函数处理，并返回一个数值，然后再除以索引中主分片的数目 number_of_primary_shards，所得的余数作为主分片的编号，取值一般在 0 到 number_of_primary_shards - 1 之间的余数范围中。通过这种方法计算出该数据是存储到哪个分片中。 这就解释了为什么主分片个数在创建索引之后就不能再更改了：如果主分片个数在创建之后可以修改，那么之前所有通过公式得到的值都会失效，之前存储的文档也可能找不到。 有的人可能认为，拥有固定数量的主分片会使以后很难对索引进行扩展。实际上，有一些技术可以让你在需要的时候很轻松的扩展。可以参阅Designing for Scale。 所有的文档API（get , index , delete , bulk , update , 和 mget）都可以接受一个 routing 参数，来自定义文档与分片之间的映射。一个自定义的路由参数可以用来确保所有相关的文档，例如所有属于同一个用户的文档都被存储到同一个分片中。我们会在Designing for Scale中详细讨论为什么要这样做。 2. 主分片与副本分片如何交互假设我们有一个三个节点的集群。集群里有一个名称为 blog 的索引，有两个主分片（primary shards）。每个主分片都有两个副本。相同节点的副本不会分配到同一节点，最后如下图展示： 我们可以发送请求到集群中的任何一个节点，每个节点都有能力处理我们的请求。每个节点都知道集群中每个文档的存储位置，所以可以直接将请求转发到对应的节点上。 在下面的例子中，我们将请求都发送到节点 1 上，我们将其称为协调节点(coordinating node)。 2.1 创建，索引和删除文档创建，索引和删除请求都是写操作，所以必须在主分片上写操作完成之后才能被复制到相关的副本分片上。 交互过程如下图所示： 下面是成功在主分片和副本分片上创建，索引以及删除文档所必须的步骤： 客户端发送了一个新建，索引 或者删除文档请求给节点 1； 节点 1 通过请求文档的 id 值判断出该文档应该被存储在分片 0 中，并且知道分片 0 的主分片 P0 位于节点 3 上。因此节点 1 会把这个请求转发给节点 3； 节点 3 在主分片上执行请求。如果请求执行成功，节点 3 并行将该请求转发给节点 1 和节点 2 上的的副本分片（R0）。一旦所有的副本分片都成功地执行了请求，则向节点 3 报告成功，节点 3 向协调节点 （Node 1 ）报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下: 2.1.1 一致性默认情况下，在尝试进行写操作之前，主分片需要规定数量(quorum)或大多数(majority)的分片拷贝 shard copies （其中分片副本可以是主分片或副本分片）。这是为了防止将数据写入网络分区的“错误的一边wrong side”。 规定数量quorum定义如下：int( (primary + number_of_replicas) / 2 ) + 1 一致性consistency值可以是one（只有主分片），all（主分片和所有的副本），或者默认值quorum，或者大多数的分片副本（The allowed values for consistency are one (just the primary shard), all (the primary and all replicas), or the default quorum, or majority, of shard copies.）。 请注意，number_of_replicas是索引设置中指定的副本数，而不是当前活跃的副本数。 如果指定索引有三个副本，则quorum将如下定义：int( (primary + 3 replicas) / 2 ) + 1 = 3 但是，如果仅启动两个节点，则活跃的分片副本不满足规定数量，您将无法对任何文档进行索引或删除。 2.1.2 超时如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 timeout 参数 使它更早终止： 100 100毫秒，30s 是30秒。 新索引默认有 1 个副本分片，这意味着为满足 规定数量 应该 需要两个活动的分片副本。 但是，这些默认的设置会阻止我们在单一节点上做任何事情。为了避免这个问题，要求只有当 number_of_replicas 大于1的时候，规定数量才会执行。 2.2 检索文档我们可以从一个主分片（primary shard）或者它们任一副本中检索文档，流程如下图： 下面是从主分片或者副本分片上检索文档所需要的一系列步骤： 客户端发送了一个 Get 请求给节点 1； 节点 1 通过请求文档的 id 值判断出该文档被存储在分片 0 中。三个节点上都存有分片 0 的复制（节点1上R0，节点2上R0，节点3上P0）。这一次，它将请求转发给节点 2 。 节点 2 返回文档给节点 1 ，节点 1 在返回文档给客户端。 对于读请求，对于每一次请求，请求节点都会选择一个不同的副本分本，达到负载均衡。通过轮询所有的副本分片。 在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。 2.3 局部更新文档更新 API （Update API）融合了上面解释的两种读写模式，如下图所示： 下面是部分更新一篇文档所需要的一系列步骤： 客户端发送了一个 Update 请求给节点 1； 节点1通过请求文档的 id 值判断出该文档被存储在分片0中。并且知道分片0的主分片P0位于节点3上。因此节点1会把这个请求转发给节点 3； 节点3从主分片（P0）上检索出指定文档，并更改 _source 字段中的JSON，修改完毕之后试着重新索引文档到主分片（P0）上。如果有人已经修改了该文档，那么会重复步骤3，如果尝试 retry_on_conflict 次还没有成功则放弃。 如果节点3更新文档成功，节点3会把文档新版本并行发给节点1和节点2上的副本分片，重新索引文档。一旦所有的副本分片返回成功，节点3向协调节点返回成功，协调节点向客户端返回成功。 基于文档的复制：当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。 2.4 多文档模式mget 和 bulk API的模式类似于单文档模式。 不同的是，协调节点知道每个文档存储在哪个分片中。 它将多文档请求分解成对每个分片的多文档请求，并将请求并行转发到每个参与节点。 一旦从每个节点接收到应答，将每个节点的响应整合到单个响应中，并返回给客户端 2.4.1 mget如下图所示： 以下是使用单个 mget 请求取回多个文档所需的步骤顺序： 客户端向节点 1 发送 mget 请求。 节点 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有应答， 节点 1 构建响应并将其返回给客户端。 2.4.2 bulkbulk API，允许在单个批量请求中执行多个创建、索引、删除和更新请求，如下图所示： bulk API 按如下步骤顺序执行： 客户端向 节点 1 发送 bulk 请求。 节点 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。 bulk API 还可以在整个批量请求的最顶层使用 consistency 参数，以及在每个请求中的元数据中使用 routing 参数。 ElasticSearch版本: 2.x 原文： https://www.elastic.co/guide/en/elasticsearch/guide/2.x/distributed-docs.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 分页搜索]]></title>
    <url>%2Felasticsearch-pagination-search.html</url>
    <content type="text"><![CDATA[1. 分页之前的文章ElasticSearch 空搜索与多索引多类型搜索我们知道，我们的空搜索匹配到集群中的13个文档。 但是，命中数组中只有10个文档（文章只显示了2条数据，故意省略掉）。 我们如何查看其他文档呢？ 与SQL使用LIMIT关键字返回一个’页面’的结果数据相同，Elasticsearch 接受 from 和 size 参数： size 表示应返回的结果数，默认为10 from 表示应跳过的初始结果数，默认为0 如果想每页显示五个条数据，那么第1到3页的结果请求如下所示：curl -XGET &apos;localhost:9200/_search?size=5&amp;pretty&apos;curl -XGET &apos;localhost:9200/_search?size=5&amp;from=5&amp;pretty&apos;curl -XGET &apos;localhost:9200/_search?size=5&amp;from=10&amp;pretty&apos; Java版本:SearchRequestBuilder searchRequestBuilder = client.prepareSearch();searchRequestBuilder.setIndices(index);searchRequestBuilder.setTypes(type);searchRequestBuilder.setFrom(from);searchRequestBuilder.setSize(size);SearchResponse response = searchRequestBuilder.get(); 要避免分页太深或者一次请求太多的结果。结果在返回前要进行排序。请记住，搜索请求通常跨越多个分片。每个分片都会生成自己的排序结果，然后在协调节点集中排序，以确保整体顺序正确。 2. 深度分页为了理解深度分页为什么是有问题的，我们假设在一个有5个主分片的索引中搜索。当我们请求结果的第一页（结果从1到10），每个分片产生自己的前10个结果，并且返回给协调节点 ，协调节点对所有50个结果进行排序，最终返回全部结果的前10个。 现在假设我们请求第1000页的数据–结果从10001到10010。除了每个分片不得不产生前10010个数据以外，其他的都跟上面查询第一页一样。协调节点对全部5个分片的50050个数据进行排序，最后丢弃掉这其中的50040个(只要10个)。 你可以看到，在分布式系统中，排序结果的成本以指数级增长。好消息是，网页搜索引擎一般不会为任何查询返回超过1,000个结果。 Elasticsearch版本:2.x 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/pagination.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 空搜索与多索引多类型搜索]]></title>
    <url>%2Felasticsearch-empty-search-and-multi-index-multi-type-search.html</url>
    <content type="text"><![CDATA[1. 空搜索测试数据: https://gist.github.com/clintongormley/8579281 1.1 搜索最基本的搜索API是空搜索(empty search)，它没有指定任何的查询条件，只返回集群索引中的所有文档： curl -XGET &apos;localhost:9200/_search?pretty&apos; Java版本:SearchRequestBuilder searchRequestBuilder = client.prepareSearch();SearchResponse response = searchRequestBuilder.get(); 返回的结果如下：&#123; "took": 4, "timed_out": false, "_shards": &#123; "total": 10, "successful": 10, "failed": 0 &#125;, "hits": &#123; "total": 13, "max_score": 1.0, "hits": [ &#123; "_index": "gb", "_type": "tweet", "_id": "5", "_score": 1.0, "_source": &#123; "date": "2014-09-15", "name": "Mary Jones", "tweet": "However did I manage before Elasticsearch?", "user_id": 2 &#125; &#125;, &#123; "_index": "gb", "_type": "tweet", "_id": "13", "_score": 1.0, "_source": &#123; "date": "2014-09-23", "name": "Mary Jones", "tweet": "So yes, I am an Elasticsearch fanboy", "user_id": 2 &#125; &#125; // ... ] &#125;&#125; 1.2 hits返回结果中最重要的部分是 hits，它包含 total 字段来表示匹配到的文档总数，并且有一个 hits 数组包含所查询结果的前十个文档。 在 hits 数组中每个结果包含文档的 _index 、 _type 、 _id 以及 _source 字段。这意味着我们可以直接从返回的搜索结果中获取整个文档。这不像其他的搜索引擎，仅仅返回文档的ID，需要你自己单独去获取文档。 每个结果还有一个 _score 字段，这是一个相关性得分，它衡量了文档与查询文本的匹配程度。默认情况下，首先返回相关性最高的文档，就是说，返回文档是按照 _score 降序排列的。在这个例子中，我们没有指定任何查询，故所有的文档具有相同的相关性，因此对所有的结果都是中性的 _score 为1。 max_score 是文档与查询文本匹配度最高的 _score。 1.3 Tooktook 告诉我们整个搜索请求执行多少毫秒数。 1.4 Shards_shards 告诉我们参与查询的分片总数（total），有多少是成功的（successful），有多少的是失败的（failed）。 通常我们不希望分片失败，但是还是有可能发生。如果我们遭受一些重大故障，导致同一分片的主分片和副本分片都丢失，那么这个分片就不会响应搜索请求。这种情况下，Elasticsearch 将报告这个分片failed，但仍将继续返回剩余分片上的结果。 1.5 Timeouttime_out 值告诉我们查询是否超时。默认情况下，搜索请求不会超时。如果低响应时间比完整结果更重要，你可以将超时指定为 10 或 10ms（10毫秒）或 1s（1秒）：curl -XGET &apos;localhost:9200/_search?timeout=10ms&apos; 在请求超时之前，ElasticSearch 将返回从每个分片收集到的任何结果。 应当注意的是 timeout 不是停止执行查询，仅仅是告知协调节点返回到目前为止收集到的结果并关闭连接。在后台，其他的分片可能仍在执行查询，即使结果已经发送了。 使用超时是因为对你的 SLA(服务等级协议)来说很重要的，而不是因为想去中止长时间运行的查询。 2. 多索引和多类型搜索如果不对我们的搜索做出特定索引或者特定类型的限制，就会搜索集群中的所有文档。Elasticsearch 将搜索请求并行转发到每一个主分片或者副本分片上，收集结果以选择全部中的前10名，并且返回给我们。 但是，通常，我们希望在一个或多个特定索引中搜索，也可能需要在一个或多个特定类型上搜索。我们可以通过在 URL 中指定索引和类型来执行此操作，如下所示： 搜索 描述 /_search 在所有的索引中对所有类型进行搜索 /gb/_search 在gb索引中对所有类型进行搜索 /gb,us/_search 在gb和us索引中对所有类型进行搜索 /g*,u*/_search 在以g或者u开头的索引中对所有类型进行搜索 /gb/user/_search 在gb索引中对user类型进行搜索 /gb,us/user,tweet/_search 在gb和us索引中对user和tweet类型进行搜索 /_all/user,tweet/_search 在所有的索引中对user和tweet类型进行搜索 Java版本:SearchRequestBuilder searchRequestBuilder = client.prepareSearch();searchRequestBuilder.setIndices("*index");SearchResponse response = searchRequestBuilder.get(); 从下面源码中，我们可以知道，设置索引和类型的方法参数是可变参数，因此我们可以设置多个索引或者类型。public SearchRequestBuilder setIndices(String... indices) &#123; request.indices(indices); return this;&#125;public SearchRequestBuilder setTypes(String... types) &#123; request.types(types); return this;&#125; 当在单个索引中搜索时，Elasticsearch 将搜索请求转发到该索引中每个分片的主分片或副本分片上，然后从每个分片收集结果。在多个索引中搜索的方式完全相同 - 只是会涉及更多的分片。 搜索一个具有五个主分片的索引完全等同于搜索每个具有一个主分片的五个索引。 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/empty-search.html https://www.elastic.co/guide/en/elasticsearch/guide/2.x/multi-index-multi-type.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 内置分析器]]></title>
    <url>%2Felasticsearch-built-in-analyzer.html</url>
    <content type="text"><![CDATA[1. 配置内置分析器内置分析器可以直接使用，不需任何配置。然而，其中一些分析器支持可选配置来改变其行为。例如，标准分析器可以配置为支持停止词列表：curl -XPUT 'localhost:9200/my_index?pretty' -H 'Content-Type: application/json' -d'&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "std_english": &#123; "type": "standard", "stopwords": "_english_" &#125; &#125; &#125; &#125;, "mappings": &#123; "my_type": &#123; "properties": &#123; "my_text": &#123; "type": "text", "analyzer": "standard", "fields": &#123; "english": &#123; "type": "text", "analyzer": "std_english" &#125; &#125; &#125; &#125; &#125; &#125;&#125;'curl -XPOST 'localhost:9200/my_index/_analyze?pretty' -H 'Content-Type: application/json' -d'&#123; "field": "my_text", "text": "The old brown cow"&#125;'curl -XPOST 'localhost:9200/my_index/_analyze?pretty' -H 'Content-Type: application/json' -d'&#123; "field": "my_text.english", "text": "The old brown cow"&#125;' 我们基于标准分析器定义一个std_english分析器，同时配置删除预定义的英文词汇表:"analyzer": &#123; "std_english": &#123; "type":"standard", "stopwords": "_english_" &#125;&#125; my_text 字段直接使用标准分析器，没有任何配置："my_text": &#123; "type": "text", "analyzer": "standard"&#125; 因此，这个字段不会删除停用词。所得的词条为：[ the, old, brown, cow ] my_text.english 字段使用std_english分析器："my_text": &#123; "type": "text", "analyzer": "standard", "fields": &#123; "english": &#123; "type":"text", "analyzer": "std_english" &#125; &#125;&#125; 因此会删除停用词。得出的结果是:[ old, brown, cow ] 2. 标准分析器(Standard Analyzer)如果没有指定分析器，默认使用 standard 分析器。对于文本分析，它对于任何语言都是最佳选择（对于任何一个国家的语言，这个分析器基本够用）。它根据Unicode Consortium定义的单词边界(word boundaries)来切分文本，然后去掉大部分标点符号。最后，把所有词转为小写。 2.1 定义standard 分析器包含一下内容： (1) 分词器(Tokenizer) ：Standard Tokenizer (2) 分词过滤器(Token Filters) ： Standard Token Filter Lower Case Token Filter top Token Filter (默认情况下禁用) 2.2 输出Examplecurl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'&#123; "analyzer": "standard", "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."&#125;' Java版本:String text = "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.";String analyzer = "standard";IndicesAdminClient indicesAdminClient = client.admin().indices();AnalyzeRequestBuilder analyzeRequestBuilder = indicesAdminClient.prepareAnalyze(text);analyzeRequestBuilder.setAnalyzer(analyzer);AnalyzeResponse response = analyzeRequestBuilder.get(); 上述句子将产生以下词条：[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog&apos;s, bone ] 2.3 配置standard 分析器接受以下参数： 参数 说明 max_token_length 最大token长度。如果一个token超过此长度，则以max_token_length进行分割。默认为255。 stopwords 预定义的停用词列表，如_english_或包含一组停用词的数组。 默认为\ _none_。 stopwords_path 包含停用词文件的路径。 有关停用词配置的更多信息，请参阅Stop Token Filter。 2.4 配置Example在此示例中，我们将 standard 分析器配置max_token_length为5（用于演示目的），并使用预定义的英文停用词列表：curl -XPUT 'localhost:9200/my_index?pretty' -H 'Content-Type: application/json' -d'&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "my_english_analyzer": &#123; "type": "standard", "max_token_length": 5, "stopwords": "_english_" &#125; &#125; &#125; &#125;&#125;'curl -XPOST 'localhost:9200/my_index/_analyze?pretty' -H 'Content-Type: application/json' -d'&#123; "analyzer": "my_english_analyzer", "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."&#125;' 上述示例产生以下词条：[ 2, quick, brown, foxes, jumpe, d, over, lazy, dog&apos;s, bone ] 3. 简单分析器(Simple Analyzer)只要遇到不是字母的字符，简单的分析器将文本进行切割分解为terms。 所有terms都是小写。 3.1 定义分词器(Tokenizer) : Lower Case Tokenizer 3.2 输出Examplecurl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'&#123; "analyzer": "simple", "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."&#125;' 以上示例产生如下词条：[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ] 3.3 配置无 4. 空格分析器（Whitespace analyzer）空白分析器在遇到空格字符时将文本切分成词条。 4.1 定义分析器(Tokenizer) : Whitespace Tokenizer 4.2 输出Examplecurl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'&#123; "analyzer": "whitespace", "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."&#125;' 以上示例产生如下词条：[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog&apos;s, bone. ] 4.3 配置无 Elasticsearch 版本：5.4 原文: https://www.elastic.co/guide/en/elasticsearch/reference/5.4/configuring-analyzers.html https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-standard-analyzer.html https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-simple-analyzer.html https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis-whitespace-analyzer.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch Analysis分析]]></title>
    <url>%2Felasticsearch-how-to-use-analysis.html</url>
    <content type="text"><![CDATA[分析(analysis)是将文本（如任何电子邮件的正文）转换为添加到倒排索引中进行搜索的tokens或terms的过程。分析由 analyzer 分析器执行，分析器可以是内置分析器或者每个索引定制的自定义分析器。 1. 索引时分析(Index time analysis)例如，在索引时，内置的英文分析器将会对下面句子进行转换：&quot;The QUICK brown foxes jumped over the lazy dog!&quot; 转换为添加到倒排索引中的词条如下：[ quick, brown, fox, jump, over, lazi, dog ] 1.1 指定索引时分析器映射中的每个text字段都可以指定自己的分析器：curl -XPUT 'localhost:9200/my_index?pretty' -H 'Content-Type: application/json' -d'&#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "title": &#123; "type": "text", "analyzer": "standard" &#125; &#125; &#125; &#125;&#125;' 在索引时，如果没有指定分析器，则会在索引设置中查找一个叫做default的分析器。如果没有找到，默认使用标准分析器 standard analyzer。 2. 搜索时分析(Search time analysis)同样的分析过程也可以应用于进行全文检索搜索(例如 match query 匹配查询)时，将查询字符串的文本转换为与存储在倒排索引中相同形式的词条。 例如，用户可能搜索：&quot;a quick fox&quot; 这将由相同的英语分析器分析为以下词条(上面索引时举例使用的是英语分析器，如果不使用相同的分析器，有可能搜不到正确的结果)：[ quick, fox ] 即使在查询字符串中使用的确切单词不会出现在原始存储文本（quick vs QUICK，fox vs foxes）中，查询字符串中的词条也能够完全匹配到倒排索引中的词条，因为我们已将相同的分析器应用于文本和查询字符串上，这意味着此查询将与我们的示例文档匹配。 2.1 指定搜索时分析器通常情况下，在索引时和搜索时应该使用相同的分析器，全文查询(例如匹配查询 match query)将根据映射来查找用于每个字段的分析器。 用于搜索特定字段的分析器由一下流程决定： 在查询中指定的分析器。 search_analyzer 映射参数。 analyzer 映射参数。 索引设置中的default_search分析器。 索引设置中的default分析器。 standard 标准分析器。 ElasticSearch版本 5.4 原文：https://www.elastic.co/guide/en/elasticsearch/reference/5.4/analysis.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 分析与分析器]]></title>
    <url>%2Felasticsearch-how-to-use-analysis-and-analyser.html</url>
    <content type="text"><![CDATA[1. 分析过程分析(analysis)过程如下： 首先，将一个文本块划分为适用于倒排索引的独立的词条(term) 然后对这些词进行标准化，提高它们的’可搜索性’或’查全率’上面的工作就是由分析器(Analyzer)来完成的。 2. 分析器组成分析器（Analyzer） 一般由三部分构成，字符过滤器（Character Filters）、分词器（Tokenizers）、分词过滤器（Token filters）。 2.1 字符过滤器首先字符串要按顺序依次经过几个字符过滤器(Character Filter)。它们的任务就是在分词前对字符串进行一次处理。字符过滤器能够剔除HTML标记，或者转换 &amp; 为 and。 2.2 分词器下一步，字符串经过分词器(Tokenizers)被分词成独立的词条(term)。一个简单的分词器可以根据空格或逗号将文本分成词条。 2.3 分词过滤器最后，每个词条都要按顺序依次经过几个分词过滤器(Token Filters)。这个过程可能会改变词条（例如，将 Quick 转为小写），删除词条（例如，删除像 a、and、the 这样的停用词），或者增加词条（例如，像 jump 和 leap 这样的同义词）。 Elasticsearch提供很多开箱即用的字符过滤器，分词器和分词过滤器。这些可以组合起来创建自定义的分析器以应对不同的需求。 3. 内建分析器不过，Elasticsearch还内置了一些分析器，可以直接使用它们。下面我们列出了几个比较重要的分析器，并演示它们有啥差异。我们看看每个分析器会从下面的字符串得到哪些词条：Set the shape to semi-transparent by calling set_trans(5) 3.1 标准分析器（Standard analyzer）标准分析器是 Elasticsearch 默认使用的分析器。对于文本分析，它对于任何语言都是最佳选择（对于任何一个国家的语言，这个分析器基本够用）。它根据Unicode Consortium定义的单词边界(word boundaries)来切分文本，然后去掉大部分标点符号。最后，把所有词条转为小写。 String standardAnalyzer = "standard";String value = "Set the shape to semi-transparent by calling set_trans(5)";AnalyzeAPI.analyzeByAnalyzer(client, standardAnalyzer, value); 产生的结果为：set, the, shape, to, semi, transparent, by, calling, set_trans, 5 AnalyzeAPI.analyzeByAnalyzer 自定义的一个工具方法，具体实现请看下面介绍。 3.2 简单分析器（Simple analyzer）简单分析器将根据不是字母的任何字符来切分文本，然后将每个词条转为小写。String simpleAnalyzer = "simple";String value = "Set the shape to semi-transparent by calling set_trans(5)";AnalyzeAPI.analyzeByAnalyzer(client, simpleAnalyzer, value); 产生的结果为：set, the, shape, to, semi, transparent, by, calling, set, trans 3.3 空格分析器（Whitespace analyzer）空格分析器根据空格来切分文本。不会转换为小写。String whitespaceAnalyzer = "whitespace";String value = "Set the shape to semi-transparent by calling set_trans(5)";AnalyzeAPI.analyzeByAnalyzer(client, whitespaceAnalyzer, value); 产生结果为：Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 3.4 语言分析器（Language analyzers）特定语言分析器适用于很多语言。它们能够考虑到特定语言的特点。例如，english 分析器自带一套英语停用词库（像 and 或 the 这些与语义无关的通用词），分析器将会这些词移除。由于理解英语语法的规则，这个分词器可以提取英语单词的词干。 以英语分析器举例：String englishAnalyzer = "english";String value = "Set the shape to semi-transparent by calling set_trans(5)";AnalyzeAPI.analyzeByAnalyzer(client, englishAnalyzer, value); 产生结果为：set, shape, semi, transpar, call, set_tran, 5 注意 transparent、calling 和 set_trans 是如何转为词干的。 4. 什么时候使用分析器当我们索引一个文档时，全文字段会被分析为单独的词条来创建倒排索引。不过，当我们在全文字段搜索(search)时，我们要让查询字符串经过同样的分析流程处理，以确保这些词条在索引中存在。理解每个字段是如何定义的，这样才可以让它们做正确的事： 当你查询全文(full text)字段，查询将使用相同的分析器来分析查询字符串，以产生正确的词条列表。 当你查询一个确切值(exact value)字段，查询将不分析查询字符串，但是你可以自己指定。 5. 测试分析器尤其当你是Elasticsearch新手时，对于如何分词以及存储到索引中理解起来比较困难。为了更好的理解如何进行，你可以使用analyze API来查看文本是如何被分析的。在查询中指定要使用的分析器，以及被分析的文本。 // 使用分词器进行词条分析public static void analyzeByAnalyzer(Client client, String analyzer, String value)&#123; IndicesAdminClient indicesAdminClient = client.admin().indices(); AnalyzeRequestBuilder analyzeRequestBuilder = indicesAdminClient.prepareAnalyze(value); analyzeRequestBuilder.setAnalyzer(analyzer); AnalyzeResponse response = analyzeRequestBuilder.get(); // 打印响应信息 List&lt;AnalyzeResponse.AnalyzeToken&gt; tokenList = response.getTokens(); for(AnalyzeResponse.AnalyzeToken token : tokenList)&#123; logger.info("-------- analyzeIndex type &#123;&#125;", token.getType()); logger.info("-------- analyzeIndex term &#123;&#125;", token.getTerm()); logger.info("-------- analyzeIndex position &#123;&#125;", token.getPosition()); logger.info("-------- analyzeIndex startOffSet &#123;&#125;", token.getStartOffset()); logger.info("-------- analyzeIndex endOffSet &#123;&#125;", token.getEndOffset()); logger.info("----------------------------------"); &#125;&#125; 或者使用如下方式：GET /_analyze&#123; "analyzer": "standard", "text": "Text to analyze"&#125; 结果中每个元素代表一个单独的词条：&#123; "tokens": [ &#123; "token": "text", "start_offset": 0, "end_offset": 4, "type": "&lt;ALPHANUM&gt;", "position": 1 &#125;, &#123; "token": "to", "start_offset": 5, "end_offset": 7, "type": "&lt;ALPHANUM&gt;", "position": 2 &#125;, &#123; "token": "analyze", "start_offset": 8, "end_offset": 15, "type": "&lt;ALPHANUM&gt;", "position": 3 &#125; ]&#125; token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。 6. 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串字段，自动设置它为全文string字段并用 standard 分析器分析。 你不希望总是这样。也许你想使用一个更适合这个数据的语言分析器。或者，你只想把字符串字段当作一个普通的字段，不做任何分析，只存储确切值，就像字符串类型的用户ID或者内部状态字段或者标签。为了达到这种效果，必须手动指定这些域的映射。 XContentBuilder mappingBuilder;try &#123; mappingBuilder = XContentFactory.jsonBuilder() .startObject() .startObject(type) .startObject("properties") .startObject("club").field("type", "string").field("index", "analyzed").field("analyzer", "english").endObject() .endObject() .endObject() .endObject();&#125; catch (Exception e) &#123; //...&#125; ElasticSearch版本：5.4 原文：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/analysis-intro.html#analysis-intro]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch Mapping映射]]></title>
    <url>%2Felasticsearch-how-to-use-mapping.html</url>
    <content type="text"><![CDATA[1. 概述为了能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理成全文本（Full-text）或精确（Exact-value）的字符串值，Elasticsearch需要知道每个字段里面都包含什么数据类型。这些类型和字段的信息都存储在映射（mapping）中。 索引中的每个文档都有一个 Type。每个 Type 拥有自己的 Mapping 或者模式定义。Mapping 在 Type 中定义字段，每个字段的数据类型，以及字段被Elasticsearch处理的方式。Mapping 还可用于设置关联到 Type 上的元数据。 例如下面的Mapping："mppings":&#123; "football-type": &#123; "properties": &#123; "country": &#123; "index": "not_analyzed", "store": true, "type": "string" &#125;, "club": &#123; "store": true, "type": "string" &#125;, "name": &#123; "store": true, "type": "string" &#125; &#125; &#125;&#125; 2. 核心字段类型Elasticsearch支持以下简单字段类型： 类型 数据类型 String string Whole number byte short integer long Floating point float double Boolean boolean Date date 当你索引一个包含新字段的文档（之前没有这个字段），Elasticsearch将根据JSON中的基本数据类型使用动态映射猜测字段的类型，基于使用以下规则： JsonType FieldType Boolean: true 或者 false “boolean” Whole number: 123 “long” Floating point:123.45 “double” String, valid date: “2017-05-04” “date” String: “foo bar” “string” 这意味着，如果你索引一个带引号的数字，例如，&quot;123&quot;，会被映射为 string 类型，而不是 long 类型。然而，如果字段已经被映射为 long 类型，Elasticsearch尝试将字符串转换为 long 类型，如果强制转换失败则会抛出异常。 举例，索引下面信息（之前没有mapping）：Map&lt;String, Object&gt; map = Maps.newHashMap();map.put("name", "C罗");map.put("sex", true);map.put("age", 31);map.put("birthday", "1985-02-05");map.put("club", "皇家马德里俱乐部"); 产生的mapping结果：&#123; "properties": &#123; "birthday": &#123; "format": "strict_date_optional_time||epoch_millis", "type": "date" &#125;, "sex": &#123; "type": "boolean" &#125;, "club": &#123; "type": "string" &#125;, "name": &#123; "type": "string" &#125;, "age": &#123; "type": "long" &#125; &#125;&#125; 3. 查看映射我们可以使用mapping API来查看Elasticsearch中的映射：IndicesAdminClient indicesAdminClient = client.admin().indices();GetMappingsRequestBuilder getMappingsRequestBuilder = indicesAdminClient.prepareGetMappings(index);GetMappingsResponse response = getMappingsRequestBuilder.get();// 结果for(ObjectCursor&lt;String&gt; key : response.getMappings().keys())&#123; ImmutableOpenMap&lt;String, MappingMetaData&gt; mapping = response.getMappings().get(key.value); for(ObjectCursor&lt;String&gt; key2 : mapping.keys())&#123; try &#123; logger.info("------------- &#123;&#125;", mapping.get(key2.value).sourceAsMap().toString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果：14:45:26.453 [main] INFO com.sjf.open.api.indexAPI.IndexAPI - ------------- &#123;properties=&#123;age=&#123;type=long&#125;, birthday=&#123;type=date, format=strict_date_optional_time||epoch_millis&#125;, club=&#123;type=string&#125;, name=&#123;type=string&#125;, sex=&#123;type=boolean&#125;&#125;&#125; 4. 自定义字段映射虽然大多数情况下基本数据类型已经能够满足，但你也会经常自定义单个字段的映射，特别是字符串类型字段。自定义类型可以使你完成一下几点： 区分全文（full text）字符串字段和准确字符串字段。比如字符串”北京”，全文字符串字段默认情况下会分成”北”和“京”两个词，但大多数情况下我们需要的是一个城市名称，不需要分词，所以它应该是一个确切的字符串字段，应该设置index属性为”not_analyzed”。 使用特定语言的分析器（不同国家语言断词方式不一样，不同国家的人可能会使用不同的设置） 优化部分匹配字段 指定自定义日期格式 4.1 typeMapping 中最重要的字段参数是type。对于不是 string 类型的字段，你可能很少需要映射除type之外的其他映射：&#123; "number_of_clicks": &#123; "type": "integer" &#125;&#125; string 类型的字段，默认的，考虑到包含全文本，它们的值在索引前要经过分析器分析，并且在此字段上进行全文搜索前要把查询字符串经过分析器的处理。 4.2 index对于 string 类型字段，其中最重要的一个映射参数是index，另一个是analyzer，下面会讲解到。 index参数控制字符串以何种方式被索引。它包含三种方式： 值 说明 analyzed 首先分析这个字符串，然后索引。换言之，以全文形式索引此字段。 not_analyzed 索引这个字段，使之可以被索引，但是索引内容和指定值一样。不分析此字段。 no 不索引这个字段。这个字段不能被搜索到。 对于 string 类型字段，index默认值为 analyzed。如果我们想把字段映射为确切值，我们需要设置为 not_analyzed：&#123; "tag": &#123; "type": "string", "index": "not_analyzed" &#125;&#125; 其他简单类型（long、double、date等等）也接受index参数，但相应的值只能是no和not_analyzed，它们的值不能是analyzed。 4.3 analyzer对于index为 analyzed 的字符串字段，使用 analyzer 参数来指定在搜索和索引的时候使用哪一种分析器。Elasticsearch默认使用 standard 分析器，但是你可以通过指定一个内建的分析器来更改它，例如可以指定 whitespace、simple或 english 等分析器。&#123; "tweet": &#123; "type": "string", "analyzer": "english" &#125;&#125; 5. 更新映射你可以在第一次创建索引的时候为 Type 指定 Mapping。此外，之后你可以为一个新 Type 添加 Mapping（或者为已有的 Type 更新 Mapping）。 你可以向已经存在的 Mapping 中增加字段，但是你不能修改已经存在的字段 Mapping。如果一个字段的映射已经存在，这可能意味着那个字段的数据已经被索引。如果你改变了字段映射，那已经被索引的数据可能会出现错误，不能被正确的搜索到。 我们可以更新一个 Mapping 来增加一个新字段，但是不能把已有字段的index属性从 analyzed 改到 not_analyzed。 在使用下面代码设置映射时，首先创建一个空的索引：// mappingXContentBuilder mappingBuilder;try &#123; mappingBuilder = XContentFactory.jsonBuilder() .startObject() .startObject(type) .startObject("properties") .startObject("name").field("type", "string").field("store", "yes").endObject() .startObject("sex").field("type", "string").field("store", "yes").endObject() .startObject("college").field("type", "string").field("store", "yes").endObject() .startObject("age").field("type", "long").field("store", "yes").endObject() .startObject("school").field("type", "string").field("store", "yes").field("index", "not_analyzed").endObject() .endObject() .endObject() .endObject();&#125; catch (Exception e) &#123; logger.error("--------- putIndexMapping 创建 mapping 失败：", e); return false;&#125;IndicesAdminClient indicesAdminClient = client.admin().indices();PutMappingRequestBuilder putMappingRequestBuilder = indicesAdminClient.preparePutMapping(index);putMappingRequestBuilder.setType(type);putMappingRequestBuilder.setSource(mappingBuilder);// 结果PutMappingResponse response = putMappingRequestBuilder.get(); 产生结果："mappings":&#123; "test-type": &#123; "properties": &#123; "college": &#123; "store": true, "type": "string" &#125;, "school": &#123; "index": "not_analyzed", "store": true, "type": "string" &#125;, "sex": &#123; "store": true, "type": "string" &#125;, "name": &#123; "store": true, "type": "string" &#125;, "age": &#123; "store": true, "type": "long" &#125; &#125; &#125;&#125; 下面我们再添加一个分析器为english的字段到上面映射中，我们新的字段会合并到上面已经存在的映射中。// mappingXContentBuilder mappingBuilder;try &#123; mappingBuilder = XContentFactory.jsonBuilder() .startObject() .startObject(type) .startObject("properties") .startObject("club").field("type", "string").field("index", "analyzed").field("analyzer", "english").endObject() .endObject() .endObject() .endObject();&#125; catch (Exception e) &#123; logger.error("--------- putIndexMapping 创建 mapping 失败：", e); return false;&#125;``` 产生结果：```json"mappings":&#123; "test-type": &#123; "properties": &#123; "college": &#123; "store": true, "type": "string" &#125;, "school": &#123; "index": "not_analyzed", "store": true, "type": "string" &#125;, "sex": &#123; "store": true, "type": "string" &#125;, "club": &#123; "analyzer": "english", "type": "string" &#125;, "name": &#123; "store": true, "type": "string" &#125;, "age": &#123; "store": true, "type": "long" &#125; &#125; &#125;&#125; Elasticsearch版本： 5.4 参考：https://www.elastic.co/guide/en/elasticsearch/guide/2.x/mapping-intro.html]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 索引]]></title>
    <url>%2Felasticsearch-how-to-indexing.html</url>
    <content type="text"><![CDATA[1. 背景假设我们刚好在一家工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，它有以下不同的需求： 数据能够包含多个值的标签、数字和纯文本。 检索任何员工的所有信息。 支持结构化搜索，例如查找30岁以上的员工。 支持简单的全文搜索和更复杂的短语(phrase)搜索。 高亮搜索结果中的关键字。 能够利用图表管理分析这些数据。 2. 索引员工文档我们首先要做的是存储员工数据，每个文档代表一个员工。在Elasticsearch中存储数据的行为就叫做索引(indexing)，不过在索引之前，我们需要明确数据应该存储在哪里。 在Elasticsearch中，文档归属于一种 type (类型)，而这些 type 存在于 index (索引)中，我们可以画一个简单的对比图来类比传统关系型数据库：Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields Elasticsearch集群可以包含多个 index （数据库），每一个 index 可以包含多个 types （表），每一个 type 又可以包含多个文档（行），然后每个文档包含多个字段（列）。 你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分: 索引（名词）：如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。 索引（动词）：索引一个文档，表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引：传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。 默认情况下，文档中的所有字段都会被索引（拥有一个倒排索引），只有这样他们才是可被搜索的。 3. 创建创建员工目录，我们将进行如下操作： 为每个员工的文档建立索引，每个文档包含了相应员工的所有信息（每个员工一个文档）。 每个文档的 type 为 employee。 employee 归属的索引为 company。 company 存储在Elasticsearch集群中。 实际上这些都是很容易的（尽管看起来有许多步骤）。我们能通过一个命令执行完成的操作：curl -XPUT 'localhost:9200/company/employee/1' -d '&#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests": [ "sports", "music" ]&#125;' 输出信息：&#123; "_index": "company", "_type": "employee", "_id": "1", "_version": 1, "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125; 我们看到路径: /company/employee/1 包含三部分信息： 名字 说明 company 索引名 employee 类型名 1 员工的ID 请求JSON实体（文档），包含了这个员工的所有信息。他的名字叫“John Smith”，25岁，喜欢攀岩。 让我们比较舒服的是它不需要你做额外的管理操作，比如创建索引或者定义每个字段的数据类型。我们能够直接索引文档，Elasticsearch已经内置所有的缺省设置，所有管理操作都是透明的。 接下来，让我们在目录中加入更多员工信息：curl -XPUT 'localhost:9200/company/employee/2' -d '&#123; "first_name" : "li", "last_name" : "chen", "age" : 29, "about" : "I love to sing", "interests": [ "eat", "music" ]&#125;';curl -XPUT 'localhost:9200/company/employee/3' -d '&#123; "first_name" : "gao", "last_name" : "lin", "age" : 22, "about" : "no no no ", "interests": [ "eat", "footbal" ]&#125;';curl -XPUT 'localhost:9200/company/employee/4' -d '&#123; "first_name" : "li", "last_name" : "yuan", "age" : 19, "about" : "no haha", "interests": [ "basketball", "music" ]&#125;';curl -XPUT 'localhost:9200/company/employee/5' -d '&#123; "first_name" : "li", "last_name" : "yuan", "age" : 19, "about" : "I like playing basketball ", "interests": [ "basketball", "music" ]&#125;';curl -XPUT 'localhost:9200/company/employee/6' -d '&#123; "first_name" : "li", "last_name" : "yuan", "age" : 19, "about" : "I like playing football ", "interests": [ "football", "music" ]&#125;';]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>ElasticSearch 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch Java API之索引文档]]></title>
    <url>%2Felasticsearch-java-api-index-doc.html</url>
    <content type="text"><![CDATA[Index API 允许我们存储一个JSON格式的文档，使数据可以被搜索。文档通过index、type、id唯一确定。我们可以自己提供一个id，或者也使用Index API 为我们自动生成一个。 这里有几种不同的方式来产生JSON格式的文档(document)： 手动方式，使用原生的byte[]或者String 使用Map方式，会自动转换成与之等价的JSON 使用第三方库来序列化beans，如Jackson 使用内置的帮助类 XContentFactory.jsonBuilder() 1. 手动方式// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(json);indexRequestBuilder.setTTL(8000);// 执行IndexResponse indexResponse = indexRequestBuilder.get(); 测试，下面代码存储梅西信息到索引为football-index，类型为football-type，id为1的文档中：String index = "football-index";String type = "football-type";String id = "1";String json = "&#123;" + "\"club\":\"巴萨罗那\"," + "\"country\":\"阿根廷\"," + "\"name\":\"梅西\"" + "&#125;"; 2. Map方式// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(map);indexRequestBuilder.setTTL(8000);// 执行IndexResponse indexResponse = indexRequestBuilder.get(); 测试，下面代码存储穆勒信息到索引为football-index，类型为football-type，id为2的文档中：String index = "football-index";String type = "football-type";String id = "2";Map&lt;String, Object&gt; map = Maps.newHashMap();map.put("name", "穆勒");map.put("club", "拜仁慕尼黑俱乐部");map.put("country", "德国"); 3. 序列化方式// Bean转换为字节ObjectMapper mapper = new ObjectMapper();byte[] json;try &#123; json = mapper.writeValueAsBytes(bean);&#125; catch (JsonProcessingException e) &#123; logger.error("---------- json 转换失败 Bean:&#123;&#125;", bean.toString()); return false;&#125;// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(json);indexRequestBuilder.setTTL(8000);// 执行IndexResponse response = indexRequestBuilder.get(); 测试，下面代码存储卡卡信息到索引为football-index，类型为football-type，id为3的文档中：String index = "football-index";String type = "football-type";String id = "3";FootballPlayer footballPlayer = new FootballPlayer();footballPlayer.setName("卡卡");footballPlayer.setClub("奥兰多城俱乐部");footballPlayer.setCountry("巴西"); 4. XContentBuilder帮助类方式ElasticSearch提供了一个内置的帮助类XContentBuilder来产生JSON文档// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(xContentBuilder);indexRequestBuilder.setTTL(8000);// 执行IndexResponse response = indexRequestBuilder.get(); 测试，下面代码存储托雷斯信息到索引为football-index，类型为football-type，id为4的文档中：String index = "football-index";String type = "football-type";String id = "4";XContentBuilder xContentBuilder;try &#123; xContentBuilder = XContentFactory.jsonBuilder(); xContentBuilder .startObject() .field("name", "托雷斯") .field("club", "马德里竞技俱乐部") .field("country", "西班牙") .endObject();&#125; catch (IOException e) &#123; logger.error("----------indexDocByXContentBuilder create xContentBuilder failed", e); return;&#125; 备注:你还可以通过startArray(string)和endArray()方法添加数组。.field()方法可以接受多种对象类型。你可以给它传递数字、日期、甚至其他XContentBuilder对象。 ElasticSearch版本:2.x 参考：https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.x/java-docs-index.html#java-docs-index-generate]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch安装与启动]]></title>
    <url>%2Felasticsearch-setup-and-run.html</url>
    <content type="text"><![CDATA[1. 检查JDK版本使用如下命令检验JDK版本：xiaosi@Qunar:~$ java -versionjava version &quot;1.7.0_40&quot;Java(TM) SE Runtime Environment (build 1.7.0_40-b43)Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)xiaosi@Qunar:~$ 如果你的JDK版本为1.7，有可能会遇到如下问题：xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearchException in thread &quot;main&quot; java.lang.RuntimeException: Java version: Oracle Corporation 1.7.0_40 [Java HotSpot(TM) 64-Bit Server VM 24.0-b56] suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JAVA_OPTS environment variable.Upgrading is preferred, this workaround will result in degraded performance. at org.elasticsearch.bootstrap.JVMCheck.check(JVMCheck.java:123) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:268) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)Refer to the log for complete error details. 主要原因是Elasticsearch至少需要Java 8. 2. 安装ElasticSearch2.1 下载检查JDK版本之后，我们可以下载并运行Elasticsearch。 二进制文件可以从 www.elastic.co/downloads 获取，过去版本也可以从中获取。 对于每个版本，您可以选择zip或tar存档，或DEB或RPM软件包。 为了简单起见，我们使用tar文件。 我们下载Elasticsearch 2.3.3 为例：xiaosi@Qunar:~$ curl -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.3/elasticsearch-2.3.3.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 26.2M 100 26.2M 0 0 190k 0 0:02:21 0:02:21 --:--:-- 211k 具体的下载版本可以查看：https://www.elastic.co/downloads/elasticsearch 2.2 解压xiaosi@Qunar:~$ tar -zxvf elasticsearch-2.3.3.tar.gz -C /home/xiaosi/optelasticsearch-2.3.3/README.textileelasticsearch-2.3.3/LICENSE.txtelasticsearch-2.3.3/NOTICE.txtelasticsearch-2.3.3/modules/elasticsearch-2.3.3/modules/lang-groovy/elasticsearch-2.3.3/modules/reindex/elasticsearch-2.3.3/modules/lang-expression/elasticsearch-2.3.3/modules/lang-groovy/plugin-security.policy 解压完之后如下：xiaosi@Qunar:~/opt$ cd elasticsearch-2.3.3/xiaosi@Qunar:~/opt/elasticsearch-2.3.3$ lsbin config lib LICENSE.txt modules NOTICE.txt README.textile 2.3 启动2.3.1 启动帮助xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearch -helpNAME start - Start ElasticsearchSYNOPSIS elasticsearch startDESCRIPTION This command starts Elasticsearch. You can configure it to run in the foreground, write a pid file and configure arbitrary options that override file-based configuration.OPTIONS -h,--help Shows this message -p,--pidfile &lt;pidfile&gt; Creates a pid file in the specified path on start -d,--daemonize Starts Elasticsearch in the background -Dproperty=value Configures an Elasticsearch specific property, like -Dnetwork.host=127.0.0.1 --property=value Configures an elasticsearch specific property, like --network.host 127.0.0.1 --property value NOTE: The -d, -p, and -D arguments must appear before any --property arguments.xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ 2.3.2 启动xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearch[2016-06-23 22:06:36,034][INFO ][node ] [Venom] version[2.3.3], pid[21245], build[218bdf1/2016-05-17T15:40:04Z][2016-06-23 22:06:36,035][INFO ][node ] [Venom] initializing ...[2016-06-23 22:06:36,475][INFO ][plugins ] [Venom] modules [reindex, lang-expression, lang-groovy], plugins [], sites [][2016-06-23 22:06:36,493][INFO ][env ] [Venom] using [1] data paths, mounts [[/ (/dev/sda7)]], net usable_space [9.6gb], net total_space [41.5gb], spins? [no], types [ext4][2016-06-23 22:06:36,493][INFO ][env ] [Venom] heap size [990.7mb], compressed ordinary object pointers [true][2016-06-23 22:06:38,041][INFO ][node ] [Venom] initialized[2016-06-23 22:06:38,041][INFO ][node ] [Venom] starting ...[2016-06-23 22:06:38,097][INFO ][transport ] [Venom] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[2016-06-23 22:06:38,102][INFO ][discovery ] [Venom] elasticsearch/CCQnbPSBQQmVK3c8f4CbHg[2016-06-23 22:06:41,167][INFO ][cluster.service ] [Venom] new_master &#123;Venom&#125;&#123;CCQnbPSBQQmVK3c8f4CbHg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: zen-disco-join(elected_as_master, [0] joins received)[2016-06-23 22:06:41,190][INFO ][http ] [Venom] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2016-06-23 22:06:41,190][INFO ][node ] [Venom] started[2016-06-23 22:06:41,232][INFO ][gateway ] [Venom] recovered [0] indices into cluster_state 2.3.3 验证在浏览器中输入：http://localhost:9200/ （elasticsearch默认端口号为9200）&#123;name: &quot;Ulysses&quot;,cluster_name: &quot;elasticsearch&quot;,version: &#123;number: &quot;2.3.3&quot;,build_hash: &quot;218bdf10790eef486ff2c41a3df5cfa32dadcfde&quot;,build_timestamp: &quot;2016-05-17T15:40:04Z&quot;,build_snapshot: false,lucene_version: &quot;5.5.0&quot;&#125;,tagline: &quot;You Know, for Search&quot;&#125; health状况：xiaosi@Qunar:~$ curl &apos;localhost:9200/_cat/health?v&apos;epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1466691011 22:10:11 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0%xiaosi@Qunar:~$ 备注：（status）绿色表示一切是好的(集群功能齐全)黄色意味着所有数据是可用的，但是一些副本尚未分配(集群功能齐全)红色意味着一些数据不可用即使一个集群是红色的,它仍然是部分功能(即它将继续搜索请求从服务可用的碎片)但是你可能需要尽快修复它,因为你有缺失的数据。 2.3.5 说明刚开始安装在/opt目录下，普通用户是没有权限的，在启动elasticsearch时会告诉你权限不够：xiaosi@Qunar:~$ cd /opt/elasticsearch-2.3.3/xiaosi@Qunar:/opt/elasticsearch-2.3.3$ cd bin/xiaosi@Qunar:/opt/elasticsearch-2.3.3/bin$ ./elasticsearchlog4j:ERROR setFile(null,true) call failed.java.io.FileNotFoundException: /opt/elasticsearch-2.3.3/logs/elasticsearch.log (权限不够) at java.io.FileOutputStream.open0(Native Method) at java.io.FileOutputStream.open(FileOutputStream.java:270) 权限不够，那就用root用户去启动elasticsearch，会告诉你不能使用root用户来启动elasticsearch：xiaosi@Qunar:/opt/elasticsearch-2.3.3/bin$ sudo ./elasticsearchException in thread &quot;main&quot; java.lang.RuntimeException: don&apos;t run elasticsearch as root. at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:93) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270) at org.elasticsearch.bootstrap. 所以只好安装在自己的目录下。对此不知道有什么解决之道？ 3. 安装Kibana3.1 下载地址https://www.elastic.co/downloads/kibana 备注：Kibana 4.5.x requires Elasticsearch 2.3.x 3.2 下载xiaosi@Qunar:~/opt$ curl -L -O https://download.elastic.co/kibana/kibana/kibana-4.5.1-linux-x64.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 31.5M 100 31.5M 0 0 136k 0 0:03:56 0:03:56 --:--:-- 214k 3.3 解压tar -zxvf kibana-4.5.1-linux-x64.tar.gz 4. 安装MarvelMarvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了一个叫做Sense的交互式控制台，使用户方便的通过浏览器直接与Elasticsearch进行交互。Elasticsearch线上文档中的很多示例代码都附带一个View in Sense的链接。点击进去，就会在Sense控制台打开相应的实例。安装Marvel不是必须的。 4.1 失败方法 （已丢弃）Marvel是一个插件，可在Elasticsearch目录中运行以下命令来下载和安装：./bin/plugin -i elasticsearch/marvel/latest 但是在运行时遇到如下问题，没有找到 -i 命令：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ ./bin/plugin -i elasticsearch/marvel/latestERROR: unknown command [-i]. Use [-h] option to list available commands 提示可以使用-h 参数去查看可以使用的命令：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ ./bin/plugin -hNAME plugin - Manages pluginsSYNOPSIS plugin &lt;command&gt;DESCRIPTION Manage pluginsCOMMANDS install Install a plugin remove Remove a plugin list List installed pluginsNOTES [*] For usage help on specific commands please type &quot;plugin &lt;command&gt; -h&quot; 我们可以看到我们可以使用install命令来代替-i参数命令：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install elasticsearch/marvel/latest-&gt; Installing elasticsearch/marvel/latest...Trying https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip ...Trying https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip ...Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip ...Trying https://github.com/elasticsearch/marvel/archive/latest.zip ...Trying https://github.com/elasticsearch/marvel/archive/master.zip ...ERROR: failed to download out of all possible locations..., use --verbose to get detailed information 按照提示使用–verbose，查看报错原因：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install elasticsearch/marvel/latest --verbose-&gt; Installing elasticsearch/marvel/latest...Trying https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip ...Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip];Trying https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip ...Failed: FileNotFoundException[https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip]; nested: FileNotFoundException[https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip];Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip ...Failed: FileNotFoundException[https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip]; nested: FileNotFoundException[https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip];Trying https://github.com/elasticsearch/marvel/archive/latest.zip ...Failed: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/latest.zip]; nested: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/latest.zip];Trying https://github.com/elasticsearch/marvel/archive/master.zip ...Failed: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/master.zip]; nested: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/master.zip];ERROR: failed to download out of all possible locations..., use --verbose to get detailed information 4.2 正确方法上面方法已经不在适用了（不知道还有没有解决方法？），现在marvel用Kibana管理，所以第二步先安装Kibana。安装完进行如下操作：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install license-&gt; Installing license...Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.3.3/license-2.3.3.zip ...Downloading .......DONEVerifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.3.3/license-2.3.3.zip checksums if available ...Downloading .DONEInstalled license into /opt/elasticsearch-2.3.3/plugins/license xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install marvel-agent-&gt; Installing marvel-agent...Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.3.3/marvel-agent-2.3.3.zip ...Downloading ..........DONEVerifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.3.3/marvel-agent-2.3.3.zip checksums if available ...Downloading .DONE@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.lang.RuntimePermission setFactory* javax.net.ssl.SSLPermission setHostnameVerifierSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]yInstalled marvel-agent into /opt/elasticsearch-2.3.3/plugins/marvel-agent 使用Kibana安装最新版本的marvel：xiaosi@Qunar:~/opt/kibana-4.5.1-linux-x64/bin$ ./kibana plugin --install elasticsearch/marvel/latestInstalling marvelAttempting to transfer from https://download.elastic.co/elasticsearch/marvel/marvel-latest.tar.gzTransferring 1597693 bytes....................Transfer completeExtracting plugin archiveExtraction completeOptimizing and caching browser bundles...Plugin installation complete 5. 安装总结 步骤 命令 Step 1: 安装Marvel into Elasticsearch: bin/plugin install license和bin/plugin install marvel-agent Step 2: 安装Marvel into Kibana bin/kibana plugin –install elasticsearch/marvel/latest Step 3: 启动Elasticsearch and Kibana bin/elasticsearch和bin/kibana Step 4: 跳转到http://localhost:5601/app/marvel Step 5: 进入Getting Started Guide. 在没网的情况下运行集群? !imgoffline installation instructions. 6. 启动elasticsearch，kibana和Marvel6.1 启动elasticsearchxiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearch[2016-06-23 22:36:56,431][INFO ][node ] [Suprema] version[2.3.3], pid[21803], build[218bdf1/2016-05-17T15:40:04Z][2016-06-23 22:36:56,431][INFO ][node ] [Suprema] initializing ...[2016-06-23 22:36:56,895][INFO ][plugins ] [Suprema] modules [reindex, lang-expression, lang-groovy], plugins [], sites [][2016-06-23 22:36:56,913][INFO ][env ] [Suprema] using [1] data paths, mounts [[/ (/dev/sda7)]], net usable_space [9.6gb], net total_space [41.5gb], spins? [no], types [ext4][2016-06-23 22:36:56,913][INFO ][env ] [Suprema] heap size [990.7mb], compressed ordinary object pointers [true][2016-06-23 22:36:58,387][INFO ][node ] [Suprema] initialized[2016-06-23 22:36:58,387][INFO ][node ] [Suprema] starting ...[2016-06-23 22:36:58,464][INFO ][transport ] [Suprema] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[2016-06-23 22:36:58,468][INFO ][discovery ] [Suprema] elasticsearch/enfSRI_1RoORbk9eIqogQw[2016-06-23 22:37:01,546][INFO ][cluster.service ] [Suprema] new_master &#123;Suprema&#125;&#123;enfSRI_1RoORbk9eIqogQw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: zen-disco-join(elected_as_master, [0] joins received)[2016-06-23 22:37:01,569][INFO ][http ] [Suprema] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2016-06-23 22:37:01,572][INFO ][node ] [Suprema] started[2016-06-23 22:37:01,611][INFO ][gateway ] [Suprema] recovered [0] indices into cluster_state[2016-06-23 22:37:42,722][INFO ][cluster.metadata ] [Suprema] [.kibana] creating index, cause [api], templates [], shards [1]/[1], mappings [config][2016-06-23 22:37:43,056][INFO ][cluster.routing.allocation] [Suprema] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).[2016-06-23 22:37:45,803][INFO ][cluster.metadata ] [Suprema] [.kibana] create_mapping [index-pattern] 在浏览器中输入：http://localhost:9200/&#123; name: &quot;Suprema&quot;, cluster_name: &quot;elasticsearch&quot;, version: &#123; number: &quot;2.3.3&quot;, build_hash: &quot;218bdf10790eef486ff2c41a3df5cfa32dadcfde&quot;, build_timestamp: &quot;2016-05-17T15:40:04Z&quot;, build_snapshot: false, lucene_version: &quot;5.5.0&quot; &#125;, tagline: &quot;You Know, for Search&quot;&#125; 6.2 启动kibanaxiaosi@Qunar:~$ cd ~/opt/kibana-4.5.1-linux-x64/bin/xiaosi@Qunar:~/opt/kibana-4.5.1-linux-x64/bin$ ./kibana log [22:37:36.150] [info][status][plugin:kibana] Status changed from uninitialized to green - Ready log [22:37:36.177] [info][status][plugin:elasticsearch] Status changed from uninitialized to yellow - Waiting for Elasticsearch log [22:37:36.181] [info][status][plugin:marvel] Status changed from uninitialized to yellow - Waiting for Elasticsearch log [22:37:36.198] [info][status][plugin:kbn_vislib_vis_types] Status changed from uninitialized to green - Ready log [22:37:36.202] [info][status][plugin:markdown_vis] Status changed from uninitialized to green - Ready log [22:37:36.210] [info][status][plugin:metric_vis] Status changed from uninitialized to green - Ready log [22:37:36.218] [info][status][plugin:spyModes] Status changed from uninitialized to green - Ready log [22:37:37.455] [info][status][plugin:statusPage] Status changed from uninitialized to green - Ready log [22:37:37.462] [info][status][plugin:table_vis] Status changed from uninitialized to green - Ready log [22:37:37.468] [info][listening] Server running at http://0.0.0.0:5601 log [22:37:42.513] [info][status][plugin:elasticsearch] Status changed from yellow to yellow - No existing Kibana index found log [22:37:45.762] [info][status][plugin:elasticsearch] Status changed from yellow to green - Kibana index ready log [22:37:45.876] [info][status][plugin:marvel] Status changed from yellow to green - Marvel index ready 浏览器中输入： http://0.0.0.0:5601 6.3 启动Marvel浏览器中输入：http://localhost:5601/app/marvel 初步安装完毕，以后还需配置一些东西。。。。]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 文件处理]]></title>
    <url>%2FPython%2FPython%20%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[1. csv文件处理记录中的字段通常由逗号分隔，但其他分隔符也是比较常见的，例如制表符（制表符分隔值，TSV）、冒号、分号和竖直条等。建议在自己创建的文件中坚持使用逗号作为分隔符，同时保证编写的处理程序能正确处理使用其他分隔符的CSV文件。 备注:有时看起来像分隔符的字符并不是分隔符。通过将字段包含在双引号中，可确保字段中的分隔符只是作为变量值的一部分，不参与分割字段(如...,&quot;Hello, world&quot;,...)。 Python的csv模块提供了一个CSV读取器和一个CSV写入器。两个对象的第一个参数都是已打开的文本文件句柄(在下面的示例中，使用newline=’’选项打开文件，从而避免删除行的操作)。必要时可以通过可选参数delimiter和quotechar，提供默认的分隔符和引用字符。Python还提供了控制转义字符、行终止符等定界符的可选参数。with open("somefile.csv", newline='') as infile: reader = csv.reader(infile, delimiter=',', quotechar='"') CSV文件的第一条记录通常包含列标题，可能与文件的其余部分有所不同。这只是一个常见的做法，并非CSV格式本身的特性。 CSV读取器提供了一个可以在for循环中使用的迭代器接口。迭代器将下一条记录作为一个字符串字段列表返回。读取器不会将字段转换为任何数值数据类型，另外，除非传递可选参数skipinitialspace=True，否则不会删除前导的空白。 如果事先不知道CSV文件的大小，而且文件可能很大，则不宜一次性读取所有记录，而应使用增量的、迭代的、逐行的处理方式：读出一行，处理一行，再获取另一行。 CSV写入器提供writerow()和writerows()两个函数。writerow()将一个字符串或数字序列作为一条记录写入文件。该函数将数字转换成字符串，因此不必担心数值表示的问题。类似地，writerows()将字符串或数字序列的列表作为记录集写入文件。 在下面的示例中，使用csv模块从CSV文件中提取Answer.Age列。假设此列肯定存在，但列的索引未知。一旦获得数值，借助statistics模块就能得到年龄的平均值和标准偏差。 首先，打开文件并读取数据：with open("demographics.csv", newline='') as infile: data = list(csv.reader(infile)) 检查文件中的第一个记录 data[0] ，它必须包含感兴趣的列标题：ageIndex = data[0].index("Answer.Age") 最后，访问剩余记录中感兴趣的字段，并计算和显示统计数据：ages = [int(row[ageIndex]) for row in data[1:]]print(statistics.mean(ages), statistics.stdev(ages)) csv和statistics模块是底层的、快速而粗糙的工具。在第6章，你将了解如何在更为复杂的项目中使用pandas的数据frame，完成那些比对几列数据进行琐碎的检索要高端得多的任务。 2. Json文件处理需要注意的一点就是某些Python数据类型和结构(比如集合和复数)无法存储在JSON文件中。因此，要在导出到JSON之前，将它们转换为JSON可表示的数据类型。例如，将复数存储为两个double类型的数字组成的数组，将集合存储为一个由集合的各项所组成的数组。 将复杂数据存储到JSON文件中的操作称为JSON序列化，相应的反向操作则称为JSON反序列化。Python通过json模块中的函数，实现JSON序列化和反序列化。 函数 说明 dump() 将Python对象导出到文件中 dumps() 将Python对象编码成JSON字符串 load() 将文件导出为Python对象 loads() 将已编码的JSON字符串解码为Python对象 备注:把多个对象存储在一个JSON文件中是一种错误的做法，但如果已有的文件包含多个对象，则可将其以文本的方式读入，进而将文本转换为对象数组（在文本中各个对象之间添加方括号和逗号分隔符），并使用loads()将文本反序列化为对象列表。 Example: 以下代码片段实现了将任意（可序列化的）对象按先序列化、后反序列化的顺序进行处理：# 将Python对象编码成JSON字符串data = [&#123;'apple': 23, 'bear': 11, 'banana': 54&#125;]s = json.dumps(data)print type(s) # &lt;type 'str'&gt;print s # [&#123;"apple": 23, "bear": 11, "banana": 54&#125;]# 将Python对象编码成JSON字符串并格式化输出format_str = json.dumps(data, sort_keys=True, indent=4, separators=(',', ': '))print format_str'''[ &#123; "apple": 23, "banana": 54, "bear": 11 &#125;]'''# 将已编码的JSON字符串解码为Python对象data = '[&#123;"apple": 23, "bear": 11, "banana": 54&#125;]'o = json.loads(data)print type(o) # &lt;type 'list'&gt;print o[0].get('apple', 0) # 23# 将Python对象导出到文件中data = [&#123;'apple': 23, 'bear': 11, 'banana': 54&#125;]with open("/home/xiaosi/data.json", "w") as f_dump: s_dump = json.dump(data, f_dump, ensure_ascii=False)# 将文件导出为Python对象with open("/home/xiaosi/data.json", 'r') as f_load: ob = json.load(f_load)print type(ob) # &lt;type 'list'&gt;print ob[0].get('banana') # 54 备注:使用JSON函数需要导入json库：import json。 JSON 类型转换到 python 的类型对照表： JSON Python object dict array list string unicode number (int) int, long number (real) float true True false False null None 来自于:]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy 数组]]></title>
    <url>%2FPython%2FPython%20Numpy%20%E6%95%B0%E7%BB%84.html</url>
    <content type="text"><![CDATA[NumPy（Numeric Python，以numpy导入）是一系列高效的、可并行的、执行高性能数值运算的函数的接口。numpy模块提供了一种新的Python数据结构——数组(array)，以及特定于该结构的函数工具箱。该模块还支持随机数、数据聚合、线性代数和傅里叶变换等非常实用的数值计算工具。 下面将学习如何创建不同形状的numpy数组，基于不同的源创建numpy数组，数组的重排和切片操作，添加数组索引，以及对某些或所有数组元素进行算术运算、逻辑运算和聚合运算。 1. 创建数组numpy数组比原生的Python列表更为紧凑和高效，尤其是在多维的情况下。但与列表不同的是，数组的语法要求更为严格：数组必须是同构的。这意味着数组项不能混合使用不同的数据类型，而且不能对不同数据类型的数组项进行匹配操作。 创建numpy数组的方法很多。可以使用函数array()，基于类数组(array-like)数据创建数组。numpy基于数据本身推断出数组元素的类型，当然，你也可以给array()传递确定的dtype参数。numpy支持的数据类型接近二十种，例如bool_、int64、uint64、float64和&lt;U32（针对Unicode字符串）。 备注:所谓的类数组数据可以是列表、元组或另一个数组。 为获得较高的效率，numpy在创建一个数组时，不会将数据从源复制到新数组，而是建立起数据间的连接。也就是说，在默认情况下，numpy数组相当于是其底层数据的视图，而不是其副本。如果底层数据对象发生改变，则相应的数组数据也会随之改变。如果你不喜欢这种方式（这是默认的处理方式，除非复制的数据量过大），可以给构造函数传递copy=True。 备注:创建数组，不会将数据从源复制到新数组，相当于是其底层数据的视图，而不是其副本。 实际上，Python的”列表”(list)是以数组的方式实现的，而并非列表的方式，这与”列表”(list)的字面含义并不一致。由于未使用前向指针，所以Python并没有给列表预留前向指针的存储空间。Python的大型列表只比”真正的”numpy数组多使用约13%的存储空间，但对于一些简单的内置操作，比如sum()，使用列表则要比数组快五到十倍。因此在使用numpy之前，应该问问自己是否真的需要用到某些numpy特有的功能。 我们来创建第一个数组——前10个正整数组成的简单数组：import numpy as np# 简单数组numbers = np.array(range(1, 11), copy=True)print numbers # [ 1 2 3 4 5 6 7 8 9 10] 函数ones()、zeros()和empty()分别构造全1数组、全零数组和尚未初始化的数组。这些函数必须有数组的形状参数，该参数用一个与数组的维度相同的列表或元组来表征:# 给定数组形状shape与数据类型type 全1数组ones = np.ones([2, 4], dtype=np.float64)print ones'''[ [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]]'''# 给定数组形状shape与数据类型type 全0数组zeros = np.zeros([2, 4], dtype=np.float64)print zeros'''[ [ 0. 0. 0. 0.] [ 0. 0. 0. 0.]]'''# 给定数组形状shape与数据类型type 尚未初始化数组 其元素值不一定为零empty = np.empty([2, 4], dtype=np.float64)print empty'''[ [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]]''' numpy使用数组的ndim、shape和dtype属性分别存储数组的维数、形状和数据类型:# 只要没有经过变形(reshape) 该属性给出的就是数组的原始形状print ones.shape # (2, 4)# 等价于len(numbers.shape)print ones.ndim # 2# 数据类型print ones.dtype # float64 函数eye(N, M=None, k=0, dtype=np.float)用于构造一个N×M的眼形单位矩阵，其第k对角线上的值为1，其他地方的值为零。当k为正数时，对应的对角线位于主对角线上方的第k条。M为None（默认值）等价于M=N:# N×M的眼形单位矩阵eye = np.eye(3, k=1)print eye'''[ [ 0. 1. 0.] [ 0. 0. 1.] [ 0. 0. 0.]]''' 当需要将几个矩阵相乘时，可以使用单位矩阵作为乘法链累积器中的初始值。 除了经典的内置函数range()外，numpy有其独有的、更高效的生成等间隔数值数组的方式：函数arange([start,] stop [, step,], dtype=None)：# 等间隔数值数组double_numbers = np.arange(2, 5, 0.25)print double_numbers # [ 2. 2.25 2.5 2.75 3. 3.25 3.5 3.75 4. 4.25 4.5 4.75] numpy在创建数组时记录每一项的数据类型，不过该数据类型并非不可变的。可在数组创建后，调用函数astype(dtype, casting =&quot;unsafe&quot;, copy=True)来改变它。对于类型缩小的情况(将较抽象的数据类型转换为更具体的数据类型)，可能会丢失一些信息。这并非numpy特有的，任何缩小变换都可能会丢失信息:# 改变数组数据类型int_numbers = double_numbers.astype(np.int)print int_numbers # [2 2 2 2 3 3 3 3 4 4 4 4] 大多数numpy操作返回的是一个视图，而非原始数组的副本。为了保留原始数据，可使用copy()函数创建现有数组的副本。这样一来，对原始数组的任何更改都不会影响到副本。但如果数组较为庞大，比如有十亿个数组项，那就不要轻易进行复制:# 数组的副本double_numbers_copy = double_numbers.copy() 2. 转置和重排借助numpy可以很容易地改变数组的形状和方向，我们再也不用像“瞎猫踫到死耗子”那样看运气了。下面我们用几个标准普尔（S&amp;P）股票代码组成一个一维数组，然后用所有可能的方式改变它的形状： 来源于: Python数据科学入门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 字符串操作]]></title>
    <url>%2FPython%2FPython%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[字符串是 Python 中最常用的数据类型。我们可以使用引号(‘或”)来创建字符串。创建字符串很简单，只要为变量分配一个值即可。例如：s = &quot;Hello World&quot;print s # Hello World 1. 大小写转换函数大小写转换函数返回原始字符串s的一个副本： 函数 说明 lower() 将所有字符转换为小写 upper() 将所有字符转换为大写 capitalize() 将第一个字符转换为大写，同时将其他所有字符转换为小写 这些函数不会影响非字母字符。大小写转换函数是规范化的一个重要元素。 Example:s = "Hello World"print s # Hello World# 转为大写us = s.upper()print us # HELLO WORLD# 转为小写ls = s.lower()print ls # hello world# 首字母大写 其余小写cs = s.capitalize()print cs # Hello world 2. 判定函数判断函数根据字符串s是否属于适当的类而返回True或False： 函数 说明 islower() 检查所有字母字符是否为小写 isupper() 检查所有字母字符是否为大写 isspace() 检查所有字符是否为空格 isdigit() 检查所有字符是否为范围0～9中的十进制数字 isalpha() 检查所有字符是否为a～z或A～Z范围内的字母字符 使用这些函数，你可以识别有效的单词、非负整数、标点符号等。 Example:# 是否为大写ius = "HELLO".isupper()print ius # True# 是否为小写ils = "hello".islower()print ils # True# 是否为空格iss = " ".isspace()print iss # True# 是否为范围0～9中的十进制数字ids = "232".isdigit()print ids # True# 是否为a～z或A～Z范围内的字母字符ias = "a2".isalpha()print ias # False 3. 解码函数Python有时会将字符串数据表示为原始的二进制数组，而非字符串，尤其是当数据来自外部源（外部文件、数据库或Web）时。Python使用符号b来标识二进制数组。例如:# 二进制数组bin = b"Hello"# 字符串 s = "Hello"print bin[0]print s[0] s[0]和bin[0]分别是’H’和72，其中72是字符’H’的ASCII码。 解码函数将二进制数组转换为字符串或反之： 函数 说明 decode() 将二进制数组转换为字符串 encode() 将字符串转换为二进制数组 许多Python函数都需要将二进制数据转换为字符串，然后再做处理。 4. 去除空白函数字符串处理的第一步是去除不需要的空白(包括换行符和制表符)。 函数 说明 lstrip() left strip 在字符串的开始处删除所有空格 rstrip() right strip 在字符串的结束处删除所有空格 strip() 对整个字符串删除所有空格(不删除字符串内部空格) 经过这些删除操作后，得到的可能会是一个空字符串！ Example:ls = " Hello world ".lstrip()print ls + "," + str(len(ls)) # Hello world ,12rs = " Hello World ".rstrip()print rs + "," + str(len(rs)) # Hello World,12ss = " Hello World ".strip()print ss + "," + str(len(ss)) # Hello World,1 5. 分割函数字符串通常包含多个标记符，用空格、冒号和逗号这样的分隔符分割。函数split(delim=’’)使用delim作为分隔符，将字符串s分割为子字符串组成的一个列表。如果未指定分隔符，Python会使用空白字符来分割字符串，并将所有连续的空白合并：ss = "Hello World".split()print ss # ['Hello', 'World']ss = "Hello,World".split(",")print ss # ['Hello', 'World'] 6. 连接函数连接函数join(ls)，将字符串列表ls连接在一起，形成一个字符串，并使用特定的对象字符串作为连接符：s = ",".join("b")print s # bs = ",".join(["a", "b", "c", "d"])print s # a,b,c,d 备注:join()函数仅在字符串之间插入连接符，而在第一个字符串前或最后一个字符串后都不插入连接符。 7. 查找函数find(needle)函数返回对象字符串中子字符串needle第一次出现的索引值(下标从0开始)，当子字符串不存在时，返回-1。该函数区分大小写。index = &quot;Hello World&quot;.find(&quot;o&quot;)print index # 4 来自于]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 数据库操作]]></title>
    <url>%2FPython%2FPython%20%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[1. MySQLPython使用数据库驱动模块与MySQL通信。诸如pymysql等许多数据库驱动都是免费的。这里我们将使用pymysql，它是Anaconda的一部分。驱动程序经过激活后与数据库服务器相连，然后将Python的函数调用转换为数据库查询，反过来，将数据库结果转换为Python数据结构。 connect()函数需要以下信息：数据库(名称)、数据库服务器的位置(主机和端口号)和数据库用户(名称和密码)。如果数据库成功连接，则返回连接标识符。接下来，创建与数据库连接相关联的数据库游标：import pymysql# 连接数据库conn = pymysql.connect(host="localhost", port=3306, user="root", passwd="root", db="test")cur = conn.cursor() 备注:使用pymysql需要导入pymysql库：import pymysql。 游标的execute()函数向数据服务器提交要执行的查询命令，并返回受影响的行数(如果查询是非破坏性的，则返回零)。与命令行MySQL查询不同，pymysql查询语句不需要在结尾加上分号。# 查询query = ''' SELECT first_name, last_name FROM People ORDER BY dob LIMIT 3'''n_rows = cur.execute(query)print n_rows # 3 如果提交非破坏性查询(比如SELECT)，需要使用游标函数fetchall()获取所有匹配的记录。该函数返回一个生成器，可以将其转换为列字段的元组构成的列表：results = list(cur.fetchall())print results # [('gztAQV', 'aLhko'), ('ZXMtHd', 'cgwjI'), ('yHwDRF', 'NgBkY')] 如果查询是破坏性的(例如UPDATE、DELETE或INSERT)，则必须执行commit操作:# 修改update_query = ''' UPDATE People SET first_name = 'yoona' WHERE last_name = 'aLhko''''n_rows = cur.execute(update_query)print n_rows # 1conn.commit()select_query = ''' SELECT first_name, last_name FROM People WHERE last_name = 'aLhko''''cur.execute(select_query)results = list(cur.fetchall())print results # [('yoona', 'aLhko')] 备注:提供commit()函数的是连接本身(conn)，而不是游标(cur)。 2. MongoDB在Python中，我们用pymongo模块中MongoClient类的实例来实现MongoDB客户端。首先安装pymongo模块(ubuntu15.10):sudo pip install pymongo 下面就可以创建一个无参数的客户端(适用于典型的安装了本地服务器的情况)，也可以用服务器的主机名和端口号作为参数创建客户端，或使用服务器的统一资源标识符(URI)作为参数创建客户端：# 使用默认的初始化方式client1 = pymongo.MongoClient()# 指定主机和端口号client2 = pymongo.MongoClient("localhost", 27017)# 用URI方式指定主机和端口号client3 = pymongo.MongoClient("mongodb://localhost:27017/") 客户一旦端建立了与数据库服务器的连接，就可以选择当前激活的数据库，进而选择激活的集合。可以使用面向对象(“.”)或字典样式的符号。如果所选的数据库或集合不存在，服务器会立即创建它们：# 创建并选择活动数据库的两种方法db = client1.test_dbdb = client1["test_db"]# 创建并选择活动集合的两种方法people = db.peoplepeople = db["people"] pymongo模块用字典变量来表示MongoDB文档。表示对象的每个字典必须具有_id这个键。如果该键不存在，服务器会自动生成它。 集合对象提供用于在文档集合中插入、搜索、删除、更新、替换和聚合文档以及创建索引的功能。 函数insert_one(doc)和insert_many(docs)将文档或文档列表插入集合。它们分别返回对象InsertOneResult或InsertManyResult，这两个对象分别提供inserted_id和inserted_ids属性。当文档没有提供明确的唯一键时，就需要使用这两个属性值作为文档的唯一键。如果指定了_id键，就是用该值作为唯一键：# 插入person1 = &#123;"name": "John", "dob": "2017-11-24"&#125;person_id1 = people.insert_one(person1).inserted_idprint person_id1 # 5a1d4ba92317d71bb605f8ceperson2 = &#123;"_id": "XVT162", "name": "Jane", "dob": "2017-11-27"&#125;person_id2 = people.insert_one(person2).inserted_idprint person_id2 # XVT162persons = [&#123;"name": "Lucy", "dob": "2017-11-12"&#125;, &#123;"name": "Tom"&#125;]result = people.insert_many(persons)print result.inserted_ids # [ObjectId('5a1d4c832317d71c2c4e284f'), ObjectId('5a1d4c832317d71c2c4e2850')] 函数find_one()和find()分别给出匹配可选属性的一个或多个文档，其中find_one()返回文档，而find()返回一个游标(一个生成器)，可以使用list()函数将该游标转换为列表，或者在for循环中将其用作迭代器。如果将字典作为参数传递给这些函数中的任意一个，函数将给出与字典的所有键值相等的文档：# 查找everyone = people.find()print list(everyone) # [&#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;]print list(people.find(&#123;"dob": "2017-11-27"&#125;)) # [&#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;]one_people = people.find_one()print one_people # &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;one_people = people.find_one(&#123;"name": "Lucy"&#125;)print one_people # &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;one_people = people.find_one(&#123;"_id": "XVT162"&#125;)print one_people # &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125; 下面介绍几个实现数据聚合和排序的分组和排序函数。函数sort()对查询的结果进行排序。当以无参数的方式调用它时，该函数按键_id的升序进行排序。函数count()返回查询结果中或整个集合中的文档数量：# 聚合count = people.count()print count # 5count = people.find(&#123;"dob": "2017-11-27"&#125;).count()print count # 1people_list = list(people.find().sort("dob"))print people_list # [&#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;] 函数delete_one(doc)和delete_many(docs)从集合中删除字典doc所标识的一个或多个文档。如果要在删除所有文档的同时保留集合，需使用空字典作为参数调用函数delete_many({})：# 删除result = people.delete_many(&#123;"dob": "2017-11-27"&#125;)print result.deleted_count # 1 来自于:Python数据科学入门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 列表,元组与集合]]></title>
    <url>%2FPython%2FPython%20%E5%88%97%E8%A1%A8%2C%E5%85%83%E7%BB%84%E4%B8%8E%E9%9B%86%E5%90%88.html</url>
    <content type="text"><![CDATA[1. 选择合适的数据结构列表、元组、集合和字典是Python中最常用的复合数据结构，它们都属于容器类的数据结构。 (1) 列表 Python用数组的方式实现列表。列表的搜索时间是线性的，因此用列表来存储大量可搜索的数据是不切实际的。 (2) 元组 元组是不可变的列表，创建后就无法再更改。元组的搜索时间也是线性的。 (3) 集合 与列表和元组不同，集合不是序列：集合项不存在索引。集合最多只能存储一个项的副本，具有次线性O(log(N))的搜索时间。集合非常适合于成员查找和消除重复项。 在下面的例子中，展示了列表与集合查询速度对比，bigList是以十进制字符串表示的前1000万个整数的列表： bigList = [str(i) for i in range(10000000)]"abc" in bigList # 耗时0.2秒bigSet = set(bigList)"abc" in bigSet # 耗时15~30微秒——快了10 000倍！ (4) 字典 字典构建了从键到值的映射。任何可哈希的数据类型（数字、布尔、字符串、元组）的对象都可以作为键，且同一字典中的不同键可以属于不同的数据类型。Python对字典值的数据类型也没有限制。字典具有次线性O(log(N))搜索时间，它非常适合用于键值的查找。 你可以通过(键, 值)这样的元组列表创建字典，也可以使用内置类构造函数enumerate(seq)创建字典，这样得到的字典，其键为各项在seq中的序列号： seq = ["alpha", "bravo", "charlie", "delta"]d = dict(enumerate(seq))print d # &#123;0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'&#125; 另一种从键序列和值序列创建字典变量的巧妙方法，是使用内置类构造函数zip(kseq, vseq)(两个序列必须具有相同的长度): kseq = ["apple", "bear", "banana"]vseq = [12, 21, 3]d = dict(zip(kseq, vseq))print d # &#123;'apple': 12, 'bear': 21, 'banana': 3&#125; Python将enumerate(seq)和zip(kseq, vseq)实现为列表生成器。列表生成器提供了一个迭代器接口，这使得我们可以在for循环中使用它们。与真正的列表不同的是，列表生成器只在需要时才生成下一个元素，这可以说是一种巧妙的偷懒方式。列表生成器便于处理大型列表，甚至允许“无限”的列表。你可以通过调用list()函数，将列表生成器显式强制转换为列表。 总结： 数据结构 查询时间复杂度 说明 列表 O(N) 数组实现，不适合大数据搜索 元组 O(N) 不可变的列表，不可更改，不适合大数据搜索 集合 O(log(N)) 不是序列，不存在重复项，适合成员查找 字典 O(log(N)) 键到值的映射，键和值数据类型无限制，适合键值查找 2. 列表推导式列表推导式是一个将数据集(不一定是列表)转换为列表的表达式。通过列表推导式，可以实现对所有或某些列表元素应用相同的操作，例如将所有元素转换为大写或对其中大于0的元素进行运算等。 列表表达式的转换过程如下： (1) 表达式遍历数据集并访问集合中的每一项。 (2) 为每一项计算布尔表达式(可选，不选默认为True)。 (3) 如果布尔表达式为True，则计算当前项目的循环表达式，并将其值附加到结果列表中。 (4) 如果布尔表达式为False，则跳过该项。 Example：# 复制myList；等同于myList.copy()或者myList[:]，但二者的效率都没有列表推导式高[x for x in myList]# 提取非负项[x for x in myList if x &gt;= 0]# 用Mylist非零项的倒数构建一个新列表[1/x for x in myList if x != 0]# 从打开的infile文件中选出所有的非空行，并删除这些行开头和结尾的空格[l.strip() for l in infile if l.strip()] 如果列表推导式被包含在圆括号中，而不是在方括号中，则程序将返回一个列表生成器对象：(x**2 for x in myList) # 结果为：&lt;generator object &lt;genexpr&gt; at 0x...&gt; 3. 计数器计数器是一种字典式集合，用于给集合项目计数。计数器定义在collections模块中。你可以将要计数的集合传递给构造函数Counter，然后使用函数most_common(n)来获取n个出现频率最高的项及对应频率的列表(如果没有提供参数n，则函数返回的将是一个针对所有项目的列表)。 phrase = "a boy eat a banana"cntr = Counter(phrase.split())print cntr.most_common() # [('a', 2), ('boy', 1), ('eat', 1), ('banana', 1)] 来源于:]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
