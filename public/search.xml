<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark Task not serializable]]></title>
    <url>%2F2018%2F06%2F01%2Fspark-base-task-not-serializable%2F</url>
    <content type="text"><![CDATA[你可能会看到如下错误：org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: ... 当你在 Driver（master）上初始化变量，然后在其中一个 worker 上尝试使用它时，可能会触发上述错误。在这种情况下，Spark Streaming 会尝试序列化该对象以将其发送给 worker，如果对象不可序列化，就会失败。考虑下面的代码片段：NotSerializable notSerializable = new NotSerializable();JavaRDD&lt;String&gt; rdd = sc.textFile(&quot;/tmp/myfile&quot;);rdd.map(s -&gt; notSerializable.doSomething(s)).collect(); 这就会触发上述错误。这里有一些方法可以解决上述错误： 对该类进行序列化 仅在传递给 map 中 lambda 函数内声明实例。 将 NotSerializable 对象设置为静态，并在每台机器上创建一次。 调用 rdd.forEachPartition 并在其中创建 NotSerializable 对象，如下所示：rdd.forEachPartition(iter -&gt; &#123; NotSerializable notSerializable = new NotSerializable(); // ...Now process iter&#125;); 原文:https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/troubleshooting/javaionotserializableexception.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 任务运行失败]]></title>
    <url>%2F2018%2F05%2F21%2Fhadoop-base-task-failure%2F</url>
    <content type="text"><![CDATA[1. 常见情况任务运行失败最常见的情况是 map 任务或 reduce 任务中的用户代码抛出运行异常。如果发生这种情况，任务 JVM 会在退出之前向其父 application master 发送错误报错。错误报告最后被记入用户日志中。application master 会将此次任务尝试标记为 failed (失败)，并释放容器以便资源可以为其他任务使用。 任务运行失败另一种常见情况是任务 JVM 突然退出，可能由于 JVM 软件缺陷而导致 MapReduce 用户代码由于特殊原因造成 JVM 退出。在这种情况下，节点管理器会注意到进程已经退出，并通知 application master 将此次任务尝试标记为失败。 一旦 application master 注意到已经有一段时间没有收到进度的更新，便会将任务标记为失败。在此之后，任务 JVM 进程将被自动杀死。任务被认为失败的超时时间间隔通常为10分钟，可以以作业为基础（或以集群为基础）进行设置，对应的属性为 mapreduce.task.timeout ，单位为毫秒。 超时设置为0，将关闭超时判定，所以长时间运行的任务永远不会被标记为失败。在这种情况下，被挂起的任务永远不会释放它的容器并随着时间的推移，最终降低整个集群的效率。因此，尽量避免这种设置。 2. 失败重试application master 被告知一个任务尝试失败后，将重新调度该任务的执行。application master 会试图避免在以前失败过的节点管理器上重新调度该任务。此外，如果一个任务失败过4次，将不会再重试，整个作业都会失败，如下表。 Attempt State Node attempt_1504162679223_24764734_r_000057_0 FAILED /default-rack/l-hp609.data.cn2:8042 attempt_1504162679223_24764734_r_000057_1 FAILED /default-rack/l-hp143.data.cn2:8042 attempt_1504162679223_24764734_r_000057_2 FAILED /default-rack/l-hp618.data.cn2:8042 attempt_1504162679223_24764734_r_000057_3 FAILED /default-rack/l-hp272.data.cn2:8042 上述作业在任务失败之后会在不同节点管理器上重新调度该任务，如果任务重试４次之后还是失败则整个作业会失败：18/05/21 00:24:52 INFO mapreduce.Job: Job job_1504162679223_24764734 failed with state FAILED due to: Task failed task_1504162679223_24764734_r_000057Job failed as tasks failed. failedMaps:0 failedReduces:1 上面的４次是可以设置的：对于 map 任务，运行任务的最多尝试次数由 mapreduce.map.maxattempts 属性控制；对于 reduce 任务，则由 mapreduce.reduce.maxattempts 属性控制。默认情况下，如果任何任务失败次数大于４（或最多尝试次数被配置为４），整个作业都会失败。 3. 任务失败容忍对于一些应用程序，我们不希望一旦有少数几个任务失败就终止运行整个作业，因为即使有任务失败，作业的一些结果可能还是可用的。在这种情况下，可以为作业设置在不触发作业的情况下任务失败的最大百分比。针对 map 任务和 reduce 任务的设置可以通过 mapreduce.map.failures.maxpercent 和 mapreduce.reduce.failures.maxpercent 两个属性来完成。 4. Killed任务任务尝试也是可以终止的（killed），这与失败不同。任务尝试可以被终止是因为它是一个推测执行任务或因为它所处的节点管理器失败，导致application master 将它上面运行的所有任务尝试标记为 killed 。被中止的任务尝试不会计入任务运行尝试次数（由 mapreduce.map.maxattempts 和 mapreduce.reduce.maxattempts 属性控制），因为尝试被中止并不是任务的过错。 用户也可以使用 Web UI 或命令行来中止或取消任务尝试。也可以采用相同的机制来中止作业。 来自:Hadoop权威指南]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Block 与 InputSplit 的区别与联系]]></title>
    <url>%2F2018%2F05%2F19%2Fhadoop-base-block-and-inputsplit%2F</url>
    <content type="text"><![CDATA[相信大家都知道，HDFS 将文件按照一定大小的块进行切割，（我们可以通过 dfs.blocksize 参数来设置 HDFS 块的大小，在 Hadoop 2.x 上，默认的块大小为 128MB。）也就是说，如果一个文件大小大于 128MB，那么这个文件会被切割成很多块，这些块分别存储在不同的机器上。当我们启动一个 MapReduce 作业去处理这些数据的时候，程序会计算出文件有多少个 Splits，然后根据 Splits 的个数来启动 Map 任务。那么 HDFS 块和 Splits 到底有什么关系？ 为了简便起见，下面介绍的文件为普通文本文件。 1. HDFS 块现在我有一个名为 iteblog.txt 的文件，如下：[iteblog@iteblog.com /home/iteblog]$ ll iteblog.txt-rw-r--r-- 1 iteblog iteblog 454669963 May 15 12:07 iteblog.txt 很明显，这个文件大于一个 HDFS 块大小，所有如果我们将这个文件存放到 HDFS 上会生成 4 个 HDFS 块，如下（注意下面的输出做了一些删除操作）：[iteblog@iteblog.com /home/iteblog]$ hadoop -put iteblog.txt /tmp[iteblog@iteblog.com /home/iteblog]$ hdfs fsck /tmp/iteblog.txt -files -blocks/tmp/iteblog.txt 454669963 bytes, 4 block(s): OK0. BP-1398136447-192.168.246.60-1386067202761:blk_8133964845_1106679622318 len=134217728 repl=31. BP-1398136447-192.168.246.60-1386067202761:blk_8133967228_1106679624701 len=134217728 repl=32. BP-1398136447-192.168.246.60-1386067202761:blk_8133969503_1106679626977 len=134217728 repl=33. BP-1398136447-192.168.246.60-1386067202761:blk_8133970122_1106679627596 len=52016779 repl=3 可以看出 iteblog.txt 文件被切成 4 个块了，前三个块大小正好是 128MB（134217728），剩下的数据存放到第 4 个 HDFS 块中。 如果文件里面有一行记录的偏移量为 134217710，长度为 100，HDFS 如何处理？ 答案是这行记录会被切割成两部分，一部分存放在 block 0 里面；剩下的部分存放在 block 1 里面。具体的，偏移量为134217710，长度为18的数据存放到 block 0 里面；偏移量134217729，长度为82的数据存放到 block 1 里面。 可以将这部分的逻辑以下面的图概括： 说明： 图中的红色块代表一个文件 中间的蓝色矩形块代表一个 HDFS 块，矩形里面的数字代表 HDFS 块的编号，读整个文件的时候是从编号为0的 HDFS 块开始读，然后依次是1,2,3… 最下面的一行矩形代表文件里面存储的内容，每个小矩形代表一行数据，里面的数字代表数据的编号。红色的竖线代表 HDFS 块边界(block boundary)。 从上图我们可以清晰地看出，当我们往 HDFS 写文件时，HDFS 会将文件切割成大小为 128MB 的块，切割的时候不会判断文件里面存储的到底是什么东西，所以逻辑上属于一行的数据会被切割成两部分，这两部分的数据被物理的存放在两个不同的 HDFS 块中，正如上图中的第5、10以及14行被切割成2部分了。 2. File Split现在我们需要使用 MapReduce 来读取上面的文件，由于是普通的文本文件，所以可以直接使用 TextInputFormat 来读取。下面是使用 TextInputFormat获取到的 FileSplit 信息：scala&gt; FileInputFormat.addInputPath(job,new Path(&quot;/tmp/iteblog.txt&quot;));scala&gt; val format = new TextInputFormat;scala&gt; val splits = format.getSplits(job)scala&gt; splits.foreach(println)hdfs://iteblogcluster/tmp/iteblog.txt:0+134217728hdfs://iteblogcluster/tmp/iteblog.txt:134217728+134217728hdfs://iteblogcluster/tmp/iteblog.txt:268435456+134217728hdfs://iteblogcluster/tmp/iteblog.txt:402653184+52016779 可以看出，每个 FileSplit 的起始偏移量和上面 HDFS 每个文件块一致。但是具体读数据的时候，MapReduce 是如何处理的呢？我们现在已经知道，在将文件存储在 HDFS 的时候，文件被切割成一个一个 HDFS Block，其中会导致一些逻辑上属于一行的数据会被切割成两部分，那 TextInputFormat 遇到这样的数据是如何处理的呢？ 对于这种情况，TextInputFormat 会做出如下两种操作： 在初始化 LineRecordReader 的时候，如果 FileSplit 的起始位置 start 不等于0， 说明这个 Block 块不是第一个 Block，这时候一律丢掉这个 Block 的第一行数据。 在读取每个 Block 的时候，都会额外地多读取一行，如果出现数据被切割到另外一个 Block 里面，这些数据能够被这个任务读取。 使用图形表示可以概括如下： 说明： 图中的红色虚线代表 HDFS 块边界(block boundary)； 蓝色的虚线代表Split 读数的边界。 从图中可以清晰地看出： 当程序读取 Block 0 的时候，虽然第五行数据被分割并被存储在 Block 0 和 Block 1 中，但是，当前程序能够完整的读取到第五行的完整数据。 当程序读取 Block 1 的时候，由于其 FileSplit 的起始位置 start 不等于0，这时候会丢掉第一行的数据，也就是说 Block 1 中的第五行部分数据会被丢弃，而直接从第六行数据读取。这样做的原因是，Block 1 中的第五行部分数据在程序读取前一个 Block 的时候已经被读取了，所以可以直接丢弃。 其他剩下的 Block 读取逻辑和这个一致。 3. 总结从上面的分析可以得出以下的总结 Split 和 HDFS Block 是一对多的关系； HDFS block 是数据的物理表示，而 Split 是 block 中数据的逻辑表示； 满足数据本地性的情况下，程序也会从远程节点上读取少量的数据，因为存在行被切割到不同的 Block 上。 原文：https://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=2650715088&amp;idx=1&amp;sn=90c6ca52fcb9ba9a1f79764095a8a635&amp;pass_ticket=S4Ar8TbFuzTw%2F%2BRLzzms8abC%2BhdeIHSX65rbkBbw7R3doqhY%2FQQLwo9QJ0V8arTy]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 抽样Sampling]]></title>
    <url>%2F2018%2F05%2F12%2Fhive-base-how-to-use-sampling%2F</url>
    <content type="text"><![CDATA[1. Block 抽样Block 抽样功能在 Hive 0.8 版本开始引入。具体参阅JIRA - Input Sampling By Splits block_sample: TABLESAMPLE (n PERCENT) 该语句允许至少抽取 n% 大小的数据（注意：不是行数，而是数据大小）做为输入，仅支持 CombineHiveInputFormat ，不能够处理一些特殊的压缩格式。如果抽样失败，MapReduce 作业的输入将是整个表或者是分区的数据。由于在 HDFS 块级别进行抽样，所以抽样粒度为块大小。例如如果块大小为256MB，即使 n% 的输入仅为100MB，那也会得到 256MB 的数据。 在下面例子中 0.1% 或更多的输入数据用于查询：SELECT *FROM source TABLESAMPLE(0.1 PERCENT) s; 如果希望在不同的块中抽取相同大小的数据，可以改变下面的参数：set hive.sample.seednumber=&lt;INTEGER&gt;; 或者可以指定要读取的总长度，但与 PERCENT 抽样具有相同的限制。（从Hive 0.10.0开始 - https://issues.apache.org/jira/browse/HIVE-3401）block_sample: TABLESAMPLE (ByteLengthLiteral)ByteLengthLiteral : (Digit)+ (&apos;b&apos; | &apos;B&apos; | &apos;k&apos; | &apos;K&apos; | &apos;m&apos; | &apos;M&apos; | &apos;g&apos; | &apos;G&apos;) 在下面例子中中 100M 或更多的输入数据用于查询：SELECT *FROM source TABLESAMPLE(100M) s; Hive 还支持按行数对输入进行限制，但它与上述两种行为不同。首先，它不需要 CombineHiveInputFormat，这意味着这可以在 non-native 表上使用。其次，用户给定的行数应用到每个 InputSplit 上。 因此总行数还取决于输入 InputSplit 的个数（不同 InputSplit 个数得到的总行数也会不一样）。（从Hive 0.10.0开始 - https://issues.apache.org/jira/browse/HIVE-3401）block_sample: TABLESAMPLE (n ROWS) 例如，以下查询将从每个输入 InputSplit 中取前10行：SELECT * FROM source TABLESAMPLE(10 ROWS); 因此如果有20个 InputSplit 就会输出200条记录。 2. 分桶表抽样语法：table_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname]) TABLESAMPLE 子句允许用户编写对抽样数据的查询，而不是对整个表格进行查询。TABLESAMPLE 子句可以添加到任意表中的 FROM 子句中。桶从1开始编号。colname 表明在哪一列上对表的每一行进行抽样。colname 可以是表中的非分区列，也可以使用 rand() 表明在整行上抽样而不是在单个列上。表中的行在 colname 上进行分桶，并随机分桶到编号为1到y的桶上。返回属于第x个桶的行。下面的例子中，返回32个桶中的第3个桶中的行，s 是表的别名：SELECT * FROM source TABLESAMPLE(BUCKET 3 OUT OF 32 ON rand()) s; 通常情况下，TABLESAMPLE 将扫描整个表并抽取样本。但是，这并不是一种有效率的方式。相反，可以使用 CLUSTERED BY 子句创建该表，表示在该表的一组列上进行哈希分区/分簇。如果 TABLESAMPLE子 句中指定的列与 CLUSTERED BY 子句中的列匹配，则 TABLESAMPLE 仅扫描表中所需的哈希分区。 所以在上面的例子中，如果使用 CLUSTERED BY id INTO 32 BUCKETS 创建表 source（根据id将数据分到32个桶中）：TABLESAMPLE(BUCKET 3 OUT OF 16 ON id) 会返回第3个和第19个簇，因为每个桶由（32/16）= 2个簇组成（创建表时指定了32个桶，会对应32个簇）。为什么选择3和19呢，因为要返回的是第3个桶，而每个桶由原来的2个簇组成，3%16=3 19%16=3，第3个桶就由原来的第3个和19个簇组成。另一个例子:TABLESAMPLE(BUCKET 3 OUT OF 64 ON id) 会返回第三个簇的一半，因为每个桶将由（32/64）= 1/2个簇组成。 原文:https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Sampling]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Yarn上的调度器]]></title>
    <url>%2F2018%2F05%2F07%2Fhadoop-scheduler-of-yarn%2F</url>
    <content type="text"><![CDATA[1. 引言Yarn在Hadoop的生态系统中担任了资源管理和任务调度的角色。在讨论其构造器之前先简单了解一下Yarn的架构。 上图是Yarn的基本架构，其中 ResourceManager 是整个架构的核心组件，负责集群上的资源管理，包括内存、CPU以及集群上的其他资； ApplicationMaster 负责在生命周期内的应用程序调度； NodeManager 负责本节点上资源的供给和隔离；Container 可以抽象的看成是运行任务的一个容器。本文讨论的调度器是在 ResourceManager 进行调度，接下来在了解一下 FIFO 调度器、Capacity 调度器、Fair 调度器三个调度器。 2. FIFO调度器 上图显示了 FIFO 调度器的实现（执行过程示意图）。FIFO 调度器是先进先出（First In First Out）调度器。FIFO 调度器是 Hadoop 使用最早的一种调度策略，可以简单的将其理解为一个 Java 队列，这就意味着在集群中同时只能有一个作业运行。所有的应用程序按照提交顺序来执行，在上一个 Job 执行完成之后，下一个 Job 按照队列中的顺序执行。FIFO调度器以独占集群全部资源的方式来运行作业，这样的好处是 Job 可以充分利用集群的全部资源，但是对于运行时间短，优先级高或者交互式查询类的MR Job 需要等待它之前的 Job 完成才能被执行，这也就导致了如果前面有一个比较大的 Job 在运行，那么后面的 Job 将会被阻塞。因此，虽然 FIFO 调度器实现简单，但是并不能满足很多实际场景的要求。这也就促使 Capacity 调度器和 Fair 调度器的诞生。 增加作业优先级的功能后，可以通过设置 mapred.job.priority 属性或 JobClinet 的 setJobPriority 方法来设置优先级。在作业调度器选择要运行的下一个作业时，FIFO 调度器中不支持优先级抢占，所以高优先级的作业会受阻于前面已经开始，长时间运行的低优先级的作业。 3. Capacity调度器 上图显示了 Capacity 调度器的实现（执行过程示意图）。Capacity 调度器也称之为容器调度器。可以将它理解为一个资源队列。资源队列需要用户自己分配。例如因为 Job 需要要把整个集群分成了AB两个队列，A队列又可以继续分，比如将A队列再分为1和2两个子队列。那么队列的分配就可以参考下面的树形结构：—A[60%] |—A.1[40%] |—A.2[60%]—B[40%] 上述的树形结构可以理解为A队列占用集群全部资源的60%，B队列占用40%。A队列又分为1，2两个子队列，A.1占据40%，A.2占据60%，也就是说此时A.1和A.2分别占用A队列的40%和60%的资源。虽然此时已经对集群的资源进行了分配，但并不是说A提交了任务之后只能使用集群资源的60%，而B队列的40%的资源处于空闲。只要是其它队列中的资源处于空闲状态，那么有任务提交的队列就可以使用分配给空闲队列的那些资源，使用的多少依据具体配置。参数的配置会在后文中提到。 3.1 Capacity调度器的特性(1) 层次化的队列设计，这种层次化的队列设计保证了子队列可以使用父队列的全部资源。这样通过层次化的管理可以更容易分配和限制资源的使用。 (2) 容量，给队列设置一个容量(资源占比)，确保每个队列不会占用集群的全部资源。 (3) 安全，每个队列都有严格的访问控制。用户只能向自己的队列提交任务，不能修改或者访问其他队列的任务。 (4) 弹性分配，可以将空闲资源分配给任何队列。当多个队列出现竞争的时候，则会按照比例进行平衡。 (5) 多租户租用，通过队列的容量限制，多个用户可以共享同一个集群，colleagues 保证每个队列分配到自己的容量，并且提高利用率。 (6) 可操作性，Yarn支持动态修改容量、权限等的分配，这些可以在运行时直接修改。还提供管理员界面，来显示当前的队列状态。管理员可以在运行时添加队列；但是不能删除队列。管理员还可以在运行时暂停某个队列，这样可以保证当前队列在执行期间不会接收其他任务。如果一个队列被设置成了stopped，那么就不能向他或者子队列提交任务。 (7) 基于资源的调度，以协调不同资源需求的应用程序，比如内存、CPU、磁盘等等。 3.2 Capacity调度器的参数配置(1) capacity：队列的资源容量（百分比）。当系统非常繁忙时，应保证每个队列的容量得到满足，如果每个队列应用程序较少，可与其他队列共享剩余资源。注意，所有队列的容量之和应小于100。 (2) maximum-capacity：队列的资源使用上限（百分比）。由于资源共享，因此一个队列使用的资源量可能超过其容量，可以通过该参数来限制最多使用资源量。（这也是前文提到的队列可以占用资源的最大百分比） (3) user-limit-factor：每个用户最多可使用的资源量（百分比）。比如，如果该值为30，那么在任何时候每个用户使用的资源量都不能超过该队列容量的30%。 (4) maximum-applications ：集群中或者队列中同时处于等待和运行状态的应用程序数目上限(cluster or at the same time in a waiting in the queue and application number of the running condition limit)，这是一个强限制，一旦集群中应用程序数目超过该上限，后续提交的应用程序将被拒绝，默认值为 10000。所有队列的数目上限可通过参数 yarn.scheduler.capacity.maximum-applications 设置（可看做默认值），而单个队列可通过参数 yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-applications 设置。 (5) maximum-am-resource-percent：集群中用于运行应用程序 ApplicationMaster 的最大资源比例，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，默认是0.1，表示10%。所有队列的 ApplicationMaster 资源比例上限可通过参数 yarn.scheduler.capacity. maximum-am-resource-percent 设置（可看做默认值），而单个队列可通过参数 yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-am-resource-percent 设置。 (6) state ：队列状态可以为 STOPPED 或者 RUNNING，如果一个队列处于 STOPPED 状态，用户不可以将应用程序提交到该队列或者它的子队列中，类似的，如果 ROOT 队列处于 STOPPED 状态，用户不可以向集群中提交应用程序，但是处于 RUNNING 状态的应用程序仍可以正常运行，以便队列可以优雅地退出。 (7) acl_submit_applications：指定哪些Linux用户/用户组可向队列提交应用程序。需要注意的是，该属性具有继承性，即如果一个用户可以向某个队列提交应用程序，那么它可以向它的所有子队列提交应用程序。配置该属性时，用户之间或用户组之间用 ， 分割，用户和用户组之间用空格分割，比如 user1,user2 group1,group2。 (8) acl_administer_queue：指定队列的管理员，管理员可控制该队列的所有应用程序，例如杀死任意一个应用程序等。同样，该属性具有继承性，如果一个用户可以向某个队列提交应用程序，则它可以向它的所有子队列提交应用程序。 4. Fair调度器 上图显示了 Fair 调度器的实现（执行过程示意图）。Fair 调度器也称之为公平调度器。Fair 调度器是一种队列资源分配方式，在整个时间线上，所有的 Job 平分资源。默认情况下，Fair 调度器只是对内存资源做公平的调度和分配。当集群中只有一个任务在运行时，那么此任务会占用集群的全部资源。当有其他的任务提交后，那些释放的资源将会被分配给新的 Job，所以每个任务最终都能获取几乎一样多的资源。 Fair 调度器也可以在多个队列上工作，如上图所示，例如有两个用户A和B，他们分别拥有一个队列。当A启动一个 Job 而B没有提交任何任务时，A会获得集群全部资源；当B启动一个 Job 后，A的任务会继续运行，不过队列A会慢慢释放它的一些资源，一会儿之后两个任务会各自获得集群一半的资源。如果此时B再启动第二个 Job 并且其它任务也还在运行时，那么它将会和B队列中的的第一个 Job 共享队列B的资源，也就是队列B的两个 Job 会分别使用集群四分之一的资源，而队列A的 Job 仍然会使用集群一半的资源，结果就是集群的资源最终在两个用户之间平等的共享。 4.1 Fair调度器参数配置(1) yarn.scheduler.fair.allocation.file： allocation 文件的位置，allocation 文件是一个用来描述队列以及它们属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为 fair-scheduler.xml。 (2) yarn.scheduler.fair.user-as-default-queue：如果没有指定队列名称时，是否将与 allocation 有关的 username 作为默认的队列名称。如果设置成 false(且没有指定队列名称) 或者没有设定，所有的 jobs 将共享 default 队列。默认值为 true。 (3) yarn.scheduler.fair.preemption：是否使用抢占模式(优先权，抢占)，默认为 fasle，在此版本中此功能为测试性的。 (4) yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中发送多个容器分配信息。默认值为 false。 (5) yarn.scheduler.fair.max.assign：如果 yarn.scheduler.fair.assignmultiple 为true，那么在一次心跳中最多发送分配容器的个数。默认为-1，无限制。 (6) yarn.scheduler.fair.locality.threshold.node：0~1之间一个float值，表示在等待获取满足 node-local 条件的容器时，最多放弃不满足 node-local 的容器机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。 (7) yarn.scheduler.fair.locality.threashod.rack：同上，满足 rack-local。 (8) yarn.scheduler.fair.sizebaseweight：是否根据应用程序的大小(Job的个数)作为权重。默认为 false，如果为 true，那么复杂的应用程序会获取更多的资源。 5. 总结如果业务逻辑比较简单或者刚接触 Hadoop 的时建议使用 FIFO 调度器；如果需要控制部分应用程序的优先级，同时又想要充分利用集群资源的情况下，建议使用 Capacity 调度器；如果想要多用户或者多队列公平的共享集群资源，那么就选用Fair调度器。希望大家能够根据业务所需选择合适的调度器。 原文：http://www.cobub.com/en/the-selection-and-use-of-hadoop-yarn-scheduler/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next主题添加版权信息]]></title>
    <url>%2F2018%2F05%2F04%2Fhexo-next-add-copyright-information%2F</url>
    <content type="text"><![CDATA[1. 开启版权声明主题配置文件下,搜索关键字 post_copyright , 将 enable 改为 true：# Declare license on postspost_copyright: enable: true license: CC BY-NC-SA 4.0 license_url: https://creativecommons.org/licenses/by-nc-sa/4.0/ 这样设置之后出现一个问题:文章的链接并不是我设置域名之后的链接，对 next/layout/_macro/ 下的 post-copyright.swig 做如下修改:&lt;li class=&quot;post-copyright-link&quot;&gt; &lt;strong&gt;&#123;&#123; __(&apos;post.copyright.link&apos;) + __(&apos;symbol.colon&apos;) &#125;&#125;&lt;/strong&gt; &lt;a href=&quot;http://smartsi.club/&#123;&#123; post.path | default(post.permalink) &#125;&#125;&quot; title=&quot;&#123;&#123; post.title &#125;&#125;&quot;&gt;http://smartsi.club/&#123;&#123; post.path | default(post.permalink) &#125;&#125;&lt;/a&gt;&lt;/li&gt; 不知道还有没有什么更好的方法解决这个问题，欢迎留言指教。 2. 自定义文章底部版权声明在目录 next/layout/_macro/ 下添加 my-copyright.swig：&#123;% if page.copyright %&#125;&lt;div class=&quot;post-copyright-information&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src=&quot;https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/sweetalert/dist/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); $(&quot;.fa-clipboard&quot;).click(function()&#123; clipboard.on(&apos;success&apos;, function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, icon: &quot;success&quot;, showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 在目录 next/source/css/_common/components/post/ 下添加 post-copyright-information.styl：.post-copyright-information &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.post-copyright-information p&#123;margin:0;&#125;.post-copyright-information span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.post-copyright-information .raw &#123; margin-left: 1em; width: 5em;&#125;.post-copyright-information a &#123; color: #808080; border-bottom:0;&#125;.post-copyright-information a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.post-copyright-information:hover .fa-clipboard &#123; color: #000;&#125;.post-copyright-information .post-url:hover &#123; font-weight: normal;&#125;.post-copyright-information .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.post-copyright-information .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改 next/layout/_macro/post.swig: 在代码：&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;wechat-subscriber.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 之前添加增加如下代码：&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改 next/source/css/_common/components/post/post.styl 文件，在最后一行增加代码：@import &quot;my-post-copyright&quot; 保存重新生成即可。 如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加 copyright: true 的设置：---layout: postauthor: sjf0115title: Hexo Next主题添加版权信息date: 2018-05-04 18:33:01tags: - Hexocategories: Hexocopyright: truepermalink: hexo-next-add-copyright-information--- 原文:https://segmentfault.com/a/1190000009544924#articleHeader19]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之Shuffle调优]]></title>
    <url>%2F2018%2F04%2F28%2Fspark-performance-shuffle-tuning%2F</url>
    <content type="text"><![CDATA[1. 调优概述大多数 Spark 作业的性能主要就是消耗在了 shuffle 环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对 shuffle 过程进行调优。但是也必须提醒大家的是，影响一个 Spark 作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle 调优只能在整个 Spark 的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解 shuffle 的原理，以及相关参数的说明，同时给出各个参数的调优建议。 2. ShuffleManager发展概述在Spark的源码中，负责 shuffle 过程的执行、计算和处理的组件主要就是 ShuffleManager，也即 shuffle 管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的 shuffle 计算引擎是 HashShuffleManager。HashShuffleManager 有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的 ShuffleManager 改成了 SortShuffleManager。SortShuffleManager 相较于 HashShuffleManager 来说，有了一定的改进。主要就在于，每个 Task 在进行 shuffle 操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个 Task 就只有一个磁盘文件。在下一个 stage 的 shuffle read task 拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下 HashShuffleManager 和 SortShuffleManager 的原理。 3. HashShuffleManager运行原理3.1 未经优化的HashShuffleManager 上图说明了未经优化的 HashShuffleManager 的原理。这里我们先明确一个假设前提：每个 Executor 只有1个CPU core，也就是说，无论这个 Executor 上分配多少个 task 线程，同一时间都只能执行一个 task 线程。 我们先从 shuffle write 开始说起。shuffle write 阶段，主要就是在一个 stage 结束计算之后，为了下一个 stage 可以执行 shuffle 类的算子（比如reduceByKey），而将每个 task 处理的数据按 key 进行“分类”。所谓“分类”，就是对相同的 key 执行 hash 算法，从而将相同 key 都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游 stage 的一个 task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。 那么每个执行 shuffle write 的 task，要为下一个 stage 创建多少个磁盘文件呢？很简单，下一个 stage 的 task 有多少个，当前 stage 的每个 task 就要创建多少份磁盘文件。比如下一个 stage 总共有 100 个 task，那么当前 stage 的每个 task 需要创建 100 份磁盘文件。如果当前 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 Task，那么每个 Executor 上总共需要创建 500 个磁盘文件，所有 Executor 上会创建 5000 个磁盘文件。由此可见，未经优化的 shuffle write 操作所产生的磁盘文件的数量是极其惊人的。 接着我们来说说 shuffle read。shuffle read，通常就是一个 stage 刚开始时要做的事情。此时该 stage 的每一个 task 就需要将上一个 stage 的计算结果中的所有相同 key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行 key 的聚合或连接等操作。由于 shuffle write 的过程中，上游 stage 的每个 task 给下游 stage 的每个 task 都创建了一个磁盘文件，因此 shuffle read 的过程中，每个 task 都要从上游 stage 的所有 task 所在节点拉取属于自己的那一个磁盘文件。 shuffle read 的拉取过程是一边拉取一边进行聚合的。每个 shuffle read task 都会有一个自己的 buffer 缓冲，每次都只能拉取与 buffer 缓冲相同大小的数据，然后通过内存中的一个 Map 进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到 buffer 缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 3.2 优化后的HashShuffleManager 上图说明了优化后的 HashShuffleManager 的原理。这里说的优化，是指我们可以设置一个参数 spark.shuffle.consolidateFiles。该参数默认值为 false，将其设置为 true 即可开启优化机制。通常来说，如果我们使用 HashShuffleManager，建议开启这个选项。 开启 consolidate 机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件了。此时引入了一个 shuffleFileGroup 的概念，每个 shuffleFileGroup 会对应一批磁盘文件，磁盘文件的数量与下游 stage 的 task 数量是相同的。一个 Executor 上有多少个 CPU core，就可以并行执行多少个 task。而第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内。 当 Executor 的 CPU core 执行完一批 task，接着执行下一批 task 时，下一批 task 就会复用之前已有的 shuffleFileGroup，包括其中的磁盘文件。也就是说，此时 task 会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能。 假设第二个 stage 有 100 个 task，第一个 stage 有 50 个 task，总共还是有 10 个 Executor，每个 Executor 执行 5 个 task。那么原本使用未经优化的 HashShuffleManager 时，每个 Executor 会产生 500 个磁盘文件，所有 Executor 会产生 5000 个磁盘文件的。但是此时经过优化之后，每个 Executor 此时只会创建 100 个磁盘文件(假设每个 Executor 只有1个CPU core，所以每个 Executor 中并行任务只有一个)，所有 Executor 只会创建 1000 个磁盘文件。 每个 Executor 创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。 4. SortShuffleManager运行原理SortShuffleManager 的运行机制主要分成两种，一种是普通运行机制，另一种是 bypass 运行机制。当 shuffle read task 的数量小于等于 spark.shuffle.sort.bypassMergeThreshold 参数的值时（默认为200），就会启用 bypass 机制。 4.1 普通运行机制 上图说明了普通的 SortShuffleManager 的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的 shuffle 算子，可能选用不同的数据结构。如果是 reduceByKey 这种聚合类的 shuffle 算子，那么会选用 Map 数据结构，一边通过 Map 进行聚合，一边写入内存；如果是 join 这种普通的 shuffle 算子，那么会选用 Array 数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过 Java 的 BufferedOutputStream 实现的。BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 一个 task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是 merge 过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个 task 就只对应一个磁盘文件，也就意味着该 task 为下游 stage 的 task 准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个 task 的数据在文件中的 start offset 与 end offset。 SortShuffleManager 由于有一个磁盘文件 merge 的过程，因此大大减少了文件数量。比如第一个 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 task，而第二个 stage 有 100 个task。由于每个 task 最终只有一个磁盘文件，因此此时每个 Executor 上只有5个磁盘文件，所有 Executor 只有 50 个磁盘文件。 4.2 bypass运行机制 上图说明了 bypass SortShuffleManager 的原理。bypass 运行机制的触发条件如下： shuffle map task 数量小于 spark.shuffle.sort.bypassMergeThreshold 参数的值。 不是聚合类的shuffle算子（比如reduceByKey）。 此时 task 会为每个下游 task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的 hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read的性能会更好。 而该机制与普通 SortShuffleManager 运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write 过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 5. shuffle相关参数调优以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。 (1) spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置 shuffle write task 的 BufferedOutputStream 的 buffer 缓冲大小。将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少 shuffle write 过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 (2) spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置 shuffle read task 的 buffer 缓冲大小，而这个 buffer 缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 (3) spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task 从 shuffle write task 所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如60次），以避免由于 JVM 的 full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的 shuffle 过程，调节该参数可以大幅度提升稳定性。 (4) spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加 shuffle 操作的稳定性。 (5) spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了 Executor 内存中，分配给 shuffle read task 进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给 shuffle read 的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 (6) spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置 ShuffleManager 的类型。Spark 1.5以后，有三个可选项：hash、sort 和 tungsten-sort。HashShuffleManager 是Spark 1.2以前的默认选项，Spark 1.2以及之后的版本默认为 SortShuffleManager 了。tungsten-sort 与 sort类似，但是使用了 tungsten 计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于 SortShuffleManager 默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，使用默认的 SortShuffleManager 就可以；如果你的业务逻辑不需要对数据进行排序，建议参考后面的几个参数调优，通过 bypass 机制或优化的 HashShuffleManager 来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort 要慎用，因为之前发现了一些相应的bug。 (7) spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当 ShuffleManager 为 SortShuffleManager 时，如果 shuffle read task 的数量小于阈值（默认是200），shuffle write 过程中不会进行排序操作，而是直接按照未经优化的 HashShuffleManager 的方式去写数据，但是最后会将每个 task 产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用 SortShuffleManager 时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于 shuffle read task 的数量。那么此时就会自动启用 bypass 机制，map-side 就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 (8) spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用 HashShuffleManager，该参数有效。如果设置为 true，那么就会开启 consolidate 机制，会大幅度合并 shuffle write 的输出文件，对于 shuffle read task 数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要 SortShuffleManager 的排序机制，那么除了使用 bypass 机制，还可以尝试将 spark.shffle.manager 参数手动指定为hash，使用 HashShuffleManager，同时开启 consolidate 机制。在实践中尝试过，发现其性能比开启了 bypass 机制的 SortShuffleManager 要高出10%~30%。 原文:https://tech.meituan.com/spark-tuning-pro.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内部原理之内存管理]]></title>
    <url>%2F2018%2F04%2F25%2Fspark-internal-memory-management%2F</url>
    <content type="text"><![CDATA[Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。 1. 堆内和堆外内存规划作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。 图 1 . 堆内和堆外内存示意图 1.1 堆内内存堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。 Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程： (1) 申请内存： Spark 在代码中 new 一个对象实例 JVM 从堆内内存分配空间，创建对象并返回对象引用 Spark 保存该对象的引用，记录该对象占用的内存 (2) 释放内存： Spark 记录该对象释放的内存，删除该对象的引用 等待 JVM 的垃圾回收机制释放该对象占用的堆内内存 我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。 对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。 虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。 1.2 堆外内存为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。 在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。 1.3 内存管理接口Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存: 清单 1 . 内存管理接口的主要方法//申请存储内存def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请展开内存def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请执行内存def acquireExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long//释放存储内存def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit//释放执行内存def releaseExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit//释放展开内存def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit 我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。 MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（Unified Memory Manager）方式，1.6 之前采用的静态管理（Static Memory Manager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 2 小节会分别对这两种方式进行介绍。 2. 内存空间分配2.1 静态内存管理在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示： 图 2 . 静态内存管理图示——堆内 可以看到，可用的堆内内存的大小需要按照下面的方式计算： 清单 2 . 可用堆内内存空间可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。 堆外的空间分配较为简单，只有存储内存和执行内存，如图 3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。 图 3 . 静态内存管理图示——堆外 静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。 2.2 统一内存管理Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图 4 和图 5 所示 图 4 . 统一内存管理图示——堆内 图 5 . 统一内存管理图示——堆外 其中最重要的优化在于动态占用机制，其规则如下： 设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block） 执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间 存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂[4] 图 6 . 动态占用机制图示 凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。 3. 存储内存管理3.1 RDD 的持久化机制弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。 Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管 理 （存储内存的其他应用场景，如缓存 broadcast 数据，暂时不在本文的讨论范围之内）。 RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。 图 7 . Storage 模块示意图 在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等 7 种不同的 存储级别 ，而存储级别是以下 5 个变量的组合： 清单 3 . 存储级别class StorageLevel private(private var _useDisk: Boolean, //磁盘private var _useMemory: Boolean, //这里其实是指堆内内存private var _useOffHeap: Boolean, //堆外内存private var _deserialized: Boolean, //是否为非序列化private var _replication: Int = 1 //副本个数) 通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式： 存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。 存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。 副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。 3.2 RDD 缓存的过程RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（Iterator）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续。 RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。 因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图 8 所示。 图 8. Spark Unroll 示意图 在图 3 和图 5 中可以看到，在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。 3.3 淘汰和落盘由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。 存储内存的淘汰规则为： 被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存 新旧 Block 不能属于同一个 RDD，避免循环淘汰 旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题 遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。 落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。 4. 执行内存管理4.1 多任务间内存分配Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。 4.2 Shuffle 的内存占用执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用： (1) Shuffle Write 若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。 若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。 (2) Shuffle Read 在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。 如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。 在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。 Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划[9]，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。 Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成： 页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。 页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。 有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升[10]。 Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。 5. 结束语Spark 的内存管理是一套复杂的机制，且 Spark 的版本更新比较快，笔者水平有限，难免有叙述不清、错误的地方，若读者有好的建议和更深的理解，还望不吝赐教。 6. 参考资源 Spark Cluster Mode Overview Spark Sort Based Shuffle 内存分析 Spark OFF_HEAP Unified Memory Management in Spark 1.6 Tuning Spark: Garbage Collection Tuning Spark Architecture 《Spark 技术内幕：深入解析 Spark 内核架构于实现原理》第 8 章 Storage 模块详解 Spark Sort Based Shuffle 内存分析 Project Tungsten: Bringing Apache Spark Closer to Bare Metal Spark Tungsten-sort Based Shuffle 分析 探索 Spark Tungsten 的秘密 Spark Task 内存管理（on-heap&amp;off-heap） 原文：https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 如何使用压缩]]></title>
    <url>%2F2018%2F04%2F23%2Fhadoop-how-to-use-compression%2F</url>
    <content type="text"><![CDATA[就如上一篇文章介绍的那样，如果输入文件是压缩文件，当 MapReduce 程序读取压缩文件时，根据文件名的后缀来选择 codes，输入文件自动解压缩（我们不需要指定压缩文件是哪一种压缩格式）。 下面我们列出了一些代码，为 Hadoop 中常用的压缩格式设置输出压缩。 1. 常用压缩格式1.1 Gzip对于最终输出，我们可以使用FileOutputFormat上的静态方便方法来设置属性：FileOutputFormat.setCompressOutput(job, true);FileOutputFormat.setOutputCompressorClass(job, GzipCodec,class); 或者Configuration conf = new Configuration();conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);conf.setClass("mapreduce.output.fileoutputformat.compress.codec", GzipCodec.class, CompressionCodec.class); 对于 Map 输出：Configuration conf = new Configuration();conf.setBoolean("mapreduce.map.output.compress",true);conf.setClass("mapreduce.map.output.compress.codec", GzipCodec.class, CompressionCodec.class);Job job = Job.getInstance(conf); 1.2 LZO对于最终输出：FileOutputFormat.setCompressOutput(conf, true);FileOutputFormat.setOutputCompressorClass(conf, LzoCodec.class); 为了使LZO可分割，我们需要生成一个LZO索引文件。 对于 Map 输出：Configuration conf = new Configuration();conf.setBoolean("mapreduce.map.output.compress",true);conf.setClass("mapreduce.map.output.compress.codec", LzoCodec.class, CompressionCodec.class);Job job = Job.getInstance(conf); 1.3 Snappy对于最终输出：conf.setOutputFormat(SequenceFileOutputFormat.class);SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);SequenceFileOutputFormat.setCompressOutput(conf, true);conf.set("mapreduce.output.fileoutputformat.compress.codec","org.apache.hadoop.io.compress.SnappyCodec"); 对于Map输出：Configuration conf = new Configuration();conf.setBoolean("mapreduce.map.output.compress", true);conf.set("mapreduce.map.output.compress.codec","org.apache.hadoop.io.compress.SnappyCodec"); 2. 实验与结果2.1 Filesystem counters文件系统计数器用于分析实验结果。以下是典型的内置文件系统计数器。 FILE_BYTES_READ 是本地文件系统读取的字节数。假设所有的 map 输入数据都来自 HDFS，那么在 map 阶段，FILE_BYTES_READ 应该为零。另一方面，reducer 的输入文件是 reduce 端本地磁盘上的数据，它们是从 map 端磁盘拉取过来的。因此，reduce 端 FILE_BYTES_READ 表示 reducer 读取的总字节数。 FILE_BYTES_WRITTEN 由两部分组成。第一部分来自 mappers。所有的 mappers 都会将中间输出溢出到磁盘上。mappers 写入磁盘的所有字节将包含在 FILE_BYTES_WRITTEN 中。第二部分来自 reducers。 在 shuffle 阶段，所有 reducers 将从 mappers 中获取中间数据，合并并溢写到 reducer 端的磁盘上。reducers 写入磁盘的所有字节也将包含在 FILE_BYTES_WRITTEN 中。 HDFS_BYTES_READ 表示作业启动时 mappers 从 HDFS 上读取的字节数。这些数据不仅包括源文件的内容，还包括关于 splits 的元数据。 HDFS_BYTES_WRITTEN 表示写入 HDFS 的字节。这是最终输出的字节数。 请注意，由于 HDFS 和本地文件系统是不同的文件系统，因此来自两个文件系统的数据不会重叠。 2.2 压缩比较(1) 没有压缩 (2) 只压缩输入 我们可以看到 HDFS_BYTES_READ 明显减少。这表明 mappers 从 HDFS 上读取的总字节数显着减少。 (3) 只压缩map中间输出 我们可以看到 FILE_BYTES_READ 和 FILE_BYTES_WRITTEN 显着减少。这意味着本地文件系统节点之间的数据传输显着减少。 (4) 只压缩最终输出 我们可以看到 HDFS_BYTES_WRITTEN 显着减少。这表明 HDFS 的最终输出显着降低。 2.3 不同压缩格式的比较：gzip，lzo 正如我们所看到的，LZO 文件略大于对应的 gzip 文件，但都比原来未压缩文件小得多。另外，LZO 文件压缩速度快了近五倍，解压速度快了两倍。 我们还可以看到 Snappy 文件比相应的 LZO 文件大，但仍然是原来未压缩文件的一半。另外，Snappy 的压缩和解压缩速度都比 LZO 更快。总之，Snappy 在压缩和解压缩时间方面速度更快，但在压缩比方面效率更低。 原文：http://comphadoop.weebly.com/how-to-use-compression.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 大量小文件问题的优化]]></title>
    <url>%2F2018%2F04%2F23%2Fhadoop-small-files-problem%2F</url>
    <content type="text"><![CDATA[1. HDFS上的小文件问题小文件是指文件大小明显小于 HDFS 上块（block）大小（默认64MB，在Hadoop2.x中默认为128MB）的文件。如果存储小文件，必定会有大量这样的小文件，否则你也不会使用 Hadoop，这样的文件给 Hadoop 的扩展性和性能带来严重问题。当一个文件的大小小于 HDFS 的块大小（默认64MB）就认定为小文件，否则就是大文件。为了检测输入文件的大小，可以浏览Hadoop DFS 主页 ，并点击 Browse filesystem（浏览文件系统）。 首先，HDFS 中任何一个文件，目录或者数据块在 NameNode 节点内存中均以一个对象形式表示（元数据），而这受到 NameNode 物理内存容量的限制。每个元数据对象约占 150 byte，所以如果有1千万个小文件，每个文件占用一个block，则 NameNode 大约需要2G空间。如果存储1亿个文件，则 NameNode 需要20G空间，这毫无疑问1亿个小文件是不可取的。 其次，处理小文件并非 Hadoop 的设计目标，HDFS 的设计目标是流式访问大数据集（TB级别）。因而，在 HDFS 中存储大量小文件是很低效的。访问大量小文件经常会导致大量的 seek，以及不断的在 DatanNde 间跳跃去检索小文件。这不是一个很有效的访问模式，严重影响性能。 最后，处理大量小文件速度远远小于处理同等大小的大文件的速度。每一个小文件要占用一个 slot，而任务启动将耗费大量时间甚至大部分时间都耗费在启动任务和释放任务上。 2. MapReduce上的小文件问题Map任务一般一次只处理一个块的输入（input）（默认使用FileInputFormat）。如果文件非常小，并且有很多，那么每一个 Map 任务都仅仅处理非常小的输入数据，并会产生大量的 Map 任务，每一个 Map 任务都会额外增加 bookkeeping 开销。一个1GB大小的文件拆分成16个64M大小的块，相对于拆分成10000个100KB的块，后者每一个小文件启动一个 Map 任务，作业的运行时间将会十倍甚至百倍慢于前者。 Hadoop 中有一些特性可以用来减轻 bookkeeping 开销：可以在一个 JVM 中允许 task JVM 重用，以支持在一个 JVM 中运行多个 Map 任务，以此来减少 JVM 的启动开销(译者注：MR1中通过设置 mapred.job.reuse.jvm.num.tasks 属性)。 3. 为什么会产生大量的小文件至少在两种场景下会产生大量的小文件： 这些小文件都是一个大逻辑文件的一部分。由于 HDFS 在2.x版本开始支持对文件进行追加，所以在此之前保存无边界文件（例如，日志文件）一种常用的方式就是将这些数据以块的形式写入HDFS中（译者注：持续产生的文件，例如日志每天都会生成）。 文件本身就是很小。设想一下，我们有一个很大的图片语料库，每一个图片都是一个单独的文件，并且没有一种很好的方法来将这些文件合并为一个大的文件。 4. 解决方案这两种情况需要有不同的解决方式。 4.1 第一种情况对于第一种情况，文件是许多记录组成的，那么可以通过调用 HDFS 的 sync() 方法(和 append 方法结合使用)，每隔一定时间生成一个大文件。或者，可以通过写一个 MapReduce 程序来来合并这些小文件。 4.2 第二种情况对于第二种情况，就需要容器通过某种方式来对这些文件进行分组。Hadoop提供了一些选择： 4.2.1 HAR FileHadoop Archives （HAR files）是在 0.18.0 版本中引入到 HDFS 中的，它的出现就是为了缓解大量小文件消耗 NameNode 内存的问题。HAR 文件是通过在 HDFS 上构建一个分层文件系统来工作。HAR 文件通过 hadoop archive 命令来创建，而这个命令实际上是运行 MapReduce 作业来将小文件打包成少量的 HDFS 文件（译者注：将小文件进行合并成几个大文件）。对于客户端来说，使用 HAR 文件系统没有任何的变化：所有原始文件都可见以及可以访问（只是使用 har://URL，而不是 hdfs://URL），但是在 HDFS 中中文件个数却减少了。 读取 HAR 文件不如读取 HDFS 文件更有效，并且实际上可能更慢，因为每个 HAR 文件访问需要读取两个索引文件以及还要读取数据文件本。 尽管 HAR 文件可以用作 MapReduce 的输入，但是 Map 没有办法直接对共同驻留在 HDFS 块上的 HAR 所有文件操作。可以考虑通过创建一种 input format，充分利用 HAR 文件的局部性优势，但是目前还没有这种input format。需要注意的是：MultiFileInputSplit，即使在 HADOOP-4565 进行了改进，选择节点本地分割中的文件，但始终还是需要每个小文件的搜索。在目前看来，HAR 可能最好仅用于存储文档。 4.2.2 SequenceFile通常解决”小文件问题”的回应是：使用 SequenceFile。这种方法的思路是，使用文件名作为 key，文件内容作为 value，如下图。 在实践中这种方式非常有效。我们回到10,000个100KB大小的小文件问题上，你可以编写一个程序将合并为一个 SequenceFile，然后你可以以流式方式处理（直接处理或使用 MapReduce） SequenceFile。这样会带来两个优势： SequenceFiles 是可拆分的，因此 MapReduce 可以将它们分成块，分别对每个块进行操作； 与 HAR 不同，它们支持压缩。在大多数情况下，块压缩是最好的选择，因为它直接对几个记录组成的块进行压缩，而不是对每一个记录进行压缩。 将现有数据转换为 SequenceFile 可能会很慢。但是，完全可以并行创建一个一个的 SequenceFile 文件。Stuart Sierra 写了一篇关于将 tar 文件转换为 SequenceFile 的文章，像这样的工具是非常有用的，我们应该多看看。向前看，最好设计好数据管道，如果可能的话，将源数据直接写入 SequenceFile，而不是作为中间步骤写入小文件。 与 HAR 文件不同，没有办法列出 SequenceFile 中的所有键，所以不能读取整个文件。Map File，类似于对键进行排序的 SequenceFile，维护部分索引，所以他们也不能列出所有的键，如下图。 4.2.3 HBase如果你产生很多小文件，根据访问模式的不同，应该进行不同类型的存储。HBase 将数据存储在 Map Files（带索引的 SequenceFile）中，如果你需要随机访问来执行 MapReduce 流式分析，这是一个不错的选择。如果延迟是一个问题，那么还有很多其他选择 - 参见Richard Jones对键值存储的调查。 原文：http://blog.cloudera.com/blog/2009/02/the-small-files-problem/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 数据压缩简介]]></title>
    <url>%2F2018%2F04%2F22%2Fhadoop-data-compress%2F</url>
    <content type="text"><![CDATA[文件压缩带来两大好处：它减少了存储文件所需的空间，并加速了数据在网络或者磁盘上的传输速度。在处理大量数据时，这两项节省可能非常重要，因此需要仔细考虑如何在 Hadoop 中使用压缩。 1. 要压缩什么？1.1 压缩输入文件如果输入文件是压缩的，那么从HDFS读入的字节就会减少，这意味着读取数据的时间会减少。对于提升作业执行的性能是有帮助的。 如果输入文件被压缩，在 MapReduce 读取时会自动解压缩，根据文件扩展名来确定使用哪个编解码器。例如，以 .gz 结尾的文件可以被识别为 gzip 压缩文件，因此使用 GzipCodec 进行读取。 1.2 压缩输出文件通常我们需要将输出存储为历史文件。如果每天的输出文件很大，并且我们需要存储历史结果以供将来使用，那么这些累积结果将占用大量的 HDFS 空间。但是，这些历史文件可能不会非常频繁地被使用，导致浪费 HDFS 空间。因此，在 HDFS 上存储之前，需要压缩输出。 1.3 压缩Map输出即使你的 MapReduce 应用程序读取和写入未压缩的数据，它也可能从压缩 Map 阶段的中间输出中受益。由于 Map 输出被写入磁盘并通过网络传输到 Reducer 节点，所以通过使用 LZO 或 Snappy 等快速压缩器，由于减少了传输的数据量从而获得性能提升。 2. 常见压缩格式 2.1 Gzipgzip 是 Hadoop 内置压缩方法，基于 DEFLATE 算法，组合 LZ77 和 Huffman 编码。 2.2 Bzip2bzip2 能够进行高质量的数据压缩。它利用先进的压缩技术，能够把普通的数据文件压缩10%至15%，压缩的速度和解压的效率都非常高！支持大多数压缩格式，包括tar、gzip 等等。 2.3 LZOLZO压缩格式由许多较小（大约256K）的压缩数据块组成，因此允许作业沿着块边界进行分割。此外，设计时考虑了速度要素，目的是达到与硬盘读取速度相当的压缩速度：压缩速度是 gzip 的５倍，解压缩速度是 gzip 的2倍。同一个文件用 LZO 压缩后比用 gzip 压缩后大50％，但比压缩前小20-50％，这对改善性能非常有利，Map 阶段完成时间要快四倍。 2.4 SnappySnappy 是一个压缩/解压库。它的目标不是最大压缩率,也不关心与任何其他压缩库的兼容性。相反，它旨在提供非常高的速度和合理的压缩。例如，与 zlib 的最快压缩模式相比，Snappy 对于大多数输入都快了一个数量级，但是生成的压缩文件都要比 zlib 模式大20％到100％。在一个64位，单核酷睿i7处理器上，Snappy 压缩速度在 250 MB/秒以上，解压缩速度在 500 MB/秒以上。Snappy 广泛应用于 Google 内部，BigTable，MapReduce 以及内部 RPC 系统各个地方都在使用。 3. 折衷所有压缩算法都在空间与时间上进行权衡：更快的压缩和解压缩速度通常以更少的空间节省为代价，意味着耗费更大的空间。上表中列出的工具通常会在压缩时通过提供九种不同的选项来控制这种权衡：-1表示对速度进行优化，-9表示对空间进行优化。 不同的工具具有非常不同的压缩特性。Gzip 是一个通用压缩器，空间与时间权衡的更好一些。Bzip2 比 gzip 压缩更有效（压缩后文件更小），但速度较慢。 Bzip2 的解压缩速度比压缩速度快，但它仍然比其他方法慢。另一方面，LZO 和 Snappy 都对速度进行了优化，并且比 gzip 快一个数量级，但是压缩效率较低。解压缩方面 Snappy 明显快于LZO。 4. 有关压缩和输入拆分的问题当考虑如何压缩由 MapReduce 处理的数据时，重要的是要了解压缩格式是否支持分割。考虑存储在 HDFS 中大小为 1GB 的未压缩文件。如果 HDFS 块大小为 64MB（MR1默认64MB，MR2默认128MB），文件将存储为16个块，并且使用此文件作为输入的 MapReduce 作业将创建16个 InputSplit（输入拆分），每一个 InputSplit 作为一个独立 Map 任务的输入单独进行处理。 假设我们有一个大小为 1GB 的 gzip 压缩文件，和以前一样，HDFS 将文件存储为16块。然而，无法为每个块创建 InputSplit，因为不能从 gzip 数据流中的任意位置开始读取，因此 Map 任务不可能独立于其他 Map 任务而只读取一个 InputSplit 中的数据。gzip 格式使用 DEFLATE 算法存储压缩数据，DEFLATE 算法将数据存储为一系列压缩的数据块。问题在于，用任何方法也不能区分每个块的开始位置，每个块的开始位置保证了允许从流中的任意位置能够读到下一个块的开始位置，这就意味着能够读出单个块的数据。因此，gzip 不支持拆分。 在这种情况下，MapReduce 不会尝试对压缩文件进行分割，因为 MapReduce 知道输入文件是通过 gzip 压缩（通过查看文件扩展名），并且知道 gzip 不支持分割。这种情况下 MapReduce 还是会继续工作的，但是以牺牲数据局部性的特性为代价：单个 Map 将会处理 16个 HDFS 块，大部分都不会在 Map 本地节点。此外，使用较少的 Mapper，作业的粒度变小，因此可能运行较长时间。 假设示例中的文件是一个 LZO 文件，我们也会遇到同样的问题，因为底层的压缩格式不能提供一种方法与流同步读取。但是，可以使用 Hadoop LZO 库附带的索引器工具处理 LZO 文件。该工具建立分割点的索引，当使用恰当的 MapReduce 输入格式时，可以有效地使他们进行拆分。另一方面，bzip2 文件在块之间提供了同步标记（pi的48位近似），因此它支持拆分。 5. IO密集型与CPU密集型在 HDFS 中存储压缩数据能够进一步分配你的硬件，因为压缩数据通常是原始数据大小的25％。此外，由于 MapReduce 作业几乎都是IO密集型，存储压缩数据意味着整体上更少的IO处理，意味着作业运行更快。然而，有两个注意事项： 一些压缩格式不能拆分来并行处理 一些解压速度比较慢，作业变为CPU密集型，抵消你在IO上的收益。 gzip 压缩格式说明了第一个注意事项。假设有一个 1.1GB 的 gzip 文件，并且集群中块大小为 128MB。这个文件分割为 9 个 HDFS 块，每个大约128MB。为了在 MapReduce 作业中并行处理这些数据，每个块将由不同的 Mapper 负责。但这意味着第二个 Mapper 将在文件中大约 128MB 的任意字节处开始。gzip 用于解压缩输入的上下文字典在这为空，这意味着 gzip 解压缩器将无法正确解释字节。结果就是，Hadoop 中的大型 gzip 文件需要由单个 Mapper 处理，这违背了并行性的目的。 Bzip2压缩格式说明了作业成为CPU密集型的第二个注意事项。Bzip2文件压缩效果良好，也可以拆分，但是解压缩算法速度比较慢，无法跟上在 Hadoop 作业中常见的流式磁盘读取。虽然 Bzip2 压缩效果良好，节省了存储空间，但是正在运行的作业需要花费时间等待CPU完成解压数据，这会降低整体速度并抵消其他收益。 6. 总结6.1 需要压缩原因 数据存储但不经常处理。这是通常的 DWH 场景。在这种情况下，空间节省可能比处理开销更重要； 压缩因子非常高，节省了大量的IO； 解压缩非常快（例如 Snappy）使我们有一定的收益； 数据已经到达压缩状态(Data already arrived compressed) 6.2 不需要压缩原因 压缩数据不可拆分。 必须注意的是，现在许多格式都是以块级压缩构建的，以实现文件的拆分和部分处理； 数据在集群中创建，压缩需要很长时间。必须注意，压缩通常比解压缩需要更多的CPU； 数据几乎没有冗余，没有必要进行压缩； 原文地址：http://comphadoop.weebly.com/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark UI 之 Streaming 标签页]]></title>
    <url>%2F2018%2F04%2F19%2Fspark-streaming-ui-streaming-tab%2F</url>
    <content type="text"><![CDATA[这篇博文将重点介绍为理解 Spark Streaming 应用程序而引入的新的可视化功能。我们已经更新了 Spark UI 中的 Streaming 标签页来显示以下信息： 时间轴视图和事件率统计，调度延迟统计以及以往的批处理时间统计 每个批次中所有JOB的详细信息此外，为了理解在 Streaming 操作上下文中作业的执行情况，有向无环执行图的可视化增加了 Streaming 的信息。 让我们通过一个从头到尾分析Streaming应用程序的例子详细看一下上面这些新的功能。 1. 处理趋势的时间轴和直方图当我们调试一个 Spark Streaming 应用程序的时候，我们更希望看到数据正在以什么样的速率被接收以及每个批次的处理时间是多少。Streaming标签页中新的UI能够让你很容易的看到目前的值和之前1000个批次的趋势情况。当你在运行一个 Streaming 应用程序的时候，如果你去访问 Spark UI 中的 Streaming 标签页，你将会看到类似下面图一的一些东西（红色的字母，例如[A]，是我们的注释，并不是UI的一部分）。 第一行（标记为 [A]）展示了 Streaming 应用程序当前的状态；在这个例子中，应用已经以1秒的批处理间隔运行了将近40分钟;在它下面是输入速率（Input rate）的时间轴（标记为 [B]），显示了Streaming应用从它所有的源头以大约49 events每秒的速度接收数据。在这个例子中，时间轴显示了在中间位置（标记为[C]）平均速率有明显的下降，在时间轴快结束的地方应用又恢复了。如果你想得到更多详细的信息，你可以点击 Input Rate旁边（靠近[B]）的下拉列表来显示每个源头各自的时间轴，正如下面图2所示： 图2显示了这个应用有两个来源，(SocketReceiver-0和 SocketReceiver-1)，其中的一个导致了整个接收速率的下降，因为它在接收数据的过程中停止了一段时间。 这一页再向下（在图1中标记为 [D] ），处理时间（Processing Time）的时间轴显示，这些批次大约在平均20毫秒内被处理完成，和批处理间隔（在本例中是1s）相比花费的处理时间更少，意味着调度延迟（被定义为：一个批次等待之前批次处理完成的时间，被标记为 [E]）几乎是零，因为这些批次在创建的时候就已经被处理了。调度延迟是你的Streaming引用程序是否稳定的关键所在，UI的新功能使得对它的监控更加容易。 3. 批次细节再次参照图1，你可能很好奇，为什么向右的一些批次花费更长的时间才能完成（注意图1中的[F]）。你可以通过UI轻松的分析原因。首先，你可以点击时间轴视图中批处理时间比较长的点，这将会在页面下方产生一个关于完成批次的详细信息列表。 它将显示这个批次的所有主要信息（在上图3中以绿色高亮显示）。正如你所看到的，这个批次较之其他批次有更长的处理时间。另一个很明显的问题是：到底是哪个spark job引起了这个批次的处理时间过长。你可以通过点击Batch Time（第一列中的蓝色链接），这将带你看到对应批次的详细信息，向你展示输出操作和它们的spark job，正如图4所示。 图4显示有一个输出操作，它产生了3个spark job。你可以点击job ID链接继续深入到stages和tasks做更深入的分析。 4. Streaming RDDs的有向无环执行图一旦你开始分析批处理job产生的stages和tasks，更加深入的理解执行图将非常有用。正如之前的博文所说，Spark1.4.0加入了有向无环执行图（execution DAG ）的可视化（DAG即有向无环图），它显示了RDD的依赖关系链以及如何处理RDD和一系列相关的stages。如果在一个Streaming应用程序中，这些RDD是通过DStreams产生的，那么可视化将展示额外的Streaming语义。让我们从一个简单的Streaming字数统计（word count）程序开始，我们将统计每个批次接收的字数。程序示例 NetworkWordCount 。它使用DStream操作flatMap, map和 reduceByKey 来计算字数。任一个批次中一个Spark job的有向无环执行图将会是如下图5所示。 可视化展示中的黑点代表着在批处理时16:06:50由DStream产生的RDD。蓝色阴影的正方形是指用来转换RDD的DStream操作，粉色的方框代表这些转换操作执行的阶段。总之图5显示了如下信息： 数据是在批处理时间16:06:50通过一个socket文本流（ socket text stream ）接收的。 Job用了两个stage和flatMap , map , reduceByKey 转换操作来计算数据中的字数 尽管这是一个简单的图表，它可以通过增加更多的输入流和类似window操作和updateStateByKey操作等高级的DStream转换而变得更加复杂。例如，如果我们通过一个含三个批次的移动窗口来计算字数（即使用reduceByKeyAndWindow），它的数据来自两个socket文本流，那么，一个批处理job的有向无环执行图将会像如下图6所示。 图6显示了于一个跨3个批次统计字数的Spark job的许多相关信息： 前三个stage实际上是各自统计窗口中3个批次的字数。这有点像上面例子 NetworkWordCount 的第一个stage，使用的是map和flatmap操作。不过要注意以下不同点： 这里有两个输入RDD，分别来自两个socket文本流，这两个RDD通过union结合成一个RDD，然后进一步转换，产生每个批次的中间统计结果。 其中的两个stage都变灰了，因为两个较旧批次的中间结果已经缓存在内存中，因此不需要再次计算，只有最近的批次需要从头开始计算。最后一个右边的stage使用reduceByKeyAndWindow 来联合每个批次的统计字数最终形成一个“窗口”的字数。这些可视化使得开发人员不仅能够监控Streaming应用程序的状态和趋势，而且能够理解它们与底层spark job和执行计划的关系。 5. 未来方向Spark1.5.0中备受期待的一个重要提升是关于每个批次( JIRA , PR )中输入数据的更多信息。例如：如果你正在使用Kafka，批处理详细信息页面将会显示这个批次处理的topics, partitions和offsets，预览如下图： 原文发布时为2015.07.08 原文：https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html 译文：https://www.csdn.net/article/2015-07-15/2825214]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 启用压缩]]></title>
    <url>%2F2018%2F04%2F12%2Fhive-enable-compression%2F</url>
    <content type="text"><![CDATA[对于数据密集型任务，I/O操作和网络数据传输需要花费相当长的时间才能完成。通过在 Hive 中启用压缩功能，我们可以提高 Hive 查询的性能，并节省 HDFS 集群上的存储空间。 1. Hive中的可用压缩编解码器要在 Hive 中启用压缩，首先我们需要找出 Hadoop 集群上可用的压缩编解码器，我们可以使用下面的 set 命令列出可用的压缩编解码器。hive&gt; set io.compression.codecs;io.compression.codecs= org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec 2. 在中间数据上启用压缩提交后，一个复杂的 Hive 查询通常会转换为一系列多阶段 MapReduce 作业，这些作业将通过 Hive 引擎进行链接以完成整个查询。因此，这里的 ‘中间输出’ 是指前一个 MapReduce 作业的输出，将会作为下一个 MapReduce 作业的输入数据。 可以通过使用 Hive Shell 中的 set 命令或者修改 hive-site.xml 配置文件来修改 hive.exec.compress.intermediate 属性，这样我们就可以在 Hive Intermediate 输出上启用压缩。 &lt;property&gt; &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.intermediate.compression.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;description/&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.intermediate.compression.type&lt;/name&gt; &lt;value&gt;BLOCK&lt;/value&gt; &lt;description/&gt;&lt;/property&gt; 或者我们可以使用 set 命令在 hive shell 中设置这些属性，如下所示:hive&gt; set hive.exec.compress.intermediate=true;hive&gt; set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;hive&gt; set hive.intermediate.compression.type=BLOCK; 3. 在最终输出上启用压缩通过设置以下属性，我们可以在 Hive shell 中的最终输出上启用压缩:&lt;property&gt; &lt;name&gt;hive.exec.compress.output&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* &lt;/description&gt;&lt;/property&gt; 或者hive&gt; set hive.exec.compress.output=true;hive&gt; set mapreduce.output.fileoutputformat.compress=true;hive&gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec; hive&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK; 4. Example在下面的 shell 代码片段中，我们在 hive shell 中将压缩属性设置为 true 后，根据现有表 tmp_order_id 创建一个压缩后的表 tmp_order_id_compress。 为了对比，我们先根据现有表 tmp_order_id 创建一个不经压缩的表 tmp_order_id_2:CREATE TABLE tmp_order_id_2 ROW FORMAT DELIMITED LINES TERMINATED BY &apos;\n&apos;AS SELECT * FROM tmp_order_id; 我们看一下不设置压缩属性输出:[xiaosi@ying:~]$ sudo -uxiaosi hadoop fs -ls /user/hive/warehouse/xiaosidata.db/tmp_order_id_2/Found 1 items-rwxr-xr-x 3 xiaosi xiaosi 2565 2018-04-18 20:38 /user/hive/warehouse/hivedata.db/tmp_order_id_2/000000_0 在 Hive Shell 中设置压缩属性：hive&gt; set hive.exec.compress.output=true;hive&gt; set mapreduce.output.fileoutputformat.compress=true;hive&gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;hive&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;hive&gt; set hive.exec.compress.intermediate=true; 根据现有表 tmp_order_id 创建一个压缩后的表 tmp_order_id_compress:hive&gt; CREATE TABLE tmp_order_id_compress ROW FORMAT DELIMITED LINES TERMINATED BY &apos;\n&apos; &gt; AS SELECT * FROM tmp_order_id;Query ID = xiaosi_20180418204750_445d742f-be89-47fb-9d2c-2b0eeac76c09Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1504162679223_21624651, Tracking URL = http://xxx:9981/proxy/application_1504162679223_21624651/Kill Command = /home/xiaosi/hadoop-2.2.0/bin/hadoop job -kill job_1504162679223_21624651Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02018-04-18 20:47:59,575 Stage-1 map = 0%, reduce = 0%2018-04-18 20:48:04,711 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.28 secMapReduce Total cumulative CPU time: 1 seconds 280 msecEnded Job = job_1504162679223_21624651Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to directory hdfs://cluster/user/hive/warehouse/hivedata.db/.hive-staging_hive_2018-04-18_20-47-50_978_2339056359585839454-1/-ext-10002Moving data to directory hdfs://cluster/user/hive/warehouse/hivedata.db/tmp_order_id_compressMapReduce Jobs Launched:Stage-Stage-1: Map: 1 Cumulative CPU: 1.28 sec HDFS Read: 6012 HDFS Write: 1436 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 280 msecOKTime taken: 14.902 seconds 我们在看一下设置压缩属性后输出:[xiaosi@ying:~]$ sudo -uxiaosi hadoop fs -ls /user/hive/warehouse/hivedata.db/tmp_order_id_compress/Found 1 items-rwxr-xr-x 3 xiaosi xiaosi 1343 2018-04-18 20:48 /user/hive/warehouse/hivedata.db/tmp_order_id_compress/000000_0.gz 因此，我们可以使用 gzip 格式创建输出文件。 原文：http://hadooptutorial.info/enable-compression-in-hive/]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hive 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 共享变量]]></title>
    <url>%2F2018%2F04%2F10%2Fspark-base-shared-variables%2F</url>
    <content type="text"><![CDATA[通常情况下，传递给 Spark 操作（例如 map 或 reduce）的函数是在远程集群节点上执行的，函数中使用的变量，在多个节点上执行时是同一变量的多个副本。这些变量被拷贝到每台机器上，并且在远程机器上对变量的更新不会回传给驱动程序。在任务之间支持通用的，可读写的共享变量是效率是非常低的。所以，Spark 提供了两种类型的共享变量 : 广播变量（broadcast variables）和 累加器（accumulators）。 1. 广播变量广播变量允许程序员将一个只读的变量缓存到每台机器上，而不是给每个任务中传递一个副本。例如，使用它们我们可以以更有效的方式将一个比较大的输入数据集的副本传递给每个节点。Spark 还试图使用高效的广播算法来分发广播变量，以降低通信成本。 Spark 的 action 操作通过一系列 stage 进行执行，这些 stage 由分布式的 shuffle 操作拆分。Spark 会自动广播每个 stage 中任务所需的公共数据。这种情况下广播的数据以序列化的形式进行缓存，并在运行每个任务之前进行反序列化。这意味着只有当跨多个 stage 的任务需要相同的数据，或者以反序列化形式缓存数据非常重要时，显式创建广播变量才是有用的。 广播变量通过在一个变量 v 上调用 SparkContext.broadcast（v） 创建。广播变量是 v 的一个包装，广播变量的值可以通过调用 value 方法来访问。下面的代码显示了这一点： Java版本：Broadcast&lt;int[]&gt; broadcastVar = sc.broadcast(new int[] &#123;1, 2, 3&#125;);broadcastVar.value();// returns [1, 2, 3] Scala版本:scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)scala&gt; broadcastVar.valueres0: Array[Int] = Array(1, 2, 3) Python版本：&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 创建广播变量后，运行在集群上的任意函数中的值 v 可以使用广播变量来代替，以便 v 在节点上最多分发一次（v is not shipped to the nodes more than once）。另外，对象 v 在广播之后不应该被修改，以保证分发到所有的节点上的广播变量具有同样的值（例如，如果该变量稍后被传送到新的节点）。 2. 累加器累加器是一种仅通过关联和交换操作进行 加 的变量，因此可以在并行计算中得到高效的支持。累加器可以用来实现计数器（如在 MapReduce 中）或者求和。Spark 本身支持数字类型的累加器，程序员可以添加对新类型的支持。 作为使用者，你可以创建命名或未命名的累加器。如下图所示，命名累加器（在此为 counter 实例）会在 Web UI 中展示。 Spark 在 Tasks 任务表中显示由任务修改的每个累加器的值。 跟踪 UI 中的累加器对于理解运行的 stage 的进度很有用（注意：Python尚未支持）。 数值型的累加器可以通过调用 SparkContext.longAccumulator() 或 SparkContext.doubleAccumulator() 来创建，分别累加 Long 或 Double 类型的值。运行在集群上的任务可以使用 add 方法进行累加数值。但是，它们无法读取累加器的值。只有驱动程序可以通过使用 value 方法读取累加器的值。 下面的代码显示了一个累加器，用于累加数组的元素： Java版本:LongAccumulator accum = jsc.sc().longAccumulator();sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -&gt; accum.add(x));// ...// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 saccum.value();// returns 10 Scala版本:scala&gt; val accum = sc.longAccumulator("My Accumulator")accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))...10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 sscala&gt; accum.valueres2: Long = 10 此代码使用了内置的 Long 类型的累加器，我们还可以通过继承 AccumulatorV2 来创建我们自己的类型。 备注:在2.0.0之前的版本中，通过继承AccumulatorParam来实现，而2.0.0之后的版本需要继承AccumulatorV2来实现自定义类型的累加器。 AccumulatorV2 抽象类有几个方法必须重写： reset 将累加器重置为零 add 将另一个值添加到累加器中 merge 将另一个相同类型的累加器合并到该累加器中。 其他必须被覆盖的方法包含在API文档中。 例如，假设我们有一个表示数学上向量的 MyVector 类，我们可以这样写： Java版本:class VectorAccumulatorV2 implements AccumulatorV2&lt;MyVector, MyVector&gt; &#123; private MyVector myVector = MyVector.createZeroVector(); public void reset() &#123; myVector.reset(); &#125; public void add(MyVector v) &#123; myVector.add(v); &#125; ...&#125;// Then, create an Accumulator of this type:VectorAccumulatorV2 myVectorAcc = new VectorAccumulatorV2();// Then, register it into spark context:jsc.sc().register(myVectorAcc, "MyVectorAcc1"); Scala版本：class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] &#123; private val myVector: MyVector = MyVector.createZeroVector def reset(): Unit = &#123; myVector.reset() &#125; def add(v: MyVector): Unit = &#123; myVector.add(v) &#125; ...&#125;// Then, create an Accumulator of this type:val myVectorAcc = new VectorAccumulatorV2// Then, register it into spark context:sc.register(myVectorAcc, "MyVectorAcc1") 请注意，当程序员定义自己的 AccumulatorV2 类型时，返回值类型可以与添加的元素的类型不同。 累加器更新只能在 action 操作中执行，Spark 保证每个任务对累加器只更新一次，即使重新启动的任务也不会重新更新该值。在 transformations 操作中，我们需要知道，如果任务或作业 stage 被重新执行，每个任务的更新操作可能会执行多次。 累加器不会改变 Spark 的懒加载（Lazy）执行模型。如果它们在 RDD 上的某个操作中进行更新，那么只有在 RDD 作为 action 操作的一部分进行计算后才更新它们的值。因此，在一个像 map() 这样的 transformation 操作时，累加器的更新并没有执行。下面的代码片段演示了这个属性： Java版本:LongAccumulator accum = jsc.sc().longAccumulator();data.map(x -&gt; &#123; accum.add(x); return f(x); &#125;);// Here, accum is still 0 because no actions have caused the `map` to be computed. Scala版本:val accum = sc.longAccumulatordata.map &#123; x =&gt; accum.add(x); x &#125;// Here, accum is still 0 because no actions have caused the map operation to be computed. Python版本:accum = sc.accumulator(0)def g(x): accum.add(x) return f(x)data.map(g)# Here, accum is still 0 because no actions have caused the `map` to be computed. Spark 版本:2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#shared-variables]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用split命令分割文件]]></title>
    <url>%2F2018%2F04%2F08%2Flinux-split-file-usage%2F</url>
    <content type="text"><![CDATA[1. 概述split 命令可以将一个大文件分割成很多个小文件。在默认情况下将按照每1000行切割成一个小文件，默认前缀为 x。没有输入或输入为 - 时，从标准输入中读取。 2. 语法：split [OPTION]... [INPUT [PREFIX]] 3. 选项-l : 指定每多少行切成一个小文件。-b : 指定每多少字节切成一个小文件。-C : 每一输出档中，单行的最大 byte 数。-d ： 使用数字作为后缀。 4. 实例4.1 按行分割xiaosi@ying:~/test/input$ split -6 a.txt 或者xiaosi@ying:~/test/input$ split -l 6 a.txt 执行以上命令后，会将原来的大文件 a.txt 切割成多个以 x 开头的小文件。而在这些小文件中，每个文件都至多6行数据(最后一个文件有可能不满６行)。xiaosi@ying:~/test/input$ ll总用量 32drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:19 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xaa-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xab-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xac-rw-rw-r-- 1 xiaosi xiaosi 198 4月 8 18:19 xad-rw-rw-r-- 1 xiaosi xiaosi 132 4月 8 18:19 xaexiaosi@ying:~/test/input$ cat a.txt | wc -l28xiaosi@ying:~/test/input$ cat xae | wc -l4 4.2 按文件大小分割xiaosi@ying:~/test/input$ split -b50M b.txt 执行以上命令后，会将原来的大文件 b.txt 切割成多个以 x 开头的小文件。而在这些小文件中，每个文件大小都为50M(最后一个文件有可能不满50M)。xiaosi@ying:~/test/input$ ll总用量 322296drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:25 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 165000000 4月 8 11:53 b.txt-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:25 xaa-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:25 xab-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:25 xac-rw-rw-r-- 1 xiaosi xiaosi 7713600 4月 8 18:25 xad 4.3 修改后缀上述示例中，文件被分割成多个带有字母的后缀文件，如果想用数字后缀可使用 -d 参数，同时可以使用 -a 来指定后缀的长度：xiaosi@ying:~/test/input$ split -b50M b.txt -d -a 3 执行以上命令后，会将原来的大文件 b.txt 切割成多个以 x 开头后面为数字的小文件：xiaosi@ying:~/test/input$ ll总用量 322296drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:36 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 165000000 4月 8 11:53 b.txt-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:36 x000-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:36 x001-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:36 x002-rw-rw-r-- 1 xiaosi xiaosi 7713600 4月 8 18:36 x003 4.4 指定输出文件名前缀xiaosi@ying:~/test/input$ split -b50M b.txt split_ 执行以上命令后，会将原来的大文件 b.txt 切割成多个以 split_ 开头的小文件：xiaosi@ying:~/test/input$ ll总用量 322296drwxrwxr-x 2 xiaosi xiaosi 4096 4月 8 18:41 ./drwxrwxr-x 6 xiaosi xiaosi 4096 8月 24 2017 ../-rw-rw-r-- 1 xiaosi xiaosi 924 4月 8 18:18 a.txt-rw-rw-r-- 1 xiaosi xiaosi 165000000 4月 8 11:53 b.txt-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:41 split_aa-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:41 split_ab-rw-rw-r-- 1 xiaosi xiaosi 52428800 4月 8 18:41 split_ac-rw-rw-r-- 1 xiaosi xiaosi 7713600 4月 8 18:41 split_ad]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux 命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 使用spark-submit部署应用程序]]></title>
    <url>%2F2018%2F04%2F07%2Fspark-base-launching-applications-with-spark-submit%2F</url>
    <content type="text"><![CDATA[1. 简介Spark的 bin 目录中的 spark-submit 脚本用于在集群上启动应用程序。可以通过一个统一的接口使用 Spark 所有支持的集群管理器，因此不必为每个集群管理器专门配置你的应用程序。 2. 语法xiaosi@yoona:~/opt/spark-2.1.0-bin-hadoop2.7$ spark-submit --helpUsage: spark-submit [options] &lt;app jar | python file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (&quot;client&quot;) or on one of the worker machines inside the cluster (&quot;cluster&quot;) (Default: client). --class CLASS_NAME Your application&apos;s main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: &quot;default&quot;). --num-executors NUM Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM. --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. 3. 打包应用依赖如果你的代码依赖于其他项目，则需要将它们与应用程序一起打包，以便将代码分发到 Spark 集群上。为此，需要创建一个包含代码及其依赖关系的 assembly jar（或 Uber jar）。sbt 和 Maven 都有 assembly 插件。创建 assembly jar 时，将 Spark 和 Hadoop 的依赖设置为 provided。他们不需要打包，因为它们在运行时由集群管理器提供。一旦你有一个 assembly jar，你可以调用 bin/spark-submit 脚本，如下所示，同时传递你的 jar。 对于Python，你可以使用 spark-submit 的 --py-files 参数来添加 .py， .zip 或 .egg 文件来与应用程序一起分发。如果你依赖于多个 Python 文件，我们建议将它们打包成一个 .zip 或 .egg 文件。 4. 使用spark-submit启动应用程序用户应用程序打包成功后，就可以使用 bin/spark-submit 脚本启动应用程序。脚本负责设置 Spark 及其依赖关系的 classpath，并且可以支持不同集群管理器和部署模式（Spark所支持的）：./bin/spark-submit \ --class &lt;main-class&gt; \ --master &lt;master-url&gt; \ --deploy-mode &lt;deploy-mode&gt; \ --conf &lt;key&gt;=&lt;value&gt; \ ... # other options &lt;application-jar&gt; \ [application-arguments] 一些常用的选项： --class: 应用程序入口 (例如：com.sjf.open.spark.Java.JavaWordCount 包含包名的全路径名称) --master: 集群的 master URL (例如：spark://23.195.26.187:7077) --deploy-mode: 是在工作节点(cluster)上还是在本地作为一个外部的客户端(client)部署你的 driver (默认: client) --conf: 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。 application-jar: 包含应用程序和所有依赖关系的 jar 路径。URL必须在集群内部全局可见，例如，对所有节点上可见的 hdfs：// 路径或 file：// 路径。 application-arguments: 传递给主类 main 方法的参数（如果有的话） Example:bin/spark-submit --class com.sjf.open.spark.Java.JavaWordCount --master local common-tool-jar-with-dependencies.jar /home/xiaosi/click_uv.txt 常见的部署策略是将你的应用程序从与工作节点机器物理位置相同的网关机器（例如，独立EC2集群中的主节点）提交。在这种设置中， client 模式比较合适。在 client 模式中，驱动程序作为集群的客户端直接在 spark-submit 进程内启动。应用程序的输入和输出直接连到控制台。因此，这个模式特别适合那些涉及 REPL（例如，Spark shell）的应用程序。 如果你提交应用程序的机器远离工作节点机器（例如在笔记本电脑本地提交），则通常使用 cluster 模式来最小化 drivers 和 executors 之间的网络延迟。目前，对于 Python 应用程序而言，在独立模式上不支持集群模式。 对于Python应用程序，只需在 &lt;application-jar&gt; 位置传递一个 .py 文件来代替 JAR，然后使用 --py-files 参数将 Python 的 .zip，.egg 或 .py 文件添加到搜索路径。 有几个可用选项是特定用于集群管理器。例如，对于具有集群部署模式的Spark独立集群，可以指定 --supervise 参数以确保如果驱动程序以非零退出码失败时，可以自动重新启动。如果要列举 spark-submit 所有可用选项，可以使用 spark-submit --help 命令来查看。以下是常见选项的几个示例： # 在本地运行 8 核./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master local[8] \ /path/to/examples.jar \ 100# 以客户端部署模式在Spark独立集群上运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000# 在集群部署模式下使用supervise在Spark独立集群上运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000# 在 YARN 集群上运行export HADOOP_CONF_DIR=XXX./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ # can be client for client mode --executor-memory 20G \ --num-executors 50 \ /path/to/examples.jar \ 1000# 在 Spark 独立集群上运行Python程序./bin/spark-submit \ --master spark://207.184.161.138:7077 \ examples/src/main/python/pi.py \ 1000# 在集群部署模式下使用supervise在Mesos集群上运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master mesos://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ http://path/to/examples.jar \ 1000 5. Master Urls传递给Spark的master url 可以采用如下格式： Master URL 描述 local 使用一个线程本地运行 Spark。 local[K] 使用K个工作线程本地运行 Spark（理想情况下，设置这个值的数量为你机器内核数量）。 local[K,F] 使用K工作线程和F个 maxFailures 在本地运行 Spark（有关此变量的解释，请参阅spark.task.maxFailures） local[*] 使用与你机器上的逻辑内核一样多的工作线程在本地运行 Spark。 local[*,F] 使用与你机器上的逻辑内核一样多的工作线程以及F个 maxFailures 在本地运行 Spark。 spark://HOST:PORT 连接到给定的Spark独立集群主机。端口必须是主机配置可使用的端口，默认情况下为7077。 spark://HOST1:PORT1,HOST2:PORT2 使用 Zookeeper 连接到具有备用 masters 的给定 Spark 独立集群。该列表必须包含使用 Zookeeper 搭建的高可用集群中的所有 master 主机。端口必须是每个 master 可以配置使用的端口，默认情况下为7077。 mesos://HOST:PORT 连接到给定的Mesos集群。端口必须是主机配置可使用的端口，默认为5050。或者，对于使用ZooKeeper的Mesos集群，借助 --deploy-mode cluster 参数使用 mesos://zk:// .... 提交。 yarn 以客户端模式还是以集群模式连接到YARN群集具体取决于 --deploy-mode 的值。可以根据HADOOP_CONF_DIR或YARN_CONF_DIR变量找到集群位置 6. 从文件加载配置spark-submit 脚本可以从 properties 文件加载默认 Spark 配置选项，并将它们传递到应用程序。默认情况下，spark 从 spark 目录下的 conf/spark-defaults.conf 配置文件中读取配置选项。有关更多详细信息，请参考加载默认配置。 以这种方式加载 Spark 默认配置可以避免在 spark-submit 上添加配置选项。例如，如果默认配置文件中设置了 spark.master 属性，那么可以安全地从 spark-submit 中省略 --master 参数。一般来说，在 SparkConf 上显式设置的配置选项拥有最高优先级，然后是传递到 spark-submit 的配置选项，最后是默认配置文件中的配置选项。 如果不清楚配置选项来自哪里，可以通过使用 --verbose 选项运行 spark-submit 打印出细粒度的调试信息。 7. 高级依赖管理使用 spark-submit 时，包含在 --jars 选项中的应用程序 jar 以及其他 jar 将自动分发到集群。在 --jars 之后提供的 URL 列表必须用逗号分隔。该列表会包含在 driver 和 executor 的 classpath 中。--jars 不支持目录的形式。 Spark使用如下URL来允许以不同策略分发 jar： file : 绝对路径和 file:/URI 通过 driver 的HTTP文件服务器提供，每个 executor 从 driver HTTP服务器上拉取文件。 hdfs : http :, https :, ftp： 正如你希望的一样，从这些URI拉取文件和 JAR。 local : 以 local:/ 开头的URI应该作为每个工作节点上的本地文件存在。这意味着不会产生网络IO，适用于推送大文件或者JAR到每个工作线程或通过 NFS，GlusterFS 等方式共享这些大文件或者jar。 请注意，JAR和文件被复制到 executor 节点上每个 SparkContext 的工作目录下。随着时间的推移，这可能会占用大量的空间，需要定时清理。使用 YARN，清理会自动执行；使用 Spark 独立集群，可以使用 spark.worker.cleanup.appDataTtl 属性配置自动清理。 用户还可以通过用 --packages 提供以逗号分隔的 maven 坐标列表来包含任何其他依赖项。使用此命令时将处理所有传递依赖性。可以使用配置选项 --repositories 以逗号分隔的方式添加其他存储库（或SBT中的解析器）。pyspark，spark-shell和 spark-submit 都可以使用这些命令来包含 Spark 的 Packages。 对于Python，等价的 --py-files 选项可用于将 .egg，.zip 和 .py 库分发给执行程序。 Spark版本:2.3.0 原文:http://spark.apache.org/docs/2.3.0/submitting-applications.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之数据倾斜调优]]></title>
    <url>%2F2018%2F04%2F05%2Fspark-performance-data-skew-tuning%2F</url>
    <content type="text"><![CDATA[1. 为何要处理数据倾斜1.1 什么是数据倾斜对Spark/Hadoop这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。 何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 1.2 数据倾斜是如何造成的在Spark中，同一个Stage的不同Partition可以并行处理，而具体依赖关系的不同Stage之间是串行处理的。假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。 由于同一个Stage内的所有Task执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该Task所处理的数据量决定。 Stage的数据来源主要分为如下两类： 从数据源直接读取。如读取HDFS，Kafka 读取上一个Stage的Shuffle数据 2. 如何缓解/消除数据倾斜2.1 尽量避免数据源的数据倾斜以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。 Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。 但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的PV信息置于同一个Partition中。此时，如果产生了数据倾斜，则需要通过其它方式处理。 2.2 调整并行度分散同一个Task的不同Key2.2.1 原理Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。 如果调整Shuffle时的并行度，使得原本被分配到同一Task的不同Key发配到不同Task上处理，则可降低原Task所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。 2.2.2 案例现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5亿条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如下INSERT OVERWRITE TABLE testSELECT CASE WHEN id &lt; 940000000 THEN (9500000 + (CAST (RAND() * 8 AS INTEGER)) * 12 ) ELSE CAST(id/100 AS INTEGER) END, nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000; 通过上述处理，一份可能造成后续数据倾斜的测试数据即以准备好。接下来，使用Spark读取该测试数据，并通过groupByKey(12)对id分组处理，且Shuffle并行度为12。代码如下public class SparkDataSkew &#123; public static void main(String[] args) &#123; SparkSession sparkSession = SparkSession.builder() .appName("SparkDataSkewTunning") .config("hive.metastore.uris", "thrift://hadoop1:9083") .enableHiveSupport() .getOrCreate(); Dataset&lt;Row&gt; dataframe = sparkSession.sql( "select * from test"); dataframe.toJavaRDD() .mapToPair((Row row) -&gt; new Tuple2&lt;Integer, String&gt;(row.getInt(0),row.getString(1))) .groupByKey(12) .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123; int id = tuple._1(); AtomicInteger atomicInteger = new AtomicInteger(0); tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet()); return new Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get()); &#125;).count(); sparkSession.stop(); sparkSession.close(); &#125;&#125; 本次实验所使用集群节点数为4，每个节点可被Yarn使用的CPU核数为16，内存为16GB。使用如下方式提交上述应用，将启动4个Executor，每个Executor可使用核数为12（该配置并非生产环境下的最优配置，仅用于本文实验），可用内存为12GB。spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar GroupBy Stage的Task状态如下图所示，Task 8处理的记录数为4500万，远大于（9倍于）其它11个Task处理的500万记录。而Task 8所耗费的时间为38秒，远高于其它11个Task的平均时间（16秒）。整个Stage的时间也为38秒，该时间主要由最慢的Task 8决定。 在这种情况下，可以通过调整Shuffle并行度，使得原来被分配到同一个Task（即该例中的Task 8）的不同Key分配到不同Task，从而降低Task 8所需处理的数据量，缓解数据倾斜。 通过groupByKey(48)将Shuffle并行度调整为48，重新提交到Spark。新的Job的GroupBy Stage所有Task状态如下图所示。 从上图可知，记录数最多的Task 20处理的记录数约为1125万，相比于并行度为12时Task 8的4500万，降低了75%左右，而其耗时从原来Task 8的38秒降到了24秒。 在这种场景下，调整并行度，并不意味着一定要增加并行度，也可能是减小并行度。如果通过groupByKey(11)将Shuffle并行度调整为11，重新提交到Spark。新Job的GroupBy Stage的所有Task状态如下图所示。 从上图可见，处理记录数最多的Task 6所处理的记录数约为1045万，耗时为23秒。处理记录数最少的Task 1处理的记录数约为545万，耗时12秒。 2.2.3 总结(1) 适用场景 大量不同的Key被分配到了相同的Task造成该Task数据量过大。 (2) 解决方案 调整并行度。一般是增大并行度，但有时如本例减小并行度也可达到效果。 (3) 优势 实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用 spark.default.parallelism 设置。如果是Spark SQL，还可通过 SET spark.sql.shuffle.partitions=[num_tasks] 设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。 (4) 劣势 适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般 2.3 自定义Partitioner2.3.1 原理使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。 2.3.2 案例以上述数据集为例，继续将并发度设置为12，但是在groupByKey算子上，使用自定义的Partitioner（实现如下）.groupByKey(new Partitioner() &#123; @Override public int numPartitions() &#123; return 12; &#125; @Override public int getPartition(Object key) &#123; int id = Integer.parseInt(key.toString()); if(id &gt;= 9500000 &amp;&amp; id &lt;= 9500084 &amp;&amp; ((id - 9500000) % 12) == 0) &#123; return (id - 9500000) / 12; &#125; else &#123; return id % 12; &#125; &#125;&#125;) 由下图可见，使用自定义Partition后，耗时最长的Task 6处理约1000万条数据，用时15秒。并且各Task所处理的数据集大小相当。 2.3.3 总结(1) 适用场景 大量不同的Key被分配到了相同的Task造成该Task数据量过大。 (2) 解决方案 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。 (3) 优势 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。 (4) 劣势 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 2.4 将Reduce side Join转变为Map side Join2.4.1 原理通过Spark的Broadcast机制，将Reduce侧Join转化为Map侧Join，避免Shuffle从而完全消除Shuffle带来的数据倾斜。 2.4.2 案例通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。INSERT OVERWRITE TABLE testSELECT CAST(CASE WHEN id &lt; 980000000 THEN (95000000 + (CAST (RAND() * 4 AS INT) + 1) * 48 ) ELSE CAST(id/10 AS INT) END AS STRING), nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000; 使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。INSERT OVERWRITE TABLE test_newSELECT CAST(CAST(id/10 AS INT) AS STRING), nameFROM student_delta_externalWHERE id BETWEEN 950000000 AND 950500000; 直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id; 该SQL对应的DAG如下图所示。从该图可见，该执行过程总共分为三个Stage，前两个用于从Hive中读取数据，同时二者进行Shuffle，通过最后一个Stage进行Join并将结果写入表test_join中。 从下图可见，最近Join Stage各Task处理的数据倾斜严重，处理数据量最大的Task耗时7.1分钟，远高于其它无数据倾斜的Task约2s秒的耗时。 接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过CACHE TABLE test_new将小表test_new进行cache。现通过如下SQL进行Join。CACHE TABLE test_new;INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id; 通过如下DAG图可见，该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。 并且数据倾斜仍然存在。如下图所示，最慢的Task耗时为7.1分钟，远高于其它Task的约2秒。 正确的使用Broadcast实现Map侧Join的方式是，通过SET spark.sql.autoBroadcastJoinThreshold=104857600;将Broadcast的阈值设置得足够大。 再次通过如下SQL进行Join。SET spark.sql.autoBroadcastJoinThreshold=104857600;INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id; 通过如下DAG图可见，该方案只包含一个Stage。 并且从下图可见，各Task耗时相当，无明显数据倾斜现象。并且总耗时为1.5分钟，远低于Reduce侧Join的7.3分钟。 2.4.3 总结(1) 适用场景 参与Join的一边数据集足够小，可被加载进Driver并通过Broadcast方法广播到各个Executor中。 (2) 优势 避免了Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。 (4) 劣势 要求参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。 2.5 为skew的key增加随机前/后缀2.5.1 原理为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。 2.5.2 案例通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。 对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。INSERT OVERWRITE TABLE testSELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000 + (CAST (RAND() * 2 AS INT) + 1) * 48 ) ELSE CAST(id/100 AS INT) END AS STRING), nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000;INSERT OVERWRITE TABLE test_newSELECT CAST(CAST(id/100 AS INT) AS STRING), nameFROM student_delta_externalWHERE id BETWEEN 950000000 AND 950500000; 通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。public class SparkDataSkew&#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setAppName("DemoSparkDataFrameWithSkewedBigTableDirect"); sparkConf.set("spark.default.parallelism", parallelism + ""); JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); leftRDD.join(rightRDD, parallelism) .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2())) .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123; AtomicInteger atomicInteger = new AtomicInteger(); iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet()); &#125;); javaSparkContext.stop(); javaSparkContext.close(); &#125;&#125; 从下图可看出，整个Join耗时1分54秒，其中Join Stage耗时1.7分钟。 通过分析Join Stage的所有Task可知，在其它Task所处理记录数为192.71万的同时Task 32的处理的记录数为992.72万，故它耗时为1.7分钟，远高于其它Task的约10秒。这与上文准备数据集时，将id为9500048为9500096对应的数据量设置非常大，其它id对应的数据集非常均匀相符合。 现通过如下操作，实现倾斜Key的分散处理 将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD 将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD 将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD 将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD 对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD 通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集具体实现代码如下public class SparkDataSkew&#123; public static void main(String[] args) &#123; int parallelism = 48; SparkConf sparkConf = new SparkConf(); sparkConf.setAppName("SolveDataSkewWithRandomPrefix"); sparkConf.set("spark.default.parallelism", parallelism + ""); JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); String[] skewedKeyArray = new String[]&#123;"9500048", "9500096"&#125;; Set&lt;String&gt; skewedKeySet = new HashSet&lt;String&gt;(); List&lt;String&gt; addList = new ArrayList&lt;String&gt;(); for(int i = 1; i &lt;=24; i++) &#123; addList.add(i + ""); &#125; for(String key : skewedKeyArray) &#123; skewedKeySet.add(key); &#125; Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys = javaSparkContext.broadcast(skewedKeySet); Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList); JavaPairRDD&lt;String, String&gt; leftSkewRDD = leftRDD .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1())) .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + "," + tuple._1(), tuple._2())); JavaPairRDD&lt;String, String&gt; rightSkewRDD = rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1())) .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream() .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + "," + tuple._1(), tuple._2())) .collect(Collectors.toList()) .iterator() ); JavaPairRDD&lt;String, String&gt; skewedJoinRDD = leftSkewRDD .join(rightSkewRDD, parallelism) .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(",")[1], tuple._2()._2())); JavaPairRDD&lt;String, String&gt; leftUnSkewRDD = leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1())); JavaPairRDD&lt;String, String&gt; unskewedJoinRDD = leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2())); skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123; AtomicInteger atomicInteger = new AtomicInteger(); iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet()); &#125;); javaSparkContext.stop(); javaSparkContext.close(); &#125;&#125; 从下图可看出，整个Join耗时58秒，其中Join Stage耗时33秒。 通过分析Join Stage的所有Task可知 由于Join分倾斜数据集Join和非倾斜数据集Join，而各Join的并行度均为48，故总的并行度为96 由于提交任务时，设置的Executor个数为4，每个Executor的core数为12，故可用Core数为48，所以前48个Task同时启动（其Launch时间相同），后48个Task的启动时间各不相同（等待前面的Task结束才开始） 由于倾斜Key被加上随机前缀，原本相同的Key变为不同的Key，被分散到不同的Task处理，故在所有Task中，未发现所处理数据集明显高于其它Task的情况 实际上，由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。而本实验受限于可用总核数为48，可同时运行的总Task数为48，故而该方案只是将总耗时减少一半（效率提升一倍）。如果资源充足，可并发执行Task数增多，该方案的优势将更为明显。在实际项目中，该方案往往可提升数倍至10倍的效率。 2.5.3 总结(1) 适用场景 两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。 (2) 解决方案 将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。 (3) 优势 相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。 (4) 劣势 如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。 2.6 大表随机添加N种随机前缀，小表扩大N倍2.6.1 原理如果出现数据倾斜的Key比较多，上一种方法将这些大量的倾斜Key分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。 2.6.2 案例这里给出示例代码，读者可参考上文中分拆出少数倾斜Key添加随机前缀的方法，自行测试。public class SparkDataSkew &#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setAppName("ResolveDataSkewWithNAndRandom"); sparkConf.set("spark.default.parallelism", parallelism + ""); JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile("hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/") .mapToPair((String row) -&gt; &#123; String[] str = row.split(","); return new Tuple2&lt;String, String&gt;(str[0], str[1]); &#125;); List&lt;String&gt; addList = new ArrayList&lt;String&gt;(); for(int i = 1; i &lt;=48; i++) &#123; addList.add(i + ""); &#125; Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList); JavaPairRDD&lt;String, String&gt; leftRandomRDD = leftRDD.mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(new Random().nextInt(48) + "," + tuple._1(), tuple._2())); JavaPairRDD&lt;String, String&gt; rightNewRDD = rightRDD .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream() .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + "," + tuple._1(), tuple._2())) .collect(Collectors.toList()) .iterator() ); JavaPairRDD&lt;String, String&gt; joinRDD = leftRandomRDD .join(rightNewRDD, parallelism) .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(",")[1], tuple._2()._2())); joinRDD.foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123; AtomicInteger atomicInteger = new AtomicInteger(); iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet()); &#125;); javaSparkContext.stop(); javaSparkContext.close(); &#125;&#125; 2.6.3 总结(1) 适用场景 一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。 (2) 优势 对大部分场景都适用，效果不错。 (3) 劣势 需要将一个数据集整体扩大N倍，会增加资源消耗。 3. 总结对于数据倾斜，并无一个统一的一劳永逸的方法。更多的时候，是结合数据特点（数据集大小，倾斜Key的多少等）综合使用上文所述的多种方法。 原文:http://www.infoq.com/cn/articles/the-road-of-spark-performance-tuning]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 Input DStreams和Receivers]]></title>
    <url>%2F2018%2F04%2F05%2Fspark-streaming-input-dstreams-and-receivers%2F</url>
    <content type="text"><![CDATA[1. 输入DStream与Receiver输入 DStreams 表示从 source 中获取输入数据流的 DStreams。在入门示例中，lines 表示输入DStream，它代表从netcat服务器获取的数据流。每一个输入DStream(除 file stream)都与一个 Receiver (接收器)相关联，接收器从 source 中获取数据，并将数据存入 Spark 内存中来进行处理。输入 DStreams 表示从数据源获取的原始数据流。Spark Streaming 提供了两类内置的流源（streaming sources）： 基础数据源(Basic sources)：在 StreamingContext API 中可以直接使用的数据源。例如：文件系统(file system)和套接字连接(socket connections)。 高级数据源(Advanced sources)：例如 Kafka，Flume，Kinesis 等数据源可通过额外的utility classes获得。这些需要额外依赖。 我们将稍后讨论这两类数据源。 请注意，如果希望在流应用程序中并行的接收多个数据流，你可以创建多个输入 DStream（在性能调优部分中进一步讨论）。这需要创建多个接收器（Receivers），来同时接收多个数据流。但请注意，Spark 的 worker/executor 是一个长期运行的任务，因此会占用分配给 Spark Streaming 应用程序的其中一个核（core）。因此，记住重要的一点，Spark Streaming 应用程序需要分配足够的核（或线程，如果在本地运行）来处理接收的数据，以及来运行接收器。 注意 当在本地运行 Spark Streaming 程序时，不要使用 local 或 local [1] 作为 master 的 URL。这两个都意味着只会有一个线程用于本地任务运行。如果使用基于接收器（例如套接字，Kafka，Flume等）的输入 DStream，那么唯一的那个线程会用于运行接收器，不会有其他线程来处理接收到的数据。因此，在本地运行时，始终使用 local [n] 作为 master 的 URL，其中 n &gt; 要运行的接收器的数目。 将逻辑扩展到集群上运行，分配给 Spark Streaming 应用程序的核数量必须大于接收器的数量。否则系统将只接收数据，而无法处理。 2. 源2.1 基础数据源在入门实例中我们已经了解到 ssc.socketTextStream（...），它通过 TCP 套接字连接从数据服务器获取文本数据创建 DStream。除了套接字，StreamingContext API 也提供了把文件作为输入源创建 DStreams 的方法。 2.1.1 File Streams可以从与 HDFS API 兼容的任何文件系统（即，HDFS，S3，NFS等）上的文件读取数据，DStream 可以使用如下命令创建： Java:streamingContext.fileStream&lt;KeyClass, ValueClass, InputFormatClass&gt;(dataDirectory); Scala:streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) Spark Streaming 会监视 dataDirectory 目录并处理在该目录中创建的任何文件（不支持嵌套目录中写入的文件）。 注意 所有文件必须具有相同的数据格式 通过原子地移动或重命名它们到数据目录中，来在dataDirectory目录下创建文件。 一旦移动到dataDirectory目录后，不能进行更改。因此，如果文件被连续追加数据，新的数据将不会被读取。 对于简单的文本文件，有一个更简单的方法：streamingContext.textFileStream（dataDirectory） 文件流不需要运行接收器（Receiver），因此不需要分配核。 fileStream 在 Python API 中不可用，只有 textFileStream 可用。 2.1.2 基于自定义的Receivers的流可以使用通过自定义的接收器接收的数据流创建 DStream。有关详细信息，请参阅自定义接收器指南。 2.1.3 RDD队列作为一个流要使用测试数据测试 Spark Streaming 应用程序，还可以使用 streamingContext.queueStream（queueOfRDDs） 基于 RDD 队列创建 DStream。 推送到队列中的每个 RDD 将被视为 DStream 中的一批次数据，并像流一样处理。 2.2 高级数据源这类数据源需要使用非Spark库的外部接口，其中一些需要复杂依赖（例如，Kafka和Flume）。因此，为了尽量减少依赖的版本冲突问题，这些数据源本身不能创建 DStream 的功能，它是通过 依赖 单独的类库实现创建 DStream 的功能。 请注意，这些高级源在 Spark Shell 中不可用，因此基于这些高级数据源的应用程序无法在 shell 中测试。如果你真的想在 Spark shell 中使用它们，那么你必须下载相应的 Maven 组件的JAR及其依赖项，并将其添加到 classpath 中。 介绍一下常用的高级数据源： Kafka：Spark Streaming 2.1.0与Kafka代理版本0.8.2.1或更高版本兼容。 有关更多详细信息，请参阅Kafka集成指南。 Flume：Spark Streaming 2.1.0与Flume 1.6.0兼容。 有关更多详细信息，请参阅Flume集成指南。 Kinesis：Spark Streaming 2.1.0与Kinesis Client Library 1.2.1兼容。 有关更多详细信息，请参阅Kinesis集成指南。 3. 自定义数据源这在Python中还不支持。 输入DStreams也可以从自定义数据源中创建。如果你这样做，需要实现一个自定义接收器（Receiver），可以从自定义数据源接收数据，并推送到Spark。有关详细信息，请参阅自定义接收器指南。 4. Receiver的可靠性基于Receiver的可靠性，可以分为两种数据源。如Kafka和Flume之类的数据源允许传输的数据被确认。如果从这些可靠源接收数据，并且被确认正确的接收数据，则可以确保不会由于任何种类的故障而丢失数据。这样就出现了两种接收器（Receiver）： 可靠的接收器 - 当数据被接收并存储在Spark中，同时备份副本，可靠的接收器正确地向可靠的源发送确认。 不可靠的接收器 - 不可靠的接收器不会向数据源发送确认。这可以用在不支持确认机制的数据源上，或者甚至是可靠的数据源当你不想或者不需要进行复杂的确认的时候。 Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#input-dstreams-and-receivers]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 DStreams]]></title>
    <url>%2F2018%2F04%2F04%2Fspark-streaming-discretized-streams-dstreams%2F</url>
    <content type="text"><![CDATA[离散流或者 DStreams 是 Spark Streaming 提供的基本抽象，它代表一个连续的数据流。从 source 中获取输入流，或者是输入流通过转换算子处理后生成的数据流。在内部，DStreams 由一系列连续的 RDD 组成。它是 Spark 中一个不可改变的抽象，分布式数据集的抽象（更多细节参见Spark编程指南）。DStream 中的每个 RDD 包含来自特定间隔的数据，如下图所示： 对 DStream 应用的任何操作都会转换为对 DStream 底层的 RDD 操作。例如，在之前的示例中将行数据流转换单词数据流，flatMap 操作应用于 lines 这个 DStreams 中的每个 RDD，生成 words 这个 DStreams 的 RDD。过程如下图所示： 这些底层的 RDD 转换操作由 Spark 引擎计算。DStream 操作隐藏了大部分细节，并为开发人员提供了更高级别的API以方便使用。这些操作将在后面的章节中详细讨论。 Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#discretized-streams-dstreams]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 初始化StreamingContext]]></title>
    <url>%2F2018%2F04%2F03%2Fspark-streaming-initializing-streamingcontext%2F</url>
    <content type="text"><![CDATA[为了初始化 Spark Streaming 程序，必须创建一个 StreamingContext 对象，它是 Spark Streaming 所有流操作的主要入口。StreamingContext 对象可以用 SparkConf 对象创建。 可以使用SparkConf对象创建JavaStreamingContext对象（对于Scala和Python语言来说，创建 StreamingContext对象）： Java版本:SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(seconds)); Scala版本:import org.apache.spark._import org.apache.spark.streaming._val conf = new SparkConf().setAppName(appName).setMaster(master)val ssc = new StreamingContext(conf, Seconds(1)) Python:from pyspark import SparkContextfrom pyspark.streaming import StreamingContextsc = SparkContext(master, appName)ssc = StreamingContext(sc, 1) appName 参数是应用程序在集群UI上显示的名称。master 是Spark，Mesos或YARN集群URL，或者是以本地模式运行的特殊字符串local [*]。 实际上，当在集群上运行时，如果你不想在程序中硬编码 master(即在程序中写死)，而是希望使用 spark-submit 启动应用程序时得到 master 的值。对于本地测试和单元测试，你可以传递 local [*] 来运行 Spark Streaming 进程。注意，这里内部创建的 JavaSparkContext（所有Spark功能的起始点），可以通过 jsc.sparkContext 访问。 JavaStreamingContext对象也可以从现有的JavaSparkContext创建。对于Scala来说，StreamingContext对象也可以从现有的 SparkContext 创建： Java版本:SparkConf conf = new SparkConf().setAppName("socket-spark-stream").setMaster("local[2]");JavaSparkContext sparkContext = new JavaSparkContext(conf);JavaStreamingContext jsc = new JavaStreamingContext(sparkContext, Durations.seconds(seconds)); Scala版本:import org.apache.spark.streaming._val sc = ... // existing SparkContextval ssc = new StreamingContext(sc, Seconds(1)) 批处理间隔必须根据应用程序和可用群集资源的延迟要求进行设置。 有关更多详细信息，请参阅“性能调优”部分。 定义上下文后，您必须执行以下操作： 通过创建输入DStreams定义输入源 通过对DStreams应用转换操作（transformation）和输出操作（output）来定义流计算 可以使用streamingContext.start()方法接收和处理数据 可以使用streamingContext.awaitTermination()方法等待流计算完成（手动或由于任何错误），来防止应用退出 可以使用streamingContext.stop（）手动停止处理。 注意点: 一旦上下文已经开始，则不能设置或添加新的流计算。 上下文停止后，无法重新启动。 在同一时间只有一个StreamingContext可以在JVM中处于活动状态。 在StreamingContext上调用stop()方法，也会关闭SparkContext对象。如果只想关闭StreamingContext对象，设置stop()的可选参数为false。 一个SparkContext可以重复利用创建多个StreamingContext，只要在创建下一个StreamingContext之前停止前一个StreamingContext（而不停止SparkContext）即可。 Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#initializing-streamingcontext]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 2.2.0 Example]]></title>
    <url>%2F2018%2F04%2F02%2Fspark-streaming-first-example%2F</url>
    <content type="text"><![CDATA[1. 概述Spark Streaming 是 Spark Core API的一个扩展，它对实时流式数据的处理具有可扩展性、高吞吐量、可容错性等特点。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等许多源中提取，并且可以使用由诸如map，reduce，join或者 window 等高级函数组成的复杂算法来处理。最后，处理后的数据可以推送到文件系统、数据库、实时仪表盘中。事实上，你可以将处理后的数据应用到 Spark 的机器学习算法、 图处理算法中去。 它的内部工作原理如下图所示。Spark Streaming 接收实时输入数据流，并将数据分成多个批次，然后由 Spark 引擎处理，批量生成最终结果数据流。 Spark Streaming 提供了一个叫做离散流(discretized stream)或称作 DStream 的高级抽象，它表示连续的数据流。DStreams 可以从如 Kafka，Flume和 Kinesis 等数据源的输入数据流创建，也可以通过对其他 DStreams 应用高级操作来创建。在内部，DStream 表示为 RDD 序列，即由一系列的 RDD 组成。 本文章介绍如何使用 DStreams 编写 Spark Streaming 程序。 可以在Scala，Java或Python（在Spark 1.2中介绍）中编写Spark Streaming程序，本文只要使用Java作为演示示例，其他可以参考原文。 2. Example在我们进入如何编写自己的Spark Streaming程序之前，让我们快速看看一个简单的Spark Streaming程序的具体样子。 假设我们要计算从监听TCP套接字的数据服务器接收的文本数据中的统计文本中包含的单词数。 首先，我们创建一个JavaStreamingContext对象，这是所有流功能的主要入口点。 我们创建一个具有两个执行线程的本地StreamingContext，并且批处理间隔为1秒。 import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaStreamingContext;SparkConf conf = new SparkConf().setAppName("socket-spark-stream").setMaster("local[2]");JavaSparkContext sparkContext = new JavaSparkContext(conf);JavaStreamingContext jsc = new JavaStreamingContext(sparkContext, Durations.seconds(1)); 使用此context，我们可以创建一个DStream，表示来自TCP源的流数据，指定主机名（例如localhost）和端口（例如7777）: import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;private static String hostName = "localhost";private static int port = 7777;// 以端口7777作为输入源创建DStreamJavaReceiverInputDStream&lt;String&gt; lines = jsc.socketTextStream(hostName, port); lines DStream表示从数据服务器接收的数据流。 此流中的每个记录都是一行文本。 然后，我们要将每行文本切分为单词：// 从DStream中将每行文本切分为单词JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String x) &#123; return Arrays.asList(x.split(" ")).iterator(); &#125;&#125;); flatMap是一个DStream操作，通过从源DStream中的每个记录生成多个新记录来创建新的DStream。 在我们例子中，每一行将被拆分成多个单词，并且单词数据流用 words 这个DStream来表示。 注意，我们使用FlatMapFunction对象定义了一个转换操作。 正如我们将会发现，在Java API中有许多这样的类帮主我们定义DStream转换操作。 下一步，我们计算单词的个数：// 在每个批次中计算单词的个数JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;&gt;(s, 1); &#125;&#125;);JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125;&#125;);// 将此DStream中生成的每个RDD的前10个元素打印到控制台wordCounts.print(); 使用PairFunction对象将words 这个DStream进一步映射（一对一变换）为（word，1）键值对的DStream。 然后，使用Function2对象，计算得到每批次数据中的单词出现的频率。 最后，wordCounts.print()将打印每秒计算的词频。 这只是设定好了要进行的计算，系统收到数据时计算就会开始。要开始接收数据，必须显式调用StreamingContext的start()方法。这样，SparkStreaming 就会开始把Spark作业不断的交给SparkContext去调度。执行会在另一个线程中进行，所以需要调用awaitTermination来等待流计算完成，来防止应用退出。 // 启动流计算环境StreamingContext并等待完成jsc.start();// 等待作业完成jsc.awaitTermination(); 注意一个Streaming context 只启动一次，所以只有在配置好所有DStream以及所需的操作之后才能启动。 如果你已经下载和构建了Spark环境，你就能够用如下的方法运行这个例子。首先，你需要运行Netcat作为数据服务器：xiaosi@yoona:~$ nc -lk 7777hello I am yoona hello... 然后，在不同的终端，你能够用如下方式运行例子:xiaosi@yoona:~/opt/spark-2.1.0-bin-hadoop2.7$ bin/spark-submit --class com.sjf.open.spark.stream.SocketSparkStreaming /home/xiaosi/code/Common-Tool/target/common-tool-jar-with-dependencies.jar 输出信息：-------------------------------------------Time: 1488348756000 ms-------------------------------------------(am,1)(,1)(yoona,1)(hello,2)(I,1) 3. Maven依赖与Spark类似，Spark Streaming通过Maven Central提供。 要编写自己的Spark Streaming程序，您必须将以下依赖项添加到Maven项目中。&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 对于Spark Streaming核心API中不存在的来源（如Kafka，Flume和Kinesis）获取数据，您必须将相应的组件 spark-streaming-xyz_2.11 添加到依赖项中。 例如，一些常见的如下： Source Artifact Kafka spark-streaming-kafka-0-8_2.11 Flume spark-streaming-flume_2.11 Kinesis spark-streaming-kinesis-asl_2.11 [Amazon Software License] 为了获取最新的列表，请访问Apache repository Spark Streaming 版本: 2.2.0 原文：http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html#a-quick-example]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming与Kafka如何保证数据零丢失]]></title>
    <url>%2F2018%2F04%2F01%2Fspark-streaming-kafka-integration-achieve-zero-data-loss%2F</url>
    <content type="text"><![CDATA[Spark Streaming 是一种构建在 Spark 上的实时计算框架，它扩展了 Spark 处理大规模流式数据的能力。Spark Streaming 的优势在于： 能运行在1000+的结点上，并达到秒级延迟。 使用基于内存的 Spark 作为执行引擎，具有高效和容错的特性。 能集成 Spark 的批处理和交互查询。 为实现复杂的算法提供和批处理类似的简单接口。 为此，Spark Streaming受到众多企业的追捧，并将其大量用于生产项目；然而，在使用过程中存在一些辣手的问题。本文将介绍使用Spark Streaming进行实时处理的一个关于保证数据零丢失的经验。 在Spark Streaming的生产实践中，要做到数据零丢失，你需要满足以下几个先决条件： 输入的数据源是可靠的； 数据接收器是可靠的； 元数据持久化; 启用了WAL特性（Write ahead log）； Exactly-Once。 下面将简单地介绍这些先决条件。 1. 输入的数据源是可靠的Spark Streaming实时处理数据零丢失，需要类似Kafka的数据源： 支持在一定时间范围内重新消费； 支持高可用消费； 支持消费确认机制； 具有这些特征的数据源，可以使得消费程序准确控制消费位置，做到完全掌控消费情况的程度，为数据零丢失打下基础。 2. 数据接收器是可靠的Spark Streaming可以对已经接收的数据进行确认。输入的数据首先被接收器（Receivers）所接收，然后存储到Spark内部。数据一旦存储到Spark中，接收器可以对它进行确认。这种机制保证了在接收器突然挂掉的情况下也不会丢失数据：因为数据虽然被接收，但是没有被持久化的情况下是不会发送确认消息的。所以在接收器恢复的时候，数据可以被原端重新发送。 3. 元数据持久化可靠的数据源和接收器可以让实时计算程序从接收器挂掉的情况下恢复。但是更棘手的问题是，如果Driver挂掉如何恢复？使用Checkpoint应用程序元数据的方法可以解决这一问题。为此，Driver可以将应用程序的重要元数据（包含：配置信息、计算代码、未处理的batch数据）持久化到可靠的存储中，比如HDFS、S3；然后Driver可以利用这些持久化的数据进行恢复。 由于有了元数据的Checkpoint，所以Driver可以利用他们重构应用程序，而且可以计算出Driver挂掉的时候应用程序执行到什么位置。 通过持久化元数据，并能重构应用程序，貌似解决了数据丢失的问题，然而在以下场景任然可能导致数据丢失： 1）两个Exectuor已经从接收器中接收到输入数据，并将它缓存到Exectuor的内存中； 2）接收器通知输入源数据已经接收； 3）Exectuor根据应用程序的代码开始处理已经缓存的数据； 4）这时候Driver突然挂掉了； 5）从设计的角度看，一旦Driver挂掉之后，它维护的Exectuor也将全部被kill； 6）既然所有的Exectuor被kill了，所以缓存到它们内存中的数据也将被丢失。结果，这些已经通知数据源但是还没有处理的缓存数据就丢失了； 7）缓存的时候不可能恢复，因为它们是缓存在Exectuor的内存中，所以数据被丢失了。 这对于很多关键型的应用程序来说还是无法容忍。这时，Spark团队再次引入了WAL解决以上这些问题。 4. WAL（Write ahead log）启用了WAL机制，所以已经接收的数据被接收器写入到容错存储中，比如HDFS或者S3。由于采用了WAl机制，Driver可以从失败的点重新读取数据，即使Exectuor中内存的数据已经丢失了。在这个简单的方法下，Spark Streaming提供了一种即使是Driver挂掉也可以避免数据丢失的机制。 虽然WAL可以确保数据不丢失,它并不能对所有的数据源保证exactly-once语义。以下场景任然比较糟糕： 1）接收器接收到输入数据，并把它存储到WAL中； 2）接收器在更新Zookeeper中Kafka的偏移量之前突然挂掉了； 3）Spark Streaming假设输入数据已成功收到（因为它已经写入到WAL中），然而Kafka认为数据被没有被消费，因为相应的偏移量并没有在Zookeeper中更新； 4）过了一会，接收器从失败中恢复； 5）那些被保存到WAL中但未被处理的数据被重新读取； 6）一旦从WAL中读取所有的数据之后，接收器开始从Kafka中消费数据。因为接收器是采用Kafka的High-Level Consumer API实现的，它开始从Zookeeper当前记录的偏移量开始读取数据，但是因为接收器挂掉的时候偏移量并没有更新到Zookeeper中，所有有一些数据被处理了2次。 除了上面描述的场景，WAL还有其他两个不可忽略的缺点: 1）WAL减少了接收器的吞吐量，因为接受到的数据必须保存到可靠的分布式文件系统中。 2）对于一些输入源来说，它会重复相同的数据。比如当从Kafka中读取数据，你需要在Kafka的brokers中保存一份数据，而且你还得在Spark Streaming中保存一份。 5. Exactly-Once为了解决由WAL引入的性能损失，并且保证 exactly-once 语义，新版的Spark中引入了名为Kafka direct API。这个想法对于这个特性是非常明智的。Spark driver只需要简单地计算下一个batch需要处理Kafka中偏移量的范围，然后命令Spark Exectuor直接从Kafka相应Topic的分区中消费数据。换句话说，这种方法把Kafka当作成一个文件系统，然后像读文件一样来消费Topic中的数据。 在这个简单但强大的设计中: 1）不再需要Kafka接收器，Exectuor直接采用Simple Consumer API从Kafka中消费数据。 2）不再需要WAL机制，我们仍然可以从失败恢复之后从Kafka中重新消费数据； 3）Exactly-Once语义得以保存，我们不再从WAL中读取重复的数据。 原文： Spark Streaming And Kafka：可靠实时计算]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop中的Secondary Sort]]></title>
    <url>%2F2018%2F03%2F21%2Fhadoop-basics-secondary-sort-in-mapreduce%2F</url>
    <content type="text"><![CDATA[我们首先提出了一个查询问题，为了解决这个问题，需要在数据集的多个字段上进行排序。然后，我们将研究 MapReduce Shuff 阶段的工作原理，然后再实现我们的二次排序以获得我们想要的查询结果。 1. 查询如果我们想查看确定 state 和 city 的所有捐款的 id，捐赠者的 state，捐助者的 city 和捐款总额 total。按以下优先顺序排列结果： state - 按字母顺序升序排序（不区分大小写） city - 按字母顺序升序排序（不区分大小写） total - 按数字顺序降序排序 可以用SQL如下实现：SELECT donation_id, donor_state, donor_city, totalFROM donationsWHERE donor_state IS NOT NULL AND donor_city IS NOT NULLORDER BY lower(donor_state) ASC, lower(donor_city) ASC, total DESC; 2. 理解Shuffle阶段现在我们需要深入了解 Shuffle 阶段： 如何和在哪里工作 有哪些工具可以根据我们的需求进行自定义以及调整 以下是使用2个 mapper 和2个 reducer 任务的工作流程图： 关于不同编号的步骤的一些细节： (1) mapper 的 map 方法从 InputFormat 提供的分片中接收所有 （key，value） 键值对。这是我们通常在 Mapper 中编写的最重要的方法。 (2) 使用指定的分区器为每个用户的 map 方法输出进行分区。默认情况下，在 MapReduce 中使用 HashPartitioner。它使用 key 的 hashCode（） 值并对 reducer 的个数进行取模。这将根据 key 随机确定（key，value） 键值对存储在每个 Reducer 的不同分区中。所有具有相同 key 的键值对位于同一个分区中，并在相同的 reducer 中结束。 (3) 在写入磁盘之前，使用指定的 Sort Comparator 对数据进行排序。同一分区记录全部写入同一个临时文件。 (4) reducer 从所有 mapper 中拉取所有分配给他们的分区。分区可以写入本地临时文件，或者足够小时存储在内存中。这个过程也被称为 Shuffle，因为分区正在洗牌。 (5) Sort Comparator 在合并所有内存和磁盘中的分区时再次使用。每个 reducer 都有一个所有（key, value）键值对完全排序的列表，这些键值对是分区器分配给它们的所有键的。 (6) Group Comparator 用于将值分组成列表。每个 “不同” key，都将调用带有参数（key，list&lt;values&gt;）的 reduce 方法。 3. 二次排序二次排序是一种可用于在多个字段上排序数据的技术。它依赖于使用一个复合键，它将包含我们想要用于排序的所有值。 在本文中，我们将阅读我们的 donations Sequence File，并在 shuffling和 reducing 之前将每个捐赠记录映射为（CompositeKey，DonationWritable） 键值对。 本文中使用的所有类都可以在GitHub上查看：https://github.com/nicomak/blog/tree/master/donors/src/main/java/mapreduce/donation/secondarysort。 为了得到查询结果而执行的 MapReduce 二次排序作业位于同一个包的 OrderByCompositeKey.java 文件中。 package mapreduce.donation.secondarysort;import java.io.IOException;import org.apache.commons.lang.StringUtils;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import data.writable.DonationWritable;public class OrderByCompositeKey &#123; public static final Log LOG = LogFactory.getLog(OrderByCompositeKey.class); /** * This mapper simply outputs a (CompositeKey, DonationWritable) pair for each donation row. * It partitions map outputs by the natural key (the 'state' field), because of our NaturalKeyPartitioner class. * Within these partitions, rows are sorted by secondary key, because of our FullKeyComparator class, * which sorts on the full composite key, in the order of ('state', 'city', 'total'). * * @author Nicomak * */ public static class CompositeKeyCreationMapper extends Mapper&lt;Object, DonationWritable, CompositeKey, DonationWritable&gt; &#123; private CompositeKey compositeKey = new CompositeKey(); @Override public void map(Object key, DonationWritable donation, Context context) throws IOException, InterruptedException &#123; // Ignore entries with empty values for better readability of results if (StringUtils.isEmpty(donation.donor_state) || StringUtils.isEmpty(donation.donor_city)) &#123; return; &#125; compositeKey.set(donation.donor_state, donation.donor_city, donation.total); context.write(compositeKey, donation); &#125; &#125; /** * This reducer will fetch the partitions (from different mappers) and then sort them by ('state', 'city', 'total') order again, * because we used our FullKeyComparator as the sort comparator class. * After that, it will group all sorted partition data by natural key ('state') because we used our NaturalKeyComparator * as the grouping comparator. * The groups which are created here are lists of donation rows with the same 'state', and ordered by 'city' * These groups are passed to the "reduce" function in order or natural key, and their content is sorted in order of secondary key. * So the output of the reducer will be rows ordered by ('state', 'city', 'total'). * * @author Nicomak * */ public static class ValueOutputReducer extends Reducer&lt;CompositeKey, DonationWritable, Text, Text&gt; &#123; private Text outputKey = new Text(); private Text outputValue = new Text(); @Override public void reduce(CompositeKey key, Iterable&lt;DonationWritable&gt; donations, Context context) throws IOException, InterruptedException &#123; for (DonationWritable donation : donations) &#123; outputKey.set(donation.donation_id); outputValue.set(String.format("%s %s %.2f", donation.donor_state, donation.donor_city, donation.total)); context.write(outputKey, outputValue); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Job job = Job.getInstance(new Configuration(), "Secondary Sorting"); job.setJarByClass(OrderByCompositeKey.class); // Mapper configuration job.setMapperClass(CompositeKeyCreationMapper.class); job.setInputFormatClass(SequenceFileInputFormat.class); job.setMapOutputKeyClass(CompositeKey.class); job.setMapOutputValueClass(DonationWritable.class); // Partitioning/Sorting/Grouping configuration job.setPartitionerClass(NaturalKeyPartitioner.class); job.setSortComparatorClass(FullKeyComparator.class); job.setGroupingComparatorClass(NaturalKeyComparator.class); // Reducer configuration job.setReducerClass(ValueOutputReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setNumReduceTasks(1); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 3.1 Composite Key我们的查询想要对3个值进行排序，所以我们创建了一个名为 CompositeKey 的 WritableComparable 类，它具有以下3个属性： state（String） - 这个被用作分区的 natural key（或主键） city（String） - 在同一个分区内对具有相同 state natural key的进行排序的辅助键（译者注：即同一分区内 state 相同将根据 city 进行排序） total（float） - 当 city 相同时进一步排序的另一个辅助键（译者注：在同一分区内 state 和 city 均相同则根据 total 进行排序） package mapreduce.donation.secondarysort;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class CompositeKey implements WritableComparable&lt;CompositeKey&gt; &#123; public String state; public String city; public float total; public CompositeKey() &#123; &#125; public CompositeKey(String state, String city, float total) &#123; super(); this.set(state, city, total); &#125; public void set(String state, String city, float total) &#123; this.state = (state == null) ? "" : state; this.city = (city == null) ? "" : city; this.total = total; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(state); out.writeUTF(city); out.writeFloat(total); &#125; @Override public void readFields(DataInput in) throws IOException &#123; state = in.readUTF(); city = in.readUTF(); total = in.readFloat(); &#125; @Override public int compareTo(CompositeKey o) &#123; int stateCmp = state.toLowerCase().compareTo(o.state.toLowerCase()); if (stateCmp != 0) &#123; return stateCmp; &#125; else &#123; int cityCmp = city.toLowerCase().compareTo(o.city.toLowerCase()); if (cityCmp != 0) &#123; return cityCmp; &#125; else &#123; return Float.compare(total, o.total); &#125; &#125; &#125;&#125; 我在这个类中实现了 compareTo（），但它只是默认的自然排序，所有字段都按升序比较。我们的查询想要对 total 字段进行降序排序，为此我们将在下一段中创建一个特定的 Sort Comparator。 3.2 Sort Comparator如图所示，如果我们希望我们的结果在 CompositeKey 的所有3个属性上进行按照我们期望的方式进行排序，我们必须使用按照 [state，city，-total] 优先级顺序的 Sort Comparator。正如我们在前一部分中所做的那样，我们创建了一个继承 WritableComparator 并为我们的排序需求实现 compare（） 方法的类：import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class FullKeyComparator extends WritableComparator &#123; public FullKeyComparator() &#123; super(CompositeKey.class, true); &#125; @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable wc1, WritableComparable wc2) &#123; CompositeKey key1 = (CompositeKey) wc1; CompositeKey key2 = (CompositeKey) wc2; int stateCmp = key1.state.toLowerCase().compareTo(key2.state.toLowerCase()); if (stateCmp != 0) &#123; return stateCmp; &#125; else &#123; int cityCmp = key1.city.toLowerCase().compareTo(key2.city.toLowerCase()); if (cityCmp != 0) &#123; return cityCmp; &#125; else &#123; return -1 * Float.compare(key1.total, key2.total); &#125; &#125; &#125;&#125; 然后，我们使用 job.setSortComparatorClass（FullKeyComparator.class） 将此类设置为 Sort Comparator。 现在使用单个 reducer 将给我们完全排序的结果。仅使用一个 reducer 时，实现 Composite Key 和 Sort Comparator 就足以对多个字段进行排序。 3.3 Partitioner如果我们使用多个 reducer，会发生什么？ 默认分区器 HashPartitioner 将根据 CompositeKey 对象的 hashCode 值将其分配给 reducer。无论我们是重写了 hashcode() 方法（正确使用所有属性的哈希）还是不重写（使用默认 Object 的实现，使用内存中地址），都将 “随机” 对所有 keys 进行分区。 二次排序不会这样的。因为合并来自 mappers 的所有分区后，reducer 的 key 可能会像这样（第一列）： 在第一个输出列中，在一个 reducer 内，对于给定 state 的数据按城市名称排序，然后按总捐赠量降序排列。但这种排序没有什么意义，因为有些数据丢失了。例如，Reducer 0 有2个排序的 Los Angeles key，但来自 Reducer 1 的 Los Angeles 条目应该放在这两个 key 之间。 因此，当使用多个 reducers 时，我们想要的是将具有相同 state 的所有 （key，value） 键值对发送给同一个 reducer，就像第二列显示的那样。最简单的方法是创建我们自己的 NaturalKeyPartitioner，类似于默认的 HashPartitioner，但仅基于 state 的 hashCode，而不是完整的 CompositeKey 的 hashCode：import org.apache.hadoop.mapreduce.Partitioner;import data.writable.DonationWritable;public class NaturalKeyPartitioner extends Partitioner&lt;CompositeKey, DonationWritable&gt; &#123; @Override public int getPartition(CompositeKey key, DonationWritable value, int numPartitions) &#123; // Automatic n-partitioning using hash on the state name return Math.abs(key.state.hashCode() &amp; Integer.MAX_VALUE) % numPartitions; &#125;&#125; 我们使用 job.setPartitionerClass（NaturalKeyPartitioner.class） 将此类设置为作业的分区器。 3.4 Group ComparatorGroup Comparator 决定每次调用 reduce 方法时如何对这些值分组（译者注：一个分组调用一次 reduce 方法）。 继续使用上图中的 Reducer 0 的例子。如果合并分区后，一个 reducer 中的（key，value）键值对必须如下处理： 可以完成的可能分组如下： 说明： 没有相同 (state,city,total) 组合的 keys。因此，对于第一个分组，每个记录调用一次 reduce 方法。 第二个是根据 state, city 分组。B 和 C 键值对的 key 有相同的 state 和 city，因此它们组合在一起在一个 reducer() 中调用。传递给函数的 key 是分组中第一个键值对的 key，因此它依赖于排序。 第三个只查看 state。B， C ，D 键值对中的 key 都具有相同的 state，因此它们被组合在一起以在一个 reducer() 中调用。 在某些情况下分组可能很有用。 例如，如果你想在每个捐赠输出旁边打印给定城市的所有捐款总和，则可以使用上述示例中的第二个分组。这样做，可以在输出所有值之前，将 reduce() 函数中的所有 “总计” 字段求和。 对于我们的查询，我们只需要打印出每个记录的字段，对于分组无关紧要。调用 reduce() 函数4次，3次或2次仍然会只打印出 A，B ，C 和 D 记录的 （id，state，city，total） 字段。 对于这个作业，它对性能没有任何影响。 让我们按照 state（natural key）进行分组，只是为了使用我们自己的 Group Comparator：import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class NaturalKeyComparator extends WritableComparator &#123; public NaturalKeyComparator() &#123; super(CompositeKey.class, true); &#125; @SuppressWarnings("rawtypes") @Override public int compare(WritableComparable wc1, WritableComparable wc2) &#123; CompositeKey key1 = (CompositeKey) wc1; CompositeKey key2 = (CompositeKey) wc2; return key1.state.compareTo(key2.state); &#125;&#125; 然后我们可以通过调用 job.setGroupingComparatorClass（NaturalKeyComparator.class） 来使用这个比较器。 4. 作业运行与结果4.1 Job 1 : With a single reducer输出结果:$ hadoop jar donors.jar mapreduce.donation.secondarysort.OrderByCompositeKey donors/donations.seqfile donors/output_secondarysort$ hdfs dfs -ls -h donors/output_secondarysort-rw-r--r-- 2 hduser supergroup 0 2015-12-28 16:00 donors/output_secondarysort/_SUCCESS-rw-r--r-- 2 hduser supergroup 74.6 M 2015-12-28 16:00 donors/output_secondarysort/part-r-00000$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | head -n 8c8e871528033bd9ce6b267ed8df27698 AA Canada 100.006eb5a716f73260c53a76a5d2aeaf3820 AA Canada 100.0092db424b01676e462eff4c9361799c18 AA Canada 98.36e0f266ed8875df71f0012fdaf50ae22e AA Canada 1.64d9064b2494941725d0f93f6ca781cdc7 AA DPO 50.0083b85744490320c8154f1f5bcd703296 AA DPO 25.007133a67b51c1ee61079fa47e3b9e5160 AA Fremont 50.00f3475f346f1483dfb57efc152d3fbced AA Helsinki\, FINLAND 153.39$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | tail -n 810df7888672288077dc4f60e4a83bcc2 WY Wilson 114.980b4dda44ac5dc29a522285db56082986 WY Wilson 100.004276bc7e6df5f4643675e65bb2323281 WY Wilson 100.00519da9b977281b7623d655b8ee0c8ea5 WY Wilson 91.4792469f6f000a6cd66ba18b5fe03e6871 WY Wilson 23.32f0a9489e53a203e0f7f47e6a350bb19a WY Wilson 1.688aed3aba4473c0f9579927d0940c540f WY Worland 75.001a497106ff2e2038f41897248314e6c6 WY Worland 50.00 分析: 由于只使用1个 reducer ，所以只有1个输出文件。该文件以状态 “AA” 开始，该 state 不是 USA state，但看起来是对国外城市的重新组合。该文件以 “WY” （怀俄明州）城市 Wilson 和 Worland 结束。所有内容都按照查询中的要求排序。 4.2 Job 2 : With 3 reducers, using default partitioner这次我们通过设置 job.setNumReduceTasks（3） 来使用3个 reducer，并且我们注释掉 job.setPartitionerClass（NaturalKeyPartitioner.class） 这一行来查看会发生什么。 输出结果:$ hadoop jar donors.jar mapreduce.donation.secondarysort.OrderByCompositeKey donors/donations.seqfile donors/output_secondarysort$ hdfs dfs -ls -h donors/output_secondarysort-rw-r--r-- 2 hduser supergroup 0 2015-12-28 15:36 donors/output_secondarysort/_SUCCESS-rw-r--r-- 2 hduser supergroup 19.4 M 2015-12-28 15:36 donors/output_secondarysort/part-r-00000-rw-r--r-- 2 hduser supergroup 38.7 M 2015-12-28 15:36 donors/output_secondarysort/part-r-00001-rw-r--r-- 2 hduser supergroup 16.5 M 2015-12-28 15:36 donors/output_secondarysort/part-r-00002$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | tail -n 5234600cf0c052b95e544f690a8deecfc WY Wilson 158.0010df7888672288077dc4f60e4a83bcc2 WY Wilson 114.980b4dda44ac5dc29a522285db56082986 WY Wilson 100.004276bc7e6df5f4643675e65bb2323281 WY Wilson 100.001a497106ff2e2038f41897248314e6c6 WY Worland 50.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00001 | tail -n 5a31e6d2ddcffef3fb0c81a9b5be8a62b WY Wilson 177.60ab277b46c65df53305ceee436e775f86 WY Wilson 150.00519da9b977281b7623d655b8ee0c8ea5 WY Wilson 91.4792469f6f000a6cd66ba18b5fe03e6871 WY Wilson 23.328aed3aba4473c0f9579927d0940c540f WY Worland 75.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00002 | tail -n 5db2334f876e2a661dc66ec79b49a7073 WY Wilson 319.44f740650269f523ac94a8bc54c40ffcb8 WY Wilson 294.70dfdba1ed68a130da5337e28b45a469d1 WY Wilson 286.64e5cf931220ab071083d174461ef50411 WY Wilson 278.19f0a9489e53a203e0f7f47e6a350bb19a WY Wilson 1.68 分析: 通过查看每个输出的最后5行，我们可以注意到所有输出都与 Wilson 城市有关。其中两项输出有 Worland 的条目。 正如前面所解释的，每个输出的结果都是按照 state 和 city 的上升顺序排列，并且 donation 降序排列。但是不可能查看给定 state 或 city 的所有排序捐赠，因为它们分布在多个文件中。 4.3 Job 3 : With 3 reducers, using NaturalKeyPartitioner对于这个作业，我们只需重新设置 job.setPartitionerClass（NaturalKeyPartitioner.class） 即可使用我们的自定义分区器，同时保留3个 Reducer 任务。 输出结果:$ hadoop jar donors.jar mapreduce.donation.secondarysort.OrderByCompositeKey donors/donations.seqfile donors/output_secondarysort$ hdfs dfs -ls -h donors/output_secondarysort-rw-r--r-- 2 hduser supergroup 0 2015-12-28 16:37 donors/output_secondarysort/_SUCCESS-rw-r--r-- 2 hduser supergroup 23.2 M 2015-12-28 16:37 donors/output_secondarysort/part-r-00000-rw-r--r-- 2 hduser supergroup 22.8 M 2015-12-28 16:37 donors/output_secondarysort/part-r-00001-rw-r--r-- 2 hduser supergroup 28.7 M 2015-12-28 16:37 donors/output_secondarysort/part-r-00002$ hdfs dfs -cat donors/output_secondarysort/part-r-00000 | tail -n 5eeac9e18795680ccc00f69a42ec4fbc5 VI St. Thomas 10.00634f02e6ffc99fddd2b2c9cda7b7677c VI St. Thomas 10.009f71710178a4fab17fb020bc994be60b VI Zurich / Switzerland 75.00f81b2dc07c0cc3ccea5941dc928247a5 VI Zurich / Switzerland 50.008337fac8a67e50ad4a0dfe7decc4b8e9 VI Zurich / Switzerland 50.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00001 | tail -n 59f3ea230d660a20ec91b287b8a0f6693 WI Wrightstown 5.008b95de5d2b2027c0a3d2631c9f0f6f9b WI Wrightstown 5.009db66b1c0c71dd438a8f979dd04cdfb8 WI Wrightstown 5.00c7c60406d260915cb35a7e267c28becc WI Wrightstown 5.00c2d98a640c301cf277247302ad5014ca WI Wrightstown 5.00$ hdfs dfs -cat donors/output_secondarysort/part-r-00002 | tail -n 5519da9b977281b7623d655b8ee0c8ea5 WY Wilson 91.4792469f6f000a6cd66ba18b5fe03e6871 WY Wilson 23.32f0a9489e53a203e0f7f47e6a350bb19a WY Wilson 1.688aed3aba4473c0f9579927d0940c540f WY Worland 75.001a497106ff2e2038f41897248314e6c6 WY Worland 50.00 分析: 与 Job2 的输出相比，我们可以看到只有一个 reducer（r-00002） 具有来自 WY state（怀俄明州）的条目。因此，如果你只对来自 WY 的捐款感兴趣，那么你要查找的结果是完整且正确地排序在一个文件中的。 其他 reducer 输出以不同 state（”VI” 和 “WI”）结束，因为每个 state 都是独占一个 reducer。这是因为我们告诉我们的 NaturalKeyPartitioner 将 state 字段视为划分的决定性值。 4.4 Performance Comparison这里是本文中描述的3个作业的比较表。取自资源管理器用户界面的总时间。其他值来自MR历史服务器UI。所有指标均为2次执行的平均值。 使用3个 reducer 时，我们可以观察到总执行时间显着改善。Job2和3比Job1更快。在Job2和Job3中，每个 reducer 的shuffling/merging的时间更长一些，但是实际时间要短得多。 5. 结论在这一部分中，我们学习了如何使用一些工具在 Shuffle 阶段对分区，排序和分组进行更多控制。 我们看到了如何实现二次排序，这有助于我们： 当使用单个 reducer 时，对多个字段的数据集进行完全排序 当使用多个 reducer 时，在辅助键上对有相同 natural key 的记录进行排序。 使用多个 reducer 可以加快排序过程，但代价是只能对多个 reducer 实现在 natural key 上的部分排序。 5.1 新定义事后看来，回顾我们应用于复合 key 的不同工具的效果，除了 “在多个字段上进行排序” 外，我们还可以给出二次排序的更一般而精确的定义： 二次排序是一种技术，用于控制 Reducer 的输入对 如何 传递给 reduce 函数。 上面的 如何 可以理解为以何种顺序（Sort Comparator）以及基于 key 的对值进行分组的方式（Group Comparator） 根据这个定义，使用 Secondary Sort，我们可以对 Reducer 内的数据进行全面控制，这就是输出文件内部始终排好序的原因。 5.2 下一步使用 Secondary Sort，我们可以控制 Reducer 内的数据，但我们无法控制如何将已排序的 map 输出分发给 reducer。我们已经定义了一个分区器来确保不同的 reducer 管理他们自己的 natural keys 并保证在二级键的排序。但它并没有解决在所有输出中对所有 natural keys 进行排序的问题。 在下一篇文章中，我们将学习如何使用全排序（Total Order Sorting）来做到这一点。 原文： http://blog.ditullio.fr/2015/12/28/hadoop-basics-secondary-sort-in-mapreduce/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux rsync命令使用指南]]></title>
    <url>%2F2018%2F03%2F20%2Frsync-command-usage%2F</url>
    <content type="text"><![CDATA[1. 概述rsync 命令是一个远程数据同步工具，可通过 LAN/WAN 快速同步多台主机间的文件。rsync 使用所谓的 “rsync算法” 来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。rsync 是一个功能非常强大的工具，其命令也有很多功能特色选项。 2. 安装与配置请参考： 3. 语法rsync [OPTION...] SRC... [DEST]rsync [OPTION...] [USER@]HOST:SRC... [DEST]rsync [OPTION...] SRC... [USER@]HOST:DESTrsync [OPTION...] [USER@]HOST::SRC... [DEST]rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]rsync [OPTION...] SRC... [USER@]HOST::DESTrsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 3.1 本地模式rsync [OPTION...] [USER@]HOST:SRC... [DEST] 拷贝本地文件。当 SRC 和 DEST 路径信息都不包含有单个冒号 : 分隔符时就启动这种工作模式。例如：xiaosi@ying:~$ rsync -a adv_push adv_push_backup/xiaosi@ying:~$ ll adv_push_backup/总用量 20drwxrwxr-x 3 xiaosi xiaosi 4096 3月 20 15:29 ./drwxr-xr-x 83 xiaosi xiaosi 12288 3月 20 15:29 ../drwxrwxr-x 2 xiaosi xiaosi 4096 3月 20 15:28 adv_push/ 3.2 通过远程Shell访问-Pullrsync [OPTION...] [USER@]HOST:SRC... [DEST] 使用一个远程Shell程序(如rsh、ssh)来实现将远程机器的内容拷贝到本地机器。当 SRC 地址路径包含单个冒号 : 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a ubuntu@xxx.xxx.xxx.xxx:test/remote_content.txt /home/xiaosi/data/share/ubuntu@xxx.xxx.xxx.xxx&apos;s password: 在本地机器上查看从服务器拷贝到本地上的文件：xiaosi@ying:~$ cd /home/xiaosi/data/share/xiaosi@ying:~/data/share$ ll总用量 12drwxrwxr-x 2 xiaosi xiaosi 4096 3月 20 19:41 ./drwxr-xr-x 14 xiaosi xiaosi 4096 3月 20 19:24 ../-rw-rw-r-- 1 xiaosi xiaosi 17 3月 20 15:57 remote_content.txt ubuntu@xxx.xxx.xxx.xxx 中 ubuntu 是服务器登录名 xxx.xxx.xxx.xxx 是服务器IP地址 password 是服务器登录名 ubuntu 对应的密码 以 Shell 方式访问，首先在服务端启动 ssh 服务： service sshd start 3.3 通过远程Shell访问-Pushrsync [OPTION...] SRC... [USER@]HOST:DEST 使用一个远程shell程序(如rsh、ssh)来实现将本地机器的内容拷贝到远程机器。当 DEST 路径地址包含单个冒号 : 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a /home/xiaosi/data/exception.txt ubuntu@xxx.xxx.xxx.xxx:tmpubuntu@xxx.xxx.xxx.xxx&apos;s password: 在服务器上查看从本地拷贝到服务器上的文件：ubuntu@VM-0-7-ubuntu:~/tmp$ lltotal 24drwxrwxr-x 5 ubuntu ubuntu 4096 Mar 20 19:34 ./drwxr-xr-x 11 ubuntu ubuntu 4096 Mar 20 19:32 ../...-rw-rw-r-- 1 ubuntu ubuntu 80 Mar 20 11:03 exception.txt... ubuntu@xxx.xxx.xxx.xxx 中 ubuntu 是服务器登录名 xxx.xxx.xxx.xxx 是服务器IP地址 password 是服务器登录名 ubuntu 对应的密码 以 Shell 方式访问，首先在服务端启动 ssh 服务： service sshd start 3.4 通过rsync进程访问-Pullrsync [OPTION...] [USER@]HOST::SRC... [DEST]rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] 从远程 rsync 服务器中拷贝文件到本地机。当 SRC 路径信息包含 :: 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a test@xxx.xxx.xxx.xxx::share/remote_content.txt /home/xiaosi/data/share 在本地机器上查看从服务器拷贝到本地上的文件：xiaosi@ying:~/data/share$ ll总用量 12drwxrwxr-x 2 xiaosi xiaosi 4096 3月 20 19:06 ./drwxr-xr-x 14 xiaosi xiaosi 4096 3月 20 19:06 ../-rw-r--r-- 1 xiaosi xiaosi 17 3月 20 18:44 remote_content.txt 以后台服务方式访问 请参考：Linux rsync配置指南 test@xxx.xxx.xxx.xxx 中 test 是以后台方式访问配置的用户 3.5 通过rsync进程访问-Pushrsync [OPTION...] SRC... [USER@]HOST::DESTrsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 从本地机器拷贝文件到远程 rsync 服务器中。当 DEST 路径信息包含 :: 分隔符时启动该模式。例如：xiaosi@ying:~$ rsync -a /home/xiaosi/data/sources.list.backup test@xxx:xxx:xxx:xxx::share/config 在服务器上查看从本地拷贝到服务器上的文件：ubuntu@VM-0-7-ubuntu:/home/share/config$ lltotal 12drwxr-xr-x 2 root root 4096 Mar 20 19:02 ./drwxr-xr-x 3 root root 4096 Mar 20 19:02 ../-rw-r--r-- 1 root root 2981 Mar 20 16:30 sources.list.backup 以后台服务方式访问 请参考：Linux rsync配置指南 test@xxx.xxx.xxx.xxx 中 test 是以后台方式访问配置的用户 3.6 查阅模式只使用一个 SRC 参数，而不使用 DEST 参数将列出源文件而不是进行复制。例如：xiaosi@ying:~$ rsync -a adv_pushdrwxrwxr-x 4,096 2018/03/20 15:28:15 adv_push-rw-rw-r-- 353 2018/03/14 17:02:35 adv_push/adv_push_20180307.txt-rw-rw-r-- 353 2018/03/14 17:02:33 adv_push/adv_push_20180308.txt 4. 选项-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。-l, --links 保留软链结。-L, --copy-links 想对待常规文件一样处理软链结。--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。--safe-links 忽略指向SRC路径目录树以外的链结。-H, --hard-links 保留硬链结。-p, --perms 保持文件权限。-o, --owner 保持文件属主信息。-g, --group 保持文件属组信息。-D, --devices 保持设备文件信息。-t, --times 保持文件时间信息。-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, --dry-run现实哪些文件将被传输。-w, --whole-file 拷贝文件，不进行增量检测。-x, --one-file-system 不要跨越文件系统边界。-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。--delete 删除那些DST中SRC没有的文件。--delete-excluded 同样删除接收端那些被该选项指定排除的文件。--delete-after 传输结束以后再删除。--ignore-errors 及时出现IO错误也进行删除。--max-delete=NUM 最多删除NUM个文件。--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。--force 强制删除目录，即使不为空。--numeric-ids 不将数字的用户和组id匹配为用户名和组名。--timeout=time ip超时时间，单位为秒。-I, --ignore-times 不跳过那些有同样的时间和长度的文件。--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T --temp-dir=DIR 在DIR中创建临时文件。--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 --partial。--progress 显示备份过程。-z, --compress 对备份的文件在传输时进行压缩处理。--exclude=PATTERN 指定排除不需要传输的文件模式。--include=PATTERN 指定不排除而需要传输的文件模式。--exclude-from=FILE 排除FILE中指定模式的文件。--include-from=FILE 不排除FILE指定模式匹配的文件。--version 打印版本信息。--address 绑定到特定的地址。--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。--port=PORT 指定其他的rsync服务端口。--blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。--progress 在传输时现实传输过程。--log-format=formAT 指定日志文件格式。--password-file=FILE 从FILE中得到密码。--bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, --help 显示帮助信息。 参考: http://man.linuxde.net/rsync]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux 命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux rsync配置指南]]></title>
    <url>%2F2018%2F03%2F20%2Frsync-config-usage%2F</url>
    <content type="text"><![CDATA[1. 概述rsync 命令是一个远程数据同步工具，可通过 LAN/WAN 快速同步多台主机间的文件。rsync 使用所谓的 “rsync算法” 来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。rsync 是一个功能非常强大的工具，其命令也有很多功能特色选项。 2. 安装在 ubuntu 下安装 rsync 通过以步骤可以实现：sudo apt-get install rsync xinetd 默认情况下 ubuntu 安装了 rsync，因此只需安装 xinetd即可:sudo apt-get install xinetd 3. 配置(1) 编辑 /etc/default/rsync 启动 rsync 作为使用 xinetd 的守护进程：# 打开rsyncsudo vim /etc/default/rsync# 编辑rsyncRSYNC_ENABLE=inetd (2) 创建 /etc/xinetd.d/rsync, 通过 xinetd 使 rsync 开始工作# 创建并打开文件sudo vim /etc/xinetd.d/rsync# 编辑内容service rsync&#123; disable = no socket_type = stream wait = no user = root server = /usr/bin/rsync server_args = --daemon log_on_failure += USERID&#125; (3) 创建 /etc/rsyncd.conf ,并填写配置信息# 创建并打开文件sudo vim /etc/rsyncd.conf# 编辑配置信息max connections = 2log file = /var/log/rsync.logtimeout = 300[share] # 模块名comment = Public Share# path为需要同步的文件夹路径path = /home/shareread only = nolist = yesuid = rootgid = root# 必须和 rsyncd.secrets中的用户名对应auth users = testsecrets file = /etc/rsyncd.secrets (4) 创建 /etc/rsyncd.secrets，配置用户名和密码.# 创建并打开文件sudo vim /etc/rsyncd.secrets# 配置用户名和密码，密码可以任意设置test:123 (5) 修改 rsyncd.secrets 文件的权限sudo chmod 600 /etc/rsyncd.secrets (6) 启动/重启 xinetdsudo /etc/init.d/xinetd restart 4. 测试在客户端运行下面的命令以及输入密码，确认 rsync 是否配置成功：xiaosi@ying:/etc/apt$ rsync test@123.206.187.64::sharePassword:drwxr-xr-x 4,096 2018/03/20 18:44:51 .-rw-r--r-- 17 2018/03/20 18:44:51 remote_content.txt test 是在服务器中 rsyncd.secrets 文件中配置的用户名。 xx.xx.xx.xx 是服务器的ip地址，也可以填写服务器对应的域名。share 是 rsyncd.conf 中定义的模块 5. 问题在测试的时候出现如下问题：xiaosi@ying:/etc/apt$ rsync test@xxx:xxx:xxx:xxx::sharersync: failed to connect to xxx:xxx:xxx:xxx (xxx:xxx:xxx:xxx): Connection refused (111)rsync error: error in socket IO (code 10) at clientserver.c(128) [Receiver=3.1.1] 首先判断 873 端口是否开放，如果没有开启一下：telnet 192.168.xxx.xxx 873 或者查看 rsync 服务是否启动：ubuntu@VM-0-7-ubuntu:~$ ps -ef | grep rsyncroot 18848 1 0 17:29 ? 00:00:00 rsync --daemon --config=/etc/rsyncd.confubuntu 18850 12214 0 17:29 pts/0 00:00:00 grep --color=auto rsync 如果没有启动，启动一下 rsync 服务：sudo rsync --daemon --config=/etc/rsyncd.conf 转载于： https://segmentfault.com/a/1190000010310496 参考于： http://ju.outofmemory.cn/entry/27665]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux 命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 与 Kafka 整合的改进]]></title>
    <url>%2F2018%2F03%2F18%2Fimprovements-to-kafka-integration-of-spark-streaming%2F</url>
    <content type="text"><![CDATA[Apache Kafka 正在迅速成为最受欢迎的开源流处理平台之一。我们在 Spark Streaming 中也看到了同样的趋势。因此，在 Apache Spark 1.3 中，我们专注于对 Spark Streaming 与 Kafka 集成进行重大改进。主要增加如下： 为 Kafka 新增了 Direct API - 这允许每个 Kafka 记录在发生故障时只处理一次，并且不使用 Write Ahead Logs。这使得 Spark Streaming + Kafka 流水线更高效，同时提供更强大的容错保证。 为 Kafka 新增了 Python API - 这样你就可以在 Python 中处理 Kafka 数据。 在本文中，我们将更详细地讨论这些改进。 1. Direct APISpark Streaming 自成立以来一直支持 Kafka，Spark Streaming 与 Kafka 在生产环境中的很多地方一起使用。但是，Spark 社区要求更好的容错保证和更强的可靠性语义。为了满足这一需求，Spark 1.2 引入了 Write Ahead Logs（WAL）。它可以确保在发生故障时从任何可靠的数据源（即Flume，Kafka和Kinesis等事务源）接收的数据不会丢失（即至少一次语义）。即使对于像 plain-old 套接字这样的不可靠（即非事务性）数据源，它也可以最大限度地减少数据的丢失。 然而，对于允许从数据流中的任意位置重放数据流的数据源（例如 Kafka），我们可以实现更强大的容错语义，因为这些数据源让 Spark Streaming 可以更好地控制数据流的消费。Spark 1.3 引入了 Direct API 概念，即使不使用 Write Ahead Logs 也可以实现 exactly-once 语义。让我们来看看集成 Apache Kafka 的 Spark Direct API 的细节。 2. 我们是如何构建它？从高层次的角度看，之前的 Kafka 集成与 Write Ahead Logs（WAL）一起工作如下： (1) 运行在 Spark workers/executors 上的 Kafka Receivers 连续不断地从 Kafka 中读取数据，这用到了 Kafka 高级消费者API。 (2) 接收到的数据存储在 Spark 的 worker/executor的内存上，同时写入到 WAL（拷贝到HDFS）上。Kafka Receiver 只有在数据保存到日志后才会更新 Zookeeper中的 Kafka 偏移量。 (3) 接收到的数据及其WAL存储位置信息也可靠地存储。在出现故障时，这些信息用于从故障中恢复，重新读取数据并继续处理。 虽然这种方法可以确保从 Kafka 接收的数据不会丢失，但是在失败的时候，某些记录仍然有可能会被多次被处理（即 at-least-once 语义）。这种情况在一些接收到的数据被可靠地保存到 WAL 中，但是在更新 Zookeeper 中相应的 Kafka 偏移量之前失败时会发生(译者注：即已经保存到WAL，但是还没有来得及更新 Zookeeper 中的 Kafka 偏移量)。从而导致了不一致的情况 - Spark Streaming 认为数据已被接收，但 Kafka 认为数据还未成功发送，因为 Zookeeper 中的偏移未更新。因此，在系统从故障中恢复后，Kafka 会再一次发送数据。 出现这种不一致的原因是两个系统无法对描述已发送内容的信息进行原子更新。为了避免这种情况，只需要一个系统来维护已发送或接收的内容的一致性视图。此外，这个系统需要有从故障中恢复时重放数据流的一切控制权。因此，我们决定所有消费的偏移量信息只保存在 Spark Streaming 中，这些信息可以使用 Kafka 的 Simple Consumer API 根据故障需要重放任意偏移量的数据来从故障中恢复。 为了构建这个系统，新的 Direct Kafka API 采用与 Receivers 和 WAL 完全不同的方法。与使用 Receivers 连续接收数据并将其存储在 WAL 中不同，我们只需在给出每个批次开始时要使用的偏移量范围。之后，在执行每个批次的作业时，将从 Kafka 中读取与偏移量范围对应的数据进行处理（与读取HDFS文件的方式类似）。这些偏移量也能可靠地保存（）并用于重新计算数据以从故障中恢复。 请注意，Spark Streaming 可以在失败以后重新读取和处理来自 Kafka 的流片段以从故障中恢复。但是，由于 RDD 转换的 exactly-once 语义，最终重新计算的结果与在没有失败的结果完全相同。 因此，Direct API 消除了对 Kafka 的 WAL 和 Receivers 的依赖，同时确保每个 Kafka 记录都被 Spark Streaming 有效地接收一次。这允许我们用端到端的 exactly-once 语义将 Spark Streaming 与 Kafka 进行整合。总的来说，它使得这样的流处理流水线更加容错，高效并且更易于使用。 3. 如何来使用新的API相比之前的更加容易使用：// Define the Kafka parameters, broker list must be specifiedval kafkaParams = Map("metadata.broker.list" -&gt; "localhost:9092,anotherhost:9092")// Define which topics to read fromval topics = Set("sometopic", "anothertopic")// Create the direct stream with the Kafka parameters and topicsval kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](streamingContext, kafkaParams, topics) 由于这种 Direct API 没有使用 Receivers，因此你不必担心如何创建多个输入 DStream 来创建多个 Receivers。你也不需要考虑每个 Receiver 消费的 Kafka partition 的数量。每个 Kafka partition 将自动的并行读取。此外，每个 Kafka partition 与 RDD partition 一一对应，从而简化了并行模型。 除了新的流处理API之外，我们还引入了 KafkaUtils.createRDD()，它可用于在 Kafka 数据上运行批处理作业。 // Define the offset ranges to read in the batch jobval offsetRanges = Array( OffsetRange("some-topic", 0, 110, 220), OffsetRange("some-topic", 1, 100, 313), OffsetRange("another-topic", 0, 456, 789))// Create the RDD based on the offset rangesval rdd = KafkaUtils.createRDD[String, String, StringDecoder, StringDecoder](sparkContext, kafkaParams, offsetRanges) 如果你想了解更多关于API和它如何实现的细节，请看下面的内容: Spark Streaming + Kafka Integration Guide Exactly-once Spark Streaming from Kafka Direct API 完整 word count example: Scala 和 Java Fault-tolerance Semantics in Spark Streaming Programming Guide 4. Python 中的Kafka API在 Spark 1.2 中，添加了 Spark Streaming 的基本 Python API，因此开发人员可以使用 Python 编写分布式流处理应用程序。在 Spark 1.3 中，扩展了 Python API 来包含Kafka。借此，在 Python 中使用 Kafka 编写流处理应用程序变得轻而易举。这是一个示例代码。kafkaStream = KafkaUtils.createStream(streamingContext,"zookeeper-server:2181", "consumer-group", &#123;"some-topic": 1&#125;)lines = kafkaStream.map(lambda x: x[1]) 查看完整的示例和 python文档。运行该示例的说明可以在 Kafka 集成指南中找到。请注意，对于使用 Kafka API 运行示例或任何 python 应用程序，你必须将 Kafka Maven 依赖关系添加到路径中。这可以在 Spark 1.3 中轻松完成，因为你可以直接将 Maven 依赖关系添加到 spark-submit （推荐的方式来启动Spark应用程序）。 原文：https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 持久化]]></title>
    <url>%2F2018%2F03%2F16%2Fspark-base-rdd-persistence%2F</url>
    <content type="text"><![CDATA[1. 概述Spark 中最重要的功能之一是在操作之间将数据集持久化(缓存)在内存中。当持久化一个 RDD 时，每个节点都会保存 RDD 的任意分区（RDD在内存中计算），并且该数据集（或从其派生的数据集）上的 action 操作可以重用它。这样可以使以后的 action 操作执行的更快（通常超过10倍）。缓存是迭代算法和快速交互使用的关键工具。 可以使用 RDD 上的 persist() 或 cache() 方法来标记要持久化的 RDD (译者注：执行这两个方法不会立即持久化 RDD)。当 RDD 第一次在 action 操作中计算时，将持久化(缓存)到节点的内存中。Spark 的缓存是可容错的 - 如果 RDD 的任意分区丢失，它将使用最初创建的转换操作自动重新计算。 2. 存储级别除此之外，可以使用不同的持久化级别来存储每个持久化的 RDD，从而允许你，例如，将数据集存储在磁盘上，或者以序列化的 Java 对象形式存储在内存中(以节省空间)，或者在不同机器节点上进行备份。通过将 StorageLevel 对象传递给 persist() 方法来设置持久化级别。cache() 方法使用默认存储级别，即 StorageLevel.MEMORY_ONLY（将反序列化的对象存储在内存中）。 持久化级别 说明 MEMORY_ONLY 将 RDD 以反列化的 Java 对象存储在 JVM 中。如果没有足够的内存存储 RDD，则某些分区将不会被缓存，每次需要时都会重新计算。这是默认级别。 MEMORY_AND_DISK 将 RDD 以反序列化的 Java 对象存储在 JVM 中。如果数据在内存中放不下，则溢写到磁盘上．需要时则会从磁盘上读取 MEMORY_ONLY_SER (Java and Scala) 将 RDD 以序列化的 Java 对象(每个分区一个字节数组)的方式存储．这通常比反序列化对象更具空间效率，特别是在使用快速序列化器的情况下，但是这种方式读取数据会消耗更多的CPU。 MEMORY_AND_DISK_SER (Java and Scala) 与 MEMORY_ONLY_SER 类似，但如果数据在内存中放不下，则溢写到磁盘上，而不是每次需要时重新计算它们。 DISK_ONLY 将 RDD 分区存储在磁盘上。 MEMORY_ONLY_2, MEMORY_AND_DISK_2等 与上面的储存级别相同，只不过将持久化数据存为两份，在两个集群节点上备份每个分区。 OFF_HEAP（实验中） 与 MEMORY_ONLY_SER 类似，但将数据存储在 堆外内存 中。 这需要启用堆内存。 在 Python 中，存储对象始终使用 Pickle 库进行序列化，因此选择什么样的序列化级别是无关紧要的。Python 中的可用存储级别包括MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY和DISK_ONLY_2。 在 Shuffle 操作中(例如，reduceByKey)，即使用户没有主动对调用 persist，Spark也会对一些中间数据进行持久化。这样做是为了如果一个节点在 Shuffle 过程中发生故障避免重新计算整个输入。如果要重用，我们仍然建议用户对生成的 RDD 调用 persist。 3. 选择存储级别Spark 的存储级别旨在提供内存使用率和CPU效率之间的不同权衡。我们建议通过以下过程来选择一个： 如果你的 RDD 适合于默认存储级别（MEMORY_ONLY），那就保持不动。 这是CPU效率最高的选项，允许 RDD 上的操作尽可能快地运行。 如果不是，请尝试使用 MEMORY_ONLY_SER 并选择一个 快速的序列化库，这种方式更加节省空间，并仍然能够快速访问。 不要溢写到磁盘，除非在数据集上的计算操作成本较高，或者需要过滤大量的数据。否则，重新计算分区可能与从磁盘读取分区一样快。 如果要快速故障恢复（例如，使用Spark为Web应用程序提供服务），请使用副本存储级别（例如，MEMORY_ONLY_2）。所有存储级别通过重新计算丢失的数据来提供完整的容错能力，但副本数据可让你继续在 RDD 上运行任务，而无需重新计算丢失的分区。 4. 清除数据Spark会自动监视每个节点的缓存使用情况，并以最近最少使用（LRU）方式丢弃旧的数据分区。如果你想手动删除 RDD，而不是等待它自动从缓存中删除，请使用 RDD.unpersist() 方法。 Spark版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#rdd-persistence]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 与 Kafka0.8 整合]]></title>
    <url>%2F2018%2F03%2F16%2Fspark-streaming-kafka-0-8-integration%2F</url>
    <content type="text"><![CDATA[在这里我们解释如何配置 Spark Streaming 以接收来自 Kafka 的数据。有两种方法，一种为使用 Receivers 和 Kafka 高级API的旧方法，以及不使用 Receivers 的新方法（在 Spark 1.3 中引入）。它们具有不同的编程模型，性能特征和语义保证。就目前的 Spark 版本而言，这两种方法都被为稳定的API。 Kafka0.8 在 Spark2.3.0 版本中已经被弃用 1. 基于Receiver的方法这种方法使用 Receiver 来接收数据。Receiver 是使用 Kafka 高级消费者API实现的。与所有接收方一样，通过 Receiver 从 Kafka 接收的数据存储在 Spark executors 中，然后由 Spark Streaming 启动的作业处理数据。 但是，在默认配置下，这种方法可能会在失败时丢失数据（请参阅接收器的可靠性）。为确保零数据丢失，你不得不另外启用 Spark Streaming 中的 Write Ahead Logs （在 Spark 1.2 中引入），同时将所有收到的 Kafka 数据保存在分布式文件系统（例如HDFS）的 Write Ahead Logs 中，以便在发生故障时恢复所有数据。有关 Write Ahead Logs 的更多详细信息，请参阅流编程指南中的部署章节。 接下来，我们将讨论如何在流应用程序中使用这种方法。 1.1 引入对于使用 SBT/Maven 项目定义的 Scala/Java 应用程序，请引入如下工件（请参阅主编程指南中的Linking部分以获取更多信息）。groupId = org.apache.sparkartifactId = spark-streaming-kafka-0-8_2.11version = 2.3.0 对于Python应用程序，在部署应用程序时，必须添加上述库及其依赖项。请参阅下面的部署小节。 1.2 编程在流应用程序代码中，导入 KafkaUtils 并创建一个输入 DStream，如下所示。 Scala版本:import org.apache.spark.streaming.kafka._val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) Java版本：import org.apache.spark.streaming.kafka.*;JavaPairReceiverInputDStream&lt;String, String&gt; kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]); Python：from pyspark.streaming.kafka import KafkaUtilskafkaStream = KafkaUtils.createStream(streamingContext, \ [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) 默认情况下，Python API会将 Kafka 数据解码为 UTF8 编码的字符串。你可以指定自定义解码函数，将 Kafka 记录中的字节数组解码为任意任意数据类型。 查看API文档。 请记住: Kafka 中的 topic partition 区与 Spark Streaming 中生成的 RDD partition 没有相关性。因此增加 KafkaUtils.createStream() 中特定 topic partition 的数量仅仅增加了在单个接收器中消费 topic 使用的线程数。但是这并没有增加 Spark 在处理数据的并行度。 可以用不同的 groups 和 topics 来创建多个 Kafka 输入 DStream，用于使用多个接收器并行接收数据。之后可以利用 union 来合并成一个 Dstream。 如果你使用 HDFS 等副本文件系统去启用 Write Ahead Logs，那么接收到的数据已经在日志中备份。因此，输入流的存储级别为 StorageLevel.MEMORY_AND_DISK_SER（即使用KafkaUtils.createStream（…，StorageLevel.MEMORY_AND_DISK_SER））。 1.3 部署与任何 Spark 应用程序一样，spark-submit 用于启动你的应用程序。但是，Scala/Java 应用程序和 Python 应用程序的细节略有不同。 对于 Scala 和 Java 应用程序，如果你使用 SBT 或 Maven 进行项目管理，需要将 spark-streaming-kafka-0-8_2.11 及其依赖项打包到应用程序 JAR 中。同时确保 spark-core_2.11 和 spark-streaming_2.11 被标记为 provided 依赖关系，因为这些已经存在 Spark 的安装中。最后使用 spark-submit 启动你的应用程序。 对于缺乏 SBT/Maven 项目管理的 Python 应用程序，可以使用 –packages 直接将 spark-streaming-kafka-0-8_2.11 及其依赖添加到 spark-submit 中（请参阅应用程序提交指南）。即，./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 ... 或者，你也可以从 Maven 仓库中下载 spark-streaming-kafka-0-8-assembly 的JAR，并将其添加到 spark-submit -jars 中。 2. 不使用Receiver的方法这种新的没有接收器的 “直接” 方法已在 Spark 1.3 中引入，以确保更强大的端到端保证。这个方法不使用接收器接收数据，而是定期查询 Kafka 每个 topic+partition 中的最新偏移量，并相应地定义了要在每个批次中要处理的偏移量范围。当处理数据的作业启动后，Kafka 的简单消费者API用于从 Kafka 中读取定义的偏移量范围（类似于从文件系统读取文件）。请注意，此特征是在 Spark 1.3 中为 Scala 和 Java API 引入的，Python API 在 Spark 1.4 中引入。 与基于 Receiver 的方法相比，该方法具有以下优点： 简化并行：不需要创建多个 Kafka 输入 Stream 然后将其合并。使用 directStream ， Spark Streaming 将创建与可以消费的 Kafka partition 一样多的 RDD partition，这些 partition 将全部从 Kafka 并行读取数据。 因此，Kafka 和 RDD partition 之间有一对一的映射关系，这更易于理解和调整。 效率：在第一种方法中实现零数据丢失需要将数据存储在 Write Ahead Log 中，这会进行数据的拷贝。这样效率比较低下，因为数据被有效地复制了两次 - 一次是 Kafka 进行的，另一次是通过 Write Ahead Log 进行的。因为没有 Receiver，所以第二种方法不存在这个问题，因此不需要 Write Ahead Log。只要我们 Kafka 的数据保留足够长的时间，就可以从 Kafka 恢复信息。 Exactly-once 语义：第一种方法使用 Kafka 的高级API在 Zookeeper 中存储消费的偏移量。这是传统的从 Kafka 上消费数据的方式。尽管这种方法（结合 Write Ahead Log 使用）可以确保零数据丢失（即 at-least once 语义），但在某些失败情况下，有一些记录可能会消耗两次。发生这种情况是因为 Spark Streaming 可靠接收的数据与 Zookeeper 跟踪的偏移之间不一致。因此，在第二种方法中，我们使用不使用 Zookeeper 的简单 Kafka API。在其检查点内，Spark Streaming 跟踪偏移量。这消除了 Spark Streaming 和 Zookeeper/Kafka 之间的不一致性，因此 Spark Streaming 每条记录在即使发生故障时也可以确切地收到一次。为了实现输出结果的 exactly-once 语义，将数据保存到外部数据存储区的输出操作必须是幂等的，或者是保存结果和偏移量的原子事务（请参阅主程序中输出操作的语义指南获取更多信息）。 请注意，这种方法的一个缺点是它不会更新 Zookeeper 中的偏移量，因此基于 Zookeeper 的 Kafka 监控工具不会显示进度。但是，你可以在每个批次中访问由此方法处理的偏移量，并自己更新 Zookeeper（请参见下文）。 接下来，我们将讨论如何在流应用程序中使用这种方法。 ２.1 引入对于使用 SBT/Maven 项目定义的 Scala/Java 应用程序，请引入如下工件（请参阅主编程指南中的Linking部分以获取更多信息）。groupId = org.apache.sparkartifactId = spark-streaming-kafka-0-8_2.11version = 2.3.0 ２.2 编程在流应用程序代码中，导入 KafkaUtils 并创建一个输入 DStream，如下所示。 Scala版本:import org.apache.spark.streaming.kafka._val directKafkaStream = KafkaUtils.createDirectStream[ [key class], [value class], [key decoder class], [value decoder class] ]( streamingContext, [map of Kafka parameters], [set of topics to consume]) Java版本:import org.apache.spark.streaming.kafka.*;JavaPairInputDStream&lt;String, String&gt; directKafkaStream = KafkaUtils.createDirectStream(streamingContext, [key class], [value class], [key decoder class], [value decoder class], [map of Kafka parameters], [set of topics to consume]); Python版本:from pyspark.streaming.kafka import KafkaUtils directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], &#123;"metadata.broker.list": brokers&#125;) 你还可以将 messageHandler 传递给 createDirectStream 来访问 MessageAndMetadata，其包含了当前消息的元数据，并可以将其转换为任意所需的类型。 在 Kafka 参数中，必须指定 metadata.broker.list 或 bootstrap.servers。默认情况下，它将从每个 Kafka 分区的最新偏移量开始消费。如果你将 Kafka 参数中的 auto.offset.reset 配置为 smallest，那么它将从最小偏移量开始消费。 你也可以使用 KafkaUtils.createDirectStream 的其他变体从任意偏移量开始消费。此外，如果你想访问每个批次中消费的偏移量，你可以执行以下操作： Scala版本：// Hold a reference to the current offset ranges, so it can be used downstreamvar offsetRanges = Array.empty[OffsetRange]directKafkaStream.transform &#123; rdd =&gt; offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd&#125;.map &#123; ...&#125;.foreachRDD &#123; rdd =&gt; for (o &lt;- offsetRanges) &#123; println(s"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;") &#125; ...&#125; Java版本:// Hold a reference to the current offset ranges, so it can be used downstreamAtomicReference&lt;OffsetRange[]&gt; offsetRanges = new AtomicReference&lt;&gt;();directKafkaStream.transformToPair(rdd -&gt; &#123; OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges(); offsetRanges.set(offsets); return rdd;&#125;).map( ...).foreachRDD(rdd -&gt; &#123; for (OffsetRange o : offsetRanges.get()) &#123;System.out.println( o.topic() + " " + o.partition() + " " + o.fromOffset() + " " + o.untilOffset()); &#125; ...&#125;); Python版本:offsetRanges = []def storeOffsetRanges(rdd): global offsetRanges offsetRanges = rdd.offsetRanges() return rdddef printOffsetRanges(rdd): for o in offsetRanges: print "%s %s %s %s" % (o.topic, o.partition, o.fromOffset, o.untilOffset)directKafkaStream \ .transform(storeOffsetRanges) \ .foreachRDD(printOffsetRanges) 如果你希望基于 Zookeeper 的 Kafka 监视工具显示流应用程序的进度，你可以使用上面来更新 Zookeeper。 请注意，HasOffsetRanges 的类型转换只有在 directKafkaStream 的第一个方法调用中使用才会成功，而不是放在后面的方法链中。你可以使用 transform() 替换 foreachRDD() 作为调用的第一个方法来访问偏移量，然后再调用其他的Spark方法。但是，请注意，RDD partition 与 Kafka partition 之间的一对一映射经过任意 shuffle 或重新分区的方法（例如， reduceByKey（）或window（）之后不会保留。 另外需要注意的是，由于此方法不使用 Receivers，因此与 receiver 相关的配置（即 spark.streaming.receiver.* 形式的配置）将不再适用于由此方法创建的输入DStream（将应用于其他输入DStreams）。相反，使用 spark.streaming.kafka.* 配置。一个重要的配置是 spark.streaming.kafka.maxRatePerPartition，每个 Kafka partition 使用 direct API 读取的最大速率（每秒消息数）。 2.3 部署这与第一种方法相同。 Spark版本： 2.3.0Kafka版本：0.8 原文：http://spark.apache.org/docs/2.3.0/streaming-kafka-0-8-integration.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark内部原理之运行原理]]></title>
    <url>%2F2018%2F03%2F14%2Fspark-internal-operating-principle-one%2F</url>
    <content type="text"><![CDATA[在大数据领域，只有深挖数据科学领域，走在学术前沿，才能在底层算法和模型方面走在前面，从而占据领先地位。 Spark的这种学术基因，使得它从一开始就在大数据领域建立了一定优势。无论是性能，还是方案的统一性，对比传统的 Hadoop，优势都非常明显。Spark 提供的基于 RDD 的一体化解决方案，将 MapReduce、Streaming、SQL、Machine Learning、Graph Processing 等模型统一到一个平台下，并以一致的API公开，并提供相同的部署方案，使得 Spark 的工程应用领域变得更加广泛。 1. Spark 专业术语定义1.1 Application：Spark应用程序指的是用户编写的Spark应用程序，包含了Driver功能代码和分布在集群中多个节点上运行的Executor代码。 Spark应用程序，由一个或多个作业JOB组成，如下图所示。 1.2 Driver：驱动程序Spark 中的 Driver 即运行上述 Application 的 Main() 函数并且创建 SparkContext，其中创建 SparkContext 的目的是为了准备 Spark 应用程序的运行环境。在 Spark 中由 SparkContext 负责和 ClusterManager 通信，进行资源的申请、任务的分配和监控等；当 Executor 部分运行完毕后，Driver 负责将 SparkContext 关闭。通常 SparkContext 代表 Driver，如下图所示。 1.3 Cluster Manager：资源管理器指的是在集群上获取资源的外部服务，常用的有：Standalone，Spark 原生的资源管理器，由 Master 负责资源的分配；Haddop Yarn，由 Yarn 中的 ResearchManager 负责资源的分配；Messos，由 Messos 中的 Messos Master 负责资源管理，如下图所示。 1.4 Executor：执行器Application 运行在 Worker 节点上的一个进程，该进程负责运行 Task，并且负责将数据存在内存或者磁盘上，每个 Application 都有各自独立的一批 Executor，如下图所示。 1.5 Worker：计算节点集群中任何可以运行 Application 代码的节点，类似于 Yarn 中的 NodeManager 节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，在Spark on Messos模式中指的就是Messos Slave节点，如下图所示。 1.6 RDD：弹性分布式数据集Resillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作），如下图所示。 1.7 窄依赖父RDD每一个分区最多被一个子RDD的分区所用；表现为一个父RDD的分区对应于一个子RDD的分区，或两个父RDD的分区对应于一个子RDD 的分区。如图所示。 1.8 宽依赖父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。如图所示。 常见的窄依赖有：map、filter、union、mapPartitions、mapValues、join（父RDD是hash-partitioned ：如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖）。 常见的宽依赖有groupByKey、partitionBy、reduceByKey、join（父RDD不是hash-partitioned ：除此之外的，rdd 的join api是宽依赖）。 1.9 DAG：有向无环图Directed Acycle graph，反应RDD之间的依赖关系，如图所示。 1.10 DAGScheduler：有向无环图调度器基于 DAG 划分 Stage 并以 TaskSet 的形势把 Stage 提交给 TaskScheduler；负责将作业拆分成不同阶段的具有依赖关系的多批任务；最重要的任务之一就是：计算作业和任务的依赖关系，制定调度逻辑。在 SparkContext 初始化的过程中被实例化，一个 SparkContext 对应创建一个 DAGScheduler。 1.11 TaskScheduler：任务调度器将 Taskset 提交给 worker（集群）运行并回报结果；负责每个具体任务的实际物理调度。如图所示。 1.12 Job：作业由一个或多个调度阶段所组成的一次计算作业；包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation。如图所示。 1.13 Stage：调度阶段一个任务集对应的调度阶段；每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业分为多个阶段；Stage分成两种类型ShuffleMapStage、ResultStage。如图所示。 1.14 TaskSet：任务集 由一组关联的，但相互之间没有Shuffle依赖关系的任务所组成的任务集。如图所示。 一个Stage创建一个TaskSet； 为Stage的每个Rdd分区创建一个Task,多个Task封装成TaskSet 1.15 Task：任务被送到某个Executor上的工作任务；单个分区数据集上的最小处理流程单元。如图所示 总体如图所示： 2. Spark运行基本流程 3. Spark运行架构特点3.1 Executor进程专属每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。如图所示。 3.2 支持多种资源管理器Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了，Spark支持资源管理器包含： Standalone、On Mesos、On YARN、Or On EC2。如图所示。 3.3 Job提交就近原则提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack（机架）里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。如图所示。 3.4 移动程序而非移动数据的原则执行Task采用了数据本地性和推测执行的优化机制。关键方法：taskIdToLocations、getPreferedLocations。如图所示。 4. Spark核心原理透视4.1 计算流程 4.2 从代码构建DAG图Val lines1 = sc.textFile(inputPath1).map(···).map(···)Val lines2 = sc.textFile(inputPath2).map(···)Val lines3 = sc.textFile(inputPath3)Val dtinone1 = lines2.union(lines3)Val dtinone = lines1.join(dtinone1)dtinone.saveAsTextFile(···)dtinone.filter(···).foreach(···) Spark的计算发生在RDD的Action操作，而对Action之前的所有Transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。 Spark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。 4.3 将DAG划分为Stage核心算法Application多个job多个Stage：Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。 划分依据：Stage划分的依据就是宽依赖，何时产生宽依赖，reduceByKey, groupByKey等算子，会导致宽依赖的产生。 核心算法：从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分。Spark内核会从触发Action操作的那个RDD开始从后往前推，首先会为最后一个RDD创建一个stage，然后继续倒推，如果发现对某个RDD是宽依赖，那么就会将宽依赖的那个RDD创建一个新的stage，那个RDD就是新的stage的最后一个RDD。然后依次类推，继续继续倒推，根据窄依赖或者宽依赖进行stage的划分，直到所有的RDD全部遍历完成为止。 4.4 将DAG划分为Stage剖析从HDFS中读入数据生成3个不同的RDD，通过一系列transformation操作后再将计算结果保存回HDFS。可以看到这个DAG中只有join操作是一个宽依赖，Spark内核会以此为边界将其前后划分成不同的Stage. 同时我们可以注意到，在图中Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作，通过map操作生成的partition可以不用等待整个RDD计算结束，而是继续进行union操作，这样大大提高了计算的效率。 4.5 提交Stages调度阶段的提交，最终会被转换成一个任务集的提交，DAGScheduler通过TaskScheduler接口提交任务集，这个任务集最终会触发TaskScheduler构建一个TaskSetManager的实例来管理这个任务集的生命周期，对于DAGScheduler来说，提交调度阶段的工作到此就完成了。而TaskScheduler的具体实现则会在得到计算资源的时候，进一步通过TaskSetManager调度具体的任务到对应的Executor节点上进行运算。 4.6 监控Job、Task、ExecutorDAGScheduler监控Job与Task：要保证相互依赖的作业调度阶段能够得到顺利的调度执行，DAGScheduler需要监控当前作业调度阶段乃至任务的完成情况。这通过对外暴露一系列的回调函数来实现的，对于TaskScheduler来说，这些回调函数主要包括任务的开始结束失败、任务集的失败，DAGScheduler根据这些任务的生命周期信息进一步维护作业和调度阶段的状态信息。 DAGScheduler监控Executor的生命状态：TaskScheduler通过回调函数通知DAGScheduler具体的Executor的生命状态，如果某一个Executor崩溃了，则对应的调度阶段任务集的ShuffleMapTask的输出结果也将标志为不可用，这将导致对应任务集状态的变更，进而重新执行相关计算任务，以获取丢失的相关数据。 4.7 获取任务执行结果结果DAGScheduler：一个具体的任务在Executor中执行完毕后，其结果需要以某种形式返回给DAGScheduler，根据任务类型的不同，任务结果的返回方式也不同。 两种结果，中间结果与最终结果：对于FinalStage所对应的任务，返回给DAGScheduler的是运算结果本身，而对于中间调度阶段对应的任务ShuffleMapTask，返回给DAGScheduler的是一个MapStatus里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个调度阶段的任务获取输入数据的依据。 两种类型，DirectTaskResult与IndirectTaskResult：根据任务结果大小的不同，ResultTask返回的结果又分为两类，如果结果足够小，则直接放在DirectTaskResult对象内中，如果超过特定尺寸则在Executor端会将DirectTaskResult先序列化，再把序列化的结果作为一个数据块存放在BlockManager中，然后将BlockManager返回的BlockID放在IndirectTaskResult对象中返回给TaskScheduler，TaskScheduler进而调用TaskResultGetter将IndirectTaskResult中的BlockID取出并通过BlockManager最终取得对应的DirectTaskResult。 4.8 任务调度总体诠释 原文： https://www.toutiao.com/i6511498014832460301/?tt_from=weixin&amp;utm_campaign=client_share&amp;timestamp=1520998005&amp;app=news_article&amp;utm_source=weixin&amp;iid=26380623414&amp;utm_medium=toutiao_android&amp;wxshare_count=1]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 创建RDD]]></title>
    <url>%2F2018%2F03%2F12%2Fspark-base-build-resilient-distributed-datasets%2F</url>
    <content type="text"><![CDATA[Spark的核心概念是弹性分布式数据集（RDD），RDD 是一个可容错、并行操作的分布式元素集合。有两种方法可以创建 RDD 对象： 在驱动程序中并行化操作已存在集合来创建 RDD 从外部存储系统中引用数据集（如：共享文件系统、HDFS、HBase 或者其他 Hadoop 支持的数据源）。 1. 并行化集合在你驱动程序的现有集合上调用 JavaSparkContext 的 parallelize 方法创建并行化集合(Parallelized collections)。集合的元素被复制以形成可以并行操作的分布式数据集。 例如，下面是如何创建一个包含数字1到5的并行化集合： Java版本：List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5);JavaRDD&lt;Integer&gt; rdd = sc.parallelize(list); Scala版本：val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data) Python版本：data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) RDD 一旦创建，分布式数据集（distData）就可以并行操作。例如，我们可以调用 distData.reduce（（a，b） - &gt; a + b） 来实现对列表元素求和。我们稍后介绍分布式数据集的操作。 并行化集合的一个重要参数是将数据集分割成多少分区的 partitions 个数。Spark 集群中每个分区运行一个任务(task)。典型场景下，一般为每个CPU分配2－4个分区。但通常而言，Spark 会根据你集群的情况，自动设置分区数。当然，你可以给 parallelize 方法传递第二个参数来手动设置分区数（如：sc.parallelize(data, 10)）。 Spark代码里有些地方仍然使用分片（slice）这个术语(分区的同义词)，主要为了保持向后兼容。 2. 外部数据集Spark 可以从 Hadoop 支持的任何存储数据源创建分布式数据集，包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等。Spark 也支持文本文件，SequenceFiles 以及任何其他 Hadoop 输入格式。 文本文件 RDD 可以使用 SparkContext 的 textFile 方法创建。该方法根据URL获取文件（机器的本地路径，或 hdfs:// ， s3n:// 等等），并按行读取。下面是一个示例调用： Java版本：JavaRDD&lt;String&gt; distFile = sc.textFile("data.txt"); Scala版本：scala&gt; val distFile = sc.textFile("data.txt")distFile: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[10] at textFile at &lt;console&gt;:26 Python版本：&gt;&gt;&gt; distFile = sc.textFile(&quot;data.txt&quot;) 一旦创建完成，就可以在 distFiile 上做数据集操作。例如，我们可以用下面的方式使用 map 和 reduce 操作将所有行的长度相加：distFile.map(s -&gt; s.length()).reduce((a, b) -&gt; a + b); Spark读文件时一些注意事项： (1) 如果使用本地文件系统路径，在所有工作节点上该文件必须都能用相同的路径访问到。要么能复制文件到所有的工作节点，要么能使用网络的方式共享文件系统。 (2) Spark 所有基于文件的输入方法，包括 textFile，能很好地支持文件目录，压缩文件和通配符。例如，你可以使用:textFile(&quot;/my/directory&quot;)textFile(&quot;/my/directory/*.txt&quot;)textFile(&quot;/my/directory/*.gz&quot;) (3) textFile 方法也可以选择第二个可选参数来控制文件分区数目，默认情况下，Spark 为每一个文件块创建一个分区（HDFS中分块大小默认为128MB），你也可以通过传递一个较大数值来请求更多分区。注意的是，分区数目不能少于分块数目。 除了文本文件，Spark 的 Java API 还支持其他几种数据格式： (1) JavaSparkContext.wholeTextFiles 可以读取包含多个小文本文件的目录，并将它们以（文件名，内容）键值对返回。这与 textFile 相反，textFile 将在每个文件中每行返回一条记录。JavaPairRDD&lt;String, String&gt; rdd = sc.wholeTextFiles("/home/xiaosi/wholeText");List&lt;Tuple2&lt;String, String&gt;&gt; list = rdd.collect();for (Tuple2&lt;?, ?&gt; tuple : list) &#123; System.out.println(tuple._1() + ": " + tuple._2());&#125; (2) 对于 SequenceFiles，可以使用 SparkContext 的 sequenceFile[K，V] 方法，其中 K 和 V 是文件中的键和值的类型。这些应该是 Hadoop 的 Writable 接口的子类，如 IntWritable 和 Text。 (3) 对于其他 Hadoop InputFormats，你可以使用 JavaSparkContext.hadoopRDD 方法，该方法采用任意 JobConf 和输入格式类，键类和值类。将这些设置与使用输入源的 Hadoop 作业相同。你还可以使用基于“新” MapReduce API（org.apache.hadoop.mapreduce）的 InputFormats 的 JavaSparkContext.newAPIHadoopRDD。 (4) JavaRDD.saveAsObjectFile 和 SparkContext.objectFile 支持保存一个 RDD，保存格式是一个简单的 Java 对象序列化格式。这是一种效率不高的专有格式，如 Avro，它提供了简单的方法来保存任何一个 RDD。 Spark版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#resilient-distributed-datasets-rdds]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 RDD操作]]></title>
    <url>%2F2018%2F03%2F12%2Fspark-base-rdd-operations%2F</url>
    <content type="text"><![CDATA[RDD支持两种类型的操作： 转换操作(transformations): 从现有数据集创建一个新数据集 动作操作(actions): 在数据集上进行计算后将值返回给驱动程序 例如，map 是一个转换操作，传递给每个数据集元素一个函数并返回一个新 RDD 表示返回结果。另一方面，reduce 是一个动作操作，使用一些函数聚合 RDD 的所有元素并将最终结果返回给驱动程序（尽管还有一个并行的 reduceByKey 返回一个分布式数据集）。 在 Spark 中，所有的转换操作(transformations)都是惰性(lazy)的，它们不会马上计算它们的结果。相反，它们仅仅记录应用到基础数据集(例如一个文件)上的转换操作。只有当 action 操作需要返回一个结果给驱动程序的时候， 转换操作才开始计算。 这个设计能够让 Spark 运行得更加高效。例如，我们知道：通过 map 创建的新数据集将在 reduce 中使用，并且仅仅返回 reduce 的结果给驱动程序，而不必将比较大的映射后的数据集返回。 1. 基础为了说明 RDD 基础知识，请考虑以下简单程序： Java版本:JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());int totalLength = lineLengths.reduce((a, b) -&gt; a + b); Scala版本:val lines = sc.textFile("data.txt")val lineLengths = lines.map(s =&gt; s.length)val totalLength = lineLengths.reduce((a, b) =&gt; a + b) Python版本:lines = sc.textFile("data.txt")lineLengths = lines.map(lambda s: len(s))totalLength = lineLengths.reduce(lambda a, b: a + b) 第一行定义了一个来自外部文件的基本 RDD。这个数据集并未加载到内存中或做其他处理：lines 仅仅是一个指向文件的指针。第二行将 lineLengths 定义为 map 转换操作的结果。其次，由于转换操作的惰性(lazy)，lineLengths 并没有立即计算。最后，我们运行 reduce，这是一个动作操作。此时，Spark 把计算分成多个任务(task)，并让它们运行在多台机器上。每台机器都运行 map 的一部分以及本地 reduce。然后仅仅将结果返回给驱动程序。 如果稍后还会再次使用 lineLength，我们可以在运行 reduce 之前添加： Java版本:lineLengths.persist(StorageLevel.MEMORY_ONLY()); Scala版本:lineLengths.persist() Python版本:lineLengths.persist() 这将导致 lineLength 在第一次计算之后被保存在内存中。 2. 传递函数给SparkSpark 的 API 很大程度上依赖于运行在集群上的驱动程序中的函数。 2.1 Java版本在 Java 中，函数由 org.apache.spark.api.java.function 接口实现。创建这样的函数有两种方法： 在你自己类中实现 Function 接口，作为匿名内部类或命名内部类，并将其实例传递给Spark。 使用 lambda 表达式 来简洁地定义一个实现。 虽然本指南的大部分内容都使用 lambda 语法进行简明说明，但很容易以长格式使用所有相同的API。例如，我们可以按照以下方式编写我们的代码： 匿名内部类JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new Function&lt;String, Integer&gt;() &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;);int totalLength = lineLengths.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;); 或者命名内部类class GetLength implements Function&lt;String, Integer&gt; &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());int totalLength = lineLengths.reduce(new Sum()); 请注意，Java中的匿名内部类也可以访问封闭范围内的变量，只要它们标记为final。 Spark会将这些变量的副本发送给每个工作节点，就像其他语言一样。 2.2 Scala版本有两种推荐的的方法可以做到这一点： 匿名函数语法，可用于短片段代码。 全局单例对象中的静态方法。例如，您可以定义对象 MyFunctions，然后传递 MyFunctions.func1，如下所示： object MyFunctions &#123; def func1(s: String): String = &#123; ... &#125;&#125;myRdd.map(MyFunctions.func1) 虽然也可以在类实例中传递方法的引用（与单例对象相反），但这需要将包含该类的对象与方法一起发送。 例如，考虑： class MyClass &#123; def func1(s: String): String = &#123; ... &#125; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(func1) &#125;&#125; 在这里，如果我们创建一个新的 MyClass 实例并调用 doStuff 方法，那么其中的 map 会引用该 MyClass 实例的 func1 方法，因此需要将整个对象发送到集群。它类似于编写 rdd.map（x =&gt; this.func1（x）） 。 以类似的方式，访问外部对象的字段将引用整个对象：class MyClass &#123; val field = "Hello" def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(x =&gt; field + x) &#125;&#125; 等价于 rdd.map（x =&gt; this.field + x），它引用了 this 对象的所有东西 。为了避免这个问题，最简单的方法是将字段复制到本地变量中，而不是从外部访问它：def doStuff(rdd: RDD[String]): RDD[String] = &#123; val field_ = this.field rdd.map(x =&gt; field_ + x)&#125; 3. 使用键值对虽然大多数 Spark 操作可以在任意类型对象的 RDD 上工作，但是还是几个特殊操作只能在键值对的 RDD 上使用。最常见的是分布式 shuffle 操作，例如按键分组或聚合元素。 3.1 Java版本在 Java 中，使用 Scala 标准库中的 scala.Tuple2 类来表示键值对。可以如下简单地调用 new Tuple2（a，b） 来创建一个元组，然后用 tuple._1（） 和 tuple._2（） 访问它的字段。 键值对的 RDD 由 JavaPairRDD 类表示。你可以使用特殊版本的 map 操作（如 mapToPair 和 flatMapToPair）从 JavaRDD 来构建 JavaPairRDD。JavaPairRDD 具有标准的 RDD 函数以及特殊的键值对函数。 例如，以下代码在键值对上使用 reduceByKey 操作来计算每行文本在文件中的出现次数：JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaPairRDD&lt;String, Integer&gt; pairs = lines.mapToPair(s -&gt; new Tuple2(s, 1));JavaPairRDD&lt;String, Integer&gt; counts = pairs.reduceByKey((a, b) -&gt; a + b); 例如，我们也可以使用 counts.sortByKey（） 来按字母顺序来对键值对排序，最后使用 counts.collect（） 将结果作为对象数组返回到驱动程序。 3.2 Scala版本在 Scala 中，这些操作在包含 Tuple2 对象的 RDD 上可以自动获取（内置元组，通过简单写入（a，b）创建）。在 PairRDDFunctions 类中提供了键值对操作，该类自动包装元组的 RDD。 例如，以下代码在键值对上使用 reduceByKey 操作来计算每行文本在文件中的出现次数：val lines = sc.textFile("data.txt")val pairs = lines.map(s =&gt; (s, 1))val counts = pairs.reduceByKey((a, b) =&gt; a + b) 例如，我们也可以使用 counts.sortByKey（） 来按字母顺序来对键值对排序，最后使用 counts.collect（） 将结果作为对象数组返回到驱动程序。 在键值对操作时使用一个自定义对象作为 key 的时候，你需要确保自定义 equals() 方法和 hashCode() 方法是匹配的。更加详细的内容，查看 Object.hashCode()) 文档)中的契约概述。 4. 转换操作下面列出了Spark支持的一些常见转换函数。 有关详细信息，请参阅RDD API文档（Scala，Java，Python，R）和RDD函数doc（Scala，Java）。 4.1 map(func) 映射将函数应用于 RDD 中的每个元素，将返回值构成新的 RDD。List&lt;String&gt; aList = Lists.newArrayList("a", "B", "c", "b");JavaRDD&lt;String&gt; rdd = sc.parallelize(aList);// 小写转大写JavaRDD&lt;String&gt; upperLinesRDD = rdd.map(new Function&lt;String, String&gt;() &#123; @Override public String call(String str) throws Exception &#123; if (StringUtils.isBlank(str)) &#123; return str; &#125; return str.toUpperCase(); &#125;&#125;);// A B C B 4.2 filter(func) 过滤返回通过选择那些 func 函数返回 true 的元素形成一个新的 RDD。List&lt;String&gt; list = Lists.newArrayList("a", "B", "c", "b");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);// 只返回以a开头的字符串JavaRDD&lt;String&gt; filterRDD = rdd.filter(new Function&lt;String, Boolean&gt;() &#123; @Override public Boolean call(String str) throws Exception &#123; return !str.startsWith("a"); &#125;&#125;);// B c b 4.3 flatMap(func) 一行转多行类似于 map 函数，但是每个输入项可以映射为0个输出项或更多输出项（所以 func 应该返回一个序列而不是一个条目）。List&lt;String&gt; list = Lists.newArrayList("a 1", "B 2");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);// 一行转多行 以空格分割JavaRDD&lt;String&gt; resultRDD = rdd.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) throws Exception &#123; if (StringUtils.isBlank(s)) &#123; return null; &#125; String[] array = s.split(" "); return Arrays.asList(array).iterator(); &#125;&#125;);// a// 1// B// 2 4.4 distinct([numTasks]))去重List&lt;String&gt; aList = Lists.newArrayList("1", "3", "2", "3");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);// 去重JavaRDD&lt;String&gt; rdd = aRDD.distinct(); // 1 2 3 4.5 union(otherDataset) 并集生成一个包含两个 RDD 中所有元素的 RDD。如果输入的 RDD 中有重复数据，union() 操作也会包含这些重复的数据．List&lt;String&gt; aList = Lists.newArrayList("1", "2", "3");List&lt;String&gt; bList = Lists.newArrayList("3", "4", "5");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 并集JavaRDD&lt;String&gt; rdd = aRDD.union(bRDD); // 1 2 3 3 4 5 4.6 intersection(otherDataset) 交集求两个 RDD 共同的元素的 RDD。 intersection() 在运行时也会去掉所有重复的元素，尽管 intersection() 与 union() 的概念相似，但性能却差的很多，因为它需要通过网络混洗数据来发现共同的元素。List&lt;String&gt; aList = Lists.newArrayList("1", "2", "3");List&lt;String&gt; bList = Lists.newArrayList("3", "4", "5");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 交集JavaRDD&lt;String&gt; rdd = aRDD.intersection(bRDD); // 3 4.7 subtract(otherDataset) 差集subtract 接受另一个 RDD 作为参数，返回一个由只存在第一个 RDD 中而不存在第二个 RDD 中的所有元素组成的 RDDList&lt;String&gt; aList = Lists.newArrayList("1", "2", "3");List&lt;String&gt; bList = Lists.newArrayList("3", "4", "5");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 差集JavaRDD&lt;String&gt; rdd = aRDD.subtract(bRDD); // 1 2 4.8 groupByKey 分组根据键值对 key 进行分组。 在（K，V）键值对的数据集上调用时，返回（K，Iterable ）键值对的数据集。 如果分组是为了在每个 key 上执行聚合（如求总和或平均值），则使用 reduceByKey 或 aggregateByKey 会有更好的性能。默认情况下，输出中的并行级别取决于父 RDD 的分区数。你可以传递一个可选参数 numTasks 来设置任务数量。 Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Banana", 9);Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("Apple", 4);List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Lists.newArrayList();list.add(t1);list.add(t2);list.add(t3);list.add(t4);JavaPairRDD&lt;String, Integer&gt; rdd = sc.parallelizePairs(list);// 分组JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupRDD = rdd.groupByKey();// Apple --- 4// Pear --- 5// Banana --- 10 9 4.9 reduceByKey(func, [numTasks]) 根据key聚合当在（K，V）键值对的数据集上调用时，返回（K，V）键值对的数据集，使用给定的reduce函数 func 聚合每个键的值，该函数类型必须是（V，V）=&gt; V。类似于 groupByKey，可以通过设置可选的第二个参数来配置reduce任务的数量。 Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Banana", 9);Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("Apple", 4);List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Lists.newArrayList();list.add(t1);list.add(t2);list.add(t3);list.add(t4);JavaPairRDD&lt;String, Integer&gt; rdd = sc.parallelizePairs(list);// 分组计算JavaPairRDD&lt;String, Integer&gt; reduceRDD = rdd.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125;&#125;);// Apple --- 4// Pear --- 5// Banana --- 19 4.10 sortByKey([ascending], [numPartitions])根据key进行排序。在（K，V）键值对的数据集调用，其中 K 实现 Ordered 接口，按照升序或降序顺序返回按键排序的（K，V）键值对的数据集。Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Apple", 4);List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Lists.newArrayList();list.add(t1);list.add(t2);list.add(t3);JavaPairRDD&lt;String, Integer&gt; rdd = sc.parallelizePairs(list);// 根据key排序JavaPairRDD&lt;String, Integer&gt; sortRDD = rdd.sortByKey(true);// Apple --- 4// Banana --- 10// Pear --- 5 4.11 coalesce(numPartitions) 合并分区对 RDD 分区进行合并，合并后分区数目为 numPartitions。大型数据集过滤之后可以对高效地运行操作很有帮助。List&lt;String&gt; list = Lists.newArrayList("1", "2", "3", "4");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);JavaRDD&lt;String&gt; coalesceRDD = rdd.coalesce(2, false);System.out.println("分区个数:" + coalesceRDD.partitions().size());// 分区个数:1JavaRDD&lt;String&gt; coalesceRDD2 = rdd.coalesce(2, true);System.out.println("分区个数:" + coalesceRDD2.partitions().size());// 分区个数:2 如果可选参数 shuff 为 false 时，传入的参数大于现有的分区数目，RDD 的分区数不变，也就是说不经过shuffle，是无法将 RDD 的分区数增大。 4.12 repartition(numPartitions) 重新分区对 RDD 中的数据重新洗牌来重新分区，分区数目可以增大也可以减少，并在各分区之间进行数据平衡。这总是通过网络混洗所有数据。List&lt;String&gt; list = Lists.newArrayList("1", "2", "3", "4");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);JavaRDD&lt;String&gt; coalesceRDD = rdd.repartition(2);System.out.println("分区个数:" + coalesceRDD.partitions().size());// 分区个数:2 我们从源码看到 repartition 只是 coalesce(numPartitions, shuffle = true) 的一个简易实现：def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; coalesce(numPartitions, shuffle = true)&#125; 4.13 cartesian(otherDataset) 笛卡尔积对两个 RDD 中的所有元素进行笛卡尔积操作。在类型为 T 和 U 的两个数据集上调用时，返回（T，U）键值对（所有元素对）数据集。List&lt;String&gt; aList = Lists.newArrayList("1", "2");List&lt;String&gt; bList = Lists.newArrayList("3", "4");JavaRDD&lt;String&gt; aRDD = sc.parallelize(aList);JavaRDD&lt;String&gt; bRDD = sc.parallelize(bList);// 笛卡尔积JavaPairRDD&lt;String, String&gt; cartesianRDD = aRDD.cartesian(bRDD);// (1, 3)// (1, 4)// (2, 3)// (2, 4) 4.14 cogroup(otherDataset, [numPartitions])在类型（K，V）和（K，W）两个数据集上调用时，返回（K，（Iterable ，Iterable ））元组的数据集。这个操作也被称为 groupWith。Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("Banana", 10);Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("Pear", 5);Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("Apple", 4);Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("Banana", 2);Tuple2&lt;String, Integer&gt; t5 = new Tuple2&lt;String, Integer&gt;("Apple", 11);Tuple2&lt;String, Integer&gt; t6 = new Tuple2&lt;String, Integer&gt;("Banana", 7);List&lt;Tuple2&lt;String, Integer&gt;&gt; list1 = Lists.newArrayList();list1.add(t1);list1.add(t2);list1.add(t3);List&lt;Tuple2&lt;String, Integer&gt;&gt; list2 = Lists.newArrayList();list2.add(t4);list2.add(t5);list2.add(t6);JavaPairRDD&lt;String, Integer&gt; rdd1 = sc.parallelizePairs(list1);JavaPairRDD&lt;String, Integer&gt; rdd2 = sc.parallelizePairs(list2);JavaPairRDD&lt;String, Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt;&gt; cogroupRDD = rdd1.cogroup(rdd2);cogroupRDD.foreach(new VoidFunction&lt;Tuple2&lt;String, Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt;&gt; group) throws Exception &#123; String key = group._1; Tuple2&lt;Iterable&lt;Integer&gt;, Iterable&lt;Integer&gt;&gt; value = group._2; System.out.println(key + " --- " + value.toString()); &#125;&#125;);// Apple --- ([4],[11])// Pear --- ([5],[])// Banana --- ([10],[2, 7]) 5. 动作操作 (Action)下面列出了Spark支持的一些常见操作。 5.1 reduce接收一个函数作为参数，这个函数要操作两个相同元素类型的RDD并返回一个同样类型的新元素．List&lt;String&gt; aList = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(aList);String result = rdd.reduce(new Function2&lt;String, String, String&gt;() &#123; @Override public String call(String v1, String v2) throws Exception &#123; return v1 + "#" + v2; &#125;&#125;);System.out.println(result); // aa#bb#cc#dd 5.2 collect将整个RDD的内容返回．List&lt;String&gt; list = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);List&lt;String&gt; collect = rdd.collect();System.out.println(collect); // [aa, bb, cc, dd] 5.3 take(n)返回 RDD 中的n个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合．需要注意的是，这些操作返回元素的顺序与你的预期可能不一样．List&lt;String&gt; list = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);List&lt;String&gt; collect = rdd.take(3);System.out.println(collect); // [aa, bb, cc] 5.4 takeSample有时需要在驱动器程序中对我们的数据进行采样，takeSample(withReplacement, num, seed) 函数可以让我们从数据中获取一个采样，并指定是否替换． 5.5 saveAsTextFile(path)将数据集的元素写入到本地文件系统，HDFS 或任何其他 Hadoop 支持的文件系统中的给定目录的文本文件（或文本文件集合）中。Spark 在每个元素上调用 toString 方法将其转换为文件中的一行文本。List&lt;String&gt; list = Lists.newArrayList("aa", "bb", "cc", "dd");JavaRDD&lt;String&gt; rdd = sc.parallelize(list);rdd.saveAsTextFile("/home/xiaosi/output"); 输出:xiaosi@ying:~/output$ cat *aabbccdd 5.6 saveAsSequenceFile(path)将数据集的元素写入到本地文件系统，HDFS 或任何其他 Hadoop 支持的文件系统中的给定路径下的 Hadoop SequenceFile中。这在实现 Hadoop 的 Writable 接口的键值对的 RDD 上可用。 在 Scala 中，它也可用于可隐式转换为 Writable 的类型（Spark包含Int，Double，String等基本类型的转换）。 5.7 foreach(func)在数据集的每个元素上运行函数 func。这通常用于副作用，如更新累加器或与外部存储系统交互。 修改foreach（）之外的变量而不是累加器可能会导致未定义的行为。有关更多详细信息，请参阅了解闭包 Spark版本:2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#rdd-operations]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 初始化]]></title>
    <url>%2F2018%2F03%2F12%2Fspark-base-initializing-spark%2F</url>
    <content type="text"><![CDATA[1. 初始化Spark 程序必须做的第一件事是创建一个 JavaSparkContext 对象(Scala和Python中是SparkContext对象)，这告诉了 Spark 如何访问集群。要创建 SparkContext，你首先需要构建一个包含有关应用程序信息的 SparkConf 对象。 Java版本：private static String appName = "JavaWordCountDemo";private static String master = "local";// 初始化Sparkprivate static SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);private static JavaSparkContext sc = new JavaSparkContext(conf); Scala版本：val conf = new SparkConf().setAppName(appName).setMaster(master)new SparkContext(conf) 每个 JVM 只能有一个 SparkContext 处于活跃状态。在创建新的 SparkContext 之前，必须先调用 stop() 方法停止之前活跃的 SparkContext。 Python版本：conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) appName 参数是应用程序在集群UI上显示的名称。master 是 Spark，Mesos 或 YARN 集群的 URL，或以本地模式运行的特殊字符串 local。实际上，当在集群上运行时，你不需要在程序中写死 master，而是使用 spark-submit 启动应用程序并以参数传递进行接收。但是，对于本地测试和单元测试，你可以通过 local 来运行 Spark 进程。 2. 使用Shell在 Spark shell 中，已经为你创建了一个专有的 SparkContext，可以通过变量 sc 访问。你自己创建的 SparkContext 将无法工作。可以用 --master 参数来设置 SparkContext 要连接的集群，用 --jars 来设置需要添加到 classpath 中的 JAR 包，如果有多个 JAR 包使用逗号分割符连接它们。你还可以通过 --packages 参数提供逗号分隔的 maven 坐标列表，将依赖关系（例如Spark Packages）添加到 shell 会话中。依赖项存在的任何可选存储库（例如Sonatype）可以传递给 --repositories 参数。例如：在一个拥有 4 核的环境上运行 bin/spark-shell，使用： ./bin/spark-shell --master local[4] 或者，还可以将 code.jar 添加到其 classpath 中，请使用：./bin/spark-shell --master local[4] --jars code.jar 使用maven坐标来包含依赖项：./bin/spark-shell --master local[4] --packages &quot;org.example:example:0.1&quot; 可以执行 spark-shell --help 获取完整的选项列表。spark-shell 调用的是更常用的spark-submit脚本。 Spark 版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#initializing-spark]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.3.0 引入Spark]]></title>
    <url>%2F2018%2F03%2F12%2Fspark-base-linking-with-spark%2F</url>
    <content type="text"><![CDATA[1. Java版Spark 2.3.0 支持用于简洁编写函数的 lambda 表达式，你也可以使用 org.apache.spark.api.java.function 包中的类。 请注意，在 Spark 2.2.0 中删除了对 Java 7 的支持。 要在 Java 中编写 Spark 应用程序，需要在 Spark 上添加依赖项。Spark可通过 Maven 仓库获得：groupId = org.apache.sparkartifactId = spark-core_2.11version = 2.3.0 另外，如果希望访问 HDFS 集群，需要根据你的 HDFS 版本添加 hadoop-client 的依赖：groupId = org.apache.hadoopartifactId = hadoop-clientversion = &lt;your-hdfs-version&gt; 最后，你需要将一些 Spark 类导入到程序中。 添加以下行：import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.SparkConf; 2. Scala版默认情况下，Spark 2.3.0 在 Scala 2.11 上构建并分布式运行。（Spark 可以与其他版本的 Scala 一起构建。）要在 Scala 中编写应用程序，需要使用兼容的 Scala 版本（例如2.11.X）。 要编写 Spark 应用程序，需要在 Spark 上添加依赖项。Spark 可通过 Maven 仓库获得：groupId = org.apache.sparkartifactId = spark-core_2.11version = 2.3.0 另外，如果希望访问 HDFS 集群，则需要根据你的 HDFS 版本添加 hadoop-client 的依赖：groupId = org.apache.hadoopartifactId = hadoop-clientversion = &lt;your-hdfs-version&gt; 最后，需要将一些 Spark 类导入到程序中。 添加以下行：import org.apache.spark.SparkContextimport org.apache.spark.SparkConf 备注 在 Spark 1.3.0 之前，需要明确导入 org.apache.spark.SparkContext._ 以启用基本的隐式转换。 备注 Spark版本: 2.3.0 原文：http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#linking-with-spark]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 第一个Spark程序WordCount]]></title>
    <url>%2F2018%2F03%2F11%2Fspark-first-application-word-count%2F</url>
    <content type="text"><![CDATA[1 Maven 依赖&lt;spark.version&gt;2.1.0&lt;/spark.version&gt;&lt;!-- spark --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2. JavaWordCountpackage com.sjf.open.spark;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.sql.SparkSession;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;import java.util.List;import java.util.regex.Pattern;/** * Created by xiaosi on 17-2-13. * * Spark 测试程序 WordCount * */public final class JavaWordCount &#123; private static final Pattern SPACE = Pattern.compile(" "); public static void main(String[] args) throws Exception &#123; if (args.length &lt; 1) &#123; System.err.println("Usage: JavaWordCount &lt;file&gt;"); System.exit(1); &#125; SparkSession spark = SparkSession.builder().appName("JavaWordCount").getOrCreate(); JavaRDD&lt;String&gt; lines = spark.read().textFile(args[0]).javaRDD(); JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) &#123; return Arrays.asList(SPACE.split(s)).iterator(); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;); List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect(); for (Tuple2&lt;?, ?&gt; tuple : output) &#123; System.out.println(tuple._1() + ": " + tuple._2()); &#125; spark.stop(); &#125;&#125; 3. 命令行执行使用Maven 进行打包：mvn cleanmvn package 使用上述命令打包后，会在项目根目录下的target目录生成jar包。打完jar包后，我们可以使用spark-submit提交任务：bin/spark-submit --class com.sjf.open.spark.JavaWordCount --master local /home/xiaosi/code/Common-Tool/target/common-tool-jar-with-dependencies.jar /home/xiaosi/a.txtee: 1aa: 3dd: 2vvv: 1ff: 2bb: 3cc: 1 4. Idea本地调试运行配置]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 分布式数据流的轻量级异步快照]]></title>
    <url>%2F2018%2F03%2F06%2Flightweight-asynchronous-snapshots-for-distributed-dataflows%2F</url>
    <content type="text"><![CDATA[1. 概述分布式有状态流处理支持在云中部署和执行大规模连续计算，主要针对低延迟和高吞吐量。这种模式的一个最根本的挑战就是在可能的失败情况下提供处理保证。现有方法依赖于可用于故障恢复的周期性全局状态快照。这些方法有两个主要缺点。首先，他们经常拖延影响数据摄取的整体计算过程。其次，持久化存储所有传输中的记录以及算子状态，这会导致比所需的快照要更大。 因此，提出了一种新的分布式快照的算法，即在 Apache Flink 中的异步屏障快照（Asynchronous Barrier Snapshotting (ABS)）。这是一种适用于现代数据流执行引擎的轻量级算法，可最大限度地减少空间需求，让快照发生时对系统的影响降到最低。这种算法不会停止流处理，它只会引入很少的运行时间开销，而且对于整个无环图的拓扑结构，只对有状态的算子进行快照，因此快照的大小只会占用很小的空间。该算法不会对执行产生重大影响，保证线性可伸缩性，并且可以在频繁的快照下正常运行。 这里所说的新型的快照算法，既适用于有向无环图，也适用于有向有环图。本文重点关注在有向无环图中的应用。 2. Apache Flink SystemApache Flink 围绕通用运行时引擎进行架构，可以统一处理批处理和流式作业。Flink 中的作业被编译成任务的有向图。数据元素从外部数据源获取，并以流水线方式通过任务图。基于接收到的输入，任务不断操作其内部状态，并产生新的输出。 2.1 流处理编程模型可以从外部来源（例如消息队列，套接字流，自定义生成器）或通过调用其他 DataStream 上的操作来创建 DataStreams。DataStreams 支持多种算子，如 map，filter 和 reduce 等形式的高阶函数，这些函数在每个记录上逐步应用并生成新的 DataStream。每个算子可以通过将并行实例放置在相应流的不同分区上运行来并行化，从而允许分布式执行流转换。 下面的代码示例中显示了如何在 Apache Flink 中实现简单的 Word Count 程序。在此程序中，从文本文件中读取单词，并将每个单词的当前计数打印到标准输出上。这是一个有状态的流处理程序，所以数据源需要知道它们在文件中的当前偏移量，并且需要计数器来将每个单词的当前计数保持在内部状态中。 2.2 分布式数据流执行当用户执行一个应用程序时，所有的 DataStream 算子都将编译成一个执行图，原理上为一个有向图 G =（T，E），其中顶点 T 表示任务，边 E 表示两个任务之间的 data channels。上图就描绘了一个 Word Count 例子的执行图。如图所示，算子的每个实例都封装在相应的任务上。任务可以进一步细分为没有 input channels 的 Source 以及没有 output channels 的 Sink。此外，M 表示任务在并行执行期间传输的所有记录的集合。每个任务 t ∈ T 封装了一个算子实例的独立运行，其由以下内容组成： 一组 input channels 和 output channels: It , Ot ⊆ E 算子状态 St 用户自定义函数 ft 。在执行过程中，每个任务消耗输入记录，更新算子状态并根据其用户自定义函数生成新的记录。对于流入算子的每一条数据 r ∈ M，通过 UDF，产生一个新的状态值 st’，同时产生一个新的输出的集合 D ⊆ M。 3. Asynchronous Barrier Snapshotting为了提供一致性结果，分布式处理系统需要对失败任务进行恢复。提供这种弹性的一种方法是定期捕获执行图的快照，然后可以用它来从故障中恢复。快照是执行图的全局状态，捕获所有必要信息以从该特定执行状态重新开始计算。 3.1 问题定义我们定义了一个执行图 G =（T，E） 的全局快照 G * =（T *，E *），其中 T * 和 E * 分别表示所有任务和边的状态集合。更详细地说，T * 由所有算子状态 St * ∈ T * 组成， ∀t ∈ T， E * 是所有 channels 状态 e * ∈ E * 的集合，其中 e * 由在 e 上传输的记录组成。 我们确保每个快照 G * 都保留某些属性，例如最终性 Termination 和可行性 Feasibility，以便在故障恢复后保证结果的正确性。最终性保证，如果所有进程都处于活跃状态，那么快照算法最终会在启动后的有限时间内完成。可行性表达了快照的意义，即在快照过程中关于计算的信息不会丢失。 3.2 非循环数据流的ABS当一个执行过程被分成多个阶段 (stage)，在不保留 channels 状态的情况下执行快照是可行的。stage 将注入的数据流和所有相关的计算划分为一系列可能的执行过程，其中所有先前的输入和生成的输出已经完全处理。在一个 Stage 结束时的算子状态集合反映了整个执行历史，因此它可以用于快照。我们算法背后的核心思想是在保持连续数据摄入的同时使用分段快照创建一致性快照（create identical snapshots with staged snapshotting）。 在我们的方法中，在持续的数据流执行中模拟 stage 是通过向数据流中周期性注入特殊屏障 barrier 标记完成的，这些标记在整个执行图中一直传输到 sink。全局快照是随着每个任务接收表示执行 stage 的 barrier 而逐步构建的。 我们进一步为我们的算法做出以下假设： 网络通道是准确可靠的，遵循 FIFO 先进先出的顺序，并且可以被阻塞以及解除阻塞。当一个 channels 被阻塞时，所有的消息都会被缓存，但是不会被传递，直到它被解除阻塞。 任务可以触发其通道组件上的操作，例如阻塞，解除阻塞和发送消息。所有输出通道都支持广播消息。 在 source 任务中注入的消息(即 stage barrier )被解析为 Nil。 算法执行过程如下： (1) 中央协调器周期性的给所有 source 注入 stage barrier（黑色实线）。当 source 接收到 barrier 时，会为当前的状态生成一个快照，然后将 barrier 广播到它的所有输出中（图（a））。 (2) 当一个非 source 任务接收到其中一个输入的 barrier 时，它会阻塞该输入，直到它接收到来自所有输入的 barrier（第9行 图2（b））。 (3) 当收到来自所有输入的 barrier 时，该任务会生成当前状态的一个快照并将其 barrier 广播到其输出（第12-13行 图2（c））。 (4) 然后，该任务解除输入通道的阻塞来继续后续的计算（第15行，图2（d））。完整的全局快照 G * =（T *，E *） 仅包含所有算子状态 T *，其中 E * = 0。 伪代码如下： 如前所述，快照算法应该保证最终性和可行性。最终性由通道和非循环执行图属性保证。channels 的可靠性确保只要任务存活，发送的每个 barrier 最终都会被接收的。此外，由于始终存在来自 source 的一条路径，因此 DAG 拓扑中的每个任务最终都将从其所有 input channels 接收 barrier 并生成快照。 对于可行性，足以证明在全局快照中的算子状态仅反映了直到最后阶段处理的记录的历史过程。这可以通过 channels 的 FIFO 顺序特性以及 barrier 全部接收之前阻塞 input channels 保证 stage 的 post-shot 记录不会在快照生成之前处理。 3.3 循环数据流的ABS在存在有向循环的执行图中的情况下，上面的 ABS 算法不会终止而会导致死锁，因为一个循环中的任务将无限期地等待接收来自其所有输入的 barrier。此外，在循环中传输的记录不会包含在快照中，因此违反了可行性。因此，为了可行性需要在快照中包含在循环中生成的所有记录，并在恢复时将这些记录重新传输。我们处理循环图的方法继承了基本算法，而不会像上面算法中看到的那样引起任何额外的 channels 阻塞。首先，我们通过静态分析来识别执行图中循环中的 back-edge L。根据控制流图理论，有向图中的 back-edge 是指在深度优先搜索中已经访问过的顶点的边。执行图 G（T，E \ L） 是一个包含拓扑中所有任务的 DAG。从该 DAG 的角度来看，该算法与以前一样运行，但是，我们需要在快照期间对下游 back-edge 接收的记录进行备份。barrier 将循环中的所有记录都推送到下游日志中，以便将它们包含在一致的快照中。 4. 故障恢复在这提供关于故障恢复操作的简要说明。有几种故障恢复方案可用于一致性快照。最简单的是，整个执行图可以从上一个全局快照重新启动，每个任务 t ，如下所示： 从持久性存储中检索与快照 St 相关联的状态并将其设置为其初始状态 恢复备份的日志以及处理所包含的记录 从其 input channels 开始摄取记录 仅通过重新调度上游任务依赖（其包含到失败任务的 output channels）以及它们各自直到 source 的上游任务，重新恢复调度部分图也是可能的。下图显示了一个恢复示例。为了提供 exactly-once 的语义，应该在所有下游节点中忽略重复记录以避免重新计算。为了达到这个目的，我们用来自source 的序列号标记记录，因此，每个下游节点都可以丢弃序号小于他们已经处理的记录。 5. 实现我们向 Apache Flink 提供了 ABS 算法的实现，以便为流式运行提供 exactly-once 处理语义。在我们当前的实现中，阻塞通道将所有传入的记录存储在磁盘上，而不是将它们保存在内存中以增加可扩展性。虽然此技术可确保鲁棒性，但会增加 ABS 算法的运行时影响。 为了区分算子状态和数据，我们引入了一个显式的 OperatorState 接口，该接口包含更新状态以及对状态进行检查点的方法。我们为 Apache Flink 支持的有状态运行时算子（例如基于偏移量的源或聚合）提供了 OperatorState 实现。 6. 评估评估的目标是将 ABS 的运行时间开销与 Naiad 中采用的全局同步快照算法进行比较，并测试算法在大数量节点上的可伸缩性。用于评估的执行拓扑结构（如下图）由6个不同的算子组成，其并行度等于集群节点的个数，转换为 6 * 集群大小 个任务顶点。执行包含3个完整的网络 shuffle，以突显 ABS 中通道阻塞的可能影响。source 产生总共10亿条记录，这些记录在 source 实例中均匀分布。拓扑中算子的状态是按键的聚合以及 source 的偏移量。在 Amazon EC2 集群上使用多达40个 m3.medium 实例在运行实验。 我们测量了在不同快照间隔下 ABS 和同步快照两种快照方案运行的运行时间开销。我们实现了在 Apache Flink Naiad 上使用的同步快照算法，以便在相同终端上执行进行比较。该实验在10节点集群上运行。为了评估我们算法的可伸缩性，我们处理固定数量的输入记录（10亿），同时将我们拓扑的并行度从5个增加到40个节点。 在下图中，我们描述了两种算法对基线的运行时影响（无容错）。当快照时间间隔较小时，同步快照的性能影响尤其明显。这是由于系统花费更多时间来获取全局快照而不是处理数据。ABS 对运行时的影响要低得多，因为它可以持续运行而不会阻碍整体执行，同时保持相当稳定的吞吐率。当快照时间间隔变大时，同步算法的影响逐渐变小。 在下图中，我们使用3秒快照间隔的 ABS 拓扑与基准（无容错）进行比较可扩展性。很明显，基准作业和 ABS 都实现了线性可扩展性。 7. 总结我们的目的是解决在分布式数据流系统上执行定期全局快照的问题。我们引入了 ABS，这是一种新的快照技术，可实现良好的吞吐量。ABS 是第一种考虑非循环执行拓扑的最小可能状态的算法。此外，我们通过仅存储需要在恢复时重新处理的记录来扩展 ABS 以在循环执行图上使用。我们在 Apache Flink 上实现了 ABS，并对比同步快照算法评估了我们算法的性能。在早期阶段，ABS 显示出良好的结果，对整体执行吞吐量影响较小并具有线性可扩展性。 原文: http://pdfs.semanticscholar.org/541e/cf8c5b9db97eb5fcd1ffdf863d948b8954cc.pdf]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 HDFS Connector]]></title>
    <url>%2F2018%2F03%2F02%2Fflink-stream-hdfs-connector%2F</url>
    <content type="text"><![CDATA[此连接器提供一个 Sink，将分区文件写入 Hadoop FileSystem 支持的任何文件系统。要使用此连接器，添加以下依赖项：&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-filesystem_2.10&lt;/artifactId&gt; &lt;version&gt;1.4-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 备注 streaming 连接器目前还不是二进制发布包的一部分，请参阅此处来了解有关如何将程序与Libraries打包以进行集群执行的信息。 文件分桶的Sink(Bucketing File Sink)分桶(Bucketing)行为以及写入数据操作都可以配置，我们稍后会讲到。下面展示了如何通过默认配置创建分桶的Sink，输出到按时间切分的滚动文件中： Java版本:DataStream&lt;String&gt; input = ...;input.addSink(new BucketingSink&lt;String&gt;("/base/path")); Scala版本:val input: DataStream[String] = ...input.addSink(new BucketingSink[String]("/base/path")) 这里唯一必需的参数是这些分桶文件存储的基本路径 /base/path。可以通过指定自定义 bucketer，writer 和 batch 大小来进一步配置 sink。 默认情况下，分桶 sink 根据元素到达时当前系统时间来进行切分，并使用 yyyy-MM-dd--HH 时间格式来命名这些分桶。这个时间格式传递给当前的系统时间的 SimpleDateFormat 来命名桶的路径。每当遇到一个新的时间就会创建一个新的桶。例如，如果你有一个包含分钟的最细粒度时间格式，那么你将会每分钟获得一个新桶。每个桶本身就是一个包含 part 文件的目录：Sink的每个并行实例都将创建自己的 part 文件，当 part 文件变得太大时，会紧挨着其他文件创建一个新的 part 文件。当一个桶在最近没有被写入数据时被视为非活跃的。当桶变得不活跃时，打开的 part 文件将被刷新(flush)并关闭。默认情况下，sink 每分钟都会检查非活跃的桶，并关闭一分钟内没有写入数据的桶。可以在 BucketingSink上 使用 setInactiveBucketCheckInterval() 和 setInactiveBucketThreshold() 配置这些行为。 你还可以使用 BucketingSink上 的 setBucketer() 指定自定义 bucketer。如果需要，bucketer 可以使用元素或元组的属性来确定 bucket目录。 默认的 writer 是StringWriter。对传入的元素调用 toString()，并将它们写入 part 文件，用换行符分隔。要在 BucketingSink 上指定一个自定义的 writer，使用 setWriter() 方法即可。如果要写入 Hadoop SequenceFiles 文件中，可以使用提供的 SequenceFileWriter，并且可以配置使用压缩格式。 最后一个配置选项是 batch 大小。这指定何时关闭 part 文件，并开启一个新文件。(默认part文件大小为384MB)。 Java版本:DataStream&lt;Tuple2&lt;IntWritable,Text&gt;&gt; input = ...;BucketingSink&lt;String&gt; sink = new BucketingSink&lt;String&gt;("/base/path");sink.setBucketer(new DateTimeBucketer&lt;String&gt;("yyyy-MM-dd--HHmm"));sink.setWriter(new SequenceFileWriter&lt;IntWritable, Text&gt;());sink.setBatchSize(1024 * 1024 * 400); // this is 400 MB,input.addSink(sink); Scala版本:val input: DataStream[Tuple2[IntWritable, Text]] = ...val sink = new BucketingSink[String]("/base/path")sink.setBucketer(new DateTimeBucketer[String]("yyyy-MM-dd--HHmm"))sink.setWriter(new SequenceFileWriter[IntWritable, Text]())sink.setBatchSize(1024 * 1024 * 400) // this is 400 MB,input.addSink(sink) 上面例子将创建一个sink，写入遵循下面格式的分桶文件中：/base/path/&#123;date-time&#125;/part-&#123;parallel-task&#125;-&#123;count&#125; 其中 date-time 是从日期/时间格式获得的字符串， parallel-task 是并行 sink 实例的索引，count 是由于 batch大小而创建的part文件的运行编号。 备注: Sink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/filesystem_sink.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 窗口触发器与Evictors]]></title>
    <url>%2F2018%2F03%2F01%2Fflink-stream-windows-trigger-and-evictor%2F</url>
    <content type="text"><![CDATA[1. 窗口触发器触发器(Trigger)决定了窗口(请参阅窗口概述)博文)什么时候准备好被窗口函数处理。每个窗口分配器都带有一个默认的 Trigger。如果默认触发器不能满足你的要求，可以使用 trigger(...) 指定自定义的触发器。 触发器接口有五个方法来对不同的事件做出响应：public abstract TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx) throws Exception;public abstract TriggerResult onProcessingTime(long time, W window, TriggerContext ctx) throws Exception;public abstract TriggerResult onEventTime(long time, W window, TriggerContext ctx) throws Exception;public void onMerge(W window, OnMergeContext ctx) throws Exception &#123; throw new UnsupportedOperationException("This trigger does not support merging.");&#125;public abstract void clear(W window, TriggerContext ctx) throws Exception; onElement() 方法，当每个元素被添加窗口时调用。 onEventTime() 方法，当注册的事件时间计时器被触发时调用。 onProcessingTime() 方法，当注册的处理时间计时器被触发时调用。 onMerge() 方法，与状态 触发器相关，并且在相应的窗口合并时合并两个触发器的状态。例如，使用会话窗口时。 clear() 方法，在删除相应窗口时执行所需的任何操作。 以上方法有两件事要注意: (1) 前三个函数决定了如何通过返回一个 TriggerResult 来对其调用事件采取什么操作。TriggerResult可以是以下之一： CONTINUE 什么都不做 FIRE_AND_PURGE 触发计算，然后清除窗口中的元素 FIRE 触发计算 PURGE 清除窗口中的元素 (2) 上面任何方法都可以用于注册处理时间计时器或事件时间计时器以供将来的操作使用。 1.1 触发与清除一旦触发器确定窗口准备好可以处理数据，就将触发，即，它返回 FIRE 或 FIRE_AND_PURGE。这是窗口算子发出当前窗口结果的信号。给定一个带有 ProcessWindowFunction 的窗口，所有的元素都被传递给 ProcessWindowFunction (可能在将所有元素传递给 evictor 之后)。带有 ReduceFunction， AggregateFunction 或者 FoldFunction 的窗口只是简单地发出他们急切希望得到的聚合结果。 触发器触发时，可以是 FIRE 或 FIRE_AND_PURGE 。FIRE 保留窗口中的内容，FIRE_AND_PURGE 会删除窗口中的内容。默认情况下，内置的触发器只返回 FIRE，不会清除窗口状态。 备注清除只是简单地删除窗口的内容，并保留窗口的元数据信息以及完整的触发状态。 1.2 窗口分配器的默认触发器窗口分配器的默认触发器适用于许多情况。例如，所有的事件时间窗口分配器都有一个 EventTimeTrigger 作为默认触发器。一旦 watermark 到达窗口末尾，这个触发器就会被触发。 备注全局窗口(GlobalWindow)的默认触发器是永不会被触发的 NeverTrigger。因此，在使用全局窗口时，必须自定义一个触发器。 备注通过使用 trigger() 方法指定触发器，将会覆盖窗口分配器的默认触发器。例如，如果你为 TumblingEventTimeWindows 指定 CountTrigger，那么不会再根据时间进度触发窗口，而只能通过计数。目前为止，如果你希望基于时间以及计数进行触发，则必须编写自己的自定义触发器。 1.3 内置触发器和自定义触发器Flink带有一些内置触发器: EventTimeTrigger 根据 watermarks 度量的事件时间进度进行触发。 ProcessingTimeTrigger 基于处理时间触发。 CountTrigger 一旦窗口中的元素数量超过给定限制就会触发。 PurgingTrigger 将其作为另一个触发器的参数，并将其转换为带有清除功能(transforms it into a purging one)。 如果需要实现一个自定义的触发器，你应该看看Trigger抽象类。请注意，API仍在发展中，在Flink未来版本中可能会发生改变。 2. 窗口驱逐器Flink 的窗口模型允许在窗口分配器和触发器之外指定一个可选的驱逐器(Evictor)。可以使用 evictor(...) 方法来完成。驱逐器能够在触发器触发之后，以及在应用窗口函数之前或之后从窗口中移除元素。为此，Evictor接口有两种方法： /** * Optionally evicts elements. Called before windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The &#123;@link Window&#125; * @param evictorContext The context for the Evictor */void evictBefore(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext);/** * Optionally evicts elements. Called after windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The &#123;@link Window&#125; * @param evictorContext The context for the Evictor */ void evictAfter(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, int size, W window, EvictorContext evictorContext); evictBefore()包含驱逐逻辑，在窗口函数之前使用。而evictAfter()在窗口函数之后使用。在使用窗口函数之前被逐出的元素将不被处理。 Flink带有三个内置的驱逐器: CountEvictor：保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。 DeltaEvictor：使用 DeltaFunction和 一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。 TimeEvictor：以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。 备注默认情况下，所有内置的驱逐器在窗口函数之前使用 备注指定驱逐器可以避免预聚合(pre-aggregation)，因为窗口内所有元素必须在应用计算之前传递给驱逐器。 备注Flink不保证窗口内元素的顺序。这意味着虽然驱逐者可以从窗口的开头移除元素，但这些元素不一定是先到的还是后到的。 备注Flink版本:1.4 原文: https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#triggers]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 窗口函数]]></title>
    <url>%2F2018%2F03%2F01%2Fflink-stream-windows-function%2F</url>
    <content type="text"><![CDATA[在定义窗口分配器之后，我们需要在每个窗口上指定我们要执行的计算。这是窗口函数的责任，一旦系统确定窗口准备好处理数据，窗口函数就处理每个窗口中的元素。 窗口函数可以是 ReduceFunction， AggregateFunction, FoldFunction 或 ProcessWindowFunction。前两个函数执行效率更高，因为 Flink 可以在每个窗口中元素到达时增量地聚合。ProcessWindowFunction 将获得一个窗口内所有元素的迭代器以及元素所在窗口的附加元信息。 使用 ProcessWindowFunction 的窗口转换操作不能像其他那样有效率，是因为 Flink 在调用该函数之前必须在内部缓存窗口中的所有元素。这可以通过将 ProcessWindowFunction 与 ReduceFunction， AggregateFunction 或 FoldFunction 组合使用来获得窗口元素的增量聚合以及WindowFunction接收的附加窗口元数据。 1. ReduceFunctionReduceFunction 指定如何组合输入数据的两个元素以产生相同类型的输出元素。Flink 使用 ReduceFunction 增量聚合窗口的元素。 ReduceFunction可以如下定义和使用: Java版本:DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123; public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2) &#123; return new Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1); &#125;&#125;); Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .reduce &#123; (v1, v2) =&gt; (v1._1, v1._2 + v2._2) &#125; 上述示例获得窗口中的所有元素元组的第二个字段之和。 2. FoldFunctionFoldFunction 指定窗口的输入元素如何与输出类型的元素合并。FoldFunction 会被每一个加入到窗口中的元素和当前的输出值增量地调用，第一个元素与一个预定义的输出类型的初始值合并。 FoldFunction 可以如下定义和使用: Java版本:DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .fold("", new FoldFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;&gt; &#123; public String fold(String acc, Tuple2&lt;String, Long&gt; value) &#123; return acc + value.f1; &#125;&#125;); Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .fold("") &#123; (acc, v) =&gt; acc + v._2 &#125; 上述示例将所有输入元素的Long值追加到初始化为空的字符串中。 备注 fold()不能应用于会话窗口或者其他可合并的窗口中。 3. AggregateFunctionAggregateFunction 是 ReduceFunction 的通用版本，具有三种类型：输入类型（IN），累加器类型（ACC）和输出类型（OUT）。输入类型是输入流中元素的类型，AggregateFunction 有一个用于将一个输入元素添加到累加器的方法。该接口还具有创建初始累加器的方法，用于将两个累加器合并到一个累加器中，并从累加器中提取输出（类型为OUT）。我们将在下面的例子中看到它是如何工作的。 与 ReduceFunction 相同，Flink 将在窗口到达时递增地聚合窗口的输入元素。 Java版本:/** * The accumulator is used to keep a running sum and a count. The &#123;@code getResult&#125; method * computes the average. */private static class AverageAggregate implements AggregateFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123; @Override public Tuple2&lt;Long, Long&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0L, 0L); &#125; @Override public Tuple2&lt;Long, Long&gt; add(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123; return new Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); &#125; @Override public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123; return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125;&#125;DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(new AverageAggregate()); Scala版本:/** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] &#123; override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2)&#125;val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .aggregate(new AverageAggregate) 上面的例子计算窗口中元素的第二个字段的平均值。 4. ProcessWindowFunctionProcessWindowFunction 获得一个窗口内所有元素的 Iterable，以及一个可以访问时间和状态信息的 Context 对象，这使得它可以提供比其他窗口函数更大的灵活性。这是以牺牲性能和资源消耗为代价的，因为元素不能增量地聚合，而是需要在内部进行缓冲，直到窗口被认为准备好进行处理为止。 ProcessWindowFunction 的结构如下所示： Java版本:public abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; implements Function &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception; /** * The context holding window metadata. */ public abstract class Context implements java.io.Serializable &#123; /** * Returns the window that is being evaluated. */ public abstract W window(); /** Returns the current processing time. */ public abstract long currentProcessingTime(); /** Returns the current event-time watermark. */ public abstract long currentWatermark(); /** * State accessor for per-key and per-window state. * * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt;If you use per-window state you have to ensure that you clean it up * by implementing &#123;@link ProcessWindowFunction#clear(Context)&#125;. */ public abstract KeyedStateStore windowState(); /** * State accessor for per-key global state. */ public abstract KeyedStateStore globalState(); &#125;&#125; Scala版本:abstract class ProcessWindowFunction[IN, OUT, KEY, W &lt;: Window] extends Function &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** * The context holding window metadata */ abstract class Context &#123; /** * Returns the window that is being evaluated. */ def window: W /** * Returns the current processing time. */ def currentProcessingTime: Long /** * Returns the current event-time watermark. */ def currentWatermark: Long /** * State accessor for per-key and per-window state. */ def windowState: KeyedStateStore /** * State accessor for per-key global state. */ def globalState: KeyedStateStore &#125;&#125; 关键参数是通过 KeySelector 提取为 keyBy（） 调用指定的键。在元组索引键或字符串字段引用的情况下，此键类型始终为元组，并且必须手动将其转换为正确大小的元组以提取关键字段。一个 ProcessWindowFunction 可以像这样定义和使用： Java版本:DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .process(new MyProcessWindowFunction());/* ... */public class MyProcessWindowFunction implements ProcessWindowFunction&lt;Tuple&lt;String, Long&gt;, String, String, TimeWindow&gt; &#123; void process(String key, Context context, Iterable&lt;Tuple&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) &#123; long count = 0; for (Tuple&lt;String, Long&gt; in: input) &#123; count++; &#125; out.collect("Window: " + context.window() + "count: " + count); &#125;&#125; Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .process(new MyProcessWindowFunction())/* ... */class MyProcessWindowFunction extends ProcessWindowFunction[(String, Long), String, String, TimeWindow] &#123; def apply(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]): () = &#123; var count = 0L for (in &lt;- input) &#123; count = count + 1 &#125; out.collect(s"Window $&#123;context.window&#125; count: $count") &#125;&#125; 上述示例显示了一个 ProcessWindowFunction，用于统计窗口中的元素。另外，窗口函数将有关窗口的信息添加到输出中。 备注 使用ProcessWindowFunction进行简单聚合（如count）的效率非常低。 下一节将展示ReduceFunction或AggregateFunction如何与ProcessWindowFunction组合以获得增量聚合以及ProcessWindowFunction的附加信息。 5. 使用增量聚合的ProcessWindowFunctionProcessWindowFunction 可以与 ReduceFunction ， AggregateFunction 或 FoldFunction 组合使用，以便在元素到达窗口时增量聚合元素。当窗口关闭时，ProcessWindowFunction提供聚合结果。这允许增量计算窗口，同时也可以访问 ProcessWindowFunction 额外的窗口元信息。 备注 你也可以使用传统WindowFunction而不是ProcessWindowFunction进行增量窗口聚合。 5.1 使用ReduceFunction的增量窗口聚合以下示例展现了如何将增量式 ReduceFunction 与 ProcessWindowFunction 结合以返回窗口中的最小事件以及窗口的开始时间。 Java版本:DataStream&lt;SensorReading&gt; input = ...;input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .reduce(new MyReduceFunction(), new MyWindowFunction());// Function definitionsprivate static class MyReduceFunction implements ReduceFunction&lt;SensorReading&gt; &#123; public SensorReading reduce(SensorReading r1, SensorReading r2) &#123; return r1.value() &gt; r2.value() ? r2 : r1; &#125;&#125;private static class MyWindowFunction implements WindowFunction&lt;SensorReading, Tuple2&lt;Long, SensorReading&gt;, String, TimeWindow&gt; &#123; public void apply(String key, TimeWindow window, Iterable&lt;SensorReading&gt; minReadings, Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out) &#123; SensorReading min = minReadings.iterator().next(); out.collect(new Tuple2&lt;Long, SensorReading&gt;(window.getStart(), min)); &#125;&#125; Scala版本:val input: DataStream[SensorReading] = ...input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .reduce( (r1: SensorReading, r2: SensorReading) =&gt; &#123; if (r1.value &gt; r2.value) r2 else r1 &#125;, ( key: String, window: TimeWindow, minReadings: Iterable[SensorReading], out: Collector[(Long, SensorReading)] ) =&gt; &#123; val min = minReadings.iterator.next() out.collect((window.getStart, min)) &#125; ) 5.2 使用AggregateFunction的增量窗口聚合以下示例显示了如何将增量式 AggregateFunction 与 ProcessWindowFunction 结合来计算平均值，并将键与平均值一起输出。 Java版本:DataStream&lt;Tuple2&lt;String, Long&gt; input = ...;input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction());// Function definitions/** * The accumulator is used to keep a running sum and a count. The &#123;@code getResult&#125; method * computes the average. */private static class AverageAggregate implements AggregateFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123; @Override public Tuple2&lt;Long, Long&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0L, 0L); &#125; @Override public Tuple2&lt;Long, Long&gt; add(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123; return new Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); &#125; @Override public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123; return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125;&#125;private static class MyProcessWindowFunction implements ProcessWindowFunction&lt;Double, Tuple2&lt;String, Double&gt;, String, TimeWindow&gt; &#123; public void apply(String key, Context context, Iterable&lt;Double&gt; averages, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out) &#123; Double average = averages.iterator().next(); out.collect(new Tuple2&lt;&gt;(key, average)); &#125;&#125; Scala版本:val input: DataStream[(String, Long)] = ...input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction())// Function definitions/** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] &#123; override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2)&#125;class MyProcessWindowFunction extends ProcessWindowFunction[Double, (String, Double), String, TimeWindow] &#123; def apply(key: String, context: Context, averages: Iterable[Double], out: Collector[(String, Double]): () = &#123; var count = 0L for (in &lt;- input) &#123; count = count + 1 &#125; val average = averages.iterator.next() out.collect((key, average)) &#125;&#125; 5.3 使用FoldFunction的增量窗口聚合以下示例展现了增量式 FoldFunction 如何与 WindowFunction 结合以提取窗口中的事件数，并返回窗口的键和结束时间。 Java版本:DataStream&lt;SensorReading&gt; input = ...;input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .fold(new Tuple3&lt;String, Long, Integer&gt;("",0L, 0), new MyFoldFunction(), new MyWindowFunction())// Function definitionsprivate static class MyFoldFunction implements FoldFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt; &gt; &#123; public Tuple3&lt;String, Long, Integer&gt; fold(Tuple3&lt;String, Long, Integer&gt; acc, SensorReading s) &#123; Integer cur = acc.getField(2); acc.setField(2, cur + 1); return acc; &#125;&#125;private static class MyWindowFunction implements WindowFunction&lt;Tuple3&lt;String, Long, Integer&gt;, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt; &#123; public void apply(String key, TimeWindow window, Iterable&lt;Tuple3&lt;String, Long, Integer&gt;&gt; counts, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) &#123; Integer count = counts.iterator().next().getField(2); out.collect(new Tuple3&lt;String, Long, Integer&gt;(key, window.getEnd(),count)); &#125;&#125; Scala版本:val input: DataStream[SensorReading] = ...input .keyBy(&lt;key selector&gt;) .timeWindow(&lt;window assigner&gt;) .fold ( ("", 0L, 0), (acc: (String, Long, Int), r: SensorReading) =&gt; &#123; ("", 0L, acc._3 + 1) &#125;, ( key: String, window: TimeWindow, counts: Iterable[(String, Long, Int)], out: Collector[(String, Long, Int)] ) =&gt; &#123; val count = counts.iterator.next() out.collect((key, window.getEnd, count._3)) &#125; ) 备注 Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#window-functions]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 用于外部数据访问的异步IO]]></title>
    <url>%2F2018%2F02%2F28%2Fflink-stream-asynchronous-io-for-external-data-access%2F</url>
    <content type="text"><![CDATA[1. 异步IO操作的必要性当与外部系统交互时（例如，使用存储在数据库中数据丰富流事件），需要注意与外部系统的通信延迟并不决定流应用程序的整体工作。访问外部数据库中的数据（例如在 MapFunction 中）通常意味着同步交互：将请求发送到数据库，MapFunction 会等待直到收到响应。在许多情况下，这个等待时间占了该函数绝大部分时间。 与外部数据库进行异步交互意味着一个并行函数实例可以并发地处理多个请求和并发地接收多个响应。那样的话，可以通过发送其他请求和接收响应来重叠等待时间。至少，等待时间可以被多个请求平摊，这在很多情况下会导致更高的流吞吐量。 通过扩展 MapFunction 到一个很高的并发度来提高吞吐量在一定程度上是可行的，但是常常会导致很高的资源成本：有更多的并行 MapFunction 实例意味着更多的任务、线程、Flink内部网络连接、与数据库之间的网络连接、缓存以及通常的内部开销。 2. 前提条件如上面的部分所述，实现数据库（或key/value存储系统）适当的异步I/O访问需要该数据库的客户端支持异步请求。许多流行的数据库提供这样的客户端。在没有这样的客户端的情况下，可以尝试创建多个客户端并使用线程池处理同步调用，从而将同步客户端转换为有限的并发客户端。但是，这种方法通常比适当的异步客户端效率低。 3. Async I/O APIFlink 的异步 I/O API允许用户在数据流中使用异步请求客户端。API处理与数据流的集成，以及处理顺序，事件时间，容错等。 假设有一个用于目标数据库的异步客户端，要实现一个通过异步I/O来操作数据库还需要三个步骤： 实现调度请求的 AsyncFunction 获取操作结果并把它传递给 ResultFuture 的 callBack 将异步 I/O 操作作为转换操作应用于 DataStream 以下代码示例说明了基本模式： Java版本:// This example implements the asynchronous request and callback with Futures that have the// interface of Java 8's futures (which is the same one followed by Flink's Future)/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */class AsyncDatabaseRequest extends RichAsyncFunction&lt;String, Tuple2&lt;String, String&gt;&gt; &#123; /** The database specific client that can issue concurrent requests with callbacks */ private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception &#123; client = new DatabaseClient(host, post, credentials); &#125; @Override public void close() throws Exception &#123; client.close(); &#125; @Override public void asyncInvoke(final String str, final ResultFuture&lt;Tuple2&lt;String, String&gt;&gt; resultFuture) throws Exception &#123; // 发出异步请求，返回结果的 Future Future&lt;String&gt; resultFuture = client.query(str); // 一旦客户端的请求完成，执行回调函数 // 回调函数只是将结果转发给 resultFuture resultFuture.thenAccept( (String result) -&gt; &#123; resultFuture.complete(Collections.singleton(new Tuple2&lt;&gt;(str, result))); &#125;); &#125;&#125;// create the original streamDataStream&lt;String&gt; stream = ...;// apply the async I/O transformationDataStream&lt;Tuple2&lt;String, String&gt;&gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100); Scala版本:/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */class AsyncDatabaseRequest extends AsyncFunction[String, (String, String)] &#123; /** The database specific client that can issue concurrent requests with callbacks */ lazy val client: DatabaseClient = new DatabaseClient(host, post, credentials) /** The context used for the future callbacks */ implicit lazy val executor: ExecutionContext = ExecutionContext.fromExecutor(Executors.directExecutor()) override def asyncInvoke(str: String, resultFuture: ResultFuture[(String, String)]): Unit = &#123; // issue the asynchronous request, receive a future for the result val resultFuture: Future[String] = client.query(str) // set the callback to be executed once the request by the client is complete // the callback simply forwards the result to the result future resultFuture.onSuccess &#123; case result: String =&gt; resultFuture.complete(Iterable((str, result))) &#125; &#125;&#125;// create the original streamval stream: DataStream[String] = ...// apply the async I/O transformationval resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100) 重要提示ResultFuture是在第一次调用 ResultFuture.complete 时已经完成。所有后续的 complete 调用都将被忽略。 以下两个参数控制异步操作： 超时：超时定义了异步请求在被认为失败之前可能需要多长时间。该参数防止死亡/失败请求。 容量：该参数定义可以同时进行多少个异步请求。尽管异步I/O方法通常会有更好的吞吐量，但是算子仍然可能是流应用程序中的瓶颈。限制并发请求的数量可以确保算子不会积压不断增长的未处理请求，但一旦容量耗尽，它将触发背压。 4. 结果顺序由 AsyncFunction 发出的并发请求经常是以无序的形式完成，取决于哪个请求先完成。为了控制结果记录发出的顺序，Flink 提供了两种模式： Unordered：异步请求结束后立即输出结果记录。在经过异步I/O算子之后，流中记录的顺序与之前会不一样。当使用处理时间作为基本时间特性时，该模式具有最低延迟和最低开销的特性。在这种模式下使用 AsyncDataStream.unorderedWait（...） 函数。 Ordered：在这种情况下，保留流的顺序。结果记录输出的顺利与异步请求触发的顺序(算子输入记录的顺序)一致。为此，算子必须缓冲结果记录，直到其前面所有的记录输出（或超时）为止。这通常会导致在检查点中出现一定量的额外延迟和一些开销，因为与 Unordered 模式相比，结果的记录在检查点状态中保持较长的一段时间。在这种模式下使用 AsyncDataStream.orderedWait（...） 函数。 5. 事件时间当流式应用程序使用事件时间时，异步 I/O 算子能正确处理 watermarks。这意味着对于两个顺序模式具体如下： Unordered： watermarks 不会超过记录，反之亦然，这意味着 watermarks 建立起顺序边界。记录只在 watermarks 之间无序排列。只有在发布 watermarks 后才会发出某个 watermarks 后发生的记录。反过来，只有在发布 watermarks 前的所有输入结果记录之后才会发送 watermarks。这意味着，在有 watermarks 的情况下，Unordered 模式与 Ordered 模式一样，都引入了延迟和开销。该开销取决于 watermarks 发送频率。 Ordered：保存记录的 watermarks 顺序，就像保存记录之间的顺序一样。与处理时间相比，开销没有显着变化。 请记住，提取时间是事件时间的特例，自动生成的 watermarks 基于数据源的处理时间。 6. 容错保证异步 I/O 算子提供 exactly-once 语义容错保证。它将检查点中正在进行的异步请求记录存储起来，并在从故障中恢复时恢复/重新触发请求。 原文： https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/asyncio.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 2.3.0 重要特性介绍]]></title>
    <url>%2F2018%2F02%2F28%2Fintroducing-apache-spark-2-3%2F</url>
    <content type="text"><![CDATA[为了继续实现 Spark 更快，更轻松，更智能的目标，Spark 2.3 在许多模块都做了重要的更新，比如 Structured Streaming 引入了低延迟的持续处理；支持 stream-to-stream joins；通过改善 pandas UDFs 的性能来提升 PySpark；支持第四种调度引擎 Kubernetes clusters（其他三种分别是自带的独立模式Standalone，YARN、Mesos）。除了这些比较具有里程碑的重要功能外，Spark 2.3 还有以下几个重要的更新： 引入 DataSource v2 APIs [SPARK-15689, SPARK-20928] 矢量化的 ORC reader [SPARK-16060] Spark History Server v2 with K-V store [SPARK-18085] 基于 Structured Streaming 的机器学习管道API模型 [SPARK-13030, SPARK-22346, SPARK-23037] MLlib 增强 [SPARK-21866, SPARK-3181, SPARK-21087, SPARK-20199] Spark SQL 增强 [SPARK-21485, SPARK-21975, SPARK-20331, SPARK-22510, SPARK-20236] 这篇文章将简单地介绍上面一些高级功能和改进，更多的特性请参见 Spark 2.3 release notes。 1. 毫秒延迟的持续流处理出于某些原因的考虑，Spark 2.0 引入的 Structured Streaming 将微批次处理从高级 API 中解耦出去。首先，它简化了 API 的使用，API 不再负责进行微批次处理。其次，开发者可以将流看成是一个没有边界的表，并基于这些 表 运行查询。 不过，为了给开发者提供更多的流式处理体验，Spark 2.3 引入了毫秒级延迟的持续流式处理模式。 从内部来看，Structured Streaming 引擎基于微批次增量执行查询，时间间隔视具体情况而定，不过这样的延迟对于真实世界的流式应用来说都是可接受的。 在持续模式下，流处理器持续不断地从数据源拉取和处理数据，而不是每隔一段时间读取一个批次的数据，这样就可以及时地处理刚到达的数据。如下图所示，延迟被降低到毫秒级别，完全满足了低延迟的要求。 持续模式目前支持的 Dataset 操作包括 Projection、Selection 以及除 current_timestamp()、current_date()、聚合函数之外的 SQL 操作。它还支持将 Kafka 作为数据源和数据池（Sink），也支持将控制台和内存作为数据池。 开发者可以根据实际的延迟需求来选择使用持续模式还是微批次模式，总之，Structured Streaming 为开发者提供了容错和可靠性方面的保证。 简单地说，Spark 2.3 的持续模式所能做到的是： 端到端的毫秒级延迟 至少一次处理保证 支持 Dataset 的映射操作 2. 流到流的连接Spark 2.0 的 Structured Streaming 已经可以支持 DataFrame/Dataset 的连接操作，但只是流到静态数据集的连接，而 Spark 2.3 带来了期待已久的流到流的连接，支持内连接和外连接，可用在大量的实时场景中。 广告变现是流到流连接的一个典型应用场景。例如，广告 impression 流和用户点击流包含相同的键（如 adld）和相关数据，而你需要基于这些数据进行流式分析，找出哪些用户的点击与 adld 相关。 虽然看起来很简单，但实际上流到流的连接解决了一些技术性难题： 将迟到的数据缓冲起来，直到在另一个流中找到与之匹配的数据。 通过设置水位（Watermark）防止缓冲区过度膨胀。 用户可以在资源消耗和延迟之间作出权衡。 静态连接和流连接之间的 SQL 语法是一致的。 3. Spark 和 KubernetesSpark 和 Kubernetes 这两个开源项目之间的功能组合也在意料之内，用于提供大规模分布式的数据处理和编配。在 Spark 2.3 中，用户可在 Kubernetes 集群上原生地运行 Spark，从而更合理地使用资源，不同的工作负载可共享 Kubernetes 集群。 Spark 可以使用 Kubernetes 的所有管理特性，如资源配额、可插拔的授权和日志。另外，要在已有的 Kubernetes 集群上启动 Spark 工作负载就像创建一个 Docker 镜像那么简单。 4. 用于 PySpark 的 Pandas UDFPandas UDF，也被称为向量化的 UDF，为 PySpark 带来重大的性能提升。Pandas UDF 以 Apache Arrow 为基础，完全使用 Python 开发，可用于定义低开销、高性能的 UDF。 Spark 2.3 提供了两种类型的 Pandas UDF：标量和组合 map。来自 Two Sigma 的 Li Jin 在之前的一篇博客中通过四个例子介绍了如何使用 Pandas UDF。 一些基准测试表明，Pandas UDF 在性能方面比基于行的 UDF 要高出一个数量级。 包括 Li Jin 在内的一些贡献者计划在 Pandas UDF 中引入聚合和窗口功能。 5. MLlib 方面的改进Spark 2.3 带来了很多 MLlib 方面的改进，包括算法、特性、性能、伸缩性和可用性。 首先，可通过 Structured Streaming 作业将 MLlib 的模型和管道部署到生产环境，不过一些已有的管道可能需要作出修改。 其次，为了满足深度学习图像分析方面的需求，Spark 2.3 引入了 ImageSchema，将图像表示成 Spark DataFrame，还提供工具用于加载常用的图像格式。 最后，Spark 2.3 带来了改进过的 Python API，用于开发自定义算法，包括 UnaryTransformer 以及用于保存和加载算法的自动化工具。 译文：https://mp.weixin.qq.com/s/SJ2P4oJtvsMgzCSH3DH4vA 原文：https://databricks.com/blog/2018/02/28/introducing-apache-spark-2-3.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 窗口概述]]></title>
    <url>%2F2018%2F02%2F28%2Fflink-stream-windows-overall%2F</url>
    <content type="text"><![CDATA[Windows(窗口)是处理无限数据流的核心。窗口将流分解成有限大小的”桶”，在上面我们可以进行计算。本文将重点介绍 Flink 中的窗口，以及常见的窗口类型。 一个窗口化的 Flink 程序一般结构如下。第一个片段指的是指定 key 的数据流（keyed streams），而第二个未指定key的数据流。可以看出，唯一的区别是指定 key 的数据流调用了 keyBy() 以及 window() 方法变为未指定 key 数据流下的 windowAll() 方法。 Keyed Windows：stream .keyBy(...) &lt;- keyed versus non-keyed windows .window(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness()] &lt;- optional, else zero .reduce/fold/apply() &lt;- required: &quot;function&quot; Non-Keyed Windows：stream .windowAll(...) &lt;- required: &quot;assigner&quot; [.trigger(...)] &lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness()] &lt;- optional, else zero .reduce/fold/apply() &lt;- required: &quot;function&quot; 在上面，方括号 [...] 中的命令是可选的。这表明 Flink 允许你可以以多种不同的方式自定义你的窗口逻辑，以便更好的满足你的需求。 1. 窗口生命周期简而言之，一旦属于这个窗口的第一个元素到达，就会创建该窗口，当时间(事件时间或处理时间)到达其结束时间戳加上用户指定的可允许延迟的时间戳，窗口将被完全删除。Flink 保证仅对基于时间的窗口进行删除，并不适用于其他类型的窗口，例如，全局窗口（具体请参阅下面的窗口分配器）。举个例子，使用基于事件时间的窗口策略，每隔5分钟创建一个不重叠的窗口，并且允许可以有1分钟的延迟时间。当第一个带有时间戳的元素落入12:00至12:05时间间隔内时，Flink 创建一个新窗口，当时间戳到达 12:06 时，窗口将被删除。 另外，每个窗口都将有一个触发器和一个函数(例如 WindowFunction， ReduceFunction 或 FoldFunction)。这个函数包含应用于窗口内容的计算，而触发器指定了窗口使用该函数的条件。触发策略可能是”当窗口中的元素个数大于4”，或”当 watermark 通过窗口末尾”。触发器还可以决定在创建窗口和删除窗口之间的任何时间内清除窗口内容。在这种情况下，清除仅指窗口中的元素，而不是窗口的元数据。这意味着新数据仍然可以添加到该窗口。 除了上述之外，你还可以指定一个 Evictor，在触发器触发之后以及在应用该函数之前和/或之后从窗口中移除元素。 2. Keyed vs Non-Keyed Windows首先要指定的第一件事就是你的数据流是否应该指定 key。这必须在定义窗口之前完成。使用 keyBy() 可以将无限数据流分解成指定 key 的逻辑上的数据流。如果未调用 keyBy()，则你的数据流未指定 key。 在指定 key 的数据流的情况下，你传入进来的事件的任何属性都可以用作 key(更多细节)。指定 key 的数据流可以允许通过多个任务并行执行窗口计算，因为每个逻辑数据流可以独立于其余的进行处理。指向相同 key 的所有元素将被发送到相同的并行任务上。 在未指定 key 的数据流的情况下，你的原始数据流不会被分割成多个逻辑数据流，并且所有窗口逻辑将由单个任务执行，即以1个的并行度执行。 3. 窗口分配器在确定数据流是否指定 key 之后，下一步就是定义窗口分配器。窗口分配器定义了元素如何分配给窗口。这通过在 window()(指定key数据流)或 windowAll()(未指定key数据流)调用中指定你选择的窗口分配器来完成。 窗口分配器负责将每个传入的元素分配给一个或多个窗口。Flink 内置了一些用于解决常见用例的窗口分配器，有滚动窗口，滑动窗口，会话窗口和全局窗口。你还可以通过继承 WindowAssigner 类实现自定义窗口分配器。所有内置窗口分配器(全局窗口除外)根据时间将元素分配给窗口，可以是处理时间或事件时间。请参阅事件时间，了解处理时间和事件时间之间的差异以及如何生成时间戳和watermarks。 在下文中，我们将展示 Flink 的内置窗口分配器的工作原理以及它们在 DataStream 程序中的使用方式。以下图形可视化每个分配器的运行。紫色圆圈表示数据流中的元素，它们被某些key（在我们这个例子下为用 user1，user2 和 user3）分区。x轴显示时间进度。 3.1 滚动窗口滚动窗口分配器将每个元素分配给固定大小的窗口。滚动窗口大小固定且不重叠。例如，如果指定大小为5分钟的滚动窗口，则将执行当前窗口，并且每五分钟将启动一个新窗口，如下图所示: 以下代码显示如何使用滚动窗口： Java版本:DataStream&lt;T&gt; input = ...;// 基于事件事件的滚动窗口input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// 基于处理时间的滚动窗口input .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// daily tumbling event-time windows offset by -8 hours.input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...// tumbling event-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// tumbling processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// daily tumbling event-time windows offset by -8 hours.input .keyBy(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 可以通过使用 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x) 来指定时间间隔。 如上一个例子中所示，滚动窗口分配器还可以使用一个可选的偏移量参数，可以用来改变窗口的对齐方式。例如，没有偏移量的情况下，按小时滚动窗口与 epoch （指的是一个特定的时间：1970-01-01 00:00:00 UTC）对齐，那么你将获得如1：00：00.000 - 1：59：59.999，2：00：00.000 - 2：59：59.999等窗口。如果你想改变，你可以给一个偏移量。以15分钟的偏移量为例，那么你将获得1：15：00.000 - 2：14：59.999，2：15：00.000 - 3：14：59.999等。偏移量的一个重要用例是将窗口调整为时区而不是UTC-0。例如，在中国，你必须指定 Time.hours(-8) 的偏移量。 3.2 滑动窗口滑动窗口分配器将每个元素分配给固定窗口大小的窗口。类似于滚动窗口分配器，窗口的大小由窗口大小(window size)参数配置。另外一个窗口滑动参数控制滑动窗口的启动频率(译者注：滑动大小)。因此，如果滑动大小小于窗口大小，滑动窗口可以重叠。在这种情况下，元素可以被分配到多个窗口。 例如，你可以使用窗口大小为10分钟的窗口，滑动大小为5分钟。这样，每5分钟会生成一个窗口，包含最后10分钟内到达的事件，如下图所示。 以下代码显示如何使用滑动窗口: Java版本:DataStream&lt;T&gt; input = ...;// sliding event-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// sliding processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// sliding processing-time windows offset by -8 hoursinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...// sliding event-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// sliding processing-time windowsinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// sliding processing-time windows offset by -8 hoursinput .keyBy(&lt;key selector&gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 可以通过使用 Time.milliseconds(x)，Time.seconds(x)，Time.minutes(x) 来指定时间间隔。 如上一个例子所示，滑动窗口分配器也可以使用一个可选的偏移量参数，用来改变窗口的对齐方式。例如，没有偏移量的情况下，按小时滑动窗口大小为30分钟的例子下，你将获得如1：00：00.000 - 1：59：59.999，1：30：00.000 - 2：29：59.999等窗口。如果你想改变，你可以给一个偏移量。以15分钟的偏移量为例，那么你将获得1：15：00.000 - 2：14：59.999，1：45：00.000 - 2：44：59.999等。偏移量的一个重要用例是将窗口调整为时区而不是UTC-0。例如，在中国，你必须指定Time.hours(-8)的偏移量。 3.3 会话窗口会话窗口分配器通过活动会话对元素进行分组。与滚动窗口和滑动窗口相比，会话窗口不会重叠，也没有固定的开始和结束时间。相反，当会话窗口在一段时间内没有接收到元素时会关闭，即当发生不活动的间隙时。会话窗口分配器配置一个会话间隙，定义了所需的不活动时长。当此时间段到期时，当前会话关闭，后续元素被分配到新的会话窗口。 以下代码显示如何使用会话窗口: Java版本:DataStream&lt;T&gt; input = ...;// event-time session windowsinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;);// processing-time session windowsinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...// event-time session windowsinput .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;)// processing-time session windowsinput .keyBy(&lt;key selector&gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&lt;windowed transformation&gt;(&lt;window function&gt;) 可以通过使用 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x) 来指定时间间隔。 由于会话窗口没有固定的开始时间和结束时间，因此它们的执行与滚动窗口和滑动窗口不同。在内部，会话窗口算子为每个到达记录创建一个新窗口，如果它们之间的距离比定义的间隙更近，则将窗口合并在一起。为了可合并，会话窗口算子需要一个合并触发器和合并窗口函数，例如 ReduceFunction 或 WindowFunction（FoldFunction无法合并）。 3.4 全局窗口全局窗口分配器将具有相同 key 的所有元素分配给同一个全局窗口。这个窗口规范是仅在你同时指定自定义触发器时有用。否则，不会执行计算，因为全局窗口没有我们可以处理聚合元素的自然结束的点（译者注：即本身自己不知道窗口的大小，计算多长时间的元素）。 以下代码显示如何使用会话窗口: Java版本:DataStream&lt;T&gt; input = ...;input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;); Scala版本:val input: DataStream[T] = ...input .keyBy(&lt;key selector&gt;) .window(GlobalWindows.create()) .&lt;windowed transformation&gt;(&lt;window function&gt;) 备注: Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 Operator概述]]></title>
    <url>%2F2018%2F02%2F28%2Fflink-stream-operators-overall%2F</url>
    <content type="text"><![CDATA[算子(Operator)将一个或多个 DataStream 转换为新的 DataStream。程序可以将多个转换组合成复杂的数据流拓扑。 本节将介绍基本转换(transformations)操作，应用这些转换后的有效物理分区以及深入了解 Flink 算子链。 1. DataStream Transformations1.1 MapDataStream → DataStream 输入一个元素并生成一个对应的元素。下面是一个将输入流的值加倍的 map 函数： Java版本：DataStream&lt;Integer&gt; dataStream = //...dataStream.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer value) throws Exception &#123; return 2 * value; &#125;&#125;); scala版本：dataStream.map &#123; x =&gt; x * 2 &#125; 1.2 FlatMapDataStream → DataStream 输入一个元素并生成零个，一个或多个元素。下面是个将句子拆分为单词的 flatMap 函数： Java版本：dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for(String word: value.split(" "))&#123; out.collect(word); &#125; &#125;&#125;); Scala版本:dataStream.flatMap &#123; str =&gt; str.split(" ") &#125; 1.3 FilterDataStream → DataStream 为每个元素计算一个布尔值的函数并保留函数返回 true 的那些元素。下面是一个筛选出零值的 filter 函数： Java版本:dataStream.filter(new FilterFunction&lt;Integer&gt;() &#123; @Override public boolean filter(Integer value) throws Exception &#123; return value != 0; &#125;&#125;); Scala版本:dataStream.filter &#123; _ != 0 &#125; 1.4 KeyByDataStream → KeyedStream 逻辑上将一个流分成不相交的分区，每个分区包含相同键的元素。在内部，这是通过哈希分区实现的。参阅博文Flink1.4 定义keys的几种方法来了解如何指定键。这个转换返回一个 KeyedStream。 Java版本:dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple Scala版本:dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple 备注 在以下情况，不能指定为key： POJO类型，但没有覆盖hashCode()方法并依赖于Object.hashCode()实现。 任意类型的数组。 1.5 ReduceKeyedStream → DataStream 键控数据流的”滚动” reduce。将当前元素与上一个 reduce 后的值组合，并生成一个新值。下面是一个创建局部求和流的 reduce 函数： Java版本:keyedStream.reduce(new ReduceFunction&lt;Integer&gt;() &#123; @Override public Integer reduce(Integer value1, Integer value2) throws Exception &#123; return value1 + value2; &#125;&#125;); Scala版本:keyedStream.reduce &#123; _ + _ &#125; 1.6 FoldKeyedStream → DataStream 在具有初始值的键控数据流上”滚动” fold。将当前元素与上一个 fold 后的值组合，并生成一个新值。下面是 fold 函数在在序列（1,2,3,4,5）的演示，生成序列 “start-1”，”start-1-2”，”start-1-2-3”，… : Java版本:DataStream&lt;String&gt; result = keyedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String current, Integer value) &#123; return current + "-" + value; &#125; &#125;); Scala版本:val result: DataStream[String] = keyedStream.fold("start")((str, i) =&gt; &#123; str + "-" + i &#125;) 1.7 AggregationsKeyedStream → DataStream 在键控数据流上滚动聚合。min 和 minBy 之间的差别是 min 返回最小值，而 minBy 返回在该字段上具有最小值的元素（max 和 maxBy 相同）。 Java版本:keyedStream.sum(0);keyedStream.sum("key");keyedStream.min(0);keyedStream.min("key");keyedStream.max(0);keyedStream.max("key");keyedStream.minBy(0);keyedStream.minBy("key");keyedStream.maxBy(0);keyedStream.maxBy("key"); Scala版本:keyedStream.sum(0)keyedStream.sum("key")keyedStream.min(0)keyedStream.min("key")keyedStream.max(0)keyedStream.max("key")keyedStream.minBy(0)keyedStream.minBy("key")keyedStream.maxBy(0)keyedStream.maxBy("key") 1.8 WindowKeyedStream → WindowedStream 可以在已分区的 KeyedStream 上定义窗口。窗口根据某些特性（例如，在最近5秒内到达的数据）对每个键的数据进行分组。请参阅窗口以获取窗口的详细说明。 Java版本:dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data Scala版本:dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data 1.9 WindowAllDataStream → AllWindowedStream 可以在常规的 DataStream 上定义窗口。窗口根据某些特征（例如，在最近5秒内到达的数据）对所有流事件进行分组。请参阅窗口以获取窗口的详细说明。 警告 在很多情况下是非并行转换。所有记录将被收集到windowAll算子的一个任务中。 Java版本:dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data Scala版本:dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data 1.10 Window ApplyWindowedStream → DataStreamAllWindowedStream → DataStream 将常规函数应用于整个窗口。以下是手动对窗口元素求和的函数。 注意 如果你使用的是windowAll转换，则需要使用AllWindowFunction。 Java版本：windowedStream.apply (new WindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Tuple, Window&gt;() &#123; public void apply (Tuple tuple, Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;);// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply (new AllWindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Window&gt;() &#123; public void apply (Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;); Scala版本:windowedStream.apply &#123; WindowFunction &#125;// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply &#123; AllWindowFunction &#125; 1.11 Window ReduceWindowedStream → DataStream 在窗口中应用功能性 reduce 函数并返回 reduce 后的值。 Java版本:windowedStream.reduce (new ReduceFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return new Tuple2&lt;String,Integer&gt;(value1.f0, value1.f1 + value2.f1); &#125;&#125;); Scala版本:windowedStream.reduce &#123; _ + _ &#125; 1.12 Window FoldWindowedStream → DataStream 将功能性 fold 函数应用于窗口并返回 fold 后值。例如，应用于序列（1,2,3,4,5）时，将序列 fold 为字符串 start-1-2-3-4-5： Java版本:windowedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; public String fold(String current, Integer value) &#123; return current + "-" + value; &#125;&#125;); Scala版本:val result: DataStream[String] = windowedStream.fold("start", (str, i) =&gt; &#123; str + "-" + i &#125;) 1.13 Aggregations on windowsWindowedStream → DataStream 聚合一个窗口的内容。min 和 minBy 之间的差别是 min 返回最小值，而 minBy 返回该字段中具有最小值的元素（max 和 maxBy 相同）。 Java版本:windowedStream.sum(0);windowedStream.sum("key");windowedStream.min(0);windowedStream.min("key");windowedStream.max(0);windowedStream.max("key");windowedStream.minBy(0);windowedStream.minBy("key");windowedStream.maxBy(0);windowedStream.maxBy("key"); Scala版本:windowedStream.sum(0)windowedStream.sum("key")windowedStream.min(0)windowedStream.min("key")windowedStream.max(0)windowedStream.max("key")windowedStream.minBy(0)windowedStream.minBy("key")windowedStream.maxBy(0)windowedStream.maxBy("key") 1.14 UnionDataStream* → DataStream 合并两个或更多数据流，创建一个包含所有流中所有元素的新流。 注意 如果你与自己进行合并，你将在结果流中获取每个元素两次。 Java版本:dataStream.union(otherStream1, otherStream2, ...); Scala版本:dataStream.union(otherStream1, otherStream2, ...) 1.15 Window JoinDataStream,DataStream → DataStream 在给定的键和公共窗口上对两个数据流进行 join。 Java版本:dataStream.join(otherStream) .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new JoinFunction () &#123;...&#125;); Scala版本:dataStream.join(otherStream) .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply &#123; ... &#125; 1.16 Window CoGroupDataStream,DataStream → DataStream 在给定键和公共窗口上对两个数据流进行组合。 Java版本:dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new CoGroupFunction () &#123;...&#125;); Scala版本:dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply &#123;&#125; 1.17 SplitDataStream → SplitStream 根据一些标准将流分成两个或更多流。 Java版本:SplitStream&lt;Integer&gt; split = someDataStream.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) &#123; output.add("even"); &#125; else &#123; output.add("odd"); &#125; return output; &#125;&#125;); Scala版本:val split = someDataStream.split( (num: Int) =&gt; (num % 2) match &#123; case 0 =&gt; List("even") case 1 =&gt; List("odd") &#125;) 1.18 SelectSplitStream → DataStream 从分流中选择一个或多个流。 Java版本：SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select("even");DataStream&lt;Integer&gt; odd = split.select("odd");DataStream&lt;Integer&gt; all = split.select("even","odd"); Scala版本:val even = split select "even"val odd = split select "odd"val all = split.select("even","odd") 1.19 Extract TimestampsDataStream → DataStream 从记录中提取时间戳，以便与使用事件时间语义的窗口一起工作。 Java版本:stream.assignTimestamps (new TimeStampExtractor() &#123;...&#125;); Scala版本:stream.assignTimestamps &#123; timestampExtractor &#125; 2. Physical partitioning通过以下功能，Flink 还可以在转换后的确切流分区上进行低层次的控制（如果需要）。 2.1 Custom partitioningDataStream → DataStream 使用用户自定义的分区器为每个元素选择指定的任务。 dataStream.partitionCustom(partitioner, "someKey");dataStream.partitionCustom(partitioner, 0); 2.2 Random partitioningDataStream → DataStream 根据均匀分布随机分配元素。 dataStream.shuffle(); 2.3 Rebalancing (Round-robin partitioning)DataStream → DataStream 对元素循环分区，为每个分区创建相同的负载。在在数据倾斜时用于性能优化。 dataStream.rebalance(); 2.4 RescalingDataStream → DataStream 为下游操作的子集循环分配元素。这非常有用，如果你想要在管道中使用，例如，从一个数据源的每个并行实例中输出到几个映射器的子集上来分配负载，但不希望发生 rebalance() 的完全重新平衡。这只需要本地数据传输，而不是通过网络传输数据，具体取决于其他配置值，例如 TaskManager 的插槽数。 上游操作向其发送元素的下游操作的子集取决于上游和下游操作的并行度。例如，如果上游操作并行度为2并且下游操作并行度为4，则一个上游操作将向两个下游操作分配元素，而另一个上游操作将分配给另外两个下游操作。另一方面，如果下游操作并行度为2而上游操作并行度为4，则两个上游操作将分配给一个下游操作，而另外两个上游操作将分配给另一个下游操作。 存在不同并行度不是成倍数关系，或者多个下游操作具有来自上游操作的不同数量的输入的情况。 这个图显示了在上面的例子中的连接模式： dataStream.rescale(); 2.5 BroadcastingDataStream → DataStream 将元素广播到每个分区。dataStream.broadcast() 3. 任务链 和 资源组链接两个连续的转换操作意味着将它们共同定位在同一个线程中以获得更好的性能。如果可能的话，Flink默认链接算子（例如，两个连续的 map 转换）。如果需要，API可以对链接进行精细控制。 如果要禁用整个作业中的链接，请使用 StreamExecutionEnvironment.disableOperatorChaining（）。对于更细粒度的控制，可用使用以下函数。请注意，这些函数只能在 DataStream 转换操作之后使用，因为它们引用上一个转换。例如，你可以使用 someStream.map（...）.startNewChain（），但不能使用 someStream.startNewChain（）。 资源组是 Flink 中的插槽，请参阅插槽。如果需要，你可以在不同的插槽中手动隔离算子。 3.1 开始一个新链从这个算子开始，开始一个新的链。将这两个 mapper 链接，并且 filter 不会链接到第一个 mapper。someStream.filter(...).map(...).startNewChain().map(...); 3.2 取消链不会将map算子链接到链上：someStream.map(...).disableChaining(); 3.3 设置插槽共享组设置操作的插槽共享组。Flink会将使用相同插槽共享组的操作放入同一插槽，同时保持在其他插槽中没有插槽共享组的操作。这可以用来隔离插槽。如果所有输入操作位于同一个插槽共享组中，则插槽共享组将继承自输入操作。缺省插槽共享组的名称为 default，可通过调用 slotSharingGroup（“default”）将操作显式放入此组。someStream.filter(...).slotSharingGroup("name"); 备注： Flink 版本： 1.4 原文： https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/index.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之提取器]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-apply-update%2F</url>
    <content type="text"><![CDATA[1. apply和update方法Scala允许你使用如下函数调用语法:f(arg1, arg2, ...) 扩展到可以应用于函数之外的值．如果 f 不是函数或方法，那么这个表达式就等同于调用:f.apply(arg1, arg2, ...) 如果它出现在赋值语句的等号左侧:f(arg1, arg2, ...) = value 则等同于调用:f.update(arg1, arg2, ..., value) 应用场景: (1) 常被用于数组和映射:val scores = new scala.collection.mutable.HashMap[String, Int]scores("Bob") = 100 // 调用scores.update("Bob", 100)val bobScores = scores("Bob") // 调用scores.apply("Bob") (2) 同样经常用在伴生对象中，用来构造对象而不用显示的使用new:class Fraction (n: Int, d: Int)&#123; ...&#125;object Fraction&#123; def apply(n: Int, d: Int) = new Fraction(n, d)&#125;// 使用val result = Fraction(3, 4) * Fraction(2, 5) 2. 提取器所谓提取器就是一个带有 unapply 方法的对象．可以把 unapply 方法理解为伴生对象中 apply 方法的反向操作. apply 方法接受构造参数，然后将他们变成对象．而 unapply 方法接受一个对象，然后从中提取值(通常这些值就是当初用来构造该对象的值)． 例如上面例子中的 Fraction 类， apply 方法从分子和分母创建出一个分数，而 unapply 方法则是去取出分子和分母: (1) 可以在变量定义时使用:// a b 分别被初始化成运算结果的分子和分母var Fraction(a, b) = Fraction(3, 4) * Fraction(2, 5) (2) 也可以用于模式匹配:// a 和 b 分别绑到分子和分母case Fraction(a, b) =&gt; ... 通常而言，模式匹配可能会失败，因此 unapply 方法返回的是一个Option．它包含一个元组，每个匹配到的变量各有一个值与之对应．下面中返回一个 Option[(Int, Int)]class Fraction (n: Int, d: Int)&#123; ...&#125;object Fraction&#123; def apply(n: Int, d: Int) = new Fraction(n, d) def unapply(input: Fraction) = if( input.den == 0 ) None else Some( (input.num, input.den) )&#125; 备注分母为0时返回None，表示无匹配 在上面例子中，apply 和 unapply 互为反向，但不一定总是互为反向．我们可以用提取器从任何类型的对象中提取信息．例如我们可以从字符串中提取名字和姓氏:// 提取器object Name&#123; def unapply(input: String) = &#123; val pos = input.indexOf(" ") if(pos == -1) Node else Some( (input.substring(0, pos), input.substring(pos + 1)) ) &#125;&#125;val author = "Lionel Messi"// 调用Name.unapply(author)val Name(first, last) = author// First Name is Lionel and last name is Messiprintln("First Name is " + first + " and last name is " + last) 3. 带单个参数或无参数的提取器在Scala中，并没有只带一个组件的元组．如果 unapply 方法要提取单值，则应该返回一个目标类型的 Option:object Number &#123; def unapply(input: String) : Option[Int] = &#123; try&#123; Some (Integer.parseInt(input.trim)) &#125; catch&#123; case ex: NumberFormatException =&gt; None &#125; &#125;&#125; 可以使用这个提取器，从字符串中提取数字:val Number(n) = "1990" 提取器也可以只是测试输入的数据而并不将其值提取出来，只需unapply方法返回Boolean:object IsContainZero&#123; def unapply(input: String) = input.contains("0")&#125; 4. unapplySeq方法如果要提取任意长度的值的序列，我们需要使用 unapplySeq 来命名我们的方法．它返回一个 Option[Seq[A]]，其中A是被提取的值的类型:object Names&#123; def unapplySeq(input: String): Option[Seq[String]] = &#123; if(input.trim == "") None else Some(input.trim.split("\\s+")) &#125;&#125;val namesStr = "Tom Lily Lucy"val Names(first, second, third) = namesStr// the first name is Tom and the second name is Lily and the third name is Lucyprintln(s"the first name is $first and the second name is $second and the third name is $third") 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之正则表达式]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-regex%2F</url>
    <content type="text"><![CDATA[1. Regex对象我们可以使用 scala.util.matching.Regex 类使用正则表达式．要构造一个 Regex 对象，使用 String 类的 r 方法即可:val numPattern = "[0-9]+".r 如果正则表达式包含反斜杠或引号的话，那么最好使用”原始”字符串语法 &quot;&quot;&quot;...&quot;&quot;&quot;:val positiveNumPattern = """^[1-9]\d*$""" 如果在Java中使用上述正则表达式，则应该使用下面方式(需要进行转义):val positiveNumPattern = "^[1-9]\\d*$" 相对于在Java中的使用方式，Scala这种写法可能更易读一些． 2. findAllInfindAllIn 方法返回遍历所有匹配项的迭代器．可以在 for 循环中使用它:val str = "a b 27 c 6 d 1"val numPattern = "[0-9]+".rfor(matchingStr &lt;- numPattern.findAllIn(str))&#123; println(matchingStr)&#125; 或者将迭代器转成数组:val str = "a b 27 c 6 d 1"val numPattern = "[0-9]+".rval matches = numPattern.findAllIn(str).toArray// Array(27,6,1) 3. findPrefixOf检查某个字符串的前缀是否能匹配，可以使用findPrefixOf方法:val str = "3 a b 27 c 6 d 1"val str2 = "a b 27 c 6 d 1"val numPattern = "[0-9]+".rval matches = numPattern.findPrefixOf(str)val matches2 = numPattern.findPrefixOf(str2)println(matches) // Some(3)println(matches2) // None 4. replaceFirstIn replaceAllIn可以使用如下命令替换第一个匹配项或者替换全部匹配项:val str = "3 a b 27 c 6 d 1"val numPattern = "[0-9]+".rval matches = numPattern.replaceFirstIn(str, "*")val matches2 = numPattern.replaceAllIn(str, "*")println(matches) // * a b 27 c 6 d 1println(matches2) // * a b * c * d * 5. 正则表达式组分组可以让我们方便的获取正则表达式的子表达式．在你想要提取的子表达式两侧加上圆括号:val str = "3 a"val numPattern = "([0-9]+) ([a-z]+)".rval numPattern(num, letter) = strprintln(num) // 3println(letter) // a 上述代码将num设置为3，letter设置为a 如果想从多个匹配项中提取分组内容，可以使用如下命令:val str = "3 a b c 4 f"val numPattern = "([0-9]+) ([a-z]+)".rfor(numPattern(num, letter) &lt;- numPattern.findAllIn(str))&#123; println(num + "---"+letter)&#125;// 3---a// 4---f 来源于: 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之高阶函数]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-higher-order-functions%2F</url>
    <content type="text"><![CDATA[Scala混合了面向对象和函数式的特性．在函数式编程语言中，函数可以像任何其他数据类型一样被传递和操作．如果想要给算法传入明细动作时，只需要将明细动作包在函数当中作为参数传入即可． 1. 作为值的函数在Scala中，函数就和数字一样，可以在变量中存放:import scala.math._val num = 3.14 // num: Double = 3.14val fun = ceil _ // fun: Double =&gt; Double = &lt;function1&gt;println(num) // 3.14println(fun(num)) // 4.0 上述代码将num设置为3.14，将fun设置为 ceil 函数．num的类型为 Double，fun的类型为 (Double) =&gt; Double (即接受并返回Double的函数) 备注 ceil函数后的 _ 表示确实指的是ceil这个函数，而不是碰巧忘记了给它传递参数 可以对函数做如下两件事: 调用它 传递它 存放在变量中，或者作为参数传递给另一个函数 Example:// 调用fun(num) // 4.0// 传递Array(3.14, 2.14, 1.14).map(fun) // Array(4.0, 3.0, 2.0) 备注map方法接受一个函数参数，将它应用到数组中的所有值，然后返回结果的数组 2. 匿名函数在Scala中，不需要给每一个函数命名，就像不用给每个数字命名一样:(x: Double) =&gt; 3 * x 上述代码表示该函数将传递给它的参数乘以3． 对与上述匿名函数我们可以如下操作: (1) 可以将函数存放在变量中:val triple = (x: Double) =&gt; 3 * xtriple(2) // 6.0 上述代码等价于:def triple(x:Double) = 3 * x (2) 可以不用命名直接将函数传递给另一个函数:Array(3.14, 2.14, 1.14).map((x: Double) =&gt; 3 * x) 3. 带函数参数的函数下面是一个接受一个函数作为参数的函数:def valueAtOneQuarter(fun: (Double) =&gt; Double) = fun(0.25)valueAtOneQuarter(sqrt _) // 0.5 即 sqrt(0.25) 上述函数的参数类型为 (Double) =&gt; Double，即接受任何 Double 并返回 Double 的函数． 4. 参数类型推断当你将一个匿名函数传递给一个函数时，Scala会尽可能帮助你推断出类型信息．不需要将代码写成如下:valueAtOneQuarter( (x:Double) =&gt; 3 * x ) // 0.75 由于valueAtOneQuarter方法知道你会传入一个类型为(x:Double) =&gt; Double的函数，可以将上述代码改为:valueAtOneQuarter( (x) =&gt; 3 * x ) 或者valueAtOneQuarter( x =&gt; 3 * x ) // 只有一个参数的函数 可以省略括号 如果参数在=&gt;右侧只出现一次，可以使用 _ 替换，因此进一步改写:valueAtOneQuarter(3 * _) 5. 柯里化柯里化是指将原来接受两个参数的函数变成一个新的接受一个参数的函数的过程．新的函数返回一个以原有第二个参数作为参数的函数．def mul (x: Int, y: Int) = x * y 以下函数接受一个参数，生成另一个接受单个参数的函数:def mulOneAtATime(x: Int) = (y: Int) =&gt; x * y 要计算两个数的乘积，需要调用:mulOneAtATime(6)(7) 细分说明一下，mulOneAtATime(6)返回的是一个函数 (y: Int) =&gt; 6 * y．而这个函数又被应用到7，因此最终的结果为42． Scala支持如下简写来定义柯里化函数:def mulOneAtATime(x: Int) (y: Int) = x * y 我们可以看到多参数只是个虚饰，不是什么编程语言的特质． 6. 控制抽象在Scala中，我们可以将一系列语句组成不带参数也没有返回值的函数．如下函数在线程中执行某段代码:def runInThread(block: ()=&gt;Unit)&#123; new Thread&#123; override def run()&#123; block() &#125; &#125;.start()&#125; 上述函数为带函数参数的函数，函数参数类型为()=&gt;Unit(表示没有参数也没有返回值)．但是如此一来，当你调用该函数时，需要写不美观的()=&gt;:runInThread&#123; () =&gt; println("Hello");Thread.sleep(10000);println("Bye"); &#125; 要想在调用中省掉()=&gt;，可以使用换名调用表示法：在参数声明和调用该函数参数的地方略去()，但保留=:def runInThread(block: =&gt; Unit)&#123; // (1) new Thread&#123; override def run() &#123; block &#125; // (2) &#125;.start()&#125; 这样，我们如下调用:runInThread&#123; println("Hello");Thread.sleep(10000);println("Bye"); &#125; Scala程序员可以构建控制抽象:看上去像编程语言的关键字的函数．例如，下面我们定一个until语句，工作原理类似while，只不过把条件反过来用:def until (condition: =&gt; Boolean) (block: =&gt; Unit) &#123; if(!condition)&#123; block until (condition) (block) &#125;&#125; 使用:var x = 4until (x == 0) &#123; println(x) x = x -1&#125;// 4// 3// 2// 1var x = 0until (x == 0) &#123; println(x) x = x -1&#125;// 无输出 这样的函数参数有一个专业术语:换名调用参数．和常规的参数不同，函数被调用时，参数表达式不会被求值．毕竟，在调用until时，不希望x == 0被求值得到false．与之相反，表达式成为无参函数的函数体，而该函数被当做参数传递下去．仔细看一下until函数的定义．注意它是柯里化的:函数首先处理掉condition，然后把block当做完全独立的另一个参数．如果没有柯里化，调用就会变成如下:until (x == 0, &#123;...&#125;) 7. return表达式在Scala中，不需要使用return语句返回函数值．函数的返回值是函数体的值．不过，可以使用return来从一个匿名函数中返回值给包含这个匿名函数的带名函数．这对于抽象控制是很有用的:def indexOf(str: String, ch: Char) : Int = &#123; var i = 0; until (i == str.length)&#123; if(str(i) == ch) return i i += 1 &#125; return -1&#125; 在这里，匿名函数{ if(str(i) == ch) return i; i += 1 }被传递给until．当return表达式被执行时，包含它的带名函数indexOf终止并返回给定的值．如果要在带名函数中使用return的话，则需要给出其返回类型．例如上例中，编译器没法推断出它会返回Int，因此需要给出返回类型Int． 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之Object]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-object%2F</url>
    <content type="text"><![CDATA[1. 单例对象Scala没有静态方法或静态字段，可以使用 object 来达到这个目的，对象定义了某个类的单个实例:object Account&#123; private var lastNumber = 0 def newUniqueNumber () = &#123;lastNumber += 1; lastNumber&#125;&#125; 当你在应用程序中需要一个新的唯一账号时，调用 Account.newUniqueNumber() 即可．对象的构造器在该对象第一次被使用时调用．在本例中，Account 的构造器在 Account.newUniqueNumber 的首次调用时执行．如果一个对象从未被使用，那么构造器也不会被执行． 对象本质上可以拥有类的所有特性，但是不能提供构造器参数． 在Scala中可以用对象来实现: 作为存放工具函数或常量的地方 高效的共享单个不可变实例 需要用单个实例来协调某个服务时(参考单例模式) 2. 伴生对象在Java中，通常会用到既有实例方法又有静态方法的类，在Scala中，可以通过类和类同名的 伴生对象 来达到同样的目的:class Account&#123; val id = Account.newUniqueNumber() private var balance = 0.0 def deposit(amount : Double) &#123; balance += amount &#125; ...&#125;// 伴生对象object Account&#123; private var lastNumber = 0 def newUniqueNumber () = &#123;lastNumber += 1; lastNumber&#125; &#125; 类和它的伴生对象可以相互访问私有特性．它们必须在同一个源文件中． 3. apply方法我们通常会定义和使用对象的 apply 方法．当遇到如下形式的表达式时，apply 方法就会被调用:Object(参数1，参数2，...，参数N) 通常，这样一个 apply 方法返回的是伴生类的对象．举例来说，Array对象定义了 apply 方法，让我们可以用下面这样的表达式来创建数组:Array("Mary", "had", "a", "little", "lamb") 不使用构造器，而是使用apply方法，对于使用嵌套表达式而言，省去new关键字会方便很多:Array(Array(1,7), Array(2,9)) 下面有一个定义apply方法的示例:class Account private (val id :Int, initialBalance: Double)&#123; private var balance = initialBalance ...&#125;// 伴生对象object Account&#123; def apply(initialBalance : Double)&#123; new Account(newUniqueNumber(), initialBalance) &#125; ...&#125; 这样我们就可以使用如下方式创建账号了:val acct = Account(1000.0) 4. 应用程序对象每个Scala程序都必须从一个对象的main方法开始，这个方法的类型为 Array[String]=&gt;Unit:object Hello&#123; def main(args: Array[String])&#123; println("Hello world!") &#125;&#125; 除了每次都提供自己main方法外，你可以扩展App特质，然后将程序代码放入构造器方法体内:object Hello extends App&#123; println("Hello world!")&#125; 如果需要命令行参数，则可以通过args属性得到:object Hello extends App&#123; if(args.length &gt; 0)&#123; println("Hello, " + args(0)) &#125; else&#123; println("Hello world!") &#125;&#125; 5. 枚举不同于Java，Scala中没有枚举类型，需要我们通过标准库类 Enumeration 来实现:object BusinessType extends Enumeration&#123; var FLIGHT, HOTEL, TRAIN, COACH = Value&#125; 继承 Enumeration 类，实现一个 BusinessType 对象，并以 Value 方法调用初始化枚举中的所有可选值．在这里我们定义了４个业务线类型，然后用Value调用它们初始化． 每次调用Value方法都返回内部类的新实例，该内部类也叫做Value．或者，可以向Value方法传入ID，名称:val FLIGHT = Value(0, "FLIGHT")val HOTEL = Value(10) // 名称为"HOTEL"val TRAIN = Value("TRAIN") // ID为11 如果不指定ID，ID为上一个枚举值上加一，如果不指定名称，名称默认为字段名．定义完成后，可以使用 BusinessType.FLIGHT，BusinessType.HOTEL，BusinessType.TRAIN 等来引用:def businessHandle(business: BusinessType.Value): Unit =&#123; if(business == BusinessType.FLIGHT)&#123; println("this is a flight behavior") &#125; else if(business == BusinessType.HOTEL)&#123; println("this ia a hotel behavior") &#125;&#125;def main(args: Array[String]): Unit = &#123; val business = BusinessType.FLIGHT businessHandle(business) // this is a flight behavior&#125; 如果觉的BusinessType.FLIGHT比较冗长繁琐，可以使用如下方式引入枚举值:import BusinessType._ 使用时直接使用枚举值名称即可:def businessHandle(business: BusinessType.Value): Unit =&#123; if(business == FLIGHT)&#123; println("this is a flight behavior") &#125; else if(business == HOTEL)&#123; println("this ia a hotel behavior") &#125;&#125; 记住枚举值的类型是BusinessType.Value而不是BusinessType，后者是拥有这些值的对象，可以增加一个类型别名:object BusinessType extends Enumeration&#123; type BusinessType = Value var FLIGHT, HOTEL, TRAIN, COACH = Value&#125; 如下使用:def businessHandle(business: BusinessType): Unit =&#123; if(business == FLIGHT)&#123; println("this is a flight behavior") &#125; else if(business == HOTEL)&#123; println("this ia a hotel behavior") &#125;&#125; 枚举值的ID可以通过id方法返回，名称通过toString方法返回:val business = FLIGHTprintln("ID:" + business.id + " name:" + business.toString) // ID:0 name:FLIGHT 可以通过如下方式输出所有的枚举值:for(business &lt;- BusinessType.values)&#123; println("ID:" + business.id + " name:" + business.toString)&#125;ID:0 name:FLIGHTID:1 name:HOTELID:2 name:TRAINID:3 name:COACH 你也可以通过枚举的ID或名称来进行查找定位:val FLIGHT1 = BusinessType(0)println("ID:" + FLIGHT1.id + " name:" + FLIGHT1.toString)val FLIGHT2 = BusinessType.withName("FLIGHT")println("ID:" + FLIGHT2.id + " name:" + FLIGHT2.toString)ID:0 name:FLIGHTID:0 name:FLIGHT 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之文件操作]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-file%2F</url>
    <content type="text"><![CDATA[1. 读取行读取文件，可以使用 scala.io.Source 对象的 fromFile 方法．如果读取所有行可以使用 getLines 方法:val source = Source.fromFile("/home/xiaosi/exception.txt", "UTF-8")val lineIterator = source.getLines()for(line &lt;- lineIterator)&#123; println(line)&#125;source.close() source.getLines 返回结果为一个迭代器，可以遍历迭代器逐条处理行． 如果想把整个文件当做一个字符串处理，可以调用mkString方法:val content = source.mkString 备注 在用完 Source 对象后，记得调用 close 方法进行关闭 2. 读取字符读取字符，可以直接把 Source 对象当做迭代器使用，因为 Source 类扩展了Iterator[Char]:val source = Source.fromFile("/home/xiaosi/exception.txt", "UTF-8")for(c &lt;- source)&#123; print(c + " ")&#125; 3. 从URL或其他源读取数据Source 对象有读取非文件源的方法:// 从URL中读取数据val sourceUrl = Source.fromURL("http://xxx", "UTF-8")// 从字符串中读取数据val sourceStr = Source.fromString("Hello World!")// 从标准输入读取数据val sourceStd = Source.stdin 4. 读取二进制文件Scala并没有提供读取二进制文件的方法．但是你可以使用Java类库来完成读取操作:val file = new File(fileName)val in = new FileInputStream(file)val bytes = new Array[byte](file.length.toInt)in.read(bytes)in.close() 5. 写入文本文件Scala并没有内置的对写入文件的支持．但是可以使用 java.io.PrintWriter 来完成:val out = new PrintWriter("/home/xiaosi/exception.txt")out.println("Hello World")out.println("Welcome")out.close() 6. 访问目录目前Scala并没有用来访问某个目录中的所有文件，或者递归的遍历所有目录的类，我们只能寻求一些替代方案. 利用如下代码可以实现递归遍历所有的子目录:// 递归遍历目录def subDirs(dir: File) : Iterator[File] = &#123; val children = dir.listFiles().filter(_.isDirectory) children.toIterator ++ children.toIterator.flatMap(subDirs _)&#125;val file = new File("/home/xiaosi/test")val iterator = subDirs(file)for(d &lt;- iterator)&#123; println(d)&#125; 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之类]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-class%2F</url>
    <content type="text"><![CDATA[1. 简单类与无参方法class Person &#123; var age = 0 // 必须初始化字段 def getAge() = age // 方法默认为公有的&#125; 备注 在Scala中，类并不声明为public． Scala源文件可以包含多个类，所有这些类都具有公有可见性．属性不声明默认为public． 使用类:val p = new Person // 或者new Person()p.age = 23println(p.getAge()) // 23 调用无参方法时，可以写上圆括号，也可以不写:p.getAge() // 23p.getAge // 23 2. 带getter和setter的属性2.1 Java getter和setter在Java类中，我们并不喜欢使用公有字段:public class Person&#123; public int age; // Java中不推荐使用这种方式&#125; 更倾向于使用getter和setter方法:public class Person&#123; private int age; public int getAge() &#123;return age;&#125; public void setAge(int age) &#123;this.age = age;&#125;&#125; 像这样的一对getter/setter通常被称为属性．我们会说Person类有一个age属性． 2.２ Scala getter和setter在Scala中对每个字段都提供了getter和setter方法:class Person&#123; var age = 0&#125; scala生成面向JVM的类，会生成一个私有的age字段以及相应的getter和setter方法．这两个方法都是公有的，因为我们没有将age声明为private．(对于私有字段而言,getter和setter方法也是私有的) 在scala中getter和setter方法分别叫age和age_=．使用的时候如下:val p = new Personp.age = 21 // 调用p.age_=(21)println(p.age) // 调用p.age()方法 备注 在scala中，getter和setter方法并非被命名为getXXX和setXXX，不过用意相同． 任何时候我们都可以自己重新定义getter和setter方法:class Person &#123; private var privateAge = 0 def age = privateAge def age_= (newAge : Int): Unit = &#123; if(newAge &gt; 150)&#123; privateAge = 150 &#125; else if(newAge &lt; 0)&#123; privateAge = 0 &#125; &#125;&#125; 使用:val p = new Personp.age = -1;println(p.age) // 0p.age = 189println(p.age) // 150 备注 Scala对每个字段生成getter和setter方法听上去有些恐怖，不过你可以控制这个过程: 如果字段是私有的，则getter和setter方法也是私有的 如果字段是val，则只有getter方法被生成 如果你不需要任何的getter和setter方法，可以将字段声明为private[this] 2.3 Example(1) 对于公有字段,getter和setter方法是公有的:class Student &#123; var age = 22&#125;val stu = new Studentstu.age = 23println(stu.age) // 23 (2) 对于私有字段,getter和setter方法是私有的:class Student &#123; private var age = 22&#125;val stu = new Student//stu.age = 23 // symbol age is inaccessible from this place//println(stu.age) // symbol age is inaccessible from this place (3) 如果字段是val，则只有getter方法被生成:class Student &#123; val age = 22&#125;val stu = new Student// stu.age = 23 // reassignment to valprintln(stu.age) // 22 3. 只带getter的属性如果只想需要一个只读的属性，有getter但没有setter，属性的值在对象构建完成之后就不再改变了，可以使用val字段:class Student &#123; val age = 22&#125; Scala会生成一个私有的final字段和一个getter方法，但没有setter方法 4. 对象私有字段在Scala中，方法可以访问该类的所有对象的私有字段:class Counter &#123; private var value = 0 def increment(): Unit = &#123; value += 1 &#125; // 对象可以访问另一个对象的私有字段 def isLess (other : Counter) = value &lt; other.value&#125; 之所以访问 other.value 是合法的，是因为 other 也是Counter对象，这与Java的private权限不一样. Scala允许我们定义更加严格的访问限制，通过private[this]这个修饰符来实现:private[this] var value = 0 这样 other.value 是不被允许访问的，这样以来Counter类只能访问当前对象的value字段，而不能访问同样是Counter类型的其他对象的字段． Scala允许你将访问权限赋予指定得类，private[类名]可以定义仅有指定类的方法可以访问给定的字段．这里的类名必须是当前定义的类，或者是包含该类的外部类． 备注 对于类私有的字段(private)，Scala会生成私有的getter和setter方法，但是对于对象私有的字段，不会生成getter和setter方法． 5. Bean属性Scala对于你定义的字段提供了getter和setter方法，但是并不是Java工具所期望的．JavaBeans规范把Java属性定义为一对getXXX/setXXX方法．很多Java工具都依赖这样的命令习惯． Scala给我们提供了@BeanProperty注解，这样就会字段生成我们期望的getXXX和setXXX方法:class Student &#123; @BeanProperty var age = 22&#125;val stu = new Studentstu.setAge(25)println(stu.getAge()) // 25 总结 scala字段 生成的方法 何时使用 val/var name 公有的name name_=(仅限var) 实现一个可以被公开访问并且背后是以字段形式保存的属性 @BeanProperty val/var name 公有的name getName() name_=(仅限var) setName() (仅限var) 与JavaBeans互操作 private val/var name 私有的name name_=(仅限var) 用于将字段访问限制在本类的方法．尽量使用private，除非真的需要一个公有属性 private[this] val/var name 无 用于将字段访问限制在同一个对象上调用的方法．不经常用 private[类名] val/var name 依赖于具体实现 将访问权限赋予外部类．不经常使用 6. 辅助构造器Scala可以有任意多的构造器，不过，Scala有一个构造器比其他所有构造器都重要，就是主构造器，除了主构造器之外，类还有任意多的辅助构造器．其同Java中的构造器十分相似，只有两处不同: 辅助构造器的名称为this 每一个辅助构造器都必须以一个先前已定义的其他辅助构造器或主构造器的调用开始 class Person &#123; private var name = "" private var age = 0 def this (name : String)&#123; this() // 调用主构造器 this.name = name &#125; def this (name : String, age : Int)&#123; this(name) // 调用前一个辅助构造器 this.age = age &#125;&#125; 可以使用如下三种方式构造对象:val p1 = new Person // 调用主构造器val p2 = new Person("Bob") // 调用第一个辅助构造器val p3 = new Person("Bob", 25) // 调用第二个辅助构造器 7. 主构造器在Scala中，每个类都有主构造器．主构造器并不以this方法定义，而是与类定义交织在一起． (1) 主构造器的参数直接放在类名之后class Person(val name:String) &#123; private var age = 0 def this (name : String, age : Int)&#123; this(name) // 调用主构造器 this.age = age &#125;&#125; 主构造器的参数被编译成字段，其值被初始化成构造时传入的参数．上述示例中name和age为Person类的字段． (2) 主构造器会执行类定义中的所有语句class Person(val name:String) &#123; println("constructed a person ...") private var age = 0 def this (name : String, age : Int)&#123; this(name) // 调用主构造器 this.age = age &#125;&#125; println语句是主构造器的一部分．每当有对象被构造出来时．上述代码就会被执行 (3) 通常可以在主构造器中使用默认参数来避免使用过多的辅助构造器class Person(val name:String = "", val age: Int = 0) &#123;&#125; 备注 如果类名之后没有参数，则该类具备一个无参主构造器. 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之Map与Tuple]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-map-and-tuple%2F</url>
    <content type="text"><![CDATA[1. 构造映射可以使用如下命令构造一个映射:scala&gt; val scores = Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 89, Bob -&gt; 98) 上面构造了一个不可变的Map[String, Int]，其值也不能被改变．如果想要一个可变映射，使用如下命令创建:scala&gt; val scores = scala.collection.mutable.Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.mutable.Map[String,Int] = Map(Bob -&gt; 98, Alice -&gt; 90, Kim -&gt; 89) 如果只想创建一个空的映射:scala&gt; val scores = new scala.collection.mutable.HashMap[String, Int]scores: scala.collection.mutable.HashMap[String,Int] = Map() 从上面我们可以知道使用-&gt;操作符来创建映射的键值对元素&quot;Alice&quot; -&gt; 90 我们也可以使用下面的方式定义映射:scala&gt; val scores = Map(("Alice",90), ("Kim",89), ("Bob",98))scores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 89, Bob -&gt; 98) 2. 获取映射中的值可以使用()来查找某个键对应的值:scala&gt; val bobscores = scores("Bob")bobscores: Int = 98 如果映射中并不包含对应键的值，则会抛出异常，这与Java返回null不同:scala&gt; val tomScores = scores("Tom")java.util.NoSuchElementException: key not found: Tom at scala.collection.MapLike$class.default(MapLike.scala:228) at scala.collection.AbstractMap.default(Map.scala:59) at scala.collection.MapLike$class.apply(MapLike.scala:141) at scala.collection.AbstractMap.apply(Map.scala:59) ... 32 elided 所以在获取某个键对应的值之前，要先检查映射中是否存在指定的键:scala&gt; val tomScores = if(scores.contains("Tom")) scores("Tom") else 0tomScores: Int = 0 以下是一个快捷写法:scala&gt; val tomScores = scores.getOrElse("Tom", 0)tomScores: Int = 0 3. 更新映射中的值在可变映射中，可以更新某个映射的值，也可以添加一个新的键值对:scala&gt; val scores = scala.collection.mutable.Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.mutable.Map[String,Int] = Map(Bob -&gt; 98, Alice -&gt; 90, Kim -&gt; 89)scala&gt; scores("Alice")=100 // 更新键值对scala&gt; scores("Tom")=67 // 添加键值对scala&gt; println(scores)Map(Bob -&gt; 98, Tom -&gt; 67, Alice -&gt; 100, Kim -&gt; 89) 还可以使用+=操作符来添加多个关系:scala&gt; scores += ("Bob" -&gt; 78, "Fred" -&gt; 89)res3: scores.type = Map(Bob -&gt; 78, Fred -&gt; 89, Tom -&gt; 67, Alice -&gt; 100, Kim -&gt; 89) 还可以使用-=操作符移除某个键对应的值:scala&gt; scores -= "Tom"res4: scores.type = Map(Bob -&gt; 78, Fred -&gt; 89, Alice -&gt; 100, Kim -&gt; 89) 虽然不可以更新一个不可变的映射，但是我们利用一些操作产生一个新的映射，并可以对原映射中的键值对进行修改或者添加新的键值对:scala&gt; val scores = Map("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 89, Bob -&gt; 98)scala&gt; val newScores = scores + ("Kim" -&gt; 78, "Tom" -&gt; 54)newScores: scala.collection.immutable.Map[String,Int] = Map(Alice -&gt; 90, Kim -&gt; 78, Bob -&gt; 98, Tom -&gt; 54) 上例中scores是不可变映射，我们在它基础上对”Kim”进行了修改，添加了”Tom”，产生了一个新的映射newScores 4. 迭代映射可以使用如下命令迭代映射:scala&gt; for( key &lt;- scores.keySet ) println(key + "---" + scores(key))Alice---90Kim---89Bob---98 或者scala&gt; for( value &lt;- scores.values ) println(value)908998 5. 排序映射在操作映射时，我们需要选定一个映射(哈希表还是平衡树)．默认情况下，scala给的是哈希表．有时候我们想对键进行一个排序，顺序访问键，这就需要一个树形映射:scala&gt; val scores = scala.collection.immutable.SortedMap("Alice" -&gt; 90, "Kim" -&gt; 89, "Bob"-&gt; 98)scores: scala.collection.immutable.SortedMap[String,Int] = Map(Alice -&gt; 90, Bob -&gt; 98, Kim -&gt; 89) 6. 与Java互操作如果你有一个Java映射，想要转换为Scala映射，以便便捷的使用Scala映射的方法，只需要增加如下语句:import scala.collection.JavaConversions.mapAsScalaMap 然后指定Scala映射类型来触发转换:scala&gt; val scores : scala.collection.mutable.Map[String,Int] = new java.util.TreeMap[String, Int]scores: scala.collection.mutable.Map[String,Int] = Map() 还可以将java.util.Properties到Map[String, String]的转换:scala&gt; import scala.collection.JavaConversions.propertiesAsScalaMapimport scala.collection.JavaConversions.propertiesAsScalaMapscala&gt; val props : scala.collection.Map[String, String] = System.getProperties()props: scala.collection.Map[String,String] =Map(env.emacs -&gt; "", java.runtime.name -&gt; Java(TM) SE Runtime Environment, sun.boot.library.path -&gt; /home/xiaosi/opt/jdk-1.8.0/jre/lib/amd64, java.vm.version -&gt; 25.91-b14, java.vm.vendor -&gt; Oracle Corporation, ... 相反，如果想要把Scal映射转换为Java映射，只需要提供相反的隐式转换即可:scala&gt; import scala.collection.JavaConversions.mapAsJavaMapimport scala.collection.JavaConversions.mapAsJavaMapscala&gt; import java.awt.font.TextAttribute._ // 引入下面的映射会用到的键import java.awt.font.TextAttribute._scala&gt; val attrs = Map(FAMILY -&gt; "Serif", SIZE -&gt; 12) // Scala映射attrs: scala.collection.immutable.Map[java.awt.font.TextAttribute,Any] = Map(java.awt.font.TextAttribute(family) -&gt; Serif, java.awt.font.TextAttribute(size) -&gt; 12)scala&gt; val font = new java.awt.Font(attrs) // Java映射font: java.awt.Font = java.awt.Font[family=Serif,name=Serif,style=plain,size=12] 7. 元组Tuple元组是不同类型的值的聚合，元组的值通过将单个的值包含在圆括号中构成的：scala&gt; val bobScore = (1, 98.5, "Bob")bobScore: (Int, Double, String) = (1,98.5,Bob) 可以使用方法_1，_2，_3访问其组员:scala&gt; val bobScore = (1, 98.5, "Bob")bobScore: (Int, Double, String) = (1,98.5,Bob)scala&gt; bobScore._1res10: Int = 1scala&gt; bobScore._3res11: String = Bob 通常，使用模式匹配的方式来获取元组的组元:scala&gt; val (id, score, name) = bobScore // 将变量id赋值为1，变量score赋值为98.5，变量name赋值为Bob val bobScore: (Int, Double, String)scala&gt; val (id, score, name) = bobScoreid: Int = 1score: Double = 98.5name: String = Bobscala&gt; println("name = " + name + ", score = " + score + ", name = " + name)name = Bob, score = 98.5, name = Bob 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之数组]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-array%2F</url>
    <content type="text"><![CDATA[1. 定长数组如果你需要一个长度不变的数组，可以使用Scala中的 Array．val nums = new Array[Int](10) // 10个整数的数组 所有元素初始化为0val strs = new Array[String](10) // 10个字符串的数组 所有元素初始化为nullval s = Array("Hello", "World") // 长度为2的Array[String] 类型是推断出来的 s(0) = "GoodBye" // Array("GoodBye"，"World") 备注 已提供初始值时不要使用new，例如上面的数组s 使用()而不是[]来访问元素 在JVM中，Scala的Array以Java数组方式实现． 2. 变长数组对于那种长度按需要变化的数组，Java有 ArrayList．Scala中等效数据结构为 ArrayBuffer．import scala.collection.mutable.ArrayBufferval b = ArrayBuffer[Int]() // 或者 new ArrayBuffer[Int]() 创建一个空的数组缓冲来存放整数b += 1 // ArrayBuffer(1) 用+=在尾端添加元素b += (1,2,3,5) // ArrayBuffer(1,1,2,3,5) 在尾端添加多个元素b ++= Array(8, 13, 21) // ArrayBuffer(1,1,2,3,5,8,13,21) 用++=操作追加任何集合b.trimEnd(5) // ArrayBuffer(1,1,2) 移除最后５个元素 可以在任意位置插入或移除元素，但这样的操作不如在尾端添加或移除元素操作那么高效:b.insert(2,6) // ArrayBuffer(1,1,6,2) 在下标2之前插入b.insert(2,7,8,9) // ArrayBuffer(1,1,7,8,9,6,2) 插入任意多的元素b.remove(2) // ArrayBuffer(1,1,8,9,6,2) 删除下标２的元素b.remove(2,3) // ArrayBuffer(1,1,2) 第二个参数的含义是要移除多少个元素 有时需要构建一个Array，但不知道最终需要装多少元素．这种情况下可以先构建一个数组缓冲，然后调用:b.toArray // Array(1,1,2) 3. 遍历数组和数组缓冲使用for循环遍历数组和数组缓冲:val b = Array(6,5,4,3,2,1)for(i &lt;- 0 until b.length)&#123; println(i + "-" + b(i))&#125; 输出结果:0-61-52-43-34-25-1 备注 until 是 RichInt 类的方法，返回所有小于(但不包括)上限的数字 如果想要每两个元素一跳，可以让i这样来进行遍历:val b = Array(6,5,4,3,2,1)for(i &lt;- 0 until (b.length, 2))&#123; println(i + "-" + b(i))&#125; 输出结果：0-62-44-2 如果要从数组的尾端开始:val b = Array(6,5,4,3,2,1)for(i &lt;- (0 until b.length).reverse)&#123; println(i + "-" + b(i))&#125; 如果在循环体中不需要用到数组下标，我们也可以直接访问数组元素:for(elem &lt;- b)&#123; println(elem)&#125; 4. 数组转换从一个数组(数组缓冲)出发，以某种方式对它进行转换是很简单的．这些转换操作不会修改原是数组，而是产生一个全新的数组:val a = Array(1,2,3,4)val result = for(elem &lt;- a) yield 2 * elem // result 是Array(2,4,6,8) for(...) yield循环创建了一个类型与原实际和相同的新集合．新元素为yield之后的表达式的值，每次迭代对应一个． 当你遍历一个集合时，如果只想处理满足特定条件的元素．可以通过for中的if来实现:val a = Array(1,2,3,4)val result = for(elem &lt;- a if elem % 2 == 0) yield 2 * elem 上面实例中对每个偶数元素翻倍，并丢掉奇数元素． 5. 常用操作5.1 sumval a = Array(6,1,7,4)a.sum // 18 要使用sum方法，元素类型必须是数值类型:整型，浮点数或者BigInteger/BigDecimal 5.2 min maxval a = Array(6,1,7,4)a.min // 1a.max // 7 min和max输出数组或数组缓冲中最小和最大的元素 5.3 sortedval a = Array(6,1,7,4)val asorted = a.sorted // Array(1, 4, 6, 7)val a = ArrayBuffer(6,1,7,4)val asorted = a.sortWith(_ &gt; _) // ArrayBuffer(7, 6, 4, 1) sorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，不会修改原始数组．可以使用sortWith方法提供一个比较函数． 5.4 mkStringval a = Array(6,1,7,4)a.mkString(" and ") // 6 and 1 and 7 and 4 如果想要显示数组或者数组缓冲的内容，可以使用mkString，允许指定元素之间的分隔符val a = Array(6,1,7,4)a.mkString("&lt;", ",", "&gt;") // &lt;6,1,7,4&gt; 该方法的另一个重载版本可以让你指定前缀和后缀 来源于: 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 学习笔记之基础语法]]></title>
    <url>%2F2018%2F02%2F27%2Fscala-notes-basis%2F</url>
    <content type="text"><![CDATA[1. 变量val定义的值实际上是一个常亮，无法改变其内容scala&gt; val num = 0num: Int = 0scala&gt; num = 2&lt;console&gt;:12: error: reassignment to val num = 2 ^ 如果要声明其值可变的变量，可以使用varscala&gt; var number = 0number: Int = 0scala&gt; number = 2number: Int = 2 在Scala中，建议使用val，除非你真的需要改变它的内容． 备注 不需要给出值或者变量的类型，可以从你用来初始化它的表达式推断出来．只声明值或者变量但不做初始化会报错：scala&gt; val str: String&lt;console&gt;:11: error: only classes can have declared but undefined members val str: String ^scala&gt; val str: String = "Hello"str: String = Hello 2. 常用类型常用类型： Byte Char Short Int Long Float Double Boolean 跟Java不同的是，这些类型是类．Scala并不刻意区分基本类型和引用类型．你可以对数字执行方法：scala&gt; 1.toString()res2: String = 1 3. 条件表达式Scala的 if/else 的语法结构和Java的一样．不过，在Scala中 if/else 表达式有值，这个值就是跟在 if 或 else 之后的表达式的值:if(x &gt; 0) 1 else -1 上述表达式的值是１或者-1，具体是哪一个取决于x的值．你可以将 if/else 表达式的值赋值给变量：val s = if(x &gt; 0) 1 else -1 等同于:if(x &gt; 0) s = 1 else s = -1 相对于第二种写法，第一种写法更好一些，因为它可以用来初始化一个val，而第二种写法当中，s必须是var． 备注 Scala中每个表达式都有一个类型scala&gt; val s = if(x &gt; 0) "positive" else -1;s: Any = positive 上述表达式的类型是两个分支类型的公共超类型．在这个例子中，其中一个分支是java.lang.String，而另一个分支是Int．它们的公共超类型是Any．if(x &gt; 0) 1 那么有可能if语句没有输出值．但是在Scala中，每个表达式都应该有某种值．这个问题的解决方案是引入一个 Unit 类，写作 ()．不带 else 的这个 if 语句等同于:if(x &gt; 0) 1 else () 4. 循环Scala拥有与Java和C++相同的while和do循环：while(n &gt; 2)&#123; println("num-&gt;" + n) n = n -1&#125; 但是Scala没有与for(初始化变量;检查变量是否满足某条件;更新变量)循环直接对应的结构．如果你需要这样的循环，有两个选择：一是选择while循环，二是使用如下for语句:for(i &lt;- 1 to n)&#123; println("num-&gt;" + i)&#125; 上述表达式的目标是让变量i遍历&lt;-右边的表达式的所有值．至于如何遍历，则取决于表达式的类型． 遍历字符串或者数组时，你通常需要使用从0到n-1的区间．这个时候你可以使用util方法而不是to方法．util方法返回一个并不包含上限的区间:val s = "Hello"for(i &lt;- 0 until s.length)&#123; println(i + " = " + s(i))&#125; 或者for(ch &lt;- "Hello")&#123; println(ch)&#125; 5. 函数要定义函数，需要给出函数的名称，参数和函数体:def abs (x: Double) = if (x &gt;= 0) x else -x 必须给出所有参数的类型，只要函数不是递归的，就可以不需要指定返回类型．Scala编译器可以通过=符号右侧的表达式的类型推断出返回类型．如果函数体需要多个表达式完成，可以使用代码块．块中最后一个表达式的值就是函数的返回值:def fac(n: Int) = &#123; var r = 1 for(i &lt;- 1 to n)&#123; r = r * i &#125; r&#125; 上例中函数返回值为r的值 备注 虽然在函数中使用 return 并没有什么不对，我们还是最好适应没有 return 的日子．之后，我们会使用大量的匿名函数，这些函数中 return 并不返回值给调用者．它跳出到包含它的函数中．我们可以把 return 当做是函数版的 break 语句，仅在需要时使用． 对于递归函数，我们必须指定返回类型：def fac(n: Int) : Int = if(n &lt; 0) 1 else n * fac(n-1) 6. 默认参数和带名参数我们在调用某些函数时并不显示的给出所有参数值，对于这些函数我们可以使用默认参数：def decorate (str : String, left : String = "[" , right : String = "]") &#123; left + str + right&#125; 这个函数带有两个参数，left 和 right，带有默认值 [ 和 ]:decorate("Hello") // [Hello]decorate("Hello", "&lt;", "&gt;") // &lt;Hello&gt; 你可以在提供参数值的时候指定参数名(带名参数)：decorate(left = "&lt;&lt;", str = "Hello", right = "&gt;&gt;") // &lt;&lt;Hello&gt;&gt; 你可以混用未命名参数和带名参数，只要那些未命名的参数是排在前面即可:decorate("Hello", right = "]###") // 实际调用 decorate("Hello", "[", "]###") 备注 带名参数并不需要跟参数列表的顺序完全一致 7. 变长参数可以实现一个接受可变长度参数列表的函数:def sum(args : Int *) = &#123; var result = 0 for(arg &lt;- args)&#123; result += arg &#125; result&#125; 可以使用任意多的参数来调用该函数:val result = sum(4, 5, 1) // 10 8. 过程Scala对于不返回值的函数有特殊的表示法．如果函数体包含在花括号当中但没有前面的=符号，那么返回类型就是Unit，这样的函数被称为过程:def welcome(str : String) &#123; println("welcome " + str)&#125; 或者显示声明Unit返回类型:def welcome(str : String) : Unit = &#123; println("welcome " + str)&#125; 9. 懒值当val被声明为lazy时，它的初始化将被推迟，直到我们首次对它取值:lazy val words = scala.io.Source.fromFile("/usr/share/dict/words").mkString 如果程序从不访问words，那么文件也不会被打开． 懒值对于开销较大的初始化语句而言十分有用． 备注 懒值并不是没有额外的开销．我们每次访问懒值，都会有一个方法被调用，而这个方法将会以线程安全的方式检查该值是否已被初始化． 10. 异常Scala的异常工作机制跟Java一样．当你抛出异常时:throw new IllegalArgumentException("x should not be negative") 当前的运算被终止，运行时系统查找可以接受 IllegalArgumentException 的异常处理器．控制权将在离抛出点最近的处理器中恢复．如果没有找到符合要求的异常处理器，则程序退出． 和Java一样，抛出的对象必须是 java.lang.Throwable 的子类．不过，与Java不同的是，Scala没有”受检”异常，你不需要声明函数或者方法可能会抛出某种异常． throw 表达式有特殊的类型Nothing．这在if/else表达式中很有用．如果一个分支的类型是Nothing，那么 if/else 表达式的类型就是另一个分支的类型:if (x &gt; 0) &#123; sqrt(x)&#125;else&#123; throw new IllegalArgumentException("x should not be negative")&#125; 第一个分支的类型是Double，第二个分支的类型是Nothing，因此if/else表达式的类型是Double 捕获异常的语法采用的是模式匹配的语法:try&#123; process(new URL("Http://hortsman.com/fred-tiny.gif"))&#125;catch &#123; case _: MalformedURLException =&gt; println ("Bad URL:" + url) case ex: IOException =&gt; ex.printStackTrace()&#125; 与Java一样，更通用的异常应该排在更具体的异常之后． try/finally 语句可以释放资源，不论有没有异常发生:var in = new URL("").openStream()try&#123; process (in)&#125;finally &#123; in.close()&#125; 来源于： 快学Scala]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 处理背压]]></title>
    <url>%2F2018%2F02%2F11%2Fhow-flink-handles-backpressure%2F</url>
    <content type="text"><![CDATA[人们经常会问Flink是如何处理背压(backpressure)效应的。 答案很简单：Flink不使用任何复杂的机制，因为它不需要任何处理机制。它只凭借数据流引擎，就可以从容地应对背压。在这篇博文中，我们介绍一下背压。然后，我们深入了解 Flink 运行时如何在任务之间传送缓冲区中的数据，并展示流数传输自然双倍下降的背压机制(how streaming data shipping naturally doubles down as a backpressure mechanism)。 我们最终通过一个小实验展示了这一点。 1. 什么是背压像Flink这样的流处理系统需要能够从容地处理背压。背压是指系统在一个临时负载峰值期间接收数据的速率大于其处理速率的一种场景(备注:就是处理速度慢，接收速度快，系统处理不了接收的数据)。许多日常情况都会导致背压。例如，垃圾回收卡顿可能导致流入的数据堆积起来，或者数据源可能出现发送数据过快的峰值。如果处理不当，背压会导致资源耗尽，甚至导致数据丢失。 让我们看一个简单的例子。 假设一个数据流管道包含一个数据源，一个流作业和一个接收器sink，它稳定的以每秒500万个元素的速度处理数据，如下所示(一个黑条代表100万个元素，下图是系统某一秒的快照)： 在某些时候，流处理作业或sink有1秒的卡顿，导致500多万个元素的堆积。或者，数据源可能出现了一个峰值，在一秒内以双倍的速度产生数据。 我们如何处理这样的情况(如上数据源出现一个峰值，一秒内以双倍的速度产生数据)呢？ 当然，可以放弃这些元素(一秒内只能处理一半的数据)。但是，对于许多按Exactly One处理语义处理记录的流式应用程序来说，数据丢失是不可接受的。额外的数据可以缓存在某个地方。缓存也应该是可持久化的，因为在失败的情况下，这些数据需要被重新读取以防止数据丢失。理想情况下，这些数据应该被缓存在一个持久化的通道中(例如，如果数据源自己能保证持久性，Apache Kafka 就是这样的一种数据源)。理想状态下应对背压的措施是将整个管道从 sink 回压到数据源，并对源头进行限流，以将速度调整到管道最慢部分的速度，从而达到稳定状态: 2. Flink中的背压Flink运行时的构建组件是算子和流。每个算子消费中间数据流，并对其进行转换，并产生新的数据流。描述这种机制的最好比喻是Flink充分使用有界容量的分布式阻塞队列。与 Java 连接线程的常规阻塞队列一样，一旦队列的有效缓冲耗尽(有界容量)，较慢的接收者就会使发送者放慢发送速度。 以两个任务之间的简单流程为例，说明 Flink 如何实现背压： (1) 记录 A 进入Flink并由任务1处理。 (2) 记录被序列化在缓冲区， (3) 缓冲区输送到任务2中，然后任务2从缓冲区中读取记录。 为了使记录通过Flink进行处理，缓冲区必须是可用的。在Flink中，这些分布式队列被认为是逻辑数据流，通过生产流和消费流管理的缓冲池来实现有界容量。缓冲池是缓冲区的集合，它们在使用后会被回收。总体思路很简单：从缓冲池中取出一个缓冲区，填充数据，在数据消耗完后，将缓冲区放回缓冲池中，之后还可以再次使用它。 缓冲池的大小在运行时会动态变化。网络堆栈中的内存缓冲区的数量(=队列的容量)决定了系统在不同发送/接收速度可以进行的缓冲量。Flink保证始终有足够的缓冲区来进行进程处理(enough buffers to make some progress)，但是这个进程的速度取决于用户程序和可用内存的数量。更多的内存意味着系统可以轻松地缓冲一定的瞬时背压(短时间段，短 GC)。越少的内存意味着需要对背压进行直接响应(没有足够的缓冲区进行缓存，只能响应处理)。 从上面的简单例子可以看出：在任务1输出端和任务2输入端都有一个与其关联的缓冲池。如果有一个可用于序列化 A 的缓冲区，我们将其序列化并分配缓冲区。 我们在这里有必要看两个case： (1) 本地交换：如果任务1和任务2在同一个工作节点(TaskManager)上运行，缓冲区可以直接交给下一个任务。一旦任务2消费完，它就会被回收。如果任务2比任务1慢，则缓冲区将以低于任务1填充的速度进行回收，从而导致任务1速度变慢。 (2) 远程交换：如果任务1和任务2在不同的工作节点上运行，缓冲区一旦发送到线路中(TCP通道)就可以被回收。在接收端，数据从线路复制到输入缓冲池的缓冲区。如果没有缓冲区可用，从TCP连接读取操作将被中断。输出端通过一个简单的 watermark 机制保证不会在线上放置太多的数据。如果有足够的数据处在可发送状态，我们会一直复制更多的数据到线路中直到低于某个阈值时。这保证了没有太多的数据在传输途中。如果接收端没有消费新的数据(因为没有缓冲区可用)，这会减慢发送方的速度。 这种简单的在固定大小缓冲池之间的缓冲区流使Flink能够拥有一个强大的背压机制，在这种机制下，任务生产数据速度不会比消费的快。 我们描述的两个任务之间的数据传输的机制可以自然的推广到复杂管道上，保证背压在整个管道内传播。 让我们看看一个简单的实验，展示了Flink在背压情况下的行为。我们运行一个简单的生产者-消费者流式拓扑，其中任务在本地交换数据，在这里我们可以变换任务产生记录的速度。对于这个测试，我们使用比默认更少的内存，以使得背压效果更明显。我们使用每个任务有2个大小为4096字节的缓冲区。在通常的Flink部署中，任务将具有更大更多缓冲区，这会提高性能。这个测试在单个JVM中运行，但使用完整的Flink代码堆栈。 图中显示了生产者任务(黄色)和消费者任务(绿色)随着时间变化所达到的最大吞吐量(单个JVM中每秒达到800万个元素)的平均吞吐量占比(average throughput as a percentage of the maximum attained throughput of the producing (yellow) and consuming (green) tasks)。为了衡量平均吞吐量，我们每5秒测量一次任务处理的记录数量。 首先，我们以60％的速度运行生产任务(我们通过调用Thread.sleep()来模拟减速)。消费者以相同的速度处理数据，不会产生延迟。然后我们把消费者任务放慢到全速的30％。在这里，背压效果产生作用，因为我们看到生产者也自然放缓到全速的30％。然后，我们取消消费者任务的人为减速，并且这两项任务都达到最大吞吐量。我们再次把消费者任务放慢到全速的30％，管道立即响应，生产者任务也全速下降到30％。最后，我们再次停止减速，两项任务都以100％的速度持续下去。总而言之，我们看到生产者和消费者在管道上相互跟随彼此的吞吐量，这是我们在流水线中期望的行为。 3. 结论Flink与像Kafka这样的可持久化数据源，让你可以立即响应处理背压而不会丢失数据。Flink不需要专门的机制来处理背压，因为data shipping in Flink doubles as a backpressure mechanism。 因此，Flink实现了管道最慢部分允许的最大吞吐量。 原文:https://data-artisans.com/blog/how-flink-handles-backpressure]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 四种优化Flink应用程序的方法]]></title>
    <url>%2F2018%2F02%2F11%2Ffour-ways-to-optimize-your-flink-applications%2F</url>
    <content type="text"><![CDATA[Flink 是一个复杂的框架，并提供了许多方法来调整其执行。在本文中，我将展示四种不同的方法来提高 Flink 应用程序的性能。如果你不熟悉 Flink，你可以阅读其他介绍性的文章，比如这个，这个和这个。如果你已经熟悉 Apache Flink，本文将帮助你更快地创建应用程序。 1. 使用 Flink tuples当你使用像 groupBy，join 或 keyBy 这样的操作时， Flink 提供了多种方式在数据集中选择key。你可以使用 key 选择器函数：// Join movies and ratings datasetsmovies.join(ratings) // Use movie id as a key in both cases .where(new KeySelector&lt;Movie, String&gt;() &#123; @Override public String getKey(Movie m) throws Exception &#123; return m.getId(); &#125; &#125;) .equalTo(new KeySelector&lt;Rating, String&gt;() &#123; @Override public String getKey(Rating r) throws Exception &#123; return r.getMovieId(); &#125; &#125;) 或者你可以在 POJO 类型中指定一个字段名称：movies.join(ratings) // Use same fields as in the previous example .where("id") .equalTo("movieId") 但是，如果你正在使用 Flink tuple 类型，你可以简单地指定将要作为 key 的字段在元组中的位置：DataSet&lt;Tuple2&lt;String, String&gt;&gt; movies ...DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; ratings ...movies.join(ratings) // Specify fields positions in tuples .where(0) .equalTo(1) 最后一种方式会给你最好的性能，但可读性呢？ 这是否意味着你的代码现在看起来像这样：DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; result = movies.join(ratings) .where(0) .equalTo(0) .with(new JoinFunction&lt;Tuple2&lt;Integer,String&gt;, Tuple2&lt;Integer,Double&gt;, Tuple3&lt;Integer, String, Double&gt;&gt;() &#123; // What is happening here? @Override public Tuple3&lt;Integer, String, Double&gt; join(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second) throws Exception &#123; // Some tuples are joined with some other tuples and some fields are returned??? return new Tuple3&lt;&gt;(first.f0, first.f1, second.f1); &#125; &#125;); 在这种情况下，提高可读性的常见方法是创建一个继承自 TupleX 类的类，并为这些字段实现 getter 和 setter 方法。在这里，下面是 Flink Gelly 库的 Edge 类的大体实现，具有三个字段并继承了 Tuple3 类：public class Edge&lt;K, V&gt; extends Tuple3&lt;K, K, V&gt; &#123; public Edge(K source, K target, V value) &#123; this.f0 = source; this.f1 = target; this.f2 = value; &#125; // Getters and setters for readability public void setSource(K source) &#123; this.f0 = source; &#125; public K getSource() &#123; return this.f0; &#125; // Also has getters and setters for other fields ...&#125; 2. 重用 Flink对象另一个可以用来提高 Flink 应用程序性能的方法是当你从自定义函数中返回数据时使用可变对象。看看这个例子：stream .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123; @Override public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123; long changesCount // A new Tuple instance is created on every execution collector.collect(new Tuple2&lt;&gt;(userName, changesCount)); &#125; &#125; 正如你所看到的，在 apply 函数的每次执行中，我们都创建一个 Tuple2 类型的实例，这会给垃圾收集器造成很大压力。解决这个问题的一种方法是重复使用同一个实例：stream .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123; // Create an instance that we will reuse on every call private Tuple2&lt;String, Long&gt; result = new Tuple&lt;&gt;(); @Override public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123; long changesCount = ... // Set fields on an existing object instead of creating a new one result.f0 = userName; // Auto-boxing!! A new Long value may be created result.f1 = changesCount; // Reuse the same Tuple2 object collector.collect(result); &#125; &#125; 上述代码会更好些。虽然我们在每次调用的时候只创建了一个 Tuple2 实例，但是我们还是间接地创建了 Long 类型的实例。为了解决这个问题， Flink 提供了很多的值类（value classes），IntValue, LongValue, StringValue, FloatValue 等。这些类的目的是为内置类型提供可变版本，所以我们可以在用户自定义函数中重用这些类型，下面就是如何使用的例子：stream .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123; // Create a mutable count instance private LongValue count = new IntValue(); // Assign mutable count to the tuple private Tuple2&lt;String, LongValue&gt; result = new Tuple&lt;&gt;(&quot;&quot;, count); @Override // Notice that now we have a different return type public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, LongValue&gt;&gt; collector) throws Exception &#123; long changesCount = ... // Set fields on an existing object instead of creating a new one result.f0 = userName; // Update mutable count value count.setValue(changesCount); // Reuse the same tuple and the same LongValue instance collector.collect(result); &#125; &#125; 上面这些使用习惯在 Flink 类库中被普遍使用，比如 Flink Gelly。 3. 使用函数注解优化 Flink 应用程序的另一种方法是提供关于用户自定义函数对输入数据做什么的一些信息。由于 Flink 无法解析和理解代码，因此你可以提供关键信息，这将有助于构建更高效的执行计划。有三个注解我们可以使用： @ForwardedFields - 指定输入值中的哪些字段保持不变并在输出值中使用。 @NotForwardedFields - 指定在输出中同一位置不保留的字段。 @ReadFields - 指定用于计算结果值的字段。你只能指定那些在计算中使用的字段，而不是仅仅将数据拷贝到输出中的字段。 我们来看看如何使用 ForwardedFields 注解：// Specify that the first element is copied without any changes@ForwardedFields("0")class MyFunction implements MapFunction&lt;Tuple2&lt;Long, Double&gt;, Tuple2&lt;Long, Double&gt;&gt; &#123; @Override public Tuple2&lt;Long, Double&gt; map(Tuple2&lt;Long, Double&gt; value) &#123; // Copy first field without change return new Tuple2&lt;&gt;(value.f0, value.f1 + 123); &#125;&#125; 上述代码意味着输入元组的第一个元素将不会改变，并且在返回时也处于同一个位置（译者注：第一个位置）。 如果你不改变字段，只是简单地将它移到不同的位置上，你同样可以使用 ForwardedFields 注解来实现。下面例子中，我们简单地将输入元组的字段进行交换（译者注：第一个字段移到第二个位置，第二个字段移到第一个位置）：// 1st element goes into the 2nd position, and 2nd element goes into the 1st position@ForwardedFields("0-&gt;1; 1-&gt;0")class SwapArguments implements MapFunction&lt;Tuple2&lt;Long, Double&gt;, Tuple2&lt;Double, Long&gt;&gt; &#123; @Override public Tuple2&lt;Double, Long&gt; map(Tuple2&lt;Long, Double&gt; value) &#123; // Swap elements in a tuple return new Tuple2&lt;&gt;(value.f1, value.f0); &#125;&#125; 上面例子中提到的注解只能应用到只有一个输入参数的函数中，比如 map 或者 flatMap。如果你有两个输入参数的函数，你可以使用 ForwardedFieldsFirst 和 ForwardedFieldsSecond 注解分别为第一和第二个参数提供信息。 下面我们看一下如何在 JoinFunction 接口的实现中使用这些注解（译者注：第一个输入元组的两个字段拷贝到输出元组的第一个和第二个位置，第二个输入元组的第二个字段拷贝到输出元组的第三个位置）：// Two fields from the input tuple are copied to the first and second positions of the output tuple@ForwardedFieldsFirst("0; 1")// The third field from the input tuple is copied to the third position of the output tuple@ForwardedFieldsSecond("2")class MyJoin implements JoinFunction&lt;Tuple2&lt;Integer,String&gt;, Tuple2&lt;Integer,Double&gt;, Tuple3&lt;Integer, String, Double&gt;&gt;() &#123; @Override public Tuple3&lt;Integer, String, Double&gt; join(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second) throws Exception &#123; return new Tuple3&lt;&gt;(first.f0, first.f1, second.f1); &#125;&#125;) Flink 同样提供了 NotForwardedFieldsFirst, NotForwardedFieldsSecond, ReadFieldsFirst, 和 ReadFirldsSecond 注解来实现同样的功能。 4. 选择 join 类型如果你告诉 Flink 一些信息，可以加快 join 的速度，但在讨论它为什么会起作用之前，让我们先来谈谈 Flink 是如何执行 join的。 当 Flink 处理批量数据时，集群中的每台机器都存储了部分数据。要执行 join 操作，Flink 需要找到两个两个数据集中满足 join 条件的所有记录对（译者注：key 相同的数据）。 要做到这一点，Flink 首先必须将两个数据集中具有相同 key 的数据放在集群中的同一台机器上。有两种策略： Repartition-Repartition 策略：在这种场景下，根据它们的 key 对两个数据集进行重新分区，通过网络发送数据。这就意味着如果数据集非常大，这将花费大量的时间将数据在网络之间进行复制。 Broadcast-Forward 策略：在这种场景下，一个数据集保持不变，将第二个数据集拷贝到集群上第二个数据集拥有第一个数据集部分数据的所有机器上（译者注：将达尔戈数据集进行分发到对应机器上）。 如果用一个较大的数据集与一个小数据集进行 join，你可以使用 Broadcast-Forward 策略并避免对第一个数据集进行重分区的昂贵代价。这很容易做到：ds1.join(ds2, JoinHint.BROADCAST_HASH_FIRST) 这表示第一个数据集比第二个数据集小得多。 Flink 支持的其他 join 提示有以下几种： BROADCAST_HASH_SECOND – 表示第二个数据集比较小 REPARTITION_HASH_FIRST – 表示第一个数据集比较小 REPARTITION_HASH_SECOND – 表示第二个数据集有点小 REPARTITION_SORT_MERGE – 表示对两个数据集重新分区并使用排序和合并策略 OPTIMIZER_CHOOSES – Flink 优化器将决定如何连接数据集 原文： https://brewing.codes/2017/10/17/flink-optimize/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 垃圾收集器]]></title>
    <url>%2F2018%2F02%2F10%2Fjvm-common-garbage-collector%2F</url>
    <content type="text"><![CDATA[本文“垃圾收集器”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。Java 虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。下面讨论的是基于 JDK 1.7 Update 14 之后的 HotSpot 虚拟机。这个虚拟机包含的所有收集器如下图所示： 图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。下面会介绍这些收集器的特性，基本原理和使用场景。 1. Serial 收集器Serial 收集器是最基本，发展历史最悠久的收集器，曾经（JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，只会使用一个CPU 或一条收集线程去完成垃圾收集工作。重要的是，它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。在用户不可见的情况下把用户正常工作的线程全部停掉，这对应用程序来说都是很难接受的。但是虚拟机的设计者表示完全理解，却又表示很无奈。举个例子来说：你妈妈给你打扫房间的时候，肯定也会让你老老实实的在椅子或房间外待着，如果她一边打扫卫生，你一边乱扔垃圾，这房间还嫩打扫完吗？ Serial 收集器看起来是一个鸡肋收集器，但是到现在为止，它依然是虚拟机运行在 Client 模式下的默认新生代收集器。这是因为它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说， 该收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集的效率。 Serial 收集器在新生代收集时采用复制算法。 2. ParNew 收集器ParNew 收集器其实是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 Serial收集器可用的所有控制参数，收集算法，Stop The World，对象分配规则，回收策略等都与 Serial 收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。 ParNew 收集器除了多线程收集之外，其他与 Serial收集器相比并没有太多的创新之处，但它却是许多运行在 Server 模型下的虚拟机首选的新生代收集器。其中一个与性能无关但是很重要的原因是，除了 Serial收集器之外，目前只有它能与 CMS 收集器配合工作。在 JDK 1.5 HotSpot推出了一款强交互应用中划时代的垃圾收集器 - CMS 收集器 （Concurrent Mark Sweep）。这款收集器是 HotSpot 虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。不幸的是 CMS 收集器作为老年代的收集器，却无法与 JDK 1.4.0中已经存在的 Parallel Scavenge收集器配合工作，所以在 JDK 1.5 中使用 CMS 来收集老年代的时候，新生代智能选择 ParNew 或者 Serial收集器。 ParNew 收集器在单 CPU 的环境中绝对不会有比 Serial 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个 CPU 的环境中都不能百分百的保证可以超越 Serial 收集器。 随着可以使用的 CPU 的数量的增加，它对于 GC 时系统资源的有效利用还是很有好处的。 与 Serial 收集器一样，也是在新生代收集时采用复制算法。 3. Parallel Scavenge 收集器Parallel Scavenge 收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去跟 ParNew 收集器一样。 Parallel Scavenge 收集器的特点是它的关注点与其他的收集器不同， CMS 等收集器的关注点是尽可能的缩短垃圾收集时的用户线程的停顿时间， 而 Parallel Scavenge 收集器的目标则是达到一个可控制的吞吐量。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。由于与吞吐量关系密切，该收集器也经常成为 吞吐量优先 收集器。Parallel Scavenge 收集器可以通过设置 -XX:+UseAdaptivSizePolicy 参数开启 GC 自适应调节策略，不需要手工指定新生代的大小(-Xmn)，Eden与Survivor区的比例(-XX:SurvivorRatio)，晋升老年代对象年龄等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。自适应调节策略也是Parallel Scavenge 收集器与 ParNew收集器的一个重要区别。 备注:吞吐量 = 运行用户代码的时间 / （运行用户代码的时间 + 垃圾收集的时间）Example:虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，则吞吐量为99% 4. Serial Old 收集器Serial Old 收集器是 Serial 收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是在于 Client 模式下的虚拟机使用。如果在 Server 模式下，那么它主要有两大用途： 一种用途是在 JDK1.5 以及之前的版本中与 Parallel Scavenge 收集器搭配使用， 另一种用途是作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器Parallel Old 收集器是 Parallel Scavenge 收集器的老年代版本，使用多线程和 标记-整理算法。这个收集器是在 JDK 1.6 中才开始提供的。在此之前新生代 Parallel Scavenge 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 Parallel Scavenge 收集器，老年代除了 Serial Old收集器外别无选择。由于老年代 Serial Old 收集器在服务端应用性能上 拖累，使用了 Parallel Scavenge 收集器也未必能在整体应用上获得吞吐量最大化的效果。由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew 加 CMS 的组合给力。 直到 Parallel Old 收集器出现后。吞吐量优先 收集器终于有了名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器CMS (Concurrent Mark Sweep) 收集器是一种以获得最短回收停顿时间为目标的收集器。CMS 收集器非常符合重视响应时间速度以及希望系统停顿时间最短的应用。从名字上就可以看出，CMS 收集器是基于 标记-清除 算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些。整个过程可以分为4个步骤： 初始标记 并发标记 重新标记 并发清除 初始标记与重新标记这两个步骤仍然需要 Stop The World。 初始标记仅仅是标记一下 GC Roots 能直接关联到的对象，速度很快，并发标记是进行 GC Roots Tracing 的过程，而重新标记则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发比较的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS 收集器的内存回收过程是与用户线程一起并发执行的。 CMS 是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。但是 CMS 收集器还远达不到完美的程度，它有以下三个明显的缺点： (1) CMS 收集器对CPU资源非常敏感 其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS 默认启动的回收线程数是 （CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程多于 25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS 对用户程序的影响就可能变得很大。 (2) CMS 收集器无法处理浮动垃圾 CMS 收集器无法处理浮动垃圾，可能出现 Concurrent Mode Failure 失败而导致另一次 Full GC 的产生。由于 CMS 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS 无法在当次收集中处理掉它们，只好留待下一次 GC 时再清理掉。这一部分垃圾就称为 浮动垃圾。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此 CMS 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 JDK 1.5 默认设置下，CMS 收集器当老年代使用了 68% 的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高触发百发比。在 JDK1.6 中， CMS收集器的启动阈值已经提升至 92%。运行期间预留的内存无法满足程序需要，就会出现一次 Concurrent Mode Failure 失败，这时虚拟机将启动后备预案：临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说，如果启动阈值设置的太高很容易导致大量这样的失败，性能反而会降低。 (3) CMS 收集器会产生大量空间碎片 CMS 是一款基于 标记—清除 算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器G1（Garbage-First）是一款面向服务端应用的垃圾收集器。HotSpot 开发团队赋予它的使命是未来可以替换掉 JDK 1.5 中发布的 CMS 收集器。与其他 GC 收集器相比，G1 具备如下特点： (1) 并行与并发。G1 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短 Stop-The-World 停顿的时间，部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 Java 程序继续执行。 (2) 分代收集。与其他收集器一样，分代概念在 G1 中依然得以保留。虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 GC 的旧对象以获取更好的收集效果。 (3) 空间整合。与 CMS 的 标记—清理 算法不同，G1 从整体来看是基于 标记—整理 算法实现的收集器，从局部（两个 Region 之间）上来看是基于 复制 算法实现的，但无论如何，这两种算法都意味着 G1 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。 (4) 可预测的停顿。这是 G1 相对于 CMS 的另一大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 在 G1 之前的其他收集器进行收集的范围都是整个新生代或者老年代，而 G1 不再是这样。使用 G1 收集器时，Java 堆的内存布局就与其他收集器有很大差别，它将整个 Java 堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 Region（不需要连续）的集合。 G1 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 Java 堆中进行全区域的垃圾收集。G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region（这也就是 Garbage-First 名称的来由）。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率。 如果不计算维护 Remembered Set的操作， G1 收集器的运作大致可划分为以下几个步骤： (1) 初始标记。初始标记阶段仅仅只是标记一下 GC Roots 能直接关联到的对象，并且修改 TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 Region 中创建新对象，这阶段需要停顿线程，但耗时很短。 (2) 并发标记。并发标记阶段是从 GC Root 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 (3) 最终标记。最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中，这阶段需要停顿线程，但是可并行执行。 (4) 筛选回收。筛选回收阶段首先对各个 Region 的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 垃圾收集算法]]></title>
    <url>%2F2018%2F02%2F04%2Fjvm-garbage-collection-algorithm%2F</url>
    <content type="text"><![CDATA[本文“垃圾收集算法”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】 由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本节不打算过多地讨论算法的实现，只是介绍几种算法的思想及其发展过程。 1. 标记-清除算法最基础的收集算法是 标记-清除 （Mark-Sweep）算法，如它的名字一样，算法分为 标记 和 清除 两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个： 一个是效率问题，标记和清除过程的效率都不高； 另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-清除算法的执行过程如下图所示： 2. 复制算法为了解决效率问题，一种称为 复制 （Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。复制算法的执行过程如下图所示： 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 的专门研究表明，新生代中的对象 98% 是朝生夕死的，所以并不需要按照 1∶1 的比例来划分内存空间，而是将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中的一块 Survivor。当回收时，将 Eden 和 Survivor 中还存活着的对象一次性地拷贝到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才用过的 Survivor 的空间。HotSpot 虚拟机默认 Eden 和 Survivor 的大小比例是 8∶1，也就是每次新生代中可用内存空间为整个新生代容量的 90%（80%+10%），只有 10% 的内存是会被 浪费 的。当然，98% 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在 98% 的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块 Survivor 空间没有足够的空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。关于对新生代进行分配担保的内容，本章稍后在讲解垃圾收集器执行规则时还会再详细讲解。 3. 标记-整理算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 50% 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 100% 存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种 标记-整理 （Mark-Compact）算法，标记过程仍然与 标记-清除 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，标记-整理 算法的示意图如下图所示: 4. 分代收集算法当前商业虚拟机的垃圾收集都采用 分代收集 （Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 标记-清理 或 标记-整理 算法来进行回收。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 堆内内存与堆外内存]]></title>
    <url>%2F2018%2F02%2F04%2Fjava-on-off-heap-memory%2F</url>
    <content type="text"><![CDATA[一般情况下，Java 中分配的非空对象都是由 Java 虚拟机的垃圾收集器管理的，也称为堆内内存（on-heap memory）。虚拟机会定期对垃圾内存进行回收，在某些特定的时间点，它会进行一次彻底的回收（full gc）。彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对 Java 应用造成的影响，跟堆的大小是成正比的。过大的堆会影响 Java 应用的性能。 对于这个问题，一种解决方案就是使用堆外内存（off-heap memory）。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。 但是 Java 本身也在不断对堆内内存的实现方式做改进。两者各有什么优缺点？ Vanilla Java 博客作者 Peter Lawrey 撰写了一篇文章，在文中他对三种方式：用new来分配对象、对象池（object pool）和堆外内存，进行了详细的分析。 用new来分配对象内存是最基本的一种方式，Lawery提到： 在Java 5.0之前，分配对象的代价很大，以至于大家都使用内存池。但是从5.0开始，对象分配和垃圾回收变得快多了，研发人员发现了性能的提升，纷纷简化他们的代码，不再使用内存池，而直接用new来分配对象。从5.0开始，只有一些分配代价较大的对象，比如线程、套接字和数据库链接，用内存池才会有明显的性能提升。 对于内存池，Lawery认为它主要用于两类对象。第一类是生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能。第二种情况是加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。对此，Lawery还以 StringInterner 为例进行了说明。 最后Lawery分析了堆外内存，它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。堆外内存有以下特点： 对于大内存有良好的伸缩性 对垃圾回收停顿的改善可以明显感觉到 在进程间可以共享，减少虚拟机间的复制 Lawery还提到堆外内存最重要的还不是它能改进性能，而是它的确定性。 当然堆外内存也有它自己的问题，最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行序列化（serialization），而序列化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。 Lawery还介绍了OpenHFT公司提供三个开源库：Chronicle Queue、Chronicle Map和 Thread Affinity，这些库可以帮助开发人员使用堆外内存来保存数据。采用堆外内存有很多好处，同时也带来挑战，对堆外内存感兴趣的读者可以阅读Lawery的原文来了解更多信息。 转载于： http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache 缓存更新策略]]></title>
    <url>%2F2018%2F02%2F03%2Fcache-update-policy%2F</url>
    <content type="text"><![CDATA[看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是 错误 的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新策略。 这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 更新缓存的的策略有四种： Cache aside Read through Write through Write behind caching 我们下面一一来看一下这四种策略。 1. Cache aside这是最常用最常用的策略了。其具体逻辑如下： 失效：应用程序先从 cache 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从 cache 中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 这是标准的策略，包括Facebook的论文《Scaling Memcache at Facebook》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》，主要是怕两个并发的写操作导致脏数据。 那么，是不是 Cache Aside 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。 但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 2. Read/Write Through我们可以看到，在上面的 Cache Aside 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而 Read/Write Through 套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 Cache。 2.1 Read ThroughRead Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside 是由调用方负责把数据加载入缓存，而 Read Through 则用缓存服务自己来加载，从而对应用方是透明的。 2.2 Write ThroughWrite Through 套路和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 Cache 自己更新数据库（这是一个同步操作） 下图自来 Wikipedia 的 Cache) 词条。其中的 Memory 你可以理解为就是我们例子里的数据库。 3. Write Behind CachingWrite Behind 又叫 Write Back。一些了解 Linux 操作系统内核的同学对 write back 应该非常熟悉，这不就是 Linux 文件系统的 Page Cache 的算法吗？ 是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。 Write Back 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write back 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 Unix/Linux 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 Trade-Off。 另外，Write Back 实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的 write back 会在仅当这个 cache 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 lazy write。 在 wikipedia 上有一张 write back 的流程图，基本逻辑如下： 转载于： https://coolshell.cn/articles/17416.html]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>CaChe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 动态表的持续查询]]></title>
    <url>%2F2018%2F02%2F02%2Fflink-sql-persistent-query-of-dynamic-table%2F</url>
    <content type="text"><![CDATA[越来越多的公司采用流处理，并将现有的批处理应用迁移到流处理，或者对新的用例采用流处理实现的解决方案。其中许多应用集中在流数据分析上，分析的数据流来自各种源，例如数据库事务、点击、传感器测量或IoT 设备。 Apache Flink 非常适用于流分析应用程序，因为它支持事件时间语义，确保只处理一次，以及同时实现了高吞吐量和低延迟。因为这些特性，Flink 能够近实时对大量的输入数据计算出一个确定和精确的结果，并且在发生故障的时候提供一次性语义。 Flink 的核心流处理API，DataStream API，非常具有表现力，并且为许多常见操作提供了原语。在其他特性中，它提供了高度可定制的窗口逻辑，不同表现特征下的不同状态原语，注册和响应定时器的钩子，以及高效的异步请求外部系统的工具。另一方面，许多流分析应用遵循相似的模式，并不需要DataStream API 提供的表现力级别。他们可以使用领域特定的语言来使用更自然和简洁的方式表达。总所周知，SQL 是数据分析的事实标准。对于流分析，SQL 可以让更多的人在数据流的特定应用中花费更少的时间。然而，目前还没有开源的流处理器提供令人满意的SQL 支持。 1. 为什么流中的 SQL 很重要SQL 是数据分析使用最广泛的语言，有很多原因： SQL 是声明式的：你指定你想要的东西，而不是如何去计算； SQL 可以进行有效的优化：优化器计估算有效的计划来计算结果； SQL 可以进行有效的评估：处理引擎准确的知道计算内容，以及如何有效的执行； 最后，所有人都知道的，许多工具都理解SQL。 因此，使用SQL 处理和分析数据流，可以为更多人提供流处理技术。此外，因为SQL 的声明性质和潜在的自动优化，它可以大大减少定义高效流分析应用的时间和精力。 但是，SQL（以及关系数据模型和代数）并不是为流数据设计的。关系是（多）集合而不是无限序列的元组。当执行SQL 查询时，传统数据库系统和查询引擎读取和处理完整的可用数据集，并产生固定大小的结果。相比之下，数据流持续提供新的记录，使数据随着时间到达。因此，流查询需要不断的处理到达的数据，从来都不是“完整的”。 话虽如此，使用SQL 处理流并不是不可能的。一些关系型数据库系统维护了物化视图，类似于在流数据中评估SQL 查询。物化视图被定义为一个SQL 查询，就像常规（虚拟）视图一样。但是，查询的结果实际上被保存（或者是物化）在内存或硬盘中，这样视图在查询时不需要实时计算。为了防止物化视图的数据过时，数据库系统需要在其基础关系（定义的SQL 查询引用的表）被修改时更新更新视图。如果我们将视图的基础关系修改视作修改流（或者是更改日志流），物化视图的维护和流中的SQL 的关系就变得很明确了。 2. Flink 的关系API：Table API 和SQL从1.1.0版本（2016年8月发布）以来，Flink 提供了两个语义相当的关系API，语言内嵌的Table API（用于Java 和Scala）以及标准SQL。这两种API 被设计用于在线流和遗留的批处理数据API 的统一，这意味着无论输入是静态批处理数据还是流数据，查询产生完全相同的结果。 统一流和批处理的API 非常重要。首先，用户只需要学习一个API 来处理静态和流数据。此外，可以使用同样的查询来分析批处理和流数据，这样可以在同一个查询里面同时分析历史和在线数据。在目前的状况下，我们尚未完全实现批处理和流式语义的统一，但社区在这个目标上取得了很大的进展。 下面的代码片段展示了两个等效的Table API 和SQL 查询，用来在温度传感器测量数据流中计算一个简单的窗口聚合。SQL 查询的语法基于Apache Calcite 的分组窗口函数样式，并将在Flink 1.3.0版本中得到支持。 val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val tEnv = TableEnvironment.getTableEnvironment(env)// define a table source to read sensor data (sensorId, time, room, temp)val sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...// register the table sourcetEnv.registerTableSource(&quot;sensors&quot;, sensorTable)// Table APIval tapiResult: Table = tEnv.scan(&quot;sensors&quot;) // scan sensors table .window(Tumble over 1.hour on &apos;rowtime as &apos;w) // define 1-hour window .groupBy(&apos;w, &apos;room) // group by window and room .select(&apos;room, &apos;w.end, &apos;temp.avg as &apos;avgTemp) // compute average temperature// SQLval sqlResult: Table = tEnv.sql(&quot;&quot;&quot; |SELECT room, TUMBLE_END(rowtime, INTERVAL &apos;1&apos; HOUR), AVG(temp) AS avgTemp |FROM sensors |GROUP BY TUMBLE(rowtime, INTERVAL &apos;1&apos; HOUR), room |&quot;&quot;&quot;.stripMargin) 就像你看到的，两种API 以及Flink 主要的的DataStream 和DataSet API 是紧密结合的。Table 可以和DataSet 或DataStream 相互转换。因此，可以很简单的去扫描一个外部的表，例如数据库或者是Parquet 文件，使用Table API 查询做一些预处理，将结果转换为DataSet，并对其运行Gelly 图形算法。上述示例中定义的查询也可以通过更改执行环境来处理批量数据。 在内部，两种API 都被转换成相同的逻辑表示，由Apache Calcite 进行优化，并被编译成DataStream 或是DataSet 程序。实际上，优化和转换程序并不知道查询是通过Table API 还是SQL 来定义的。如果你对优化过程的细节感兴趣，可以看看我们去年发布的一篇博客文章。由于Table API 和SQL 在语义方面等同，只是在样式上有些区别，在这篇文章中当我们谈论SQL 时我们通常引用这两种API。 在当前的1.2.0版本中，Flink 的关系API 在数据流中，支持有限的关系操作，包括投影、过滤和窗口聚合。所有支持的操作有一个共同点，就是它们永远不会更新已经产生的结果记录。这对于时间记录操作，例如投影和过滤显然不是问题。但是，它会影响收集和处理多条记录的操作，例如窗口聚合。由于产生的结果不能被更新，在Flink 1.2.0中，输入的记录在产生结果之后不得不被丢弃。 当前版本的限制对于将产生的数据发往Kafka 主题、消息队列或者是文件这些存储系统的应用是可以被接受的，因为它们只支持追加操作，没有更新和删除。遵循这种模式的常见用例是持续的ETL 和流存档应用，将流进行持久化存档，或者是准备数据用于进一步的在线（流）或者是离线分析。由于不可能更新之前产生的结果，这一类应用必须确保产生的结果是正确的，并且将来不需要更正。下图说明了这样的应用。 虽然只支持追加查询对有些类型的应用和存储系统有用，但是还是有一些流分析的用例需要更新结果。这些流应用包括不能丢弃延迟到达的记录，需要早期的结果用于（长期运行）窗口聚合，或者是需要非窗口的聚合。在每种情况下，之前产生的结果记录都需要被更新。结果更新查询通常将其结果保存在外部数据库或者是键值存储，使其可以让外部应用访问或者是查询。实现这种模式的应用有仪表板、报告应用或者是其他的应用，它们需要及时的访问持续更新的结果。下图说明了这一类应用 3. 动态表的持续查询支持查询更新之前产生的结果是Flink 的关系API 的下一个重要步骤。这个功能非常重要，因为它大大增加了API 支持的用例的范围和种类。此外，一些新的用例可以采用DataStream API 来实现。 因此，当添加对结果更新查询的支持时，我们必须保留之前的流和批处理输入的语义。我们通过动态表的概念来实现。动态表是持续更新，并且能够像常规的静态表一样查询的表。但是，与批处理表查询终止后返回一个静态表作为结果不同的是，动态表中的查询会持续运行，并根据输入表的修改产生一个持续更新的表。因此，结果表也是动态的。这个概念非常类似我们之前讨论的物化视图的维护。 假设我们可以在动态表中运行查询并产生一个新的动态表，那会带来一个问题，流和动态表如何相互关联？答案是流和动态表可以相互转换。下图展示了在流中处理关系查询的概念模型。 首先，流被转换为动态表，动态表使用一个持续查询进行查询，产生一个新的动态表。最后，结果表被转换成流。要注意，这个只是逻辑模型，并不意味着查询是如何实际执行的。实际上，持续查询在内部被转换成传统的DataStream 程序。 随后，我们描述了这个模型的不同步骤： 在流中定义动态表 查询动态表 生成动态表 3.1 在流中定义动态表评估动态表上的SQL 查询的第一步是在流中定义一个动态表。这意味着我们必须指定流中的记录如何修改动态表。流携带的记录必须具有映射到表的关系模式的模式。在流中定义动态表有两种模式：附加模式和更新模式。 在附加模式中，流中的每条记录是对动态表的插入修改。因此，流中的所有记录都附加到动态表中，使得它的大小不断增长并且无限大。下图说明了附加模式。 在更新模式中，流中的记录可以作为动态表的插入、更新或者删除修改（附加模式实际上是一种特殊的更新模式）。当在流中通过更新模式定义一个动态表时，我们可以在表中指定一个唯一的键属性。在这种情况下，更新和删除操作会带着键属性一起执行。更新模式如下图所示。 3.2 查询动态表一旦我们定义了动态表，我们可以在上面运行查询。由于动态表随着时间进行改变，我们必须定义查询动态表的意义。假定我们有一个特定时间的动态表的快照，这个快照可以作为一个标准的静态批处理表。我们将动态表A 在点t 的快照表示为A[t]，可以使用人意的SQL 查询来查询快照，该查询产生了一个标准的静态表作为结果，我们把在时间t 对动态表A 做的查询q 的结果表示为q(A[t])。如果我们反复在动态表的快照上计算查询结果，以获取进度时间点，我们将获得许多静态结果表，它们随着时间的推移而改变，并且有效的构成一个动态表。我们在动态表的查询中定义如下语义。 查询q 在动态表A 上产生了一个动态表R，它在每个时间点t 等价于在A[t]上执行q 的结果，即R[t]=q(A[t])。该定义意味着在批处理表和流表上执行相同的查询q 会产生相同的结果。在下面的例子中，我们给出了两个例子来说明动态表查询的语义。 在下图中，我们看到左侧的动态输入表A，定义成追加模式。在时间t=8时，A 由6行（标记成蓝色）组成。在时间t=9 和t=12 时，有一行追加到A（分别用绿色和橙色标记）。我们在表A 上运行一个如图中间所示的简单查询，这个查询根据属性k 分组，并统计每组的记录数。在右侧我们看到了t=8（蓝色），t=9（绿色）和t=12（橙色）时查询q 的结果。在每个时间点t，结果表等价于在时间t 时再动态表A 上执行批查询。 这个例子中的查询是一个简单的分组（但是没有窗口）聚合查询。因此，结果表的大小依赖于输入表的分组键的数量。此外，值得注意的是，这个查询会持续更新之前产生的结果行，而不只是添加新行。 第二个例子展示了一个类似的查询，但是有一个很重要的差异。除了对属性k 分组以外，查询还将记录每5秒钟分组为一个滚动窗口，这意味着它每5秒钟计算一次k 的总数。再一次的，我们使用Calcite 的分组窗口函数来指定这个查询。在图的左侧，我们看到输入表A ，以及它在附加模式下随着时间而改变。在右侧，我们看到结果表，以及它随着时间演变。 与第一个例子的结果不同的是，这个结果表随着时间增长，例如每5秒钟计算出新的结果行（考虑到输入表在过去5秒收到更多的记录）。虽然非窗口查询（主要是）更新结果表的行，但是窗口聚合查询只追加新行到结果表中。 虽然这篇博客专注于动态表的SQL 查询的语义，而不是如何有效的处理这样的查询，但是我们要指出的是，无论输入表什么时候更新，都不可能计算查询的完整结果。相反，查询编译成流应用，根据输入的变化持续更新它的结果。这意味着不是所有的有效SQL 都支持，只有那些持续性的、递增的和高效计算的被支持。我们计划在后续的博客文章中讨论关于评估动态表的SQL 查询的详细内容。 3.3 生成动态表查询动态表生成的动态表，其相当于查询结果。根据查询和它的输入表，结果表会通过插入、更新和删除持续更改，就像普通的数据表一样。它可能是一个不断被更新的单行表，一个只插入不更新的表，或者介于两者之间。 传统的数据库系统在故障和复制的时候，通过日志重建表。有一些不同的日志技术，比如UNDO、REDO和UNDO/REDO日志。简而言之，UNDO 日志记录被修改元素之前的值来回滚不完整的事务，REDO 日志记录元素修改的新值来重做已完成事务丢失的改变，UNDO/REDO 日志同时记录了被修改元素的旧值和新值来撤销未完成的事务，并重做已完成事务丢失的改变。基于这些日志技术的原理，动态表可以转换成两类更改日志流：REDO 流和REDO+UNDO 流。 通过将表中的修改转换为流消息，动态表被转换为redo+undo 流。插入修改生成一条新行的插入消息，删除修改生成一条旧行的删除消息，更新修改生成一条旧行的删除消息以及一条新行的插入消息。行为如下图所示。 左侧显示了一个维护在附加模式下的动态表，作为中间查询的输入。查询的结果转换为显示在底部的redo+undo 流。输入表的第一条记录(1,A)作为结果表的一条新纪录，因此插入了一条消息+(A,1)到流中。第二条输入记录k=‘A’(4,A)导致了结果表中 (A,1)记录的更新，从而产生了一条删除消息-(A,1)和一条插入消息+(A,2)。所有的下游操作或数据汇总都需要能够正确处理这两种类型的消息。 在两种情况下，动态表会转换成redo 流：要么它只是一个附加表（即只有插入修改），要么它有一个唯一的键属性。动态表上的每一个插入修改会产生一条新行的插入消息到redo 流。由于redo 流的限制，只有带有唯一键的表能够进行更新和删除修改。如果一个键从动态表中删除，要么是因为行被删除，要么是因为行的键属性值被修改了，所以一条带有被移除键的删除消息发送到redo 流。更新修改生成带有更新的更新消息，比如新行。由于删除和更新修改根据唯一键来定义，下游操作需要能够根据键来访问之前的值。下图展示了如何将上述相同查询的结果表转换为redo 流。 插入到动态表的(1,A)产生了+(A,1)插入消息。产生更新的(4,A)生成了*(A,2)的更新消息。 Redo 流的通常做法是将查询结果写到仅附加的存储系统，比如滚动文件或者Kafka 主题，或者是基于键访问的数据存储，比如Cassandra、关系型DBMS以及压缩的Kafka 主题。还可以实现将动态表作为流应用的关键的内嵌部分，来评价持续查询和对外部系统的查询能力，例如一个仪表盘应用。 3.4 切换到动态表发生的改变在1.2版本中，Flink 关系API 的所有流操作，例如过滤和分组窗口聚合，只会产生新行，并且不能更新先前发布的结果。 相比之下，动态表能够处理更新和删除修改。 现在你可能会问自己，当前版本的处理模式如何与新的动态表模型相关？ API 的语义会完全改变，我们需要从头开始重新实现API，以达到所需的语义？ 所有这些问题的答案很简单。当前的处理模型是动态表模型的一个子集。 使用我们在这篇文章中介绍的术语，当前的模型通过附加模式将流转换为动态表，即一个无限增长的表。 由于所有操作仅接受插入更改并在其结果表上生成插入更改（即，产生新行），因此所有在动态附加表上已经支持的查询，将使用重做模型转换回DataStreams，仅用于附加表。 因此，当前模型的语义被新的动态表模型完全覆盖和保留。 4. 结论与展望Flink 的关系API 在任何时候都非常适合用于流分析应用，并在不同的生产环境中使用。在这篇博文中，我们讨论了Table API 和SQL 的未来。 这一努力将使Flink 和流处理更易于访问。 此外，用于查询历史和实时数据的统一语义以及查询和维护动态表的概念，将能够显着简化许多令人兴奋的用例和应用程序的实现。 由于这篇文章专注于流和动态表的关系查询的语义，我们没有讨论查询执行的细节，包括内部执行撤销，处理后期事件，支持结果预览，以及边界空间要求。 我们计划在稍后的时间点发布有关此主题的后续博客文章。 近几个月来，Flink 社区的许多成员一直在讨论和贡献关系API。 到目前为止，我们取得了很大的进步。 虽然大多数工作都专注于以附加模式处理流，但是日程上的下一步是处理动态表以支持更新其结果的查询。 如果您对使用SQL处理流程的想法感到兴奋，并希望为此做出贡献，请提供反馈，加入邮件列表中的讨论或获取JIRA 问题。 译文: http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table 原文: http://flink.apache.org/news/2017/04/04/dynamic-tables.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 脱离JVM？ Hadoop生态圈的挣扎与演化]]></title>
    <url>%2F2018%2F02%2F02%2Fhadoop-ecosystem-break-away-jvm%2F</url>
    <content type="text"><![CDATA[新世纪以来，互联网及个人终端的普及，传统行业的信息化及物联网的发展等产业变化产生了大量的数据，远远超出了单台机器能够处理的范围，分布式存储与处理成为唯一的选项。从2005年开始，Hadoop从最初Nutch项目的一部分，逐步发展成为目前最流行的大数据处理平台。Hadoop生态圈的各个项目，围绕着大数据的存储，计算，分析，展示，安全等各个方面，构建了一个完整的大数据生态系统，并有Cloudera，HortonWorks，MapR等数十家公司基于开源的Hadoop平台构建自己的商业模式，可以认为是最近十年来最成功的开源社区。 Hadoop 的成功固然是由于其顺应了新世纪以来互联网技术的发展趋势，同时其基于JVM的平台开发也为Hadoop的快速发展起到了促进作用。Hadoop生态圈的项目大都基于Java，Scala，Clojure等JVM语言开发，这些语言良好的语法规范，丰富的第三方类库以及完善的工具支持，为Hadoop这样的超大型项目提供了基础支撑。同时，作为在程序员中普及率最高的语言之一，它也降低了更多程序员使用，或是参与开发Hadoop项目的门槛。同时，基于Scala开发的Spark，甚至因为项目的火热反过来极大的促进了Scala语言的推广。但是随着Hadoop平台的逐步发展，Hadoop生态圈的项目之间的竞争加剧，越来越多的Hadoop项目注意到了这些JVM语言的一些不足之处，希望通过更有效率的处理方式，提升分布式系统的执行效率与健壮性。本文主要以Spark和Flink项目为例，介绍Hadoop社区观察到的一些因为JVM语言的不足导致的问题，以及相应的解决方案与未来可能的发展方向。 –more– 注：本文假设读者对Java和Hadoop系统有基本了解。 1. 背景目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，分布式计算引擎项目之间的竞争也相当激烈。MapReduce作为Hadoop平台的第一个分布式计算引擎，具有非常良好的可扩展性，Yahoo曾成功的搭建了上万台节点的MapReduce系统。但是MapReduce只支持Map和Reduce编程范式，使得复杂数据计算逻辑需要分割为多个Hadoop Job，而每个Hadoop Job都需要从HDFS读取数据，并将Job执行结果写回HDFS，所以会产生大量额外的IO开销，目前MapReduce正在逐渐被其他三个分布式计算引擎替代。Tez,Spark和Flink都支持图结构的分布式计算流，可在同一Job内支持任意复杂逻辑的计算流。Tez的抽象层次较低，用户不易直接使用，Spark与Flink都提供了抽象的分布式数据集以及可在数据集上使用的操作符，用户可以像操作Scala数据集合类似的方式在Spark/FLink中的操作分布式数据集，非常的容易上手，同时，Spark与Flink都在分布式计算引擎之上，提供了针对SQL，流处理，机器学习和图计算等特定数据处理领域的库。 随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了JVM本身，希望通过解决JVM本身带来的一些问题，提高分布式系统的性能或是健壮性，从而增强自身的竞争力。 JVM本身作为一个各种类型应用执行的平台，其对Java对象的管理也是基于通用的处理策略，其垃圾回收器通过估算Java对象的生命周期对Java对象进行有效率的管理。针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的JVM参数更有效率的管理Java对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身以及JVM的各参数有深入的了解，极大的提高了分布式计算平台的调优门槛（例如这篇文章中对Spark的调优 Tuning Java Garbage Collection for Spark Applications）。然而类似Spark或是Flink的分布式计算框架，框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的Java对象生命周期，从而为更有效率的管理Java对象提供了可能。 2. JVM存在的问题2.1. Java对象开销相对于c/c++等更加接近底层的语言，Java对象的存储密度相对偏低，例如【1】，“abcd”这样简单的字符串在UTF-8编码中需要4个字节存储，但Java采用UTF-16编码存储字符串，需要8个字节存储“abcd”，同时Java对象还对象header等其他额外信息，一个4字节字符串对象，在Java中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率的内存存储，则意味着CPU数据访问吞吐量更高，以及更少的磁盘落地可能。 2.2. 对象存储结构引发的cache miss为了缓解CPU处理速度与内存访问速度的差距【2】，现代CPU数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以cache line为单位加载数据，所以当CPU访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果CPU要访问的数据不在当前缓存所有的cache line中，则需要从内存中加载对应的数据，这被称为一次cache miss。当cache miss非常高的时候，CPU大部分的时间都在等待数据加载，而不是真正的处理数据。Java对象并不是连续的存储在内存上，同时很多的Java数据结构的数据聚集性也不好，在Spark的性能调优中，经常能够观测到大量的cache miss。Java社区有个项目叫做Project Valhalla，可能会部分的解决这个问题，有兴趣的可以看看这儿 OpenJDK: Valhalla。 2.3. 大数据的垃圾回收Java的垃圾回收机制，一直让Java开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面，垃圾回收也是Java应用的一颗不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大的影响了Java应用的性能和可用性。在当前的数据中心中，大容量的内存得到了广泛的应用，甚至出现了单台机器配置TB内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换，清洗，处理等步骤。在这个过程中，会产生海量的Java对象，JVM的垃圾回收执行效率对性能有很大影响。通过JVM参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及JVM的各参数有深入的了解，而且有时候这也远远不够。 2.4. OOM问题OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会出现OutOfMemoryError错误，JVM崩溃，分布式框架的健壮性和性能都会受到影响。通过JVM管理内存，同时试图解决OOM问题的应用，通常都需要检查Java对象的大小，并在某些存储Java对象特别多的数据结构中设置阈值进行控制。但是JVM并没有提供官方的检查Java对象大小的工具，第三方的工具类库可能无法准确通用的确定Java对象的大小【6】。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外的业务逻辑无关的代码。 3. 解决方案为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术： (1) 定制的序列化工具。显式内存管理的前提步骤就是序列化，将Java对象序列化成二进制数据存储在内存上（on heap或是off-heap）。通用的序列化框架，如Java默认的java.io.Serializable将Java对象以及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于Spark或是Flink这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如Hadoop的org.apache.hadoop.io.Writable，需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。 (2) 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有的内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收的压力。 (3) 缓存友好的数据结构和算法。只将操作相关的数据连续存储，可以最大化的利用L1/L2/L3缓存，减少Cache miss的概率，提升CPU计算的吞吐量。以排序为例，由于排序的主要操作是对Key进行对比，如果将所有排序数据的Key与Value分开，对Key连续存储，则访问Key时的Cache命中率会大大提高。 3.1 定制的序列化工具分布式计算框架可以使用定制序列化工具的前提是要处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。Spark与Flink数据集都支持任意Java或是Scala类型，通过自动生成定制序列化工具，Spark与Flink既保证了API接口对用户的友好度（不用像Hadoop那样数据类型需要继承实现org.apache.hadoop.io.Writable接口），同时也达到了和Hadoop类似的序列化效率。 3.1.1 Spark的序列化框架Spark 支持通用的计算框架，如 Java Serialization和 Kryo。其缺点之前也略有论述，总结如下： 占用较多内存。Kryo相对于Java Serialization更高，它支持一种类型到Integer的映射机制，序列化时用Integer代替类型信息，但还不及定制的序列化工具效率。 反序列化时，必须反序列化整个Java对象。 无法直接操作序列化后的二进制数据。 Project Tungsten 提供了一种更好的解决方式，针对于DataFrame API（Spark针对结构化数据的类SQL分析API，参考 Spark DataFrame Blog），由于其数据集是有固定Schema的Tuple（可大概类比为数据库中的行），序列化是针对每个Tuple存储其类型信息以及其成员的类型信息是非常浪费内存的，对于Spark来说，Tuple类型信息是全局可知的，所以其定制的序列化工具只存储Tuple的数据，如下图所示 对于固定大小的成员，如int，long等，其按照偏移量直接内联存储。对于变长的成员，如String，其存储一个指针，指向真正的数据存储位置，并在数据存储开始处存储其长度。通过这种存储方式，保证了在反序列化时，当只需访问某一个成员时，只需根据偏移量反序列化这个成员，并不需要反序列化整个Tuple。 Project Tungsten 的定制序列化工具应用在 Sort，HashTable，Shuffle等很多对Spark性能影响最大的地方。比如在Shuffle阶段，定制序列化工具不仅提升了序列化的性能，而且减少了网络传输的数据量，根据DataBricks的Blog介绍，相对于Kryo，Shuffle800万复杂Tuple数据时，其性能至少提高2倍以上。此外，Project Tungsten也计划通过Code generation技术，自动生成序列化代码，将定制序列化工具推广到Spark Core层，从而使得更多的Spark应用受惠于此优化。 3.1.2 Flink的序列化框架Flink在系统设计之初，就借鉴了很多传统 RDBMS 的设计，其中之一就是对数据集的类型信息进行分析，对于特定 Schema 的数据集的处理过程，进行类似RDBMS执行计划优化的优化。同时，数据集的类型信息也可以用来设计定制的序列化工具。和Spark类似，Flink支持任意的Java或是Scala类型，Flink通过Java Reflection框架分析基于Java的Flink程序UDF(User Define Function)的返回类型的类型信息，通过Scala Compiler分析基于Scala的Flink程序UDF的返回类型的类型信息。类型信息由TypeInformation类表示，这个类有诸多具体实现类，例如（更多详情参考Flink官方博客 Apache Flink: Juggling with Bits and Bytes）： BasicTypeInfo: 任意Java基本类型（装包或未装包）和String类型。 BasicArrayTypeInfo: 任意Java基本类型数组（装包或未装包）和String数组。 WritableTypeInfo: 任意Hadoop’s Writable接口的实现类. TupleTypeInfo: 任意的Flink tuple类型(支持Tuple1 to Tuple25). Flink tuples是固定长度固定类型的Java Tuple实现。 CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples). PojoTypeInfo: 任意的POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是public修饰符定义，要么有getter/setter方法。 GenericTypeInfo: 任意无法匹配之前几种类型的类。） 前6种类型数据集几乎覆盖了绝大部分的Flink程序，针对前6种类型数据集，Flink皆可以自动生成对应的TypeSerializer定制序列化工具，非常有效率的对数据集进行序列化和反序列化。对于第7中类型，Flink使用Kryo进行序列化和反序列化。此外，对于可被用作Key的类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据直接进行compare，hash等之类的操作。对于Tuple，CaseClass，Pojo等组合类型，Flink自动生成的TypeSerializer，TypeComparator同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的TypeSerializer，TypeComparator，如下图所示： 此外，如有需要，用户可通过集成TypeInformation接口，定制实现自己的序列化工具。 3.2 显式的内存管理垃圾回收的JVM内存管理回避不了的问题，JDK8的G1算法改善了JVM垃圾回收的效率和可用范围，但对于大数据处理的实际环境中，还是远远不够。这也和现在分布式框架的发展趋势有冲突，越来越多的分布式计算框架希望尽可能多的将待处理的数据集放在内存中，而对于JVM垃圾回收来说，内存中Java对象越少，存活时间越短，其效率越高。通过JVM进行内存管理的话，OutOfMemoryError也是一个很难解决的问题。同时，在JVM内存管理中，Java对象有潜在的碎片化存储问题（Java对象所有信息可能不是在内存中连续存储），也有可能在所有Java对象大小没有超过JVM分配内存时，出现OutOfMemoryError问题。 3.2.1 Flink的内存管理Flink将内存分为三个部分，每个部分都有不同的用途： Network buffers: 一些以32KB Byte数组为单位的buffer，主要被网络模块用于数据的网络传输。 Memory Manager pool: 大量以32KB Byte数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。 Remaining (Free) Heap: 主要留给UDF中用户自己创建的Java对象，由JVM管理。 Network buffers在Flink中主要基于Netty的网络传输，无需多讲。Remaining Heap用于UDF中用户自己创建的Java对象，在UDF中，用户通常是流式的处理数据，并不需要很多内存，同时Flink也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool（以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。 在Flink中，内存池由多个MemorySegment组成，每个MemorySegment代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment提供了根据偏移量访问数据的各种方法，如get/put int，long，float，double等，MemorySegment之间数据拷贝等方法，和java.nio.ByteBuffer类似。对于Flink的数据结构，通常包括多个向内存池申请的MemeorySegment，所有要存入的对象，通过TypeSerializer序列化之后，将二进制数据存储在MemorySegment中，在取出时，通过TypeSerializer反序列化。数据结构通过MemorySegment提供的set/get方法访问具体的二进制数据。 Flink这种看起来比较复杂的内存管理方式带来的好处主要有： 二进制的数据存储大大提高了数据存储密度，节省了存储空间。 所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。而对于大部分的分布式计算框架来说，这部分由于要缓存大量数据，是最有可能导致OOM的地方。 内存池虽然占据了大部分内存，但其中的MemorySegment容量较大(默认32KB)，所以内存池中的Java对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了JVM垃圾回收的压力。 Remaining Heap的内存虽然由JVM管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的Minor GC就会全部回收掉，一般不会触发Full GC。 Flink当前的内存管理在最底层是基于byte[]，所以数据最终还是on-heap，最近Flink增加了off-heap的内存管理支持，将会在下一个release中正式出现。Flink off-heap的内存管理相对于on-heap的优点主要在于（更多细节，请参考 Apache Flink: Off-heap Memory in Apache Flink and the curious JIT compiler）： 启动分配了大内存(例如100G)的JVM很耗费时间，垃圾回收也很慢。如果采用off-heap，剩下的Network buffer和Remaining heap都会很小，垃圾回收也不用考虑MemorySegment中的Java对象了。 更有效率的IO操作。在off-heap下，将MemorySegment写到磁盘或是网络，可以支持zeor-copy技术，而on-heap的话，则至少需要一次内存拷贝。 off-heap可用于错误恢复，比如JVM崩溃，在on-heap时，数据也随之丢失，但在off-heap下，off-heap的数据可能还在。此外，off-heap上的数据还可以和其他程序共享。 3.2.2 Spark的内存管理Spark的off-heap内存管理与Flink off-heap模式比较相似，也是通过Java UnSafe API直接访问off-heap内存，通过定制的序列化工具将序列化后的二进制数据存储与off-heap上，Spark的数据结构和算法直接访问和操作在off-heap上的二进制数据。Project Tungsten是一个正在进行中的项目，想了解具体进展可以访问：SPARK-7075 Project Tungsten (Spark 1.5 Phase 1)， SPARK-9697 Project Tungsten (Spark 1.6)。 3.3 缓存友好的计算磁盘IO和网络IO之前一直被认为是Hadoop系统的瓶颈，但是随着Spark，Flink等新一代的分布式计算框架的发展，越来越多的趋势使得CPU/Memory逐渐成为瓶颈，这些趋势包括： 更先进的IO硬件逐渐普及。10GB网络和SSD硬盘等已经被越来越多的数据中心使用。 更高效的存储格式。Parquet，ORC等列式存储被越来越多的Hadoop项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。 更高效的执行计划。例如Spark DataFrame的执行计划优化器的Fliter-Push-Down优化会将过滤条件尽可能的提前，甚至提前到Parquet的数据访问层，使得在很多实际的工作负载中，并不需要很多的磁盘IO。 由于CPU处理速度和内存访问速度的差距，提升CPU的处理效率的关键在于最大化的利用L1/L2/L3/Memory，减少任何不必要的Cache miss。定制的序列化工具给Spark和Flink提供了可能，通过定制的序列化工具，Spark和Flink访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时，尽量连续存储，减少内存碎片化对Cache命中率的影响，甚至更进一步，Spark与Flink可以将需要操作的部分数据（如排序时的Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能的提升Cache命中的概率。 3.3.1 Flink中的数据结构以Flink中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink通过特殊设计的排序算法，获得了非常好了性能，其排序算法的实现如下： 将待排序的数据经过序列化后存储在两个不同的MemorySegment集中。数据全部的序列化值存放于其中一个MemorySegment集中。数据序列化后的Key和指向第一个MemorySegment集中其值的指针存放于第二个MemorySegment集中。 对第二个MemorySegment集中的Key进行排序，如需交换Key位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。 排序完成后，访问数据时，按照第二个MemorySegment集中Key的顺序访问，并通过Pinter值找到数据在第一个MemorySegment集中的位置，通过TypeSerializer反序列化成Java对象返回。 这样实现的好处有： 通过Key和Full data分离存储的方式，尽量将被操作的数据最小化，提高Cache命中的概率，从而提高CPU的吞吐量。 移动数据时，只需移动Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。 TypeComparator直接基于二进制数据进行操作，节省了反序列化的时间。 3.3.2 Spark的数据结构Spark中基于off-heap的排序与Flink几乎一模一样，在这里就不多做介绍了，感兴趣的话，请参考：Project Tungsten: Bringing Apache Spark Closer to Bare Metal。 4. 总结本文主要介绍了Hadoop生态圈的一些项目遇到的一些因为JVM内存管理导致的问题，以及社区是如何应对的。基本上，以内存为中心的分布式计算框架，大都开始了部分脱离JVM，走上了自己管理内存的路线，Project Tungsten甚至更进一步，提出了通过LLVM，将部分逻辑编译成本地代码，从而更加深入的挖掘SIMD等CPU潜力。此外，除了Spark，Flink这样的分布式计算框架，HBase（HBASE-11425），HDFS（HDFS-7844）等项目也在部分性能相关的模块通过自己管理内存来规避JVM的一些缺陷，同时提升性能。 参考： project tungsten The “Memory Wall”: Modern Microprocessors flink memory management: Apache Flink: Juggling with Bits and Bytes java GC：Tuning Java Garbage Collection for Spark Applications Project Valhalla: OpenJDK: Valhalla java object size: dweiss/java-sizeof · GitHub Big Data Performance Engineering 原文: https://zhuanlan.zhihu.com/hadoop/20228397]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之资源调优]]></title>
    <url>%2F2018%2F02%2F02%2Fspark-performance-resources-tuning%2F</url>
    <content type="text"><![CDATA[1. 前言在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解资源调优。 2. 资源调优2.1 调优概述在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 2.2 Spark作业基本运行原理 详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 2.3 资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 (1) num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 (2) executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G-8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3-1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 (3) executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2-4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3-1/2左右比较合适，也是避免影响其他同学的作业运行。 (4) driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 (5) spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500-1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2-3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 (6) spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 (7) spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。 2.4 资源参数参考示例以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \ 原文: https://tech.meituan.com/spark-tuning-basic.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 性能调优之开发调优]]></title>
    <url>%2F2018%2F02%2F01%2Fspark-performance-optimization-basic%2F</url>
    <content type="text"><![CDATA[1. 前言在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优。 2. 开发调优2.1 调优概述Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 2.2 原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 一个简单的例子:// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)rdd1.reduce(...) 2.3 原则二：尽可能复用同一个RDD除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 一个简单的例子:// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 2.4 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 对多次使用的RDD进行持久化的代码示例:// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 2.5 原则四：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 Broadcast与map进行join代码示例:// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 2.6 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 2.7 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 (1) 使用reduceByKey/aggregateByKey替代groupByKey 详情见“原则五：使用map-side预聚合的shuffle操作”。 (2)使用mapPartitions替代普通map mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ (3) 使用foreachPartitions替代foreach 原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 (4) 使用filter之后进行coalesce操作 通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 (5) 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 2.8 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 广播大变量的代码示例:// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 2.9 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 2.10 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 原文: https://tech.meituan.com/spark-tuning-basic.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 保存点之回溯时间]]></title>
    <url>%2F2018%2F01%2F31%2Fflink-stream-turning-back-time-savepoints%2F</url>
    <content type="text"><![CDATA[这篇文章是系列文章的第一篇，数据工匠团队会在这里为大家展示一些Apache Flink的核心功能。 流处理通常被大家与动态数据关联起来，相应的系统差不多会在数据被创造出来的那一刻就立刻对其进行处理或响应。像延迟、吞吐量、水印和处理迟到的数据等等都是大家讨论得最多的流处理话题，通常是关注现在，而不是过去。 可在实际项目中，却有许多种场景需要你的流处理程序把以前处理过的数据再重新处理一遍。这里有些例子： 为你的程序部署一个新版本，可能是有新功能、修复了问题、或者采用了更好的机器学习模型； 使用相同的源数据流对应用程序的不同版本进行A/B测试，两边都从同一个点开始测试，这样就不会牺牲之前的状态； 评估或开展将应用程序迁移到更新版本的处理框架上，或是一个不同的集群上； Apache Flink 的保存点（Savepoint）功能可以支持上面的所有场景，并且也是让 Flink 与其它分布式开源流处理器不同的一个显著区别点。 在本文中，我们会讲述如何使用保存点功能来重新处理数据，并一定程度地深入底层，讲述这个功能在Flink中是怎么实现的。 1. “重新处理”到底是什么意思？为了保证大家对重新处理数据的理解是一致的，我们先讨论一个你可能需要重新处理数据的业务例子。想像一个社交媒体公司，她除了基本的发贴功能之外，还发布了一种付费的、或者说是推广发贴的功能。 公司的用户可以访问一个简单的、基于 Flink 实现的仪表板，显示他们的所有文章（不管是普通的还是付费的）被大家查看、点击等等的次数。几个星期之后，从用户的反馈中就可以清晰地看到，这个仪表板如果能把普通的发贴数据和付费的发贴数据区别开来，那就会更好用。 要实现这个功能，就有必要返回到付费发贴功能最初发布的那个时刻，然后从那个时刻开始，把所有数据全都重新处理一遍。这一次要把付费贴和普通贴的展示和交互全都拆开来。如果要把从公司创立伊始产生的数据全都重新处理一遍，这就实在有点强人所难，所以能够从付费发贴的功能发布的时候开始重新处理，同时还保留之前的计算结果，这个功能就很有必要了。 所以当我们用到 重新处理 这个词时，我们的意思就是回到一个系统以前的、一致的状态（按开发者的定义，不一定非要是流的最早状态），然后从那个状态开始再处理一遍，可能也要在更改了你的 Flink 程序之后。 读者们可以看到的好消息就是： Flink 为大家免费提供了上述重新处理功能，相应的功能就叫保存点。我们说”免费”，意思是只要你的程序是容错的，并且可以从错误中恢复，那你就可以在 Flink 中创建一个保存点并重新处理数据，花费的额外准备工作量几乎为零。 2. 简单说说保存点到底是什么简而言之，一个 Flink 程序的保存点就是关于以下两点的全局一致的镜像： 所有数据源的位置； 所有并行算子的状态； “全局一致” 意味着所有并行算子的状态都在所有输入的相同的明确定义的位置处被记录下来了。 如果在过去的某个时刻，你为某个应用程序记下了保存点，那你就可以从那个保存点的位置开始启动一个新程序。新的程序将使用那个保存点位置保存下来的算子的状态进行初始化，并且会从记录的保存点里各个数据源的相应位置开始，重新处理全部数据。 因为 Flink 的保存点之间是相互完全独立的，所以对每个程序你都可以有多个保存点，这样你就可以根据这些不同的保存点的信息，回到不同的位置，启动多次、甚至不同的程序（如下图所示）。这个功能对于派生你的流处理程序，或者为它们打不同的版本，是非常有用的。 我们应该注意，在从某个保存点开始重新处理数据时，对事件的时间处理是非常重要的。重新处理基本上就意味着从过去到现在进行快速回放，也就是说，是全速地从某些存储系统中读出数据，直到赶上了当前的状态，然后再继续实时地处理新到达的数据。 因为程序对于时间的处理或者插入时间都是要依赖当前的本地时间的，那么如果在根据保存点启动程序时不使用事件的时间，而使用别的时间，对程序的逻辑而言就很可能导致错误的结果。 3. 听起来不错，那我该做什么？不用做很多！事实上，所有支持故障恢复的程序都是自动支持保存点的。因此，大多数进行有状态计算的程序已经满足了需要的条件。如果没有，可以对它们进行快速更新，让它们具备： 启用检查点功能：在每种情况下，我们都推荐在构建 Flink 程序的同时，把检查点功能打开，事实上在你的 Flink 程序中加上检查点只是需要增加几行代码而已。 可以重置的数据源（即Apache Kafka、Amazon Kinesis，或者文件系统等）：数据源必须能按照你想要重新处理的点开始，重放数据。 所有的状态都通过 Flink 的管理状态接口保存：所有具体的算子的状态都必须保存在 Flink 的容错状态数据结构中，这让它可以按照某个之前的保存点位置被重置。 配置一个合适的状态后台：Flink 提供了不同的状态后台来将检查点和保存点持久化。默认地，保存点都保存在 JobManager 中，但你要为你的程序配置一个适当的后台状态程序，比如 RocksDB 等。 如果你已经在运行一个容错的程序了，那就创建一个保存点，然后从保存点的位置开始重新启动程序，这只需要在 Flink 命令行里敲几个命令就可以了。咱们接下来挨个看看。 (1) 第一步：创建一个保存点 首先，获得所有运行中的 Flink 任务的列表：user$ flink list------------Running/Restarting Jobs------------10.10.2016 16:20:33 : job_id : Sample Job (RUNNING) （运行上面的命令时，你的真实任务ID会是一个包括字母和数字的字符串。） 然后，用相应的任务ID创建一个保存点：user$ flink savepoint job_id 现在你的保存点就已经可用了。 如果你准备马上根据你的保存点来重新启动任务，你通常会想要把现在正在运行的任务先停掉。你已经有了相应任务的ID，那把它停掉只要几秒钟就够了：user$ flink cancel job_id (2) 第二步：从一个保存点开始启动任务 当你更新完程序之后，就可以从你的保存点开始启动任务了。user$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar 如果你想在一个示例程序中自己重做这些步骤，我们推荐你看看一篇之前的博客文章，我们在那里讲了怎么做这件事。 4. 如果我想升级我的程序，该怎样做？如果你想从一个保存点开始启动一个修改过的程序，有几件事是要考虑的。我们可以区别下面这两种情况： 改变一个用户定义的函数的逻辑，比如MapFunction； 改变一个程序的架构，也就是增加或减少算子等； 第一种情况很简单，不需要什么特别的准备。你可以按你的需要去修改函数代码。不过，如果你用一个修改了的架构从保存点开始启动程序，那么为了能够恢复算子的状态，Flink 必须能够将保存点程序的算子与使用了新架构的新程序的算子对应起来。 在这种情况下，你就要手动地将算子ID分配给最初的和更新了的程序。因为如果没有算子ID的话，是没办法修改程序的架构的。所以最佳实践经验就要求一定要分配算子ID。 下面的代码段显示了如何为算子分配ID：DataStream stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(“source-id”) .shuffle() // The stateful mapper with ID .map(new StatefulMapper()) .uid(“mapper-id”)// Stateless sink (no specific ID required)stream.print() 请查阅文档，了解更多关于升级程序和保存点的细节。 5. 关于保存点的最佳实践要更好的利用上文中描述的 Flink 的重新处理功能，你应该经常触发，生成新的保存点。我们建议要根据某些时刻表（比如每天一次，每周一次，等等）自动地生成保存点，而且每当你关闭某个任务或发布程序的新版本时，也最好先生成保存点。 依据你想用 Flink 做的事件不同，生成保存点的最佳方法也会不同，但总的来说，在构建你的程序时你应该花些时间考虑如何使用这些保存点。 6. 这些东西是怎么工作的呢？保存点事实上只是检查点的一个延伸，这就是 Flink 的容错机制。如果开启了检查点功能，Flink 就会周期性地为所有的算子状态生成一个一致的检查点。在文档中详细的描述了检查点的细节，如果你是个 Flink 新手，花些时间去读读是非常值得的。 你可能会以为要生成一个一致的检查点，就得暂停数据处理，因为 Flink 必须要等着，直到所有没处理完的记录全被处理掉了，然后做个镜像，镜像生成之后再回去继续处理数据。事实并非如此！ Flink 是持续处理数据的，即使在生成检查点的时候也是这样。文档中的Barriers一节讲了实现这个功能的原理。 两者之间的关键区别：检查点是基于某些规定的时间间隔自动生成的，而保存点是由用户显式地触发生成的，而且不会象检查点那样过了一定的时间之后就会被删掉。 7. 总结我们讨论了 Apache Flink 的保存点和数据重处理功能，因为我们相信这就是 Flink 与开源世界中其它流处理器之间的重要区别之一。而且最重要的，在容错的 Flink 程序中获得重处理功能几乎是不需要任何代价的，只需要很少的改动。 Flink 社区现在还在积极地工作着，要把保存点功能做得更好，包括在改变并发度的情况下保存状态的解决方案等。有些相应的功能（比如Flink-3755）已经发布到主分支上了，而且会被包含到下一个小版本Flink 1.2.0中。 所以，当你需要把程序多部署一份，或者上个新版本，或者要做A/B测试，或者要让多个程序从同一个点开始处理数据时，你可以这么做了，而且不会丢失那些宝贵的状态数据。 当有真实的需求时，流处理基于实时的特性不应该阻挡你把时间调回过去的动作。 有兴趣了解关于 Apache FLink 的保存点的更多内容吗？数据工匠CTO Stephan Ewen做了一个关于这个话题的七分钟白板演练，你可以在MapR博客上看到相关内容。 原文: https://data-artisans.com/blog/turning-back-time-savepoints 译文: http://www.infoq.com/cn/articles/turning-back-time-savepoints]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 命令行界面]]></title>
    <url>%2F2018%2F01%2F30%2Fflink-basic-command-line-interface%2F</url>
    <content type="text"><![CDATA[1. 概述Flink 提供了一个命令行接口（CLI）用来运行打成JAR包的程序，并且可以控制程序的运行。命令行接口在 Flink 安装完之后即可拥有，本地单节点或是分布式部署安装都会有命令行接口。命令行接口启动脚本是 $FLINK_HOME/bin 目录下的 flink 脚本， 默认情况下会连接运行中的 Flink master(JobManager)， JobManager 的启动脚本与 CLI 在同一安装目录下。 使用命令行接口的前提条件是 JobManager 已经被启动(通过$FLINK_HOME/bin/start-local.sh 或是 $FLINK_HOME/bin/start-cluster.sh)或是 Flink YARN 环境可用。 JobManager 可以通过如下命令启动:$FLINK_HOME/bin/start-local.sh或$FLINK_HOME/bin/start-cluster.sh 2. Example(1) 运行示例程序，不传参数：./bin/flink run ./examples/batch/WordCount.jar (2) 运行示例程序，带输入和输出文件参数：./bin/flink run ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (3) 运行示例程序，带输入和输出文件参数,并设置16个并发度：./bin/flink run -p 16 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (4) 运行示例程序，并禁止 Flink 输出日志./bin/flink run -q ./examples/batch/WordCount.jar (5) 以独立(detached)模式运行示例程序./bin/flink run -d ./examples/batch/WordCount.jar (6) 在指定 JobManager 上运行示例程序./bin/flink run -m myJMHost:6123 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (7) 运行示例程序，指定程序入口类(Main方法所在类)：./bin/flink run -c org.apache.flink.examples.java.wordcount.WordCount ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (8) 运行示例程序，使用带有2个 TaskManager 的per-job YARN 集群./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar --input hdfs:///xiaosi/a.txt --output hdfs:///xiaosi/result.txt (9) 以JSON格式输出 WordCount 示例程序优化执行计划：./bin/flink info ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt (10) 列出已经调度的和正在运行的Job(包含Job ID信息)./bin/flink list (11) 列出已经调度的Job(包含Job ID信息)./bin/flink list -s (13) 列出正在运行的Job(包含Job ID信息)./bin/flink list -r (14) 列出在Flink YARN中运行Job./bin/flink list -m yarn-cluster -yid &lt;yarnApplicationID&gt; -r (15) 取消一个Job./bin/flink cancel &lt;jobID&gt; (16) 取消一个带有保存点(savepoint)的Job./bin/flink cancel -s [targetDirectory] &lt;jobID&gt; (17) 停止一个Job(只适用于流计算Job)./bin/flink stop &lt;jobID&gt; 取消和停止一个作业的区别如下： 调用取消作业时，作业中的算子立即收到一个调用cancel()方法的指令以尽快取消它们。如果算子在调用取消操作后没有停止，Flink 将定期开启中断线程来取消作业直到作业停止。 停止作业是一种停止正在运行的流作业的更加优雅的方法。停止仅适用于使用实现StoppableFunction接口的数据源的那些作业。当用户请求停止作业时，所有数据源将收到调用stop()方法指令。但是作业还是会继续运行，直到所有数据源正确关闭。这允许作业处理完所有正在传输的数据(inflight data)。 3. 保存点保存点通过命令行客户端进行控制： 3.1 触发保存点./bin/flink savepoint &lt;jobID&gt; [savepointDirectory] 这会触发作业ID为jobId的保存点，并返回创建的保存点的路径。你需要此路径来还原和处理保存点。 此外，你可以选择指定一个目标文件系统目录来存储保存点。目录可以被 JobManager 访问。 如果你不指定目标目录，则需要配置默认目录（请参阅保存点）。 否则，触发保存点将失败。 3.2 使用YARN触发保存点./bin/flink savepoint &lt;jobId&gt; [savepointDirectory] -yid &lt;yarnAppId&gt; 这将触发作业ID为 jobId 以及 YARN 应用程序ID为 yarnAppId 的保存点，并返回创建的保存点的路径。 其他一切与上面的触发保存点中描述的相同。 3.3 根据保存点取消Job你可以自动触发一个保存点并取消作业:./bin/flink cancel -s [savepointDirectory] &lt;jobID&gt; 如果没有配置保存点目录，则需要为 Flink 安装配置默认的保存点目录(请参阅保存点）。 只有保存点触发成功，作业才被取消 3.4 恢复保存点./bin/flink run -s &lt;savepointPath&gt; ... 这个run命令提交作业时带有一个保存点标记，这使得程序可以从保存点中恢复状态。保存点路径是通过保存点触发命令得到的。 默认情况下，我们尝试将所有的保存点状态与正在提交的作业进行匹配。如果你想允许跳过无法使用新作业恢复的保存点状态，则可以设置allowNonRestoredState标志。当保存点触发时，如果想从程序中删除一个算子（作为程序的一部分），并且仍然想要使用这个保存点，则需要允许这一点。 ./bin/flink run -s &lt;savepointPath&gt; -n ... 如果想从程序中删除算子(作为保存点一部分的)，这时会非常有用。 3.5 销毁保存点./bin/flink savepoint -d &lt;savepointPath&gt; 销毁一个保存点同样需要一个路径。这个保存点路径是通过保存点触发命令得到的。 如果使用自定义状态实例（例如自定义 reducing 状态或 RocksDB 状态），则必须指定程序JAR的路径以及被触发的保存点，以便使用用户代码类加载器来销毁保存点：./bin/flink savepoint -d &lt;savepointPath&gt; -j &lt;jarFile&gt; 否则，你将遇到 ClassNotFoundException。 4. 用法下面是Flink命令行接口的用法:xiaosi@yoona:~/qunar/company/opt/flink-1.3.2$ ./bin/flink./flink &lt;ACTION&gt; [OPTIONS] [ARGUMENTS]The following actions are available:Action &quot;run&quot; compiles and runs a program. Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;run&quot; action options: -c,--class &lt;classname&gt; Class with the program entry point (&quot;main&quot; method or &quot;getPlan()&quot; method. Only needed if the JAR file does not specify the class in its manifest. -C,--classpath &lt;url&gt; Adds a URL to each user code classloader on all nodes in the cluster. The paths must specify a protocol (e.g. file://) and be accessible on all nodes (e.g. by means of a NFS share). You can use this option multiple times for specifying more than one URL. The protocol must be supported by the &#123;@link java.net.URLClassLoader&#125;. -d,--detached If present, runs the job in detached mode -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -n,--allowNonRestoredState Allow to skip savepoint state that cannot be restored. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered. -p,--parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. -q,--sysoutLogging If present, suppress logging output to standard out. -s,--fromSavepoint &lt;savepointPath&gt; Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537 ). -z,--zookeeperNamespace &lt;zookeeperNamespace&gt; Namespace to create the Zookeeper sub-paths for high availability mode Options for yarn-cluster mode: -yD &lt;arg&gt; Dynamic properties -yd,--yarndetached Start detached -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session -yj,--yarnjar &lt;arg&gt; Path to Flink jar file -yjm,--yarnjobManagerMemory &lt;arg&gt; Memory for JobManager Container [in MB] -yn,--yarncontainer &lt;arg&gt; Number of YARN container to allocate (=Number of Task Managers) -ynm,--yarnname &lt;arg&gt; Set a custom name for the application on YARN -yq,--yarnquery Display available YARN resources (memory, cores) -yqu,--yarnqueue &lt;arg&gt; Specify YARN queue. -ys,--yarnslots &lt;arg&gt; Number of slots per TaskManager -yst,--yarnstreaming Start Flink in streaming mode -yt,--yarnship &lt;arg&gt; Ship files in the specified directory (t for transfer) -ytm,--yarntaskManagerMemory &lt;arg&gt; Memory per TaskManager Container [in MB] -yz,--yarnzookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode Options for yarn mode: -ya,--yarnattached Start attached -yD &lt;arg&gt; Dynamic properties -yj,--yarnjar &lt;arg&gt; Path to Flink jar file -yjm,--yarnjobManagerMemory &lt;arg&gt; Memory for JobManager Container [in MB] -yqu,--yarnqueue &lt;arg&gt; Specify YARN queue. -yt,--yarnship &lt;arg&gt; Ship files in the specified directory (t for transfer) -yz,--yarnzookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability modeAction &quot;info&quot; shows the optimized execution plan of the program (JSON). Syntax: info [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;info&quot; action options: -c,--class &lt;classname&gt; Class with the program entry point (&quot;main&quot; method or &quot;getPlan()&quot; method. Only needed if the JAR file does not specify the class in its manifest. -p,--parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;list&quot; lists running and scheduled programs. Syntax: list [OPTIONS] &quot;list&quot; action options: -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -r,--running Show only running programs and their JobIDs -s,--scheduled Show only scheduled programs and their JobIDs Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;stop&quot; stops a running program (streaming jobs only). Syntax: stop [OPTIONS] &lt;Job ID&gt; &quot;stop&quot; action options: -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;cancel&quot; cancels a running program. Syntax: cancel [OPTIONS] &lt;Job ID&gt; &quot;cancel&quot; action options: -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -s,--withSavepoint &lt;targetDirectory&gt; Trigger savepoint and cancel job. The target directory is optional. If no directory is specified, the configured default directory (state.savepoints.dir) is used. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode:Action &quot;savepoint&quot; triggers savepoints for a running job or disposes existing ones. Syntax: savepoint [OPTIONS] &lt;Job ID&gt; [&lt;target directory&gt;] &quot;savepoint&quot; action options: -d,--dispose &lt;arg&gt; Path of savepoint to dispose. -j,--jarfile &lt;jarfile&gt; Flink program JAR file. -m,--jobmanager &lt;host:port&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. Options for yarn-cluster mode: -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session Options for yarn mode: Please specify an action. 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 外部检查点]]></title>
    <url>%2F2018%2F01%2F30%2Fflink-stream-deployment-externalized-checkpoints%2F</url>
    <content type="text"><![CDATA[1. 概述检查点通过恢复状态和对应流位置来实现 Flink 状态容错，从而为应用程序提供与无故障执行相同的语义。 请参阅检查点以了解如何为你的应用程序启用和配置检查点。 2. 外部检查点 Externalized Checkpoints默认情况下检查点不会持久化存储在外部系统中，只是用来从故障中恢复作业。当一个程序被取消时它们会被删除。但是，你可以配置检查点定期持久化存储在外部系统中，类似于保存点(savepoints)。这些外部持久化的检查点将其元数据写入持久性存储中，即使在作业失败时也不会自动清除。这样，如果你的作业失败时，你会有一个检查点用于恢复作业。 CheckpointConfig config = env.getCheckpointConfig();config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); ExternalizedCheckpointCleanup模式配置当你取消作业时外部检查点如何操作： (1) ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：作业取消时保留外部检查点。请注意，在这种情况下，你必须手动清除取消后的检查点状态。 (2) ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION: 作业取消时删除外部检查点。检查点状态只有在作业失败时才可用。 2.1 目录结构与保存点类似，外部检查点由元数据文件组成，一些其他数据文件（取决于状态后端）。外部检查点元数据的目标目录是由配置属性state.checkpoints.dir确定的，目前它只能通过配置文件来设置。 state.checkpoints.dir: hdfs:///checkpoints/ 该目录包含恢复检查点所需的检查点元数据。对于MemoryStateBackend，这个元数据文件是独立的(self-contained)，不需要其他文件。 FsStateBackend 和 RocksDBStateBackend 需要写到不同的数据文件中，只需将这些文件的路径写入元数据文件。这些数据文件存储在状态后端指定的路径上。 env.setStateBackend(new RocksDBStateBackend(&quot;hdfs:///checkpoints-data/&quot;); 2.2 与保存点的区别外部检查点与保存点有一些差异。他们 使用状态后端指定的（低层次）数据格式 可能是增量存储的 不支持 Flink 部分功能（如重新调整）。 2.3 从外部检查点恢复作业可以通过使用检查点的元数据文件从外部检查点中恢复，就像从保存点恢复一样（请参阅保存点恢复）。请注意，如果元数据文件不是独立的，jobmanager 需要访问它所引用的数据文件（参见上面的目录结构）。 $ bin/flink run -s :checkpointMetaDataPath [:runArgs] 备注:Flink版本:1.4 术语翻译: 术语 翻译 Checkpoints 检查点 Externalized Checkpoints 外部检查点 savepoints 保存点 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之作业与调度]]></title>
    <url>%2F2018%2F01%2F29%2Fflink-internals-job-scheduling%2F</url>
    <content type="text"><![CDATA[1. 调度Flink中的执行资源是通过任务槽定义。每个TaskManager都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例。请注意，Flink经常同时执行连续的任务：对于流式处理程序时刻发生，但是对于批处理程序来说却是经常发生。 下图证明了这一点。考虑一个带有数据源，一个MapFunction 和 一个ReduceFunction 的程序。数据源和 MapFunction 以并行度4运行， ReduceFunction以并行度3运行。流水线由 Source-Map-Reduce 序列组成。在具有2个TaskManager（每个有3个插槽）的集群上，程序将按照下面的描述执行: 在内部，Flink通过SlotSharingGroup和 CoLocationGroup定义哪些任务可以共享一个槽（允许），哪些任务必须严格放置在同一个槽中。 2. JobManager 数据结构在作业执行期间，JobManager 追踪分布式任务，决定何时调度下一个任务（或任务集合），并对完成的任务或执行失败的任务进行相应的处理。 JobManager 接收 JobGraph，JobGraph表示由算子（JobVertex）和中间结果（IntermediateDataSet）组成的数据流。每个算子都具有属性，如并行度和执行的代码等。另外，JobGraph还有一组附加的库，运行算子代码必需使用这些库。 JobManager 将 JobGraph 转换成 ExecutionGraph。 ExecutionGraph 是 JobGraph 的并行版本：对于每个 JobVertex，对于每个并行子任务它都包含一个 ExecutionVertex。例如并行度为100的算子会有一个 JobVertex 以及 100个 ExecutionVertices。 ExecutionVertex跟踪特定子任务的执行状态。JobVertex 中所有的 ExecutionVertices 都保存在一个 ExecutionJobVertex 中，该 ExecutionJobVertex 跟踪整个算子的状态。除了顶点之外， ExecutionGraph 还包含 IntermediateResult 和 IntermediateResultPartition。前者跟踪 IntermediateDataSet 的状态，后者追踪每个分区的状态。 每个 ExecutionGraph 都有一个与之相关的作业状态。作业状态表示作业执行的当前状态。 Flink 作业首先处于 ctreated 状态，然后切换到 running 状态，一旦所有工作完成后切换到 finished 状态。在出现故障的情况下，作业首先切换到 failing 状态，取消所有正在运行任务的地方。如果所有作业顶点已达到最终状态，并且作业不可重新启动，那么作业转换 failed 状态。如果作业可以重新启动，那么它将进入 restarting 状态。一旦作业重新启动完成后，将进入 ctreated 状态。 在用户取消作业的情况下，将进入 cancelling 状态。这也需要取消所有正在运行的任务。一旦所有正在运行的任务都达到最终状态，作业将转换到 cancelled 状态。 不同于表示全局终端状态以及触发清理工作的 finished， canceled 和 failed 状态，suspended 状态只是本地终端。本地终端的意思是作业的执行已在相应的 JobManager 上终止，但 Flink 集群的另一个 JobManager 可从持久性 HA 存储中检索作业并重新启动作业。因此，进入 suspended 状态的作业将不会完全清理。 在 ExecutionGraph 的执行过程中，每个并行任务都经历了从 ctreated 到 finished 或 failed 的多个阶段。下图说明了它们之间的状态和可能的转换。任务可以执行多次（例如在故障恢复过程中）。出于这个原因， ExecutionVertex 执行跟踪信息保存在 Execution 中。 每个 ExecutionVertex 都有一个当前的Execution，以及之前的Executions。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Shell中判断HDFS文件是否存在]]></title>
    <url>%2F2018%2F01%2F25%2Fhadoop-how-to-determine-the-hdfs-file-exists%2F</url>
    <content type="text"><![CDATA[1. 用法Hadoop提供了-test命令可以验证文件目录是否存在。我们首先看一下-test命令的使用用法:hadoop fs -help-test -[defsz] &lt;path&gt;: Answer various questions about &lt;path&gt;, with result via exit status. -d return 0 if &lt;path&gt; is a directory. -e return 0 if &lt;path&gt; exists. -f return 0 if &lt;path&gt; is a file. -s return 0 if file &lt;path&gt; is greater than zero bytes in size. -z return 0 if file &lt;path&gt; is zero bytes in size. else, return 1. 命令参数 描述 -d 如果指定路径是一个目录返回0否则返回1 -e 如果指定路径存在返回0否则返回1 -f 如果指定路径是一个文件返回0否则返回1 -s 如果指定路径文件大小大于0返回0否则返回1 -z 如果指定指定文件大小等于0返回0否则返回1 2. Example:[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -ls test/advFound 1 itemsdrwxr-xr-x - xiaosi xiaosi 0 2018-01-25 15:39 test/adv/day=20180123[xiaosi@ying:~$]$[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123[xiaosi@ying:~$]$ echo $?0[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180124[xiaosi@ying:~$]$ echo $?1 3. Shell中判断sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123if [ $? -eq 0 ] ;then echo &apos;[info]目录已存在不需要创建&apos;else sudo -uxiaosi hadoop fs -mkdir -p test/adv/day=20180123fi]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之数据流容错]]></title>
    <url>%2F2018%2F01%2F24%2Fflink-data-streaming-fault-tolerance%2F</url>
    <content type="text"><![CDATA[1. 概述Apache Flink提供了一个容错机制来持续恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态也将最终反映每条记录来自数据流严格一次exactly once。 请注意，有一个开关可以降级为保证至少一次(least once)（如下所述）。 容错机制连续生成分布式流数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁生成，而不会对性能造成太大影响。流应用程序的状态存储在可配置的位置（例如主节点或HDFS）。 如果应用程序发生故障（由于机器，网络或软件故障），Flink会停止分布式流式数据流。然后系统重新启动算子并将其重置为最新的成功检查点。输入流被重置为状态快照的时间点。作为重新启动的并行数据流处理的任何记录都保证不属于先前检查点状态的一部分。 注意:默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参阅检查点。 为了实现这个机制的保证，数据流源（如消息队列或代理）需要能够将流重放到定义的最近时间点。Apache Kafka有这个能力，而Flink的Kafka连接器就是利用这个能力。有关Flink连接器提供的保证的更多信息，请参阅数据源和接收器的容错保证。 因为Flink的检查点是通过分布式快照实现的，所以我们交替使用快照和检查点两个概念。 2. CheckpointingFlink的容错机制的核心部分是生成分布式数据流和算子状态的一致性快照。这些快照作为一个一致性检查点，在系统发生故障时可以回溯。Flink的生成这些快照的机制在分布式数据流的轻量级异步快照中进行详细的描述。它受分布式快照Chandy-Lamport算法的启发，并且专门针对Flink的执行模型量身定制。 2.1 BarriersFlink分布式快照的一个核心元素是数据流Barriers。这些Barriers被放入数据流中，并作为数据流的一部分与记录一起流动。Barriers永远不会超越记录，严格按照相对顺序流动。Barriers将数据流中的记录分成进入当前快照的记录集合和进入下一个快照的记录集合。每个Barriers都携带前面快照的ID。Barriers不会中断流的流动，因此非常轻。来自不同快照的多个Barriers可以同时在流中，这意味着不同快照可以同时发生。 Barriers在数据流源处被放入的并行数据流。快照n放入Barriers的位置（我们称之为Sn）是快照覆盖数据的源流中的位置。例如，在Apache Kafka中，这个位置是分区中最后一个记录的偏移量。该位置Sn会报告给检查点协调员（Flink的JobManager）。 Barriers向下游流动。当中间算子从其所有输入流中接收到快照n的Barriers时，它会将快照n的Barriers发送到其所有输出流中。一旦Sink算子（流式DAG的末尾）从其所有输入流中接收到Barriers n，就向检查点协调器确认快照n。在所有Sink确认了快照之后，才被确认已经完成。 一旦快照n完成，作业将不会再向数据源询问Sn之前的记录，因为那时这些记录（以及它们的后代记录）已经通过了整个数据流拓扑。 接收多个输入流的算子需要根据快照Barriers对其输入流。上图说明了这一点： 只要算子从一个输入流接收到快照Barriers n时，就不能处理来自该数据流的任何记录(译者注:进行缓存)，当从其他输入流中接收到最后一个Barriers n时，才开始处理缓存的数据(即对齐的意思)。否则，就会混合属于快照n和快照n + 1的记录。 报告Barriers n的数据流暂时搁置。从这些数据流接收到的记录不会被处理，而是放入输入缓冲区中(例如上图中的aligning部分)。 一旦接收到最后一个流的Barriers n时，算子才发送所有待发送的记录，然后才发送快照Barriers n自己(例如上图中的checkpoint部分)。 之后，恢复处理所有输入流中的记录，在处理来自数据流的记录之前优先处理来自输入缓冲区中的记录(例如上图中的continue部分)。 2.2 State当算子包含任何形式的状态时，这个状态也必须是快照的一部分。算子状态有不同的形式： 用户自定义状态：这是由转换函数（如map（）或filter（））直接创建和修改的状态。有关详细信息，请参阅状态概述 系统状态：这种状态指的是作为算子计算一部分的数据缓冲区。这种状态的一个典型例子是窗口缓冲区，在窗口缓冲区中，系统为窗口收集（以及聚合）记录，直到窗口被计算和删除。 在算子收到所有输入流中的所有快照barriers以及在barriers发送到输出流之前，算子对自己的状态进行快照。这时，At that point, all updates to the state from records before the barriers will have been made, and no updates that depend on records from after the barriers have been applied。由于快照的状态可能较大，因此需要其存储在可配置状态终端state backend中。默认情况下，会存储在JobManager的内存中，但是在生产环境下，应该配置为分布式可靠存储系统（如HDFS）。在状态被存储之后，算子确认检查点，将快照barriers发送到输出流，然后继续前行。 生成的快照包含： 对于每个并行流数据源，快照启动时在数据流中的偏移量/位置 对于每个算子，指向作为快照中一部分的状态的指针 2.3 Exactly Once vs. At Least Once对齐步骤可能会给流式传输程序造成延迟。这个额外的延迟通常大约在几毫秒的数量级，但是我们已经看到一些因为异常值造成的延迟明显增加的情况。对于所有记录需要持续较低延迟（几毫秒）的应用程序而言，Flink有一个开关可以在检查点期间跳过流对齐。一旦算子看到每个输入的检查点barriers，就会生成检查点快照。 当跳过对齐步骤时，即使在检查点n的一些检查点barriers到达之后，算子也会继续处理所有输入。这样，在为检查点n生成状态快照之前也会处理到属于检查点n+1的元素。在恢复时，这些记录将会重复出现，因为它们既包含在检查点n的状态快照中，也会在检查点n之后作为数据的一部分进行重放。 备注: 对齐仅发生在当算子具有多个输入（例如join）或者具有多个输出（在流repartitioning/shuffle之后）的情况。正因为如此，只有密集并行流操作(only embarrassingly parallel streaming operations)（map（），flatMap（），filter（）…）的数据流即使在至少一次模式下也只能提供严格一次。 2.4 异步状态快照请注意，上述机制意味着算子在状态终端存储状态快照时停止处理输入记录。这种同步状态快照在每次生成快照时都会造成延迟。 可以让算子在存储其状态快照的同时继续处理输入记录，有效地让状态快照在后台异步发生。要做到这一点，算子必须能够产生一个以某种方式存储的状态对象，以至于对算子状态的进一步的修改不会影响状态对象。例如，copy-on-write数据结构（如RocksDB中使用的数据结构）具有这种功能。 在接收到输入端的检查点barriers后，算子启动其状态的异步快照复制。barriers立即发送到输出流中，并继续进行正常的流处理。一旦后台复制过程完成，它就会向检查点协调器（JobManager）确认检查点。检查点现在只有在所有sink接收到barriers并且所有有状态的算子已经确认完成备份（可能在barriers到达sink之后）。 有关状态快照的详细信息，请参阅状态终端。 3. 恢复在这种机制下恢复很简单：一旦失败，Flink选择最近完成的检查点k。然后系统重新部署整个分布式数据流，并为每个算子提供作为检查点k一部分的快照状态。数据源被设置为开始从位置Sk读取数据流。例如在Apache Kafka中，这意味着告诉消费者从偏移量Sk处开始提取数据。 如果增量对状态进行快照，算子将从最新且完整的快照状态开始，然后对该状态应用一系列增量快照更新。 请参阅重启策略了解更多信息。 4. 实现算子快照对算子进行快照，有两部分：同步部分和异步部分。 算子和状态终端将其快照作为Java FutureTask。该任务包含的状态同步部分已经完成异步部分挂起。然后异步部分由该检查点的后台线程执行。 算子检查点只是同步返回一个已经完成的FutureTask。如果需要执行异步操作，则在FutureTask的run（）方法中执行。 任务是可取消的，所以消耗句柄的数据流和其他资源是可以被释放。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 检查点启用与配置]]></title>
    <url>%2F2018%2F01%2F17%2Fflink-stream-development-checkpointing-enable-config%2F</url>
    <content type="text"><![CDATA[Flink 中的每个函数和操作符都可以是有状态的（请参阅使用状态了解详细信息）。有状态函数在处理单个元素/事件时存储数据。 为了能够状态容错，Flink 需要对状态进行 checkpoint。检查点允许 Flink 在流中恢复状态和位置，为应用程序提供与无故障执行相同的语义。 关于 Flink 流式容错机制背后的技术请参阅流式容错的详细文档。 1. 前提条件Flink 的检查点机制与流和状态的持久存储进行交互。一般来说，它要求： 一个可持久化（或保存很长时间）的数据源，可以重放特定时间段的记录。持久消息队列是这种数据源的一个例子（例如 Apache Kafka，RabbitMQ，Amazon Kinesis，Google PubSub）或 文件系统（例如 HDFS， S3， GFS， NFS， Ceph 等）。 状态的持久化存储，通常是分布式文件系统（例如 HDFS， S3， GFS， NFS， Ceph 等） 2. 启用和配置检查点默认情况下，检查点被禁用。要启用检查点，要在 StreamExecutionEnvironment 上调用 enableCheckpointing（n），其中n是检查点时间间隔（以毫秒为单位）。 检查点的其他参数包括： (1) exactly-once 与 at-least-once：你可以选择性的将模式传递给 enableCheckpointing（n） 方法来在两个保证级别之间进行选择。对于大多数应用来说，一般都选择 exactly-once。at-least-once可能与某些超低延迟（持续几毫秒）的应用程序有关。 (2) 检查点超时：如果在规定时间之前没有完成检查点，正在进行的检查点就会被终止。 (3) 检查点之间的最小时间：为了确保流式应用程序在检查点之间有一定的进展，可以定义检查点之间的时间间隔。例如，如果此值设置为5000，不论检查点持续时间和检查点间隔是多少，下一个检查点将在上一个检查点完成之后的5秒内启动。请注意，这意味着检查点间隔 checkpoint interval 永远不会小于此参数。 通过定义 检查点之间的时间差 (time between checkpoints)而不是检查点间隔(checkpoint interval)来配置应用程序通常更容易，因为 检查点之间的时间差 不会受到检查点有时花费比平均时间更长时间的影响（例如，如果目标存储系统暂时比较慢）。 请注意，这个值也意味着并发检查点的数量为1。 (4) 并发检查点的数量：默认情况下，当一个检查点正在运行时，系统不会触发另一个检查点。这确保了拓扑结构不会在检查点上花费太多时间，并且不会在处理流时有进展(not make progress with processing the streams)。可以允许多个重叠的检查点，这对于具有一定处理延迟（例如，因为函数调用外部服务需要等待一些时间响应），但是仍然想要做非常频繁的 checkpoints（100毫秒 ）重新处理很少见的失败情况具有一定意义。 定义检查点之间的最短时间时，不能使用此选项。 (5) 外部检查点externalized checkpoints：可以配置定期检查点持久化到从外部存储中。外部检查点将其元数据写入持久性存储，作业失败时也不会自动清理。这样，如果你的作业失败，你将会有一个检查点用来恢复。有关外部检查点的部署说明中有更多详细信息。 Java版本:StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()// start a checkpoint every 1000 msenv.enableCheckpointing(1000)// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig.setCheckpointTimeout(60000)// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig.setMaxConcurrentCheckpoints(1) 3. 相关配置选项其他参数和默认值也可以通过conf/flink-conf.yaml配置文件进行设置（请参阅完整指南的配置）： (1) state.backend：如果启用了检查点，用来存储算子状态检查点的终端。支持的终端： jobmanager：内存状态，备份到 JobManager/ZooKeeper 的内存中。应在较小状态（Kafka偏移量）或测试和本地调试时使用。 文件系统：状态存储在 TaskManager 的内存中，状态快照存储在文件系统中。Flink支持所有文件系统，例如 HDFS，S3，… (2) state.backend.fs.checkpointdir：用于在 Flink 支持的文件系统中存储检查点的目录。注意：JobManager 必须可以访问状态终端，本地安装时可以使用file：//。 (3) state.backend.rocksdb.checkpointdir: 用于存储 RocksDB 文件的本地目录，或由系统目录分隔符（例如Linux/Unix上的’：’（冒号））分隔的目录列表。（默认值是taskmanager.tmp.dirs） (4) state.checkpoints.dir: 外部检查点元数据的目标目录。 (5) state.checkpoints.num-retained: 已完成的检查点实例的数量。如果最新的检查点已损坏，必须使用多个实例才可以恢复回退到较早的检查点。（默认值：1） 4. 选择状态终端Flink 的检查点机制存储定时器中所有状态和有状态算子的一致性快照，包括连接器，窗口以及任何用户自定义的状态。检查点存储的位置（例如，JobManager 的内存，文件系统，数据库）取决于状态终端的配置。 默认情况下，状态保存在 TaskManager 的内存中，检查点存储在 JobManager 的内存中。为了适当地存储较大的状态，Flink 也支持多种方法在其他状态终端存储状态以及对状态进行检查点操作。状态终端的选择可以通过 StreamExecutionEnvironment.setStateBackend（...） 来配置。 有关可用状态终端以及作业范围和群集范围内配置选项的的详细信息，请参阅状态终端。 5. 迭代作业中的状态检查点目前 Flink 只为无迭代作业提供处理保证。在迭代作业上启用检查点会导致异常。为了在迭代程序上强制进行检查点操作，用户需要在启用检查点时设置特殊标志：env.enableCheckpointing（interval，force = true）。 6. 重启策略Flink支持不同的重启策略，控制在失败情况下重启的方式。有关更多信息，请参阅重启策略。 备注: Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 状态终端]]></title>
    <url>%2F2018%2F01%2F17%2Fflink-stream-state-backends%2F</url>
    <content type="text"><![CDATA[1. 概述Flink 提供了不同的状态终端，可以指定状态的存储方式和位置。 状态可以存储在Java的堆内或堆外。根据你的状态终端，Flink 也可以管理应用程序的状态，这意味着 Flink 可以处理内存管理（可能会溢出到磁盘，如果有必要），以允许应用程序存储非常大的状态。默认情况下，配置文件 flink-conf.yaml 为所有Flink作业决定其状态终端。 但是，默认的状态终端配置也可以被每个作业的配置覆盖，如下所示。 Java版本:StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(...); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()env.setStateBackend(...) 2. 可用的状态终端开箱即用，Flink 内置了如下状态终端： MemoryStateBackend FsStateBackend RocksDBStateBackend 如果没有配置，系统默认使用MemoryStateBackend。 2.1 MemoryStateBackendMemoryStateBackend 将数据以对象的形式保存在 Java 堆上。键值对状态和窗口算子拥有保存值，触发器等的哈希表。 在进行检查点操作时，状态终端对状态进行快照，并将其作为检查点确认消息的一部分发送给 JobManager（master），并将存储在其堆上。 MemoryStateBackend 可以配置为使用异步快照。尽管我们强烈建议使用异步快照来避免阻塞管道，但请注意，这是一项新功能，目前默认情况下不会启用。要启用此功能，用户可以在实例化 MemoryStateBackend的构造函数中设置相应的布尔值 true，例如：new MemoryStateBackend(MAX_MEM_STATE_SIZE, true); MemoryStateBackend 的使用限制： 每个单独状态的大小默认限制为5 MB。这个值可以在 MemoryStateBackend 的构造函数中增加。 不考虑配置的最大状态大小，状态不能大于akka frame大小。 聚合状态必须能够放进 JobManager 内存中。 MemoryStateBackend 适用场景： 本地开发和调试 只存储较小状态的作业，例如只包含 record-at-a-time 函数的作业（Map，FlatMap，Filter，…）。 Kafka消费者只需要很少的状态。 2.2 FsStateBackendFsStateBackend 使用文件系统URL（类型，地址，路径）进行配置，如 hdfs://namenode:40010/flink/checkpoints 或 file:///data/flink/checkpoints。 FsStateBackend 将正在使用的数据保存在 TaskManager 的内存中。在进行检查点操作时，将状态快照写入配置的文件系统文件和目录中。较小的元数据存储在 JobManager 的内存中（或者在高可用性模式下，存储在元数据检查点中）。 FsStateBackend 默认使用异步快照，以避免在写入状态检查点时阻塞处理管道。如果要禁用此功能，用户可以在实例化 FsStateBackend 的构造函数中将对应的布尔值设置为 false，例如：new FsStateBackend（path，false）; FsStateBackend 适用场景： 具有大状态，长窗口，大的键/值状态的作业。 所有高可用配置。 2.3 RocksDBStateBackendRocksDBStateBackend 使用文件系统URL（类型，地址，路径）进行配置，例如 hdfs://namenode:40010/flink/checkpoints 或 file:///data/flink/checkpoints。 RocksDBStateBackend 将 正在使用的数据保存在 RocksDB 数据库中，其位于 TaskManager 数据目录下（默认情况下）。进行检查点操作时，整个 RocksDB 数据库进行检查点操作存储到配置的文件系统和目录中。较小的元数据存储在 JobManager 的内存中（或者在高可用性模式下，存储在元数据检查点中）。 RocksDBStateBackend 总是执行异步快照。 RocksDBStateBackend 使用限制： 由于 RocksDB 的JNI桥接API基于 byte []，每个键和每个值支持的最大大小为 2^31 个字节。重要的是在 RocksDB 中使用合并操作的状态（例如ListState）可以累积超过2^31字节，然后在下一次检索时会失败。目前这是 RocksDB JNI 的限制。 RocksDBStateBackend 适用场景： 具有非常大的状态，长时间窗口，大键/值状态的作业。 所有高可用配置。 请注意，你可以保存的状态数量仅受可用磁盘空间的限制。与保存状态到内存的 FsStateBackend 相比，这可以保存非常大的状态。但是，这也意味着在这个状态终端下可以达到的最大吞吐量将会降低。 RocksDBStateBackend 是目前唯一个提供增量检查点的终端（见这里）。 3. 配置状态终端如果你不指定，默认的状态终端是 jobmanager。如果你希望为集群中的所有作业建立不同的默认值，可以在 flink-conf.yaml 中定义一个新的默认状态终端来完成。默认的状态终端可以被每个作业的配置覆盖，如下所示。 3.1 设置每个作业的状态终端作业状态终端在作业的 StreamExecutionEnvironment 上设置，如下例所示： Java版本:StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints")); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()env.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints")) 3.2 设置默认状态终端可以使用配置键 state.backend 在 flink-conf.yaml 配置文件中配置默认状态终端。 配置的值可以是 jobmanager（MemoryStateBackend），filesystem（FsStateBackend），rocksdb（RocksDBStateBackend），或实现状态终端工厂 FsStateBackendFactory 类的全限定类名，例如 RocksDBStateBackend 的 org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory。 如果默认状态终端设置为 filesystem，state.backend.fs.checkpointdir 定义了检查点数据存储目录。 配置文件中的示例部分可能如下所示：# The backend that will be used to store operator state checkpointsstate.backend: filesystem# Directory for storing checkpointsstate.backend.fs.checkpointdir: hdfs://namenode:40010/flink/checkpoints 备注: Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/state_backends.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 使用状态]]></title>
    <url>%2F2018%2F01%2F16%2Fflink-stream-working-with-state%2F</url>
    <content type="text"><![CDATA[1. Keyed State 与 Operator StateFlink有两种基本的状态：Keyed State和Operator State。 1.1 Keyed StateKeyed State总是与key相关，只能在KeyedStream的函数和运算符中使用。 备注:KeyedStream继承DataStream，表示根据指定的key进行分组的数据流。使用DataStream提供的KeySelector根据key对其上的算子State进行分区。DataStream支持的典型操作也可以在KeyedStream上进行，除了诸如shuffle，forward和keyBy之类的分区方法之外。一个KeyedStream可以通过调用DataStream.keyBy()来获得。而在KeyedStream上进行任何transformation都将转变回DataStream。 你可以将 Keyed State 视为已经分区或分片的Operator State，每个 key 对应一个状态分区。每个Keyed State在逻辑上只对应一个 &lt;并行算子实例，key&gt;，并且由于每个 key “只属于” 一个Keyed Operator的一个并行实例，我们可以简单地认为成 &lt;operator，key&gt;。 Keyed State 被进一步组织成所谓的 Key Group。Key Group 是 Flink 可以重新分配 Keyed State 的最小单位；Key Group的数量与最大并行度一样多。在执行期间，Keyed Operator的每个并行实例都与一个或多个Key Group的key一起工作。 1.2 Operator State使用Operator State (或非Keyed State)，每个算子状态都绑定到一个并行算子实例。Kafka Connector 是在Flink中使用算子状态的一个很好的例子。Kafka消费者的每个并行实例都要维护一个topic分区和偏移量的map作为其Operator State。 在并行度发生变化时，Operator State接口支持在并行算子实例之间进行重新分配状态。可以有不同的方案来处理这个重新分配。 2. Raw State 与 Managed StateKeyed State和Operator State以两种形式存在：托管状态Managed State和原始状态Raw State。 Managed State由Flink运行时控制的数据结构表示，如内部哈希表或RocksDB。例如ValueState，ListState等。Flink的运行时对状态进行编码并将它们写入检查点。 Raw State是指算子保持在它们自己数据结构中的状态。当检查点时，他们只写入一个字节序列到检查点。Flink对状态的数据结构一无所知，只能看到原始字节。 所有数据流函数都可以使用Managed State，但Raw State接口只能在实现算子时使用。建议使用Managed State（而不是Raw State），因为在Managed State下，Flink可以在并行度发生变化时自动重新分配状态，并且还可以更好地进行内存管理。 备注:如果你的Managed State需要自定义序列化逻辑，请参阅相应的指南以确保将来的兼容性。Flink的默认序列化器不需要特殊处理。 3. Managed Keyed StateManaged Keyed State接口提供了对不同类型状态的访问，这些状态的作用域为当前输入元素的key。这意味着这种类型的状态只能用于KeyedStream，可以通过stream.keyBy（...）创建。 现在，我们先看看可用状态的不同类型，然后我们将看到如何在一个程序中使用它们。可用状态是： ValueState &lt;T&gt;：保存了一个可以更新和检索的值（如上所述，作用域为输入元素的key，所以操作看到的每个key可能有一个值）。该值可以使用update（T）来设置，使用T value（）来检索。 ListState &lt;T&gt;：保存了一个元素列表。可以追加元素并检索当前存储的所有元素的Iterable。使用add（T）添加元素，可以使用Iterable &lt;T&gt; get（）来检索Iterable。 ReducingState &lt;T&gt;：保存一个单一的值，表示添加到状态所有值的聚合。接口与ListState相同，但使用add（T）添加的元素，使用指定的ReduceFunction转换为聚合。 AggregatingState &lt;IN，OUT&gt;：保存一个单一的值，表示添加到状态所有值的聚合。与ReducingState不同，聚合后的类型可能与添加到状态的元素类型不同。接口与ListState相同，但使用add（IN）添加到状态的元素使用指定的AggregateFunction进行聚合。 FoldingState &lt;T，ACC&gt;：保存一个单一的值，表示添加到状态所有值的聚合。与ReducingState不同，聚合后类型可能与添加到状态的元素类型不同。接口与ListState相同，但使用add（T）添加到状态的元素使用指定的FoldFunction折叠成聚合。 MapState ：保存了一个映射列表。可以将键值对放入状态，并检索当前存储的所有映射的Iterable。使用put（UK，UV）或putAll（Map &lt;UK，UV&gt;）添加映射。与用户key相关的值可以使用get（UK）来检索。映射，键和值的迭代视图可分别使用entries（），keys（）和values（）来检索。 所有类型的状态都有一个clear（）方法，它清除了当前活跃key的状态，即输入元素的key。 备注:FoldingState和FoldingStateDescriptor已经在Flink 1.4中被弃用，将来会被彻底删除。请改用AggregatingState和AggregatingStateDescriptor。 请记住，这些状态对象仅能用于状态接口，这一点很重要。状态没有必要存储在内存中，也可以驻留在磁盘或其他地方。第二件要记住的是，你从状态获得的值取决于输入元素的key。因此，如果所涉及的key不同，那你在用户函数调用中获得的值可能与另一个调用中的值不同。 为了得到一个状态句柄，你必须创建一个StateDescriptor。它包含了状态的名字（我们将在后面看到，你可以创建多个状态，必须有唯一的名称，以便引用它们），状态值的类型，以及用户自定义函数，如ReduceFunction。根据要检索的状态类型，你可以创建一个ValueStateDescriptor，ListStateDescriptor，ReducingStateDescriptor，FoldingStateDescriptor或MapStateDescriptor。 使用RuntimeContext来访问状态，所以只能在Rich函数中使用。请参阅这里了解有关信息，我们会很快看到一个例子。 在RichFunction中可用的RuntimeContext具有下面访问状态的方法： ValueState getState(ValueStateDescriptor) ReducingState getReducingState(ReducingStateDescriptor) ListState getListState(ListStateDescriptor) AggregatingState getAggregatingState(AggregatingState) FoldingState getFoldingState(FoldingStateDescriptor) MapState getMapState(MapStateDescriptor) 下面是FlatMapFunction的一个例子： Java版本:public class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; &#123; /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; @Override public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception &#123; // access the state value Tuple2&lt;Long, Long&gt; currentSum = sum.value(); // update the count 个数 currentSum.f0 += 1; // add the second field of the input value 总和 currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 &gt;= 2) &#123; out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); &#125; &#125; @Override public void open(Configuration config) &#123; ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( "average", // the state name TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set sum = getRuntimeContext().getState(descriptor); &#125;&#125;// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(0) .flatMap(new CountWindowAverage()) .print();// the printed output will be (1,4) and (1,5) Scala版本:class CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] &#123; private var sum: ValueState[(Long, Long)] = _ override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = &#123; // access the state value val tmpCurrentSum = sum.value // If it hasn&apos;t been used before, it will be null val currentSum = if (tmpCurrentSum != null) &#123; tmpCurrentSum &#125; else &#123; (0L, 0L) &#125; // update the count val newSum = (currentSum._1 + 1, currentSum._2 + input._2) // update the state sum.update(newSum) // if the count reaches 2, emit the average and clear the state if (newSum._1 &gt;= 2) &#123; out.collect((input._1, newSum._2 / newSum._1)) sum.clear() &#125; &#125; override def open(parameters: Configuration): Unit = &#123; sum = getRuntimeContext.getState( new ValueStateDescriptor[(Long, Long)](&quot;average&quot;, createTypeInformation[(Long, Long)]) ) &#125;&#125;object ExampleCountWindowAverage extends App &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment env.fromCollection(List( (1L, 3L), (1L, 5L), (1L, 7L), (1L, 4L), (1L, 2L) )).keyBy(_._1) .flatMap(new CountWindowAverage()) .print() // the printed output will be (1,4) and (1,5) env.execute(&quot;ExampleManagedState&quot;)&#125; 这个例子实现了一个穷人的计数窗口。我们通过第一个字段键入元组（在这个例子中都有相同的key为1）。该函数将计数和总和存储在ValueState中。一旦计数达到2，就输出平均值并清除状态，以便我们从0开始。注意，如果我们元组第一个字段具有不同值，那将为每个不同的输入key保持不同的状态值。 3.1 Scala DataStream API中的状态除了上面介绍的接口之外，Scala API还具有在KeyedStream上使用单个ValueState的有状态map（）或flatMap（）函数的快捷方式。用户函数可以在Option获取ValueState的当前值，并且必须返回将用于更新状态的更新值。val stream: DataStream[(String, Int)] = ...val counts: DataStream[(String, Int)] = stream .keyBy(_._1) .mapWithState((in: (String, Int), count: Option[Int]) =&gt; count match &#123; case Some(c) =&gt; ( (in._1, c), Some(c + in._2) ) case None =&gt; ( (in._1, 0), Some(in._2) ) &#125;) 4. Managed Operator State要使用Managed Operator State，有状态函数可以实现更通用的CheckpointedFunction接口或ListCheckpointed &lt;T extends Serializable&gt;接口。 4.1 CheckpointedFunctionCheckpointedFunction接口提供了使用不同的重分配方案对非Ked State的访问。它需要实现一下两种方法：void snapshotState(FunctionSnapshotContext context) throws Exception;void initializeState(FunctionInitializationContext context) throws Exception; 每当执行检查点时，将调用snapshotState（）。每当用户自定义函数被初始化时，对应的initializeState（）都被调用，或当函数被初始化时，或者当函数实际上从早期的检查点恢复时被调用(The counterpart, initializeState(), is called every time the user-defined function is initialized, be that when the function is first initialized or be that when the function is actually recovering from an earlier checkpoint. )。鉴于此，initializeState（）不仅是初始化不同类型的状态的地方，而且还包括状态恢复逻辑的位置。 目前支持列表式的Managed Operator State。状态应该是一个可序列化的对象列表，相互间彼此独立，因此可以在扩展时重新分配。换句话说，这些对象可以在非Keyed State中重新分配比较细的粒度。根据状态访问方法，定义了以下重新分配方案： 均分再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，列表被平分为与并行算子一样多的子列表。每个算子都可以得到一个可以为空或者包含一个或多个元素的子列表。例如，如果并行度为1，算子的检查点状态包含元素element1和element2，将并行度增加到2时，element1在算子实例0上运行，而element2将转至算子实例1。 合并再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，每个算子都可以获得完整的状态元素列表。 下面是一个有状态的SinkFunction的例子，它使用CheckpointedFunction在将元素输出到外部之前进行缓冲元素。它演示了基本的均分再分配列表状态： Java版本:public class BufferingSink implements SinkFunction&lt;Tuple2&lt;String, Integer&gt;&gt;, CheckpointedFunction, CheckpointedRestoring&lt;ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; &#123; private final int threshold; private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState; private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements; public BufferingSink(int threshold) &#123; this.threshold = threshold; this.bufferedElements = new ArrayList&lt;&gt;(); &#125; @Override public void invoke(Tuple2&lt;String, Integer&gt; value) throws Exception &#123; bufferedElements.add(value); if (bufferedElements.size() == threshold) &#123; for (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123; // send it to the sink &#125; bufferedElements.clear(); &#125; &#125; @Override public void snapshotState(FunctionSnapshotContext context) throws Exception &#123; checkpointedState.clear(); for (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123; checkpointedState.add(element); &#125; &#125; @Override public void initializeState(FunctionInitializationContext context) throws Exception &#123; ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor = new ListStateDescriptor&lt;&gt;( "buffered-elements", TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;)); checkpointedState = context.getOperatorStateStore().getListState(descriptor); if (context.isRestored()) &#123; for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123; bufferedElements.add(element); &#125; &#125; &#125; @Override public void restoreState(ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; state) throws Exception &#123; // this is from the CheckpointedRestoring interface. this.bufferedElements.addAll(state); &#125;&#125; Scala版本:class BufferingSink(threshold: Int = 0) extends SinkFunction[(String, Int)] with CheckpointedFunction with CheckpointedRestoring[List[(String, Int)]] &#123; @transient private var checkpointedState: ListState[(String, Int)] = _ private val bufferedElements = ListBuffer[(String, Int)]() override def invoke(value: (String, Int)): Unit = &#123; bufferedElements += value if (bufferedElements.size == threshold) &#123; for (element &lt;- bufferedElements) &#123; // send it to the sink &#125; bufferedElements.clear() &#125; &#125; override def snapshotState(context: FunctionSnapshotContext): Unit = &#123; checkpointedState.clear() for (element &lt;- bufferedElements) &#123; checkpointedState.add(element) &#125; &#125; override def initializeState(context: FunctionInitializationContext): Unit = &#123; val descriptor = new ListStateDescriptor[(String, Int)]( &quot;buffered-elements&quot;, TypeInformation.of(new TypeHint[(String, Int)]() &#123;&#125;) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) if(context.isRestored) &#123; for(element &lt;- checkpointedState.get()) &#123; bufferedElements += element &#125; &#125; &#125; override def restoreState(state: List[(String, Int)]): Unit = &#123; bufferedElements ++= state &#125;&#125; initializeState方法以FunctionInitializationContext为参数。这用来初始化非keyed state“容器”。这是一个ListState类型的容器，非keyed state对象将在检查点时存储。 注意一下状态是如何被初始化，类似于keyed state状态，使用包含状态名称和状态值类型相关信息的StateDescriptor： Java版本:ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor = new ListStateDescriptor&lt;&gt;( "buffered-elements", TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));checkpointedState = context.getOperatorStateStore().getListState(descriptor); Scala版本:val descriptor = new ListStateDescriptor[(String, Long)]( &quot;buffered-elements&quot;, TypeInformation.of(new TypeHint[(String, Long)]() &#123;&#125;))checkpointedState = context.getOperatorStateStore.getListState(descriptor) 状态访问方法的命名约定包含其重新分配模式及其状态结构。 例如，要使用带有联合重新分配方案的列表状态进行恢复，请使用getUnionListState（descriptor）访问状态。如果方法名称不包含重新分配模式，例如 getListState（descriptor），这表示使用基本的均分重分配方案。 在初始化容器之后，我们使用上下文的isRestored（）方法来检查失败后是否正在恢复。如果是，即我们正在恢复，将会应用恢复逻辑。 如修改后的BufferingSink的代码所示，在状态初始化期间恢复的这个ListState被保存在类变量中，以备将来在snapshotState（）中使用。 在那里ListState清除了前一个检查点包含的所有对象，然后用我们想要进行检查点的新对象填充。 Keyed State也可以在initializeState（）方法中初始化。这可以使用提供的FunctionInitializationContext完成。 4.2 ListCheckpointedListCheckpointed接口是CheckpointedFunction进行限制的一种变体，它只支持在恢复时使用均分再分配方案的列表样式状态。还需要实现以下两种方法： List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;void restoreState(List&lt;T&gt; state) throws Exception; snapshotState()方法应该返回一个对象列表来进行checkpoint，而restoreState()方法在恢复时必须处理这样一个列表。如果状态是不可重分区的，则可以在snapshotState()中返回一个Collections.singletonList(MY_STATE)。 4.2.1 Stateful Source Functions与其他算子相比，有状态的数据源需要得到更多的关注。为了能更新状态以及输出集合的原子性（在失败/恢复时需要一次性语义），用户需要从数据源的上下文中获取锁。 Java版本:public static class CounterSource extends RichParallelSourceFunction&lt;Long&gt; implements ListCheckpointed&lt;Long&gt; &#123; /** current offset for exactly once semantics */ private Long offset; /** flag for job cancellation */ private volatile boolean isRunning = true; @Override public void run(SourceContext&lt;Long&gt; ctx) &#123; final Object lock = ctx.getCheckpointLock(); while (isRunning) &#123; // output and state update are atomic synchronized (lock) &#123; ctx.collect(offset); offset += 1; &#125; &#125; &#125; @Override public void cancel() &#123; isRunning = false; &#125; @Override public List&lt;Long&gt; snapshotState(long checkpointId, long checkpointTimestamp) &#123; return Collections.singletonList(offset); &#125; @Override public void restoreState(List&lt;Long&gt; state) &#123; for (Long s : state) offset = s; &#125;&#125; Scala版本:class CounterSource extends RichParallelSourceFunction[Long] with ListCheckpointed[Long] &#123; @volatile private var isRunning = true private var offset = 0L override def run(ctx: SourceFunction.SourceContext[Long]): Unit = &#123; val lock = ctx.getCheckpointLock while (isRunning) &#123; // output and state update are atomic lock.synchronized(&#123; ctx.collect(offset) offset += 1 &#125;) &#125; &#125; override def cancel(): Unit = isRunning = false override def restoreState(state: util.List[Long]): Unit = for (s &lt;- state) &#123; offset = s &#125; override def snapshotState(checkpointId: Long, timestamp: Long): util.List[Long] = Collections.singletonList(offset)&#125; 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 状态概述]]></title>
    <url>%2F2018%2F01%2F16%2Fflink-stream-state-overview%2F</url>
    <content type="text"><![CDATA[有状态的函数和算子在处理单个元素/事件时存储数据，使得状态state成为任何精细操作的关键构件。 例如： 当应用程序搜索某些特定模式事件时，状态将存储迄今为止遇到的事件序列。 当按每分钟/小时/天聚合事件时，状态保存待处理的聚合事件。 在数据流上训练机器学习模型时，状态保存当前版本的模型参数。 当需要管理历史数据时，状态允许访问过去发生的事件。 Flink 需要了解状态，以便使用检查点进行状态容错，并允许流应用程序使用保存点。 对状态进行了解有助于你对 Flink 应用程序进行扩展，这意味着 Flink 负责在并行实例之间进行重新分配状态。 Flink 的可查询状态queryable state功能允许你在 Flink 运行时在外部访问状态。 在使用状态时，阅读有关Flink的 State Backends 应该对你很有帮助。Flink 提供不同的 State Backends，并指定状态的存储方式和位置。状态可以位于Java的堆内或堆外。根据你的 State Backends，Flink也可以管理应用程序的状态，这意味着Flink进行内存管理(可能会溢写到磁盘，如果有必要)，以允许应用程序保持非常大的状态。State Backends可以在不更改应用程序逻辑的情况下进行配置。 下一步 使用状态：显示如何在Flink应用程序中使用状态，并解释不同类型的状态。 检查点：描述如何启用和配置容错检查点。 可查询状态：解释如何在Flink运行时从外部访问状态。 为Managed State自定义序列化：讨论为状态自定义序列化逻辑及其升级。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 内置的时间戳提取器和Watermark生成器]]></title>
    <url>%2F2018%2F01%2F16%2Fflink-stream-event-timestamp-and-extractors%2F</url>
    <content type="text"><![CDATA[如Flink1.4 生成时间戳与Watermarks所介绍的，Flink提供了一个抽象类，允许程序员可以分配自己的时间戳并发送Watermark。更具体地说，可以通过AssignerWithPeriodicWatermarks或AssignerWithPunctuatedWatermarks接口来实现，具体实现取决于用户具体情况。第一个接口将周期性的发送Watermark，第二个则基于传入记录的某些属性发送Watermark，例如，当在流中遇到特殊元素时。 为了进一步缓解这些任务的编程工作，Flink带有一些内置的时间戳分配器。除了开箱即用的功能外，它们的实现也可以作为自定义实现的一个例子。 1. 递增时间戳分配器周期性生成Watermark最简单的例子是给定数据源任务中的时间戳会递增顺序出现。在这种情况下，由于没有时间戳比当前时间戳还早到达的，所以当前时间戳可以始终充当Watermark。 请注意，每个并行数据源任务的时间戳必须是升序的。例如，如果在特定设置中，一个并行数据源实例读取一个Kafka分区，那么只需要确保在每个Kafka分区内时间戳是升序的即可。每当并行数据流被shuffle，union，连接或合并时，Flink的Watermark合并机制能够产生正确的watermarks。 Java版本:DataStream&lt;MyEvent&gt; stream = ...DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;MyEvent&gt;() &#123; @Override public long extractAscendingTimestamp(MyEvent element) &#123; return element.getCreationTime(); &#125;&#125;); Scala版本:val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime ) 2. 允许固定数量延迟的分配器周期性生成Watermark的另一个例子是当Watermark落后于数据流中看到的最大时间戳(事件时间)一固定数量时间(a fixed amount of time)。这种情况涵盖了事先知道流中可能遇到的最大延迟的场景，例如，当创建一个测试用的自定义数据源时，其上每个元素的时间戳分布在一个固定时间段内。对于这些情况，Flink提供了BoundedOutOfOrdernessTimestampExtractor，带有一个maxOutOfOrderness参数，即在计算给定窗口最终结果一个元素在被忽略之前允许延迟的最大时间。延迟对应于t-t_w的结果，其中t是元素的(事件时间)时间戳，t_w是前一个Watermark时间戳。如果延迟大于0，则该元素被认为是迟到的，并且在计算其相应窗口的作业结果时默认为忽略该元素。 Java版本:DataStream&lt;MyEvent&gt; stream = ...DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;MyEvent&gt;(Time.seconds(10)) &#123; @Override public long extractTimestamp(MyEvent element) &#123; return element.getCreationTime(); &#125;&#125;); Scala版本:val stream: DataStream[MyEvent] = ...val withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime )) 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 图解Watermark]]></title>
    <url>%2F2018%2F01%2F15%2Fflink-stream-graphic-watermark%2F</url>
    <content type="text"><![CDATA[如果你正在构建实时流处理应用程序，那么事件时间处理是你迟早必须使用的功能之一。因为在现实世界的大多数用例中，消息到达都是无序的，应该有一些方法，通过你建立的系统知道消息可能延迟到达，并且有相应的处理方案。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中使用它。 EventTime是事件在现实世界中发生的时间，ProcessingTime是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于处理时间的系统，看看它的缺点。 我们创建一个大小为10秒的滑动窗口，每5秒滑动一次，在窗口结束时，系统将发送在此期间收到的消息数。 一旦了解了EventTime处理在滑动窗口如何工作，那么了解其在滚动窗口中如何工作也就不是难事。所以让我们开始吧。 1. 基于处理时间的系统在这个例子中，我们期望消息具有一定格式的值，时间戳就是消息的那个值，同时时间戳是在源产生此消息的时间。由于我们正在构建基于处理时间的系统，因此以下代码忽略了时间戳部分。 我们需要知道消息中应包含消息产生时间是很重要的。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己生成这个产生时间。稍后我们将看到，事件时间处理提取此时间戳信息来处理延迟消息。 val text = senv.socketTextStream(&quot;localhost&quot;, 9999)val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125; .keyBy(0) .timeWindow(Time.seconds(10), Time.seconds(5)) .sum(1)counts.printsenv.execute(&quot;ProcessingTime processing example&quot;) 1.1 消息无延迟到达假设源分别在第13秒产生两个类型a的消息以及在第16秒产生一个消息。(小时和分钟不重要，因为窗口大小只有10秒)。 这些消息将落入如下所示窗口中。前两个在第13秒产生的消息将落入窗口1[5s-15s]和窗口2[10s-20s]中，第三个在第16秒产生的消息将落入窗口2[10s-20s]和窗口3[15s-25s]中。每个窗口得到的最终计数分别为(a，2)，(a，3)和(a，1)。 该输出跟预期的输出是一样的。现在我们看看当一个消息延迟到达系统时会发生什么。 1.2 消息延迟到达现在假设其中一条消息(在第13秒产生)可能由于网络拥塞延迟6秒(第19秒到达)。你能猜测出这个消息会落入哪个窗口？ 延迟的消息落入窗口2和窗口3中，因为19在10-20和15-25之间。窗口2的计算没有任何问题(因为消息本应该落入这个窗口)，但是它影响了窗口1和窗口3的计算结果。现在我们将尝试使用基于EventTime处理来解决这个问题。 2. 基于EventTime的系统要使用基于EventTime处理，我们需要一个时间戳提取器，从消息中提取事件时间信息。请记住，消息是有格式值，时间戳。 extractTimestamp方法获取时间戳并将其作为Long类型返回。现在忽略getCurrentWatermark方法，我们稍后会介绍： class TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable &#123; override def extractTimestamp(e: String, prevElementTimestamp: Long) = &#123; e.split(&quot;,&quot;)(1).toLong &#125; override def getCurrentWatermark(): Watermark = &#123; new Watermark(System.currentTimeMillis) &#125;&#125; 现在我们需要设置这个时间戳提取器，并将TimeCharactersistic设置为EventTime。其余的代码与ProcessingTime的情况保持一致： senv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val text = senv.socketTextStream(&quot;localhost&quot;, 9999) .assignTimestampsAndWatermarks(new TimestampExtractor)val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125; .keyBy(0) .timeWindow(Time.seconds(10), Time.seconds(5)) .sum(1)counts.printsenv.execute(&quot;EventTime processing example&quot;) 运行上述代码的结果如下图所示： 结果看起来更好一些，窗口2和3现在是正确的结果，但是窗口1仍然是有问题的。Flink没有将延迟的消息分配给窗口3，是因为在当前检查消息的事件时间，知道它不应该出现在窗口3中。但是为什么没有将消息分配给窗口1？原因是当延迟的信息到达系统时(第19秒)，窗口1的评估(evaluation)已经完成了(第15秒)。现在让我们尝试通过使用Watermark来解决这个问题。 3. WatermarkWatermark是一个非常重要概念，我将尽力给你一个简短的概述。如果你有兴趣了解更多信息，你可以从Google中观看这个演讲，还可以从dataArtisans那里阅读此博客。 Watermark本质上是一个时间戳。当Flink中的算子(operator)接收到Watermark时，它明白它不会再看到比该时间戳更早的消息。因此Watermark也可以被认为是告诉Flink在EventTime中多远的一种方式。 在这个例子的目的，就是把Watermark看作是告诉Flink一个消息可能延迟多少的方式。在上一次尝试中，我们将Watermark设置为当前系统时间。因此，期望消息没有任何的延迟。现在我们将Watermark设置为当前时间减去5秒，这就告诉Flink我们期望消息最多延迟5秒钟，这是因为每个窗口仅在Watermark通过时被评估。由于我们的Watermark是当前时间减去5秒，所以第一个窗口[5s-15s]将会在第20秒被评估。类似地，窗口[10s-20s]将会在第25秒进行评估，依此类推(译者注:窗口延迟评估)。 override def getCurrentWatermark(): Watermark = &#123; new Watermark(System.currentTimeMillis - 5000)&#125; 这里我们假定事件时间比当前系统时间晚5秒，但事实并非总是如此(有可能6秒，7秒等等)。在许多情况下，最好保留迄今为止收到的最大时间戳(从消息中提取)。使用迄今为止收到的最大时间戳减去预期的延迟时间来代替用当前系统时间减去预期的延迟时间。 进行上述更改后运行代码的结果是： 最后我们得到了正确的结果，所有窗口都按照预期输出计数，(a，2)，(a，3)和(a，1)。 4. Allowed Lateness我们也可以使用AllowedLateness功能设置消息的最大允许延迟时间来解决这个问题。 在我们之前使用Watermark - delay的方法中，只有当Watermark超过window_length + delay时，窗口才会被触发计算。如果你想要适应延迟事件，并希望窗口按时触发，则可以使用Allowed Lateness。 如果设置了允许延迟，Flink不会丢弃消息，除非它超过了window_end_time + delay的延迟时间。一旦收到一个延迟消息，Flink会提取它的时间戳并检查是否在允许的延迟时间内，然后检查是否触发窗口(按照触发器设置)。 因此，请注意，在这种方法中可能会多次触发窗口，如果你仅需要一次处理，你需要使你的sink具有幂等性。 5. 结论实时流处理系统的重要性日益增长，延迟消息的处理是你构建任何此类系统的一部分。在这篇博文中，我们看到延迟到达的消息会影响系统的结果，以及如何使用ApacheFlink的事件时间功能来解决它们。 原文:http://vishnuviswanath.com/flink_eventtime.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 生成时间戳与Watermarks]]></title>
    <url>%2F2018%2F01%2F15%2Fflink-stream-event-timestamps-and-watermark%2F</url>
    <content type="text"><![CDATA[本节适用于在事件时间上运行的程序。有关事件时间，处理时间和提取时间的介绍，请参阅Flink1.4 事件时间与处理时间。 为了处理事件时间，流处理程序需要相应地设置TimeCharacteristic。 Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Scala版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 1. 分配时间戳为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配事件时间戳。这通常通过访问/提取元素中某个字段的时间戳来完成。时间戳分配与生成watermarks相结合，告诉系统有关事件时间的进度progress。分配时间戳和生成watermarks有两种方法： 直接在数据流源中分配与生成 通过时间戳分配器/watermark生成器：在Flink时间戳分配器中也会定义要发送的watermarks 备注:时间戳和watermarks都是从Java历元1970-01-01T00：00：00Z以来的毫秒数。 1.1 带有时间戳和watermarks的数据源函数流数据源还可以直接为它们产生的元素分配时间戳，并且也可以发送watermarks。如果数据源分配了时间戳，那么就不需要时间戳分配器。 备注:如果继续使用时间戳分配器，将会覆盖数据源提供的时间戳和watermarks。 如果直接向数据源中的元素分配时间戳，数据源必须使用SourceContext上的collectWithTimestamp()方法。如果要生成watermarks，数据源必须调用emitWatermark（Watermark）函数。 以下是分配时间戳并生成watermarks的源(non-checkpointed)的简单示例： Java版本:@Overridepublic void run(SourceContext&lt;MyType&gt; ctx) throws Exception &#123; while (/* condition */) &#123; MyType next = getNext(); ctx.collectWithTimestamp(next, next.getEventTimestamp()); if (next.hasWatermarkTime()) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime())); &#125; &#125;&#125; Scala版本:override def run(ctx: SourceContext[MyType]): Unit = &#123; while (/* condition */) &#123; val next: MyType = getNext() ctx.collectWithTimestamp(next, next.eventTimestamp) if (next.hasWatermarkTime) &#123; ctx.emitWatermark(new Watermark(next.getWatermarkTime)) &#125; &#125;&#125; 1.2 时间戳分配器/Watermark生成器时间戳分配器接收数据流并产生一个新的数据流，包含带有时间戳的元素和Watermark。如果原始流已经拥有时间戳或watermarks，那么如果使用时间戳分配器将会覆盖它们。 时间戳分配器通常在数据源之后立马指定，但也不是严格遵循这样的原则。例如，一个常见的模式是在时间戳分配器之前需要进行解析(MapFunction)和过滤(FilterFunction)。无论如何，时间戳分配器都需要在第一个基于事件时间的操作(例如第一个窗口操作)之前被指定。但也有特殊情况，当使用Kafka作为流作业的数据源时，Flink允许在数据源(消费者)内部定义时间戳分配器/watermarks生成器。有关如何执行此操作的更多信息，请参见Kafka Connector文档。 备注:本节的其余部分介绍了程序员为了创建自己的时间戳提取器/watermarks生成器而必须实现的主要接口。如果要查看Flink内置的执行器，请参阅[Pre-defined Timestamp Extractors / Watermark Emitters](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html) Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);DataStream&lt;MyEvent&gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo);DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream .filter( event -&gt; event.severity() == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks());withTimestampsAndWatermarks .keyBy( (event) -&gt; event.getGroup() ) .timeWindow(Time.seconds(10)) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter());val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())withTimestampsAndWatermarks .keyBy( _.getGroup ) .timeWindow(Time.seconds(10)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...) 1.2.1 Periodic Watermarks 分配器AssignerWithPeriodicWatermarks分配时间戳并定期生成Watermarks(可能取决于流元素，或纯粹基于处理时间)。 通过ExecutionConfig.setAutoWatermarkInterval()定义Watermarks的时间间隔(每n毫秒)。每次调用分配器的getCurrentWatermark()方法，如果返回的Watermark非null，并且大于先前的Watermark，则会发送(emitted)这个新的Watermarks。 以下是带有周期性Watermark的时间戳分配器的两个简单示例: Java版本:/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */public class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks&lt;MyEvent&gt; &#123; private final long maxOutOfOrderness = 3500; // 3.5 seconds private long currentMaxTimestamp; @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; long timestamp = element.getCreationTime(); currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp); return timestamp; &#125; @Override public Watermark getCurrentWatermark() &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound return new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125;&#125;/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */public class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks&lt;MyEvent&gt; &#123; private final long maxTimeLag = 5000; // 5 seconds @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; return element.getCreationTime(); &#125; @Override public Watermark getCurrentWatermark() &#123; // return the watermark as current time minus the maximum time lag return new Watermark(System.currentTimeMillis() - maxTimeLag); &#125;&#125; Scala版本:/** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxOutOfOrderness = 3500L; // 3.5 seconds var currentMaxTimestamp: Long; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; val timestamp = element.getCreationTime() currentMaxTimestamp = max(timestamp, currentMaxTimestamp) timestamp; &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current highest timestamp minus the out-of-orderness bound new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125;&#125;/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123; val maxTimeLag = 5000L; // 5 seconds override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def getCurrentWatermark(): Watermark = &#123; // return the watermark as current time minus the maximum time lag new Watermark(System.currentTimeMillis() - maxTimeLag) &#125;&#125; 1.2.2 Punctuated Watermarks 分配器每当某个事件表明一个新的Watermarks可能要生成时，需要调用AssignerWithPunctuatedWatermarks方法来生成Watermarks(To generate watermarks whenever a certain event indicates that a new watermark might be generated, use AssignerWithPunctuatedWatermarks)。对于这个类，Flink首先调用extractTimestamp()方法为元素分配时间戳，然后立即调用该元素上的checkAndGetNextWatermark()方法。 把在extractTimestamp()方法中分配的时间戳传递给checkAndGetNextWatermark()方法，并且可以决定是否要生成Watermarks。只要checkAndGetNextWatermark()方法返回非null的Watermark，并且该Watermark比以前最新的Watermark都大，则会发送这个新的Watermark。 Java版本:public class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks&lt;MyEvent&gt; &#123; @Override public long extractTimestamp(MyEvent element, long previousElementTimestamp) &#123; return element.getCreationTime(); &#125; @Override public Watermark checkAndGetNextWatermark(MyEvent lastElement, long extractedTimestamp) &#123; return lastElement.hasWatermarkMarker() ? new Watermark(extractedTimestamp) : null; &#125;&#125; Scala版本:class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123; override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123; element.getCreationTime &#125; override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123; if (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null &#125;&#125; 备注:可以在每个单独的事件上生成Watermark。但是，由于每个Watermark在下游引起一些计算，所以过多的Watermark会降低性能。 2. 每个Kafka分区一个时间戳当使用Apache Kafka作为数据源时，每个Kafka分区都可能有一个简单的事件时间模式(时间戳按升序递增或有界无序)。然而，当消费Kafka中的流时，多个分区通常并行消费，来自多个分区的事件会交叉在一起，破坏每个分区模式。 在这种情况下，你可以使用Flink的Kafka分区感知Watermark的生成(Kafka-partition-aware watermark generation)。使用该特性，在Kafka消费者中，每个Kafka分区都生成watermark，并且每个分区的watermark的合并方式与在数据流shuffle上合并方式相同(the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.)。 例如，如果在每个Kafka分区中的事件时间戳严格递增，则使用递增时间戳watermark生成器生成每个分区的watermark，在整体watermark上产生的结果也非常好。 下图显示了如何使用每个Kafka分区生成watermark，以及在这种情况下watermark如何通过流数据流进行传播: Java版本:FlinkKafkaConsumer09&lt;MyType&gt; kafkaSource = new FlinkKafkaConsumer09&lt;&gt;("myTopic", schema, props);kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;MyType&gt;() &#123; @Override public long extractAscendingTimestamp(MyType element) &#123; return element.eventTimestamp(); &#125;&#125;);DataStream&lt;MyType&gt; stream = env.addSource(kafkaSource); Scala版本:val kafkaSource = new FlinkKafkaConsumer09[MyType](&quot;myTopic&quot;, schema, props)kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123; def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp&#125;)val stream: DataStream[MyType] = env.addSource(kafkaSource) 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream Exactly once 未必严格一次]]></title>
    <url>%2F2018%2F01%2F11%2FStream%2FExactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E6%AC%A1%2F</url>
    <content type="text"><![CDATA[分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中”严格一次（Exactly-once）” 是很多引擎追求的目标之一，很多 SPE 均宣称可提供”严格一次”的处理语义。 然而exactly-once具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用exactly-once来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在exactly-once处理语义方面的差异，以及为什么exactly-once更适合称之为有效一次(Effectively-once)。同时本文还将探讨在实现所谓exactly-once的语义过程中，各类常用技术之间需要进行的取舍。 1. 背景流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图，大部分情况（但也并非总是如此）下也可以看作有向无环图（Directed acyclic graph，DAG）。在这种图中，每个边（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边的数据或事件的算子（Operator）。有两种特殊类型的顶点，通常称之为 Source 和 Sink，Source 会消耗外部数据/事件并将其注入应用程序，而Sink通常负责收集应用程序生成的结果。图1展示了这样的一个流应用程序范例。 执行流/事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：最多一次（At-most-once）、最少一次（At-least-once），以及严格一次（Exactly-once）。这些不同处理语义可粗略理解如下： 2. 最多一次这其实是一种”尽力而为”的方法。数据或事件可以保证被应用程序中的所有算子最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图2列举了一个范例。 3. 最少一次数据或事件可保证被应用程序图中的所有算子最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为”最少一次”。图3展示了一个范例。在本例中，第一个算子最初处理事件时失败了，随后重试并成功，然后第二次重试并再次成功，然而第二次重试实际上是不必要的。 3. 严格一次事件可保证被流应用程序中的所有算子“严格一次”处理，哪怕遇到各种失败。为了实现exactly-once处理语义，通常主要会使用下列两种机制： 分布式快照/状态检查点 最少一次事件交付，外加消息去重 通过分布式快照/状态检查点方法实现的exactly-once是由 Chandy-Lamport 分布式快照算法启发而来的。在这种机制中，会定期为流应用程序中每个算子的所有状态创建检查点，一旦系统中任何位置出现失败，每个算子的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被回退到最新一致状态，并从该状态开始重新处理。图4展示了这种机制的一些基本概念。 在图4中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，算子在处理传入的数据时失败了。此时 S = 4 这个状态值已经被保存到持久存储中，而 S = 12 状态值正位于算子的内存中。为了解决这种差异，在 T3 时处理图将状态回退至 S = 4，并”重播”流中每个连续状态直到最新状态，并处理每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。 实现exactly-once的另一种方法是在实现至少一次事件交付的同时在每个算子一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个算子中移除重复的事件，随后才将结果事件发送给用户在算子中定义的逻辑。这种机制要求为每个算子保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel 以及 Apache Kafka Streams 等机制。图 5 展示了这种机制的概况。 4. 严格一次真的就一次吗？接着重新考虑一下exactly-once处理语义实际上能为最终用户提供怎样的保证。exactly-once这样的标签对于描述严格一次起到了一定的误导效果。 有些人可能认为exactly-once描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。但实际上没有一个 SPE 能完全保证只处理一次。面对各种可能的失败，根本不可能保证每个算子中由用户自定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。 假设这样一个场景：有个流处理算子需要执行 Map 操作输出传入事件的 ID，随后原样返回事件。例如这个操作可能使用了如下的虚构代码：Map (Event event) &#123; Print &quot;Event ID: &quot; + event.getId() Return event&#125; 每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑严格一次执行可以得到保证，那么事件 ID 只输出一次。然而这一点永远无法保证，因为用户自定义的逻辑执行过程中可能随时随地面临失败。SPE 无法自行判断用户自定义的逻辑到底执行到哪一步了。因此任何用户自定义逻辑都无法保证只执行一次。这也意味着用户自定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。 那么当 SPE 宣称提供exactly-once的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称exactly-once处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。 上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个算子的状态，并自动提交状态更新。对于机制1（分布式快照/状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个算子的状态检查点）；对于机制2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个算子的状态以及每个算子追踪已经被成功处理过事件的事务日志。 状态提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次(The committing of state or applying updates to the durable backend that is the source of truth can be described as occurring exactly-once)。然而在计算状态的更新/改动，例如处理在事件上执行用户自定义逻辑的事件，但是如果出现失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的效果只会在持久后端状态存储中体现一次。因此在这里我们认为”有效一次（Effectively-once）”术语可以更精确地描述这样的处理语义。 5. 分布式快照与至少一次事件交付外加去重机制的对比从语义的角度来看，分布式快照以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，有可能会产生明显的性能差异。 基于机制1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在流应用程序正常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率会更高一点，这会导致 SPE 需要暂停应用程序并回滚所有算子的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。 机制2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被算子的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个算子中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何算子遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个算子遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他算子的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格: 分布式快照/状态检查点 利 弊 性能和资源开销小 从失败中恢复时的性能影响大 随着拓扑规模的增大，对性能的潜在影响增高 至少一次交付外加去重 利 弊 失败对性能的影响更为局部 可能需要大量的存储与基础设施的支持 失败的影响未必会随着拓扑规模一起增加 每个算子处理每个事件均会产生性能开销 虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播/重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，算子实际上将具备幂等的特性。 6. 结论希望本文可以帮助大家意识到exactly-once这个术语极具误导性。提供exactly-once的处理语义实际上意味着在对流处理引擎所管理的算子的状态进行不同更新只会影响一次。exactly-once完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此这里我们更愿意使用有效一次这个术语来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只影响一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格/有效一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊。 原文:https://streaml.io/blog/exactly-once/ 译文:https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 对于流处理技术的谬见]]></title>
    <url>%2F2018%2F01%2F11%2FStream%2F%5BStream%5D%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81%2F</url>
    <content type="text"><![CDATA[我们在思考流处理问题上花了很多时间，更酷的是，我们也花了很多时间帮助其他人认识流处理，以及如何在他们的组织里应用流处理来解决数据问题。 我们首先要做的是纠正人们对流处理（作为一个快速变化的领域，这里有很多误见值得我们思考）的错误认识。 在这篇文章里，我们选出了其中的六个作为例子。因为我们对Apache Flink比较熟悉，所以我们会基于Flink来讲解这些例子。 谬见1：没有不使用批处理的流（Lambda架构） 谬见2：延迟和吞吐量：只能选择一个 谬见3：微批次意味着更好的吞吐量 谬见4：Exactly once？完全不可能 谬见5：流只能被应用在“实时”场景里 谬见6：不管怎么样，流仍然很复杂 1. 谬见1：没有不使用批处理的流（Lambda架构）Lambda架构在Apache Storm的早期阶段和其它流处理项目里是一个很有用的设计模式。这个架构包含了一个快速流层和一个批次层。 之所以使用两个单独的层，是因为Lambda架构里的流处理只能计算出大致的结果（也就是说，如果中间出现了错误，那么计算结果就不可信），而且只能处理相对少量的事件。 就算Storm的早期版本存在这样的问题，但现今的很多开源流处理框架都具有容错能力，它们可以在出现故障的前提下生成准确的计算结果，而且具有高吞吐的计算能力。所以没有必要再为了分别得到“快”和“准确”的结果而维护多层架构。现今的流处理器（比如Flink）可以同时帮你得到两种结果。 好在人们不再更多地讨论Lambda架构，说明流处理正在走向成熟。 2. 谬见2：延迟和吞吐量：只能选择一个早期的开源流处理框架要么是高吞吐的，要么是低延迟的，而海量且快速一直未能成为开源流处理框架的代名词。 不过Flink（可能还有其它的框架）就同时提供了高吞吐和低延迟。这里有一个基准测试结果的样例。 让我们从底层来剖析这个例子，特别是从硬件层，并结合具有网络瓶颈的流处理管道（很多使用Flink的管道都有这个瓶颈）。在硬件层不应该存在需要作出权衡的条件，所以网络才是影响吞吐量和延迟的主要因素。 一个设计良好的软件系统应该会充分利用网络的上限而不会引入瓶颈问题。不过对Flink来说，总是有可优化的空间，可以让它更接近硬件所能提供的效能。使用一个包含10个节点的集群，Flink现在每秒可以处理千万级别的事件量，如果扩展到1000个节点，它的延迟可以降低到几十毫秒。在我们看来，这种水平已经比很多现有的方案高出很多。 谬见3：微批次意味着更好的吞吐量我们可以从另一个角度来讨论性能，不过先让我们来澄清两个容易混淆的概念： 微批次:微批次建立在传统批次之上，是处理数据的一个执行或编程模型。通过这项技术，进程或任务可以把一个流当作一系列小型的批次或数据块(参阅:Apache Storm 微批次设计模式)。 缓冲:缓冲技术用于对网络、磁盘、缓存的访问进行优化。Wikipedia完美地把它定义为物理内存里的一块用于临时储存移动数据的区域。 那么第3个缪见就是说，使用微批次的数据处理框架能够比每次处理一个事件的框架达到更高的吞吐量，因为微批次在网络上传输的效率更高。这个缪见忽略了一个事实，流框架不会依赖任何编程模型层面的批次，它们只会在物理层面使用缓冲。Flink确实也会对数据进行缓冲，也就是说它会通过网络发送一组处理过的记录，而不是每次发送一条记录。从性能方面说，不对数据进行缓冲是不可取的，因为通过网络逐个发送记录不会带来任何性能上的好处。所以我们得承认在物理层面根本不存在类似一次一条记录这样的情况。 不过缓冲只能作为对性能的优化，所以缓冲： 对用户是不可见的 不应该对系统造成任何影响 不应该出现人为的边界 不应该限制系统功能 所以对Flink的用户来说，他们开发的程序能够单独地处理每个记录，那是因为Flink为了提升性能隐藏了使用缓冲的细节。 事实上，在任务调度里使用微批次会带来额外的开销，而如果这样做是为了降低延迟，那么这种开销会只增不减！流处理器知道该如何利用缓冲的优势而不会带来任务调度方面的开销。 4. 谬见4：Exactly once？完全不可能这个缪见包含了几个方面的内容： 从根本上说，Exactly once是不可能的 从端到端的Exactly once是不可能的 Exactly once从来都不是真实世界的需求 Exactly once以牺牲性能为代价 我们退一步讲，我们并不介意Exactly once这种观点的存在。Exactly once原先指的是一次性传递，而现在这个词被随意用在流处理里，让这个词变得令人困惑，失去了它原本的意义。不过相关的概念还是很重要的，我们不打算跳过去。 为了尽量准确，我们把一次性状态和一次性传递视为两种不同的概念。因为之前人们对这两个词的使用方式导致了它们的混淆。Apache Storm使用at least once来描述传递(Storm不支持状态)，而Apache Samza使用at least once来描述应用状态。 (1) 一次性状态是指应用程序在经历了故障以后恍如没有发生过故障一样。例如，假设我们在维护一个计数器应用程序，在发生了一次故障之后，它既不能多计数也不能少计数。在这里使用Exactly once这个词是因为应用程序状态认为每个消息只被处理了一次。 (2) 一次性传递是指接收端(应用程序之外的系统)在故障发生后会收到处理过的事件，恍如没有发生过故障一样。 流处理框架在任何情况下都不保证一次性传递，但可以做到一次性状态。Flink可以做到一次性状态，而且不会对性能造成显著影响。Flink还能在与Flink检查点相关的数据槽上做到一次性传递。 Flink检查点就是应用程序状态的快照，Flink会为应用程序定时异步地生成快照。这就是Flink在发生故障时仍然能保证一次性状态的原因：Flink定时记录（快照）输入流的读取位置和每个操作数的相关状态。如果发生故障，Flink会回滚到之前的状态，并重新开始计算。所以说，尽管记录被重新处理，但从结果来看，记录好像只被处理过一次。 那么端到端的一次性处理呢？通过恰当的方式让检查点兼具事务协调机制是可能的，换句话说，就是让源操作和目标操作参与到检查点里来。在框架内部，结果是一次性的，从端到端来看，也是一次性的，或者说接近一次性。例如，在使用Flink和Kafka作为数据源并发生数据槽（HDFS）滚动时，从Kafka到HDFS就是端到端的一次性处理。类似地，在把Kafka作为Flink的源并且把Cassandra作为Flink的槽时，如果针对Cassandra的更新是幂等时，那么就可以实现端到端的一次性处理。 值得一提的是，利用Flink的保存点，检查点可以兼具状态版本机制。使用保存点，在保持状态一致性的同时还可以“随着时间移动”。这样可以让代码的更新、维护、迁移、调试和各种模拟测试变得简单。 5. 谬见5：流只能被应用在“实时”场景里这个谬见包括几点内容： 我没有低延迟的应用，所以我不需要流处理器 流处理只跟那些持久化之前的过渡数据有关系 我们需要批处理器来完成笨重的离线计算 现在是时候思考一下数据集的类型和处理模型之间的关系了。 (1) 首先，有两种数据集： 没有边界的：从非预定义的端点持续产生的数据 有边界的：有限且完整的数据 很多真实的数据集是没有边界的，不管这些数据时存储在文件里，还是在HDFS的目录里，还是在像Kafka这样的系统里。举一些例子： 移动设备或网站用户的交互信息 物理传感器提供的度量指标 金融市场数据 机器日志数据 实际上，在现实世界中很难找到有边界的数据集，不过一个公司所有大楼的位置信息倒是有边界的（不过它也会随着公司业务的增长而变化）。 (2) 其次，有两种处理模型： 流：只要有数据生成就会一直处理 批次：在有限的时间内结束处理，并释放资源 让我们再深入一点，来区分两种没有边界的数据集：连续性流和间歇性流。 使用任意一种模型来处理任意一种数据集是完全可能的，虽然这不是最优的做法。例如，批次处理模型被长时间地应用在无边界的数据集上，特别是间歇性的无边界数据集。现实情况是，大多数批处理任务是通过调度来执行的，每次只处理无边界数据集的一小部分。这意味着流的无边界特质会给某些人带来麻烦（那些工作在流入管道上的人）。 批处理是无状态的，输出只取决于输入。现实情况是，批处理任务会在内部保留状态（比如reducer经常会保留状态），但这些状态只限在批次的边界内，而且它们不会在批次间流窜。当有人尝试实现类似带有”事件时间戳”的时间窗，那么”批次的边界内状态”就会变得很有用，这在处理无边界数据集时是个很常用的手段。处理无边界数据集的批处理器将不可避免地遇到延迟事件(因为上游的延迟)，批次内的数据有可能因此变得不完整。要注意，这里假设我们是基于事件时间戳来移动时间窗的，因为事件时间戳是现实当中最为准确的模型。在执行批处理的时候，迟到的数据会成为问题，即使通过简单的时间窗修复(比如翻转或滑动时间窗)也解决不了这个问题，特别是如果使用会话时间窗，就更难以处理了。因为完成一个计算所需要的数据不会都在一个批次里，所以在使用批次处理无边界数据集时，很难保证结果的正确性。最起码，它需要额外的开销来处理迟到的数据，还要维护批次之间的状态(要等到所有数据达到后才开始处理，或者重新处理批次)。 Flink内建了处理迟到数据的机制，迟到数据被视为真实世界无边界数据的正常现象，所以Flink设计了一个流处理器专门处理迟到数据。有状态的流处理器更适合用来处理无边界数据集，不管数据集是持续生成的还是间歇生成的。使用流处理器只是个锦上添花的事情。 6. 缪见6：不管怎么样，流仍然很复杂这是最后一个缪见。你也许会想：”理论虽好，但我仍然不会采用流技术，因为……”： 流框架难以掌握 流难以解决时间窗、事件时间戳、触发器的问题 流需要结合批次，而我已经知道如何使用批次，那为什么还要使用流？ 我们从来没有打算怂恿你使用流，虽然我们觉得流是个很酷的东西。我们相信，是否使用流完全取决于数据和代码的特点。在做决定之前问问自己：”我正在跟什么样类型的数据集打交道？” 无边界的（用户活动数据、日志、传感器数据） 有边界的 然后再问另一个问题：”哪部分变化最频繁？” 代码比数据变化更频繁 数据比代码变化更频繁 对于数据比代码变化更频繁的情况，例如在经常变化的数据集上执行一个相对固定的查询操作，这样会出现流方面的问题。所以，在认定流是一个”复杂”的东西之前，你可能在不知不觉中已经解决过流方面的问题！你可能使用过基于小时的批次任务调度，团队里的其他人可以创建和管理这些批次（在这种情况下，你得到的结果可能是不准确的，而你意识不到这样的结果是批次的时间问题和之前提过的状态问题造成的）。 为了能够提供一组封装了这些时间和状态复杂性的API，Flink社区为此工作了很长时间。在Flink里可以很简单地处理事件时间戳，只要定义一个时间窗口和一个能够抽取时间戳和水印的函数(只在每个流上调用一次)。处理状态也很简单，类似于定义Java变量，再把这些变量注册到Flink。使用Flink的StreamSQL可以在源源不断的流上面运行SQL查询。 最后一点：对代码比数据变化更频繁的情况该怎么办？对于这种情况，我们认为你遇到了探索性问题。使用笔记本或其它类似的工具进行迭代可能适合用来解决探索性问题。在代码稳定了之后，你仍然会碰到流方面的问题。我们建议从一开始就使用长远的方案来解决流方面的问题。 7. 流处理的未来随着流处理的日渐成熟和这些缪见的逐步淡去，我们发现流正朝着除分析应用之外的领域发展。正如我们所讨论的那样，真实世界正连续不断地生成数据。 原文:http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 主流流处理框架比较(2)]]></title>
    <url>%2F2018%2F01%2F10%2FStream%2F%5BStream%5D%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83(2)%2F</url>
    <content type="text"><![CDATA[在上篇文章中，我们过了下基本的理论，也介绍了主流的流处理框架：Storm，Trident，Spark Streaming，Samza和Flink。今天咱们来点有深度的主题，比如，容错，状态管理或者性能。除此之外，我们也将讨论开发分布式流处理应用的指南，并给出推荐的流处理框架。 1. 容错性流处理系统的容错性与生俱来的比批处理系统难实现。当批处理系统中出现错误时，我们只需要把失败的部分简单重启即可；但对于流处理系统，出现错误就很难恢复。因为线上许多作业都是7 x 24小时运行，不断有输入的数据。流处理系统面临的另外一个挑战是状态一致性，因为重启后会出现重复数据，并且不是所有的状态操作是幂等的。容错性这么难实现，那下面我们看看各大主流流处理框架是如何处理这一问题。 1.1 Apache StormStorm使用上游数据备份和消息确认的机制来保障消息在失败之后会重新处理。消息确认原理：每个操作都会把前一次的操作处理消息的确认信息返回。Topology的数据源备份它生成的所有数据记录。当所有数据记录的处理确认信息收到，备份即会被安全拆除。失败后，如果不是所有的消息处理确认信息收到，那数据记录会被数据源数据替换。这保障了没有数据丢失，但数据结果会有重复，这就是at-least once传输机制。 Storm采用取巧的办法完成了容错性，对每个源数据记录仅仅要求几个字节存储空间来跟踪确认消息。纯数据记录消息确认架构，尽管性能不错，但不能保证exactly once消息传输机制，所有应用开发者需要处理重复数据。Storm存在低吞吐量和流控问题，因为消息确认机制在反压下经常误认为失败。 1.2 Spark StreamingSpark Streaming实现微批处理，容错机制的实现跟Storm不一样。微批处理的想法相当简单。Spark在集群各worker节点上处理micro-batches。每个micro-batches一旦失败，重新计算就行。因为micro-batches本身的不可变性，并且每个micro-batches也会持久化，所以exactly once传输机制很容易实现。 1.3 SamzaSamza的实现方法跟前面两种流处理框架完全不一样。Samza利用消息系统Kafka的持久化和偏移量。Samza监控任务的偏移量，当任务处理完消息，相应的偏移量被移除。消息的偏移量会被checkpoint到持久化存储中，并在失败时恢复。但是问题在于：从上次checkpoint中修复偏移量时并不知道上游消息已经被处理过，这就会造成重复。这就是at least once传输机制。 1.4 Apache FlinkFlink的容错机制是基于分布式快照实现的，这些快照会保存流处理作业的状态(本文对Flink的检查点和快照不进行区分，因为两者实际是同一个事物的两种不同叫法。Flink构建这些快照的机制可以被描述成分布式数据流的轻量级异步快照，它采用Chandy-Lamport算法实现。)。如果发生失败的情况，系统可以从这些检查点进行恢复。Flink发送checkpoint的栅栏（barrier）到数据流中（栅栏是Flink的分布式快照机制中一个核心的元素），当checkpoint的栅栏到达其中一个operator，operator会接所有收输入流中对应的栅栏（比如，图中checkpoint n对应栅栏n到n-1的所有输入流，其仅仅是整个输入流的一部分）。所以相对于Storm，Flink的容错机制更高效，因为Flink的操作是对小批量数据而不是每条数据记录。但也不要让自己糊涂了，Flink仍然是原生流处理框架，它与Spark Streaming在概念上就完全不同。Flink也提供exactly once消息传输机制。 2. 状态管理大部分大型流处理应用都涉及到状态。相对于无状态的操作(其只有一个输入数据，处理过程和输出结果)，有状态的应用会有一个输入数据和一个状态信息，然后处理过程，接着输出结果和修改状态信息。因此，我们不得不管理状态信息，并持久化。我们期望一旦因某种原因失败，状态能够修复。状态修复有可能会出现小问题，它并不总是保证exactly once，有时也会出现消费多次，但这并不是我们想要的。 2.1 Apache Storm我们知道，Storm提供at-least once的消息传输保障。那我们又该如何使用Trident做到exactly once的语义。概念上貌似挺简单，你只需要提交每条数据记录，但这显然不是那么高效。所以你会想到小批量的数据记录一起提交会优化。Trident定义了几个抽象来达到exactly once的语义，见下图，其中也会有些局限。 2.2 Spark StreamingSpark Streaming是微批处理系统，它把状态信息也看做是一种微批量数据流。在处理每个微批量数据时，Spark加载当前的状态信息，接着通过函数操作获得处理后的微批量数据结果并修改加载过的状态信息。 2.3 SamzaSamza实现状态管理是通过Kafka来处理的。Samza有真实的状态操作，所以其任务会持有一个状态信息，并把状态改变的日志推送到Kafka。如果需要状态重建，可以很容易的从Kafka的topic重建。为了达到更快的状态管理，Samza也支持把状态信息放入本地key-value存储中，所以状态信息不必一直在Kafka中管理，见下图。不幸的是，Samza只提供at-least once语义，exactly once的支持也在计划中。 2.4 Apache FlinkFlink提供状态操作，和Samza类似。Flink提供两种类型的状态：一种是用户自定义状态；另外一种是窗口状态。如图，第一个状态是自定义状态，它和其它的的状态不相互作用。这些状态可以分区或者使用嵌入式Key-Value存储状态(参阅文容错和状态)。当然Flink提供exactly-once语义。下图展示Flink长期运行的三个状态。 3. 单词计数例子中的状态管理单词计数的详细代码见上篇文章，这里仅关注状态管理部分。 让我们先看Trident：public static StormTopology buildTopology(LocalDRPC drpc) &#123; FixedBatchSpout spout = ... TridentTopology topology = new TridentTopology(); TridentState wordCounts = topology.newStream(&quot;spout1&quot;, spout) .each(new Fields(&quot;sentence&quot;),new Split(), new Fields(&quot;word&quot;)) .groupBy(new Fields(&quot;word&quot;)) .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;)); ... &#125; 在第九行代码中，我们通过调用persistentAggregate创建一个状态。其中参数Count存储单词数，如果你想从状态中处理数据，你必须创建一个数据流。从代码中也可以看出实现起来不方便。 Spark Streaming声明式的方法稍微好点：// Initial RDD input to updateStateByKeyval initialRDD = ssc.sparkContext.parallelize(List.empty[(String, Int)])val lines = ...val words = lines.flatMap(_.split(&quot; &quot;))val wordDstream = words.map(x =&gt; (x, 1))val trackStateFunc = (batchTime: Time, word: String, one: Option[Int], state: State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) Some(output) &#125;val stateDstream = wordDstream.trackStateByKey( StateSpec.function(trackStateFunc).initialState(initialRDD)) 首先我们需要创建一个RDD来初始化状态（第二行代码），然后进行transformations（第五行和六行代码）。接着在第八行到十四行代码，我们定义函数来处理单词数状态。函数计算并更新状态，最后返回结果。第十六行和十七行代码，我们得到一个状态信息流，其中包含单词数。 接着我们看下Samza:class WordCountTask extends StreamTask with InitableTask &#123; private var store: CountStore = _ def init(config: Config, context: TaskContext) &#123; this.store = context.getStore(&quot;wordcount-store&quot;) .asInstanceOf[KeyValueStore[String, Integer]] &#125; override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector, coordinator: TaskCoordinator) &#123; val words = envelope.getMessage.asInstanceOf[String].split(&quot; &quot;) words.foreach &#123; key =&gt; val count: Integer = Option(store.get(key)).getOrElse(0) store.put(key, count + 1) collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;), (key, count))) &#125; &#125; 首先在第三行代码定义状态，进行Key-Value存储，在第五行到八行代码初始化状态。接着在计算中使用，上面的代码已经很直白。 最后，讲下Flink使用简洁的API实现状态管理：val env = ExecutionEnvironment.getExecutionEnvironmentval text = env.fromElements(...)val words = text.flatMap ( _.split(&quot; &quot;) )words.keyBy(x =&gt; x).mapWithState &#123; (word, count: Option[Int]) =&gt; &#123; val newCount = count.getOrElse(0) + 1 val output = (word, newCount) (output, Some(newCount)) &#125;&#125; 我们仅仅需要在第六行代码中调用mapwithstate函数，它有一个函数参数（函数有两个变量，第一个是单词，第二个是状态。然后返回处理的结果和新的状态）。 4. 流处理框架性能这里所讲的性能主要涉及到的是延迟性和吞吐量。 对于延迟性来说，微批处理一般在秒级别，大部分原生流处理在百毫秒以下，调优的情况下Storm可以很轻松的达到十毫秒。 同时也要记住，消息传输机制保障，容错性和状态恢复都会占用机器资源。例如，打开容错恢复可能会降低10％到15％的性能，Storm可能降低70%的吞吐量。总之，天下没有免费的午餐。对于有状态管理，Flink会降低25%的性能，Spark Streaming降低50%的性能。 也要记住，各大流处理框架的所有操作都是分布式的，通过网络发送数据是相当耗时的，所以要利用数据本地性，也尽量优化你的应用的序列化。 5. 项目成熟度当你为应用选型时一定会考虑项目的成熟度。下面来快速浏览一下：Storm是第一个主流的流处理框架，后期已经成为长期的工业级的标准，并在像Twitter，Yahoo，Spotify等大公司使用。Spark Streaming是最近最流行的Scala代码实现的流处理框架。现在Spark Streaming被公司（Netflix, Cisco, DataStax, Intel, IBM等）日渐接受。Samza主要在LinkedIn公司使用。Flink是一个新兴的项目，很有前景。 你可能对项目的贡献者数量也感兴趣。Storm和Trident大概有180个代码贡献者；整个Spark有720多个；根据github显示，Samza有40个；Flink有超过130个代码贡献者。 6. 小结在进行流处理框架推荐之前，先来整体看下总结表： 7. 流处理框架推荐应用选型是大家都会遇到的问题，一般是根据应用具体的场景来选择特定的流处理框架。下面给出几个作者认为优先考虑的点： High level API：具有high level API的流处理框架会更简洁和高效； 状态管理：大部分流处理应用都涉及到状态管理，因此你得把状态管理作为评价指标之一； exactly once语义：exactly once会使得应用开发变得简单，但也要看具体需求，可能at least once或者at most once语义就满足你得要求； 自动恢复：确保流处理系统能够快速恢复，你可以使用Chaos Monkey或者类似的工具进行测试。快速的恢复是流处理重要的部分。 Storm：Storm非常适合任务量小但速度要求高的应用。如果你主要在意流处理框架的延迟性，Storm将可能是你的首先。但同时也要记住，Storm的容错恢复或者Trident的状态管理都会降低整体的性能水平。也有一个潜在的Storm更新项目-Twitter的Heron，Heron设计的初衷是为了替代Storm，并在每个单任务上做了优化但同时保留了API。 Spark Streaming：如果你得基础架构中已经涉及到Spark，那Spark Streaming无疑是值得你尝试的。因为你可以很好的利用Spark各种library。如果你需要使用Lambda架构，Spark Streaming也是一个不错的选择。但你要时刻记住微批处理的局限性，以及它的延迟性问题。 Samza：如果你想使用Samza，那Kafka应该是你基础架构中的基石，好在现在Kafka已经成为家喻户晓的组件。像前面提到的，Samza一般会搭配强大的本地存储一起，这对管理大数据量的状态非常有益。它可以轻松处理上万千兆字节的状态信息，但要记住Samza只支持at least once语义。 Flink：Flink流处理系统的概念非常不错，并且满足绝大多数流处理场景，也经常提供前沿的功能函数，比如，高级窗口函数或者时间处理功能，这些在其它流处理框架中是没有的。同时Flink也有API提供给通用的批处理场景。但你需要足够的勇气去上线一个新兴的项目，并且你也不能忘了看下Flink的roadmap。 8. Dataflow和开源最后，我们来聊下Dataflow和它的开源。Dataflow是Google云平台的一部分，Google云平台包含很多组件：大数据存储，BigQuery，Cloud PubSub，数据分析工具和前面提到的Dataflow。 Dataflow是Google管理批处理和流处理的统一API。它是建立在MapReduce（批处理），FlumeJava（编程模型）和MillWheel（流处理）之上。Google最近决定开源Dataflow SDK，并完成Spark和Flink的runner。现在可以通过Dataflow的API来定义Google云平台作业、Flink作业或者Spark作业，后续会增加对其它引擎的支持。 Google为Dataflow提供Java、Python的API，社区已经完成Scalable的DSL支持。除此之外，Google及其合作者提交Apache Beam到Apache。 原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream 主流流处理框架比较(1)]]></title>
    <url>%2F2018%2F01%2F10%2FStream%2F%5BStream%5D%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83(1)%2F</url>
    <content type="text"><![CDATA[分布式流处理是对无边界数据集进行连续不断的处理、聚合和分析。它跟MapReduce一样是一种通用计算，但我们期望延迟在毫秒或者秒级别。这类系统一般采用有向无环图（DAG）。 DAG是任务链的图形化表示，我们用它来描述流处理作业的拓扑。如下图，数据从sources流经处理任务链到sinks。单机可以运行DAG，但本篇文章主要聚焦在多台机器上运行DAG的情况。 1. 关注点当选择不同的流处理系统时，有以下几点需要注意的： 运行时和编程模型：平台框架提供的编程模型决定了许多特色功能，编程模型要足够处理各种应用场景。这是一个相当重要的点，后续会继续。 函数式原语：流处理平台应该能提供丰富的功能函数，比如，map或者filter这类易扩展、处理单条信息的函数；处理多条信息的函数aggregation；跨数据流、不易扩展的操作join。 状态管理：大部分应用都需要保持状态处理的逻辑。流处理平台应该提供存储、访问和更新状态信息。 消息传输保障：消息传输保障一般有三种：at most once，at least once和exactly once。At most once的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失；At least once意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失；Exactly once的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复。 容错：流处理框架中的失败会发生在各个层次，比如，网络部分，磁盘崩溃或者节点宕机等。流处理框架应该具备从所有这种失败中恢复，并从上一个成功的状态（无脏数据）重新消费。 性能：延迟时间（Latency），吞吐量（Throughput）和扩展性（Scalability）是流处理应用中极其重要的指标。 平台的成熟度和接受度：成熟的流处理框架可以提供潜在的支持，可用的库，甚至开发问答帮助。选择正确的平台会在这方面提供很大的帮助。 2. 运行时和编程模型运行时和编程模型是一个系统最重要的特质，因为它们定义了表达方式、可能的操作和将来的局限性。因此，运行时和编程模型决定了系统的能力和适用场景。 实现流处理系统有两种完全不同的方式： (1) 一种是称作原生流处理，意味着所有输入的记录一旦到达即会一个接着一个进行处理。 (2) 第二种称为微批处理。把输入的数据按照某种预先定义的时间间隔(典型的是几秒钟)分成短小的批量数据，流经流处理系统。 两种方法都有其先天的优势和不足。首先以原生流处理开始，原生流处理的优势在于它的表达方式。数据一旦到达立即处理，这些系统的延迟性远比其它微批处理要好。除了延迟性外，原生流处理的状态操作也容易实现，后续将详细讲解。一般原生流处理系统为了达到低延迟和容错性会花费比较大的成本，因为它需要考虑每条记录。原生流处理的负载均衡也是个问题。比如，我们处理的数据按key分区，如果分区的某个key是资源密集型，那这个分区很容易成为作业的瓶颈。 接下来看下微批处理。将流式计算分解成一系列短小的批处理作业，也不可避免的减弱系统的表达力。像状态管理或者join等操作的实现会变的困难，因为微批处理系统必须操作整个批量数据。并且，batch interval会连接两个不易连接的事情：基础属性和业务逻辑。相反地，微批处理系统的容错性和负载均衡实现起来非常简单，因为微批处理系统仅发送每批数据到一个worker节点上，如果一些数据出错那就使用其它副本。微批处理系统很容易建立在原生流处理系统之上。 编程模型一般分为组合式和声明式。组合式编程提供基本的构建模块，它们必须紧密结合来创建拓扑。新的组件经常以接口的方式完成。相对应地，声明式API操作是定义的高阶函数。它允许我们用抽象类型和方法来写函数代码，并且系统创建拓扑和优化拓扑。声明式API经常也提供更多高级的操作（比如，窗口函数或者状态管理）。后面很快会给出样例代码。 3. 主流流处理系统有一系列各种实现的流处理框架，不能一一列举，这里仅选出主流的流处理解决方案，并且支持Scala API。因此，我们将详细介绍Apache Storm，Trident，Spark Streaming，Samza和Apache Flink。前面选择讲述的虽然都是流处理系统，但它们实现的方法包含了各种不同的挑战。这里暂时不讲商业的系统，比如Google MillWheel或者Amazon Kinesis，也不会涉及很少使用的Intel GearPump或者Apache Apex。 Apache Storm最开始是由Nathan Marz和他的团队于2010年在数据分析公司BackType开发的，后来BackType公司被Twitter收购，接着Twitter开源Storm并在2014年成为Apache顶级项目。毋庸置疑，Storm成为大规模流数据处理的先锋，并逐渐成为工业标准。Storm是原生的流处理系统，提供low-level的API。Storm使用Thrift来定义topology和支持多语言协议，使得我们可以使用大部分编程语言开发，Scala自然包括在内。 Trident是对Storm的一个更高层次的抽象，Trident最大的特点以batch的形式进行流处理。Trident简化topology构建过程，增加了窗口操作、聚合操作或者状态管理等高级操作，这些在Storm中并不支持。相对应于Storm的At most once流传输机制，Trident提供了Exactly once传输机制。Trident支持Java，Clojure和Scala。 当前Spark是非常受欢迎的批处理框架，包含Spark SQL，MLlib和Spark Streaming。Spark的运行时是建立在批处理之上，因此后续加入的Spark Streaming也依赖于批处理，实现了微批处理。接收器把输入数据流分成短小批处理，并以类似Spark作业的方式处理微批处理。Spark Streaming提供高级声明式API（支持Scala，Java和Python）。 Samza最开始是专为LinkedIn公司开发的流处理解决方案，并和LinkedIn的Kafka一起贡献给社区，现已成为基础设施的关键部分。Samza的构建严重依赖于基于log的Kafka，两者紧密耦合。Samza提供组合式API，当然也支持Scala。 最后来介绍Apache Flink。Flink是个相当早的项目，开始于2008年，但只在最近才得到注意。Flink是原生的流处理系统，提供high level的API。Flink也提供API来像Spark一样进行批处理，但两者处理的基础是完全不同的。Flink把批处理当作流处理中的一种特殊情况。在Flink中，所有的数据都看作流，是一种很好的抽象，因为这更接近于现实世界。 快速的介绍流处理系统之后，让我们以下面的表格来更好清晰的展示它们之间的不同： 4. Word CountWordcount之于流处理框架学习，就好比hello world之于编程语言学习。它能很好的展示各流处理框架的不同之处，让我们从Storm开始看看如何实现Wordcount：TopologyBuilder builder = new TopologyBuilder();builder.setSpout("spout", new RandomSentenceSpout(), 5);builder.setBolt("split", new Split(), 8).shuffleGrouping("spout");builder.setBolt("count", new WordCount(), 12).fieldsGrouping("split", new Fields("word")); ...Map&lt;String, Integer&gt; counts = new HashMap&lt;String, Integer&gt;();public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String word = tuple.getString(0); Integer count = counts.containsKey(word) ? counts.get(word) + 1 : 1; counts.put(word, count); collector.emit(new Values(word, count));&#125; 首先，定义topology。第二行代码定义一个spout，作为数据源。然后是一个处理组件bolt，分割文本为单词。接着，定义另一个bolt来计算单词数（第四行代码）。也可以看到魔数5，8和12，这些是并行度，定义集群每个组件执行的独立线程数。第八行到十五行是实际的WordCount bolt实现。因为Storm不支持内建的状态管理，所有这里定义了一个局部状态。 按之前描述，Trident是对Storm的一个更高层次的抽象，Trident最大的特点以batch的形式进行流处理。除了其它优势，Trident提供了状态管理，这对wordcount实现非常有用:public static StormTopology buildTopology(LocalDRPC drpc) &#123; FixedBatchSpout spout = ... TridentTopology topology = new TridentTopology(); TridentState wordCounts = topology.newStream("spout1", spout) .each(new Fields("sentence"),new Split(), new Fields("word")) .groupBy(new Fields("word")) .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count")); ... &#125; 如你所见，上面代码使用higher level操作，比如each（第七行代码）和groupby（第八行代码）。并且使用Trident管理状态来存储单词数（第九行代码）。 下面是时候祭出提供声明式API的Apache Spark。记住，相对于前面的例子，这些代码相当简单，几乎没有冗余代码。下面是简单的流式计算单词数：val conf = new SparkConf().setAppName("wordcount")val ssc = new StreamingContext(conf, Seconds(1))val text = ...val counts = text.flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _)counts.print()ssc.start()ssc.awaitTermination() 每个Spark Streaming的作业都要有StreamingContext，它是流式函数的入口。StreamingContext加载第一行代码定义的配置conf，但更重要地，第二行代码定义batch interval（这里设置为1秒）。第六行到八行代码是整个单词数计算。这些是标准的函数式代码，Spark定义topology并且分布式执行。第十二行代码是每个Spark Streaming作业最后的部分：启动计算。记住，Spark Streaming作业一旦启动即不可修改。 接下来看下Apache Samza，另外一个组合式API例子：class WordCountTask extends StreamTask &#123; override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector, coordinator: TaskCoordinator) &#123; val text = envelope.getMessage.asInstanceOf[String] val counts = text.split(&quot; &quot;).foldLeft(Map.empty[String, Int]) &#123; (count, word) =&gt; count + (word -&gt; (count.getOrElse(word, 0) + 1)) &#125; collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;), counts)) &#125; Samza的属性配置文件定义topology，为了简明这里并没把配置文件放上来。定义任务的输入和输出，并通过Kafka topic通信。在单词数计算整个topology是WordCountTask。在Samza中，实现特殊接口定义组件StreamTask，在第三行代码重写方法process。它的参数列表包含所有连接其它系统的需要。第八行到十行简单的Scala代码是计算本身。 Flink的API跟Spark Streaming是惊人的相似，但注意到代码里并未设置batch interval：val env = ExecutionEnvironment.getExecutionEnvironment val text = env.fromElements(...) val counts = text.flatMap ( _.split(&quot; &quot;) ) .map ( (_, 1) ) .groupBy(0) .sum(1) counts.print() env.execute(&quot;wordcount&quot;) 上面的代码是相当的直白，仅仅只是几个函数式调用，Flink支持分布式计算。 5. 结论上面给出了基本的理论和主流流处理框架介绍，下篇文章将会更深入的探讨其它关注点。希望你能对前面的文章感兴趣，如果有任何问题，请联系我讨论这些主题。 原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework]]></content>
      <categories>
        <category>Stream</category>
      </categories>
      <tags>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 事件时间与Watermarks]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-stream-event-time-and-watermark%2F</url>
    <content type="text"><![CDATA[1. watermarkFlink实现了数据流模型(Dataflow Model)中许多技术。如果想对事件时间(event time)和watermarks更详细的了解，请参阅下面的文章: The world beyond batch: Streaming 101 The Dataflow Model 支持事件时间的流处理器需要一种方法来衡量事件时间的进度。例如，一个构建小时窗口的窗口算子(operator)，当事件时间超过一小时末尾时需要告知窗口算子，以便算子可以关闭正在进行的窗口。 事件时间可以独立于处理时间来运行。例如，在一个程序中，算子的当前事件时间可以略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度继续运行。另一方面，另一个流式处理程序处理几个星期的事件时间只需几秒钟就可以，通过快速浏览缓存在Kafka Topic中历史数据。 Flink中测量事件时间进度的机制是watermarks。watermarks会作为数据流的一部分进行流动，并带有一个时间戳t。Watermark(t)表示数据流中的事件时间已达到时间t，意思就是说数据流之后不再有时间戳t‘&lt;= t的元素(即带时间戳的事件老于或等于watermark)。 下图显示了具有时间戳(逻辑上)的事件流以及内嵌的watermark。在这个例子中，事件是有序的(相对于它们的时间戳)，这意味着watermark只是数据流中的周期性标记。 watermark对于乱序数据流至关重要，如下图所示，事件并未按照时间戳进行排序。通常，watermark表示在数据流中那个时刻小于时间戳的所有事件都已经到达。一旦watermark到达算子，算子就可以将其内部的事件时间提到watermark的那个值。 2. 数据流中的并行Watermarkswatermarks是直接通过数据源函数(source functions)生成的或在数据源函数之后生成的。源函数的每个并行子任务通常独立生成watermarks。这些watermarks在指定并行数据源上定义事件时间。 watermarks贯穿整个流处理程序，他们会在watermark到达的算子时将事件时间提前(advance)。每当算子提前事件时间时，它都会为下游的后续算子生成一个新的watermarks(Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.)。 一些算子消耗多个输入流；例如，union操作，或者算子后面跟着keyBy(...)函数或者partition(...)函数。这样的算子的当前事件时间是其输入流的所有事件时间中的最小值。随着输入流更新事件时间，算子也会更新事件。 下图显示了事件和watermarks流经并行流的的示例，以及跟踪事件时间的算子: 3. 延迟元素某些元素可能违反watermarks条件，这意味着即使出现watermarks(t)，但是还是会出现很多的时间戳t&#39;&lt;= t的元素。事实上，在现实世界中，某些元素可能被任意地延迟，因此指定一个时间，带有事件时间戳的所有事件在此之前出现是不可能的。此外，即使延迟时间是有限制的，也不希望延迟太多的watermarks，因为它会在事件时间窗口的评估中导致太多的延迟。 因此，流处理程序中可能会明确的知道会有延迟元素。延迟元素是那些系统事件时钟(由watermark所示)已经超过了延迟元素的时间戳的那些元素。有关如何处理事件时间窗口中的延迟元素的更多信息，请参阅Allowed Lateness。 4. 调试Watermarks请参阅调试Windows和事件时间部分，以便在运行时调试Watermarks。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 事件时间与处理时间]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-stream-event-time-and-processing-time%2F</url>
    <content type="text"><![CDATA[Flink在数据流中支持几种不同概念的时间。 1. 处理时间Processing Time(处理时间)是指执行相应操作机器的系统时间(Processing time refers to the system time of the machine that is executing the respective operation.)。 当一个流程序以处理时间来运行时，所有基于时间的操作(如时间窗口)将使用运行算子(operator)所在机器的系统时间。例如:一个基于处理时间按每小时进行处理的时间窗口将包括以系统时间为标准在一个小时内到达指定算子的所有的记录(an hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour.)。 处理时间是最简单的一个时间概念，不需要在数据流和机器之间进行协调。它有最好的性能和最低的延迟。然而，在分布式或者异步环境中，处理时间具有不确定性，因为容易受到记录到达系统速度的影响(例如从消息队列到达的记录)，还会受到系统内记录流在不同算子之间的流动速度的影响(speed at which records arrive in the system, and to the speed at which the records flow between operators inside the system)。 2. 事件时间Event Time(事件时间)是每个独立事件在它生产设备上产生的时间。在进入Flink之前，事件时间通常要嵌入到记录中，并且事件时间也可以从记录中提取出来。一个基于事件时间按每小时进行处理的时间窗口将包含所有的记录，其事件时间都在这一小时之内，不管它们何时到达，以及它们以什么顺序到达。 事件时间即使在乱序事件，延迟事件以及从备份或持久化日志中的重复数据也能获得正确的结果。对于事件时间，时间的进度取决于数据，而不是任何时钟。事件时间程序必须指定如何生成事件时间的Watermarks，这是表示事件时间进度的机制。 按事件时间处理往往会导致一定的延迟，因为它要等待延迟事件和无序事件一段时间。因此，事件时间程序通常与处理时间操作相结合使用。 3. 摄入时间Ingestion Time(摄入时间)是事件进入Flink的时间。在source operator中，每个记录将源的当前时间记为时间戳，基于时间的操作(如时间窗口)会使用该时间戳。 摄入时间在概念上处于事件时间和处理时间之间。与处理时间相比，摄入时间的成本稍微更高一些，但是可以提供更可预测的结果。因为摄入时间的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，然而对于处理时间，每个窗口算子可能将记录分配给不同的窗口(基于本地系统时钟以及传输延迟)。 与事件时间相比，摄入时间程序无法处理任何无序事件或延迟事件，但程序不必指定如何生成watermarks。 在内部，摄入时间与事件时间非常相似，但事件时间会自动分配时间戳以及自动生成watermark(with automatic timestamp assignment and automatic watermark generation)。 4. 选择时间特性Flink DataStream程序的第一部分通常设置基本的时间特性(base time characteristic)。该设置定义数据流源的行为方式(例如，它们是否产生时间戳)，以及窗口操作如KeyedStream.timeWindow(Time.seconds(30))应使用哪一类型时间，是事件时间还是处理时间等。 以下示例展示了一个聚合每小时时间窗口内的事件的Flink程序。窗口的行为会与时间特性相匹配。 Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);// alternatively:// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);DataStream&lt;MyEvent&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));stream .keyBy( (event) -&gt; event.getUser() ) .timeWindow(Time.hours(1)) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)// alternatively:// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))stream .keyBy( _.getUser ) .timeWindow(Time.hours(1)) .reduce( (a, b) =&gt; a.add(b) ) .addSink(...) 备注:为了以事件时间运行此示例，程序需要使用定义了事件时间并自动产生watermarks的源，或者程序必须在源之后设置时间戳分配器和watermarks生成器。上述函数描述了如何获取事件时间戳，以及展现事件流的无序程度。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 重启策略]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-restart-strategy%2F</url>
    <content type="text"><![CDATA[Flink支持不同的重启策略，重启策略控制在作业失败后如何重启。可以使用默认的重启策略启动集群，这个默认策略在作业没有特别指定重启策略时使用。如果在提交作业时指定了重启策略，那么此策略将覆盖集群的默认配置策略。 1. 概述默认的重启策略通过Flink的配置文件flink-conf.yaml进行设置。配置参数restart-strategy定义了采取哪种策略。如果未启用检查点，那么将使用不重启策略。如果启用检查点且重启策略尚未配置，则固定延迟重启策略与Integer.MAX_VALUE一起使用进行尝试重启。请参阅下面可用的重启策略列表以了解支持哪些值。 每个重启策略都有自己的一套控制其行为的参数。这些值也在配置文件中配置。每个重启策略的描述都包含有关各个配置值的更多信息。 重启策略 值 固定延迟重启策略 fixed-delay 失败率重启策略 failure-rate 不重启策略 none 除了定义一个默认的重启策略之外，还可以为每个Flink作业定义一个指定的重启策略。此重启策略通过调用ExecutionEnvironment上的setRestartStrategy方法以编程的方式进行设置。请注意，这也适用于StreamExecutionEnvironment。 以下示例显示了如何为作业设置固定延迟重启策略。如果发生故障，系统将尝试每10s重新启动一次作业，最多重启3次。 Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)) 2. 重启策略下面介绍几种重启策略的配置选项。 2.1 固定延迟重启策略固定延迟重启策略尝试一定次数来重新启动作业。如果超过最大尝试次数，那么作业最终将失败。在两次连续的尝试重启之间，重启策略会等待一段固定的时间(译者注:连续重启时间间隔)。 通过在flink-conf.yaml中设置以下配置参数，可以将此策略默认启用：restart-strategy: fixed-delay 配置参数 描述 默认值 restart-strategy.fixed-delay.attempts 在声明作业失败之前，Flink重试执行的次数 1或者如果启用检查点，则为Integer.MAX_VALUE restart-strategy.fixed-delay.delay 延迟重试意味着在执行失败后，重新执行不会立即开始，而只会在某个延迟之后开始。当程序与外部系统进行交互时，延迟重试会很有帮助 akka.ask.timeout，或10s(如果通过检查点激活) Example:restart-strategy.fixed-delay.attempts: 3restart-strategy.fixed-delay.delay: 10 s 固定延迟重启策略也可以通过编程来设置： Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay)) 2.2 失败率重启策略失败率重启策略在失败后重新启动作业，但当超过失败率(每个时间间隔的失败)时，作业最终会失败。在两次连续的重启尝试之间，重启策略会等待一段固定的时间。 通过在flink-conf.yaml中设置以下配置参数，可以将此策略默认启用: 配置参数 描述 默认值 restart-strategy.failure-rate.max-failures-per-interval 在一个作业声明失败之前，在给定时间间隔内最大的重启次数 1 restart-strategy.failure-rate.failure-rate-interval 计算失败率的时间间隔 1分钟 restart-strategy.failure-rate.delay 两次连续重启尝试之间的时间间隔 akka.ask.timeout Example:restart-strategy.failure-rate.max-failures-per-interval: 3restart-strategy.failure-rate.failure-rate-interval: 5 minrestart-strategy.failure-rate.delay: 10 s 失败率重新启动策略也可以通过编程来设置： Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per interval Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay)); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per unit Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay)) 2.3 不重启策略作业直接失败，不会尝试重新启动:restart-strategy: none 不重启策略也可以通过编程来设置： Java版本:ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart()); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.noRestart()) 2.4 回退重启策略使用集群定义的重启策略(The cluster defined restart strategy is used. )。这有助于启用检查点的流式传输程序。默认情况下，如果没有定义其他重启策略，则选择固定延时重启策略。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 数据流类型与转换关系]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-stream-dataflow-type-and-transformation%2F</url>
    <content type="text"><![CDATA[Flink 为流处理和批处理分别提供了 DataStream API 和 DataSet API。正是这种高层的抽象和 flunent API 极大地便利了用户编写大数据应用。不过很多初学者在看到官方文档中那一大坨的转换时，常常会蒙了圈，文档中那些只言片语也很难讲清它们之间的关系。所以本文将介绍几种关键的数据流类型，它们之间是如何通过转换关联起来的。下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。 1. DataStreamDataStream 是 Flink 流处理 API 中最核心的数据结构。它代表了一个运行在多个分区上的并行流。一个 DataStream 可以从 StreamExecutionEnvironment 通过 env.addSource(SourceFunction) 获得。 DataStream 上的转换操作都是逐条的，比如 map()，flatMap()，filter()。DataStream 也可以执行 rebalance（再平衡，用来减轻数据倾斜）和 broadcaseted（广播）等分区转换。 val stream: DataStream[MyType] = env.addSource(new FlinkKafkaConsumer08[String](...))val str1: DataStream[(String, MyType)] = stream.flatMap &#123; ... &#125;val str2: DataStream[(String, MyType)] = stream.rebalance()val str3: DataStream[AnotherType] = stream.map &#123; ... &#125; 上述 DataStream 上的转换在运行时会转换成如下的执行图： 如上图的执行图所示，DataStream 各个算子会并行运行，算子之间是数据流分区。如 Source 的第一个并行实例（S1）和 flatMap() 的第一个并行实例（m1）之间就是一个数据流分区。而在 flatMap() 和 map() 之间由于加了 rebalance()，它们之间的数据流分区就有3个子分区（m1的数据流向3个map()实例）。这与 Apache Kafka 是很类似的，把流想象成 Kafka Topic，而一个流分区就表示一个 Topic Partition，流的目标并行算子实例就是 Kafka Consumers。 2. KeyedStreamKeyedStream 用来表示根据指定的 key 进行分组的数据流。一个 KeyedStream可以通过调用 DataStream.keyBy() 来获得。而在 KeyedStream 上进行任何 transformation 都将转变回 DataStream。在实现中，KeyedStream 是把 key 的信息写入到了 transformation 中。每条记录只能访问所属 key 的状态，其上的聚合函数可以方便地操作和保存对应 key 的状态。 3. WindowedStream &amp; AllWindowedStreamWindowedStream代表了根据 key 分组，并且基于 WindowAssigner 切分窗口的数据流。所以 WindowedStream 都是从 KeyedStream 衍生而来的。而在 WindowedStream 上进行任何 transformation 也都将转变回 DataStream。 val stream: DataStream[MyType] = ...val windowed: WindowedDataStream[MyType] = stream .keyBy(&quot;userId&quot;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of dataval result: DataStream[ResultType] = windowed.reduce(myReducer) 上述 WindowedStream 的样例代码在运行时会转换成如下的执行图： Flink 的窗口实现中会将到达的数据缓存在对应的窗口buffer中（一个数据可能会对应多个窗口）。当到达窗口发送的条件时（由Trigger控制），Flink 会对整个窗口中的数据进行处理。Flink 在聚合类窗口有一定的优化，即不会保存窗口中的所有值，而是每到一个元素执行一次聚合函数，最终只保存一份数据即可。 在key分组的流上进行窗口切分是比较常用的场景，也能够很好地并行化（不同的 key 上的窗口聚合可以分配到不同的 task 去处理）。不过有时候我们也需要在普通流上进行窗口的操作，这就是 AllWindowedStream。AllWindowedStream 是直接在 DataStream 上进行 windowAll(...) 操作。AllWindowedStream 的实现是基于 WindowedStream 的（Flink 1.1.x 开始）。Flink 不推荐使用 AllWindowedStream，因为在普通流上进行窗口操作，就势必需要将所有分区的流都汇集到单个的 Task 中，而这个单个的 Task 很显然就会成为整个Job的瓶颈。 4. JoinedStreams &amp; CoGroupedStreams双流 Join 也是一个非常常见的应用场景。深入源码你可以发现，JoinedStreams 和 CoGroupedStreams 的代码实现有80%是一模一样的，JoinedStreams 在底层又调用了 CoGroupedStreams 来实现 Join 功能。除了名字不一样，一开始很难将它们区分开来，而且为什么要提供两个功能类似的接口呢？ 实际上这两者还是很点区别的。首先 co-group 侧重的是 group，是对同一个 key 上的两组集合进行操作，而 join 侧重的是 pair，是对同一个 key 上的每对元素进行操作。co-group 比 join 更通用一些，因为 join 只是 co-group 的一个特例，所以 join 是可以基于 co-group 来实现的（当然有优化的空间）。而在 co-group 之外又提供了 join 接口是因为用户更熟悉 join（源于数据库吧），而且能够跟 DataSet API 保持一致，降低用户的学习成本。 JoinedStreams 和 CoGroupedStreams 是基于 Window 上实现的，所以 CoGroupedStreams 最终又调用了 WindowedStream 来实现。 val firstInput: DataStream[MyType] = ...val secondInput: DataStream[AnotherType] = ...val result: DataStream[(MyType, AnotherType)] = firstInput.join(secondInput) .where(&quot;userId&quot;).equalTo(&quot;id&quot;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new JoinFunction () &#123;...&#125;) 上述 JoinedStreams 的样例代码在运行时会转换成如下的执行图： 双流上的数据在同一个 key 的会被分别分配到同一个 window 窗口的左右两个篮子里，当 window 结束的时候，会对左右篮子进行笛卡尔积从而得到每一对 pair，对每一对 pair 应用 JoinFunction。不过目前（Flink 1.1.x） JoinedStreams 只是简单地实现了流上的 join 操作而已，距离真正的生产使用还是有些距离。因为目前 join 窗口的双流数据都是被缓存在内存中的，也就是说如果某个 key 上的窗口数据太多就会导致 JVM OOM（然而数据倾斜是常态）。双流 join 的难点也正是在这里，这也是社区后面对 join 操作的优化方向，例如可以借鉴 Flink 在批处理 join 中的优化方案，也可以用 ManagedMemory 来管理窗口中的数据，并当数据超过阈值时能spill到硬盘。 5. ConnectedStreams在 DataStream 上有一个 union 的转换 dataStream.union(otherStream1, otherStream2, ...)，用来合并多个流，新的流会包含所有流中的数据。union 有一个限制，就是所有合并的流的类型必须是一致的。ConnectedStreams 提供了和 union 类似的功能，用来连接两个流，但是与 union 转换有以下几个区别： ConnectedStreams 只能连接两个流，而 union 可以连接多于两个流。 ConnectedStreams 连接的两个流类型可以不一致，而 union 连接的流的类型必须一致。 ConnectedStreams 会对两个流的数据应用不同的处理方法，并且双流之间可以共享状态。这在第一个流的输入会影响第二个流时, 会非常有用。 如下 ConnectedStreams 的样例，连接 input 和 other 流，并在 input 流上应用 map1 方法，在 other 上应用 map2 方法，双流可以共享状态（比如计数）。 val input: DataStream[MyType] = ...val other: DataStream[AnotherType] = ...val connected: ConnectedStreams[MyType, AnotherType] = input.connect(other)val result: DataStream[ResultType] = connected.map(new CoMapFunction[MyType, AnotherType, ResultType]() &#123; override def map1(value: MyType): ResultType = &#123; ... &#125; override def map2(value: AnotherType): ResultType = &#123; ... &#125; &#125;) 当并行度为2时，其执行图如下所示： 6. 总结本文介绍通过不同数据流类型的转换图来解释每一种数据流的含义、转换关系。后面的文章会深入讲解 Window 机制的实现，双流 Join 的实现等。 原文:http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 执行计划]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-execution-plans%2F</url>
    <content type="text"><![CDATA[根据各种参数(如数据大小或集群中的机器数量)，Flink的优化器自动会为你的程序选择一个执行策略。很多情况下，准确的知道Flink如何执行你的程序是很有帮助的。 1. 计划可视化工具Flink内置一个执行计划的可视化工具。包含可视化工具的HTML文档位于tools/planVisualizer.html下。用JSON表示作业执行计划，并将其可视化为具有执行策略完整注释的图(visualizes it as a graph with complete annotations of execution strategies)。 备注:打开可视化工具的方式有所改变:由本地文件 tools/planVisualizer.html 改为 url http://flink.apache.org/visualizer/index.html 以下代码显示了如何从程序中打印执行计划的JSON： Java版本:final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();...System.out.println(env.getExecutionPlan()); Scala版本:val env = ExecutionEnvironment.getExecutionEnvironment...println(env.getExecutionPlan()) 要可视化执行计划，请执行以下操作： (1) 使用浏览器打开planVisualizer.html(或者直接在浏览器中输入http://flink.apache.org/visualizer/index.html 网址) (2) 将JSON字符串粘贴到文本框中 (3) 点击Draw按钮 完成上面这些步骤后，将会显示详细的执行计划。 2. Web界面Flink提供了一个用于提交和执行作业的Web界面。这个界面是JobManager Web监控界面的一部分，默认情况下在端口8081上运行。通过这个界面提交作业需要你在flink-conf.yaml中设置jobmanager.web.submit.enable：true。 你可以在作业执行之前指定程序参数。执行计划可视化器使你能够在执行Flink作业之前查看执行计划。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 并发执行]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-parallel-execute%2F</url>
    <content type="text"><![CDATA[本节介绍如何在Flink中配置程序的并行执行。一个Flink程序由多个任务(transformations/operators，data sources和sinks)组成。一个任务被分成多个并发实例来执行，每个并发实例只处理任务输入数据的一个子集。一个任务的并发实例的个数称为并发度(parallelism)。 如果你想使用保存点，也应该考虑设置最大并发度。从保存点恢复时，可以更改特定算子或整个程序的并发度，并且此配置指定了并发的上限。 1. 设置并发度一个任务的并发度可以在Flink中指定不同级别。 1.1 算子级别单个算子，数据源，sink可以通过调用setParallelism()方法来定义并发度。例如，像这样： Java版本:DataStream&lt;String&gt; text = [...]DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text .flatMap(new LineSplitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1).setParallelism(5);wordCounts.print();env.execute("Word Count Example"); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentval text = [...]val wordCounts = text .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1).setParallelism(5)wordCounts.print()env.execute(&quot;Word Count Example&quot;) 1.2 执行环境级别如这所述，Flink程序是在执行环境的上下文中执行的。执行环境为它执行的所有算子，数据源和数据sink提供了默认的并发度。执行环境的并发度可以通过显式配置一个算子的并发度来覆盖。 执行环境的默认并发度可以通过调用setParallelism()方法来指定。要为执行的所有算子，数据源和sink设置并发度为3，请按如下方式设置执行环境的默认并发度： Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(3);DataStream&lt;String&gt; text = [...]DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = [...]wordCounts.print();env.execute("Word Count Example"); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(3)val text = [...]val wordCounts = text .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1)wordCounts.print()env.execute(&quot;Word Count Example&quot;) 1.3 客户端级别在向Flink提交作业时，可以在客户端设置并发度。客户端可以是Java或Scala程序。Flink的命令行接口(CLI)就是一种客户端。 对于CLI客户端，可以使用-p指定并发度参数。 例如：./bin/flink run -p 10 ../examples/*WordCount-java*.jar 在Java/Scala程序中，并发度设置如下： Java版本:try &#123; PackagedProgram program = new PackagedProgram(file, args); InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport("localhost:6123"); Configuration config = new Configuration(); Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader()); // set the parallelism to 10 here client.run(program, 10, true);&#125; catch (ProgramInvocationException e) &#123; e.printStackTrace();&#125; Scala版本:try &#123; PackagedProgram program = new PackagedProgram(file, args) InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(&quot;localhost:6123&quot;) Configuration config = new Configuration() Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader()) // set the parallelism to 10 here client.run(program, 10, true)&#125; catch &#123; case e: Exception =&gt; e.printStackTrace&#125; 1.4 系统级别可以通过在./conf/flink-conf.yaml中设置parallelism.default属性来为所有执行环境定义全系统默认并发度。详细信息请参阅配置文档。 2. 设置最大并发度最大并发度可以在可以设置并发度的地方设置(客户端级别和系统级别除外)。你可以调用setMaxParallelism()取代setParallelism()方法来设置最大并发度。 最大并发度的默认设置大致为operatorParallelism +（operatorParallelism / 2），下限为127，上限为32768。 备注:将最大并发度设置为非常大的数值可能会对性能造成不利影响，因为一些后端状态必须保持在内部数据结构，而这些内部数据结构随key-groups(这是可扩展状态的内部实现机制)的数量进行扩展。(some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state).) 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 累加器与计数器]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-accumulators-counters%2F</url>
    <content type="text"><![CDATA[1. 概述累加器(Accumulators)是一个简单的构造器，具有加法操作和获取最终累加结果操作，在作业结束后可以使用。 最直接的累加器是一个计数器(counter)：你可以使用Accumulator.add()方法对其进行累加。在作业结束时，Flink将合并所有部分结果并将最终结果发送给客户端。在调试过程中，或者你快速想要了解有关数据的更多信息，累加器很有用。 目前Flink拥有以下内置累加器。它们中的每一个都实现了累加器接口： (1) IntCounter, LongCounter 以及 DoubleCounter: 参阅下面示例中使用的计数器。 (2) Histogram：为离散数据的直方图(A histogram implementation for a discrete number of bins.)。内部它只是一个整数到整数的映射。你可以用它来计算值的分布，例如 单词计数程序的每行单词分配。 2. 如何使用首先，你必须在你要使用的用户自定义转换函数中创建一个累加器(accumulator)对象(这里是一个计数器):private IntCounter numLines = new IntCounter(); 其次，你必须注册累加器(accumulator)对象，通常在rich函数的open()方法中注册。在这里你也可以自定义累加器的名字:getRuntimeContext().addAccumulator("num-lines", this.numLines); 现在你就可以在算子函数中的任何位置使用累加器，包括在open()和close()方法中:this.numLines.add(1); 最后结果将存储在JobExecutionResult对象中，该对象从执行环境的execute()方法返回(当前仅当执行等待作业完成时才起作用):JobExecutionResult result = env.execute();long lineCounter = result.getAccumulatorResult("num-lines");System.out.println(lineCounter); 每个作业的所有累加器共享一个命名空间。因此，你可以在作业的不同算子函数中使用同一个累加器。Flink在内部合并所有具有相同名称的累加器。 备注:目前累加器的结果只有在整个工作结束之后才可以使用。我们还计划在下一次迭代中可以使用前一次迭代的结果。你可以使用聚合器来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。 3. Exampleimport com.google.gson.Gson;import com.google.gson.GsonBuilder;import com.qunar.innovation.data.bean.AdsPushBehavior;import com.qunar.innovation.data.utils.ConstantUtil;import org.apache.flink.api.common.accumulators.LongCounter;import org.apache.flink.api.common.functions.RichMapFunction;import org.apache.flink.configuration.Configuration;public class AdsPushParseMap extends RichMapFunction&lt;String, AdsPushBehavior&gt; &#123; private static Gson gson = new GsonBuilder().setDateFormat("yyyy-MM-dd HH:mm:ss").create(); private final LongCounter behaviorCounter = new LongCounter(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); getRuntimeContext().addAccumulator(ConstantUtil.ADS_PUSH_APP_CODE, behaviorCounter); &#125; @Override public AdsPushBehavior map(String content) throws Exception &#123; try&#123; // 解析 AdsPushBehavior adsPushBehavior = gson.fromJson(content, AdsPushBehavior.class); this.behaviorCounter.add(1); return adsPushBehavior; &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125;&#125; import com.qunar.innovation.data.TestFlink;import com.qunar.innovation.data.functions.*;import com.qunar.innovation.data.utils.ConstantUtil;import org.apache.flink.api.common.JobExecutionResult;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.core.fs.FileSystem;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class AdsPushLocalStream &#123; private final static Logger LOGGER = LoggerFactory.getLogger(TestFlink.class); public static void main(String[] args) &#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;String&gt; dataSet = env.readTextFile("file:///home/xiaosi/input.txt"); // 处理数据 DataSet&lt;String&gt; adsPushDataSet = dataSet.map(new ContentMap()).name("contentMap").setParallelism(1). map(new AdsPushParseMap()).name("behaviorMap").setParallelism(1) .map(new AdsPushFeatureMap()).name("featureMap").setParallelism(1) .filter(new AdsPushFeatureFilter()).name("featureFilter").setParallelism(1); adsPushDataSet.writeAsText("file:///home/xiaosi/output", FileSystem.WriteMode.OVERWRITE); try &#123; JobExecutionResult result = env.execute(); long behaviorCounter = result.getAccumulatorResult(ConstantUtil.ADS_PUSH_APP_CODE); System.out.println(behaviorCounter); &#125; catch (Exception e) &#123; LOGGER.error(e.getMessage(), e); &#125; &#125;&#125; 3. 自定义累加器为了实现你自己的累加器，你只需要编写你的Accumulator接口的实现。如果你认为你的自定义累加器应与Flink一起传输，请随意创建一个拉取请求(Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.)。 你可以选择实现Accumulator或SimpleAccumulator。 Accumulator&lt;V，R&gt;非常灵活：它为要添加的值定义一个类型V，并为最终结果定义一个结果类型R。例如，对于直方图，V是数字，R是直方图。SimpleAccumulator适用于两种类型相同的情况，例如，计数器。 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 定义keys的几种方法]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-how-to-specifying-keys%2F</url>
    <content type="text"><![CDATA[一些转换(例如，join，coGroup，keyBy，groupBy)要求在一组元素上定义一个key。其他转换(Reduce，GroupReduce，Aggregate，Windows)允许在使用这些函数之前根据key对数据进行分组。 一个DataSet进行分组如下:DataSet&lt;...&gt; input = // [...]DataSet&lt;...&gt; reduced = input.groupBy(/*define key here*/).reduceGroup(/*do something*/); DataStream也可以指定一个key:DataStream&lt;...&gt; input = // [...]DataStream&lt;...&gt; windowed = input.keyBy(/*define key here*/).window(/*window specification*/); Flink的数据模型不是基于键值对。因此，没有必要将数据集类型打包成keys和values。keys是”虚拟”：它们只是被定义在实际数据之上的函数，以指导分组算子使用。 备注:在下面的讨论中，我们将使用DataStream API和keyBy。对于DataSet API，你只需要替换为DataSet和groupBy即可。 下面介绍几种Flink定义keys方法。 1. 为Tuples类型定义keys最简单的情况就是在元组的一个或多个字段上对元组进行分组。下面是在元组的第一个字段(整数类型)上进行分组： Java版本:DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = // [...]KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(0) Scala版本:val input: DataStream[(Int, String, Long)] = // [...]val keyed = input.keyBy(0) 下面，我们将在复合key上对元组进行分组，复合key包含元组的第一个和第二个字段: Java版本:DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = // [...]KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(0,1) Scala版本:val input: DataSet[(Int, String, Long)] = // [...]val grouped = input.groupBy(0,1) 如果你有一个包含嵌套元组的DataStream，例如：DataStream&lt;Tuple3&lt;Tuple2&lt;Integer, Float&gt;,String,Long&gt;&gt; ds; 如果指定keyBy(0)，则使用整个Tuple2作为key(以Integer和Float为key)。如果要使用嵌套中Tuple2的某个字段，则必须使用下面介绍的字段表达式指定keys。 2. 使用字段表达式定义keys你可以使用基于字符串的字段表达式来引用嵌套字段以及定义keys来进行分组，排序，连接或coGrouping。字段表达式可以非常容易地选择(嵌套)复合类型(如Tuple和POJO类型)中的字段。 在下面的例子中，我们有一个WC POJO，它有两个字段word和count。如果想通过word字段分组，我们只需将word传递给keyBy()函数即可。 // some ordinary POJO (Plain old Java Object)public class WC &#123; public String word; public int count;&#125;DataStream&lt;WC&gt; words = // [...]DataStream&lt;WC&gt; wordCounts = words.keyBy("word").window(/*window specification*/); 字段表达式语法: (1) 按其字段名称选择POJO字段。例如，user是指向POJO类型的user字段。 (2) 通过字段名称或0到offset的数值字段索引来选择元组字段(field name or 0-offset field index)。例如，f0和5分别指向Java元组类型的第一和第六字段。 (3) 你可以在POJO和元组中选择嵌套字段。例如，user.zip是指POJO类型user字段中的zip字段。支持POJO和Tuples的任意嵌套和组合，如f1.user.zip或user.f3.1.zip。 (4) 你可以使用*通配符表达式选择所有类型。这也适用于不是元组或POJO类型的类型。 Example:public static class WC &#123; public ComplexNestedClass complex; //nested POJO private int count; // getter / setter for private field (count) public int getCount() &#123; return count; &#125; public void setCount(int c) &#123; this.count = c; &#125;&#125;public static class ComplexNestedClass &#123; public Integer someNumber; public float someFloat; public Tuple3&lt;Long, Long, String&gt; word; public IntWritable hadoopCitizen;&#125; 下面是上述示例代码的有效字段表达式：count：WC类中的count字段。complex：递归地选择复合字段POJO类型ComplexNestedClass的所有字段。complex.word.f2：选择嵌套字段Tuple3的最后一个字段。complex.hadoopCitizen：选择Hadoop IntWritable类型。 3. 使用key Selector 函数定义keys定义key的另一种方法是key选择器函数。key选择器函数将单个元素作为输入，并返回元素的key。key可以是任何类型的。 以下示例显示了一个key选择器函数，它只返回一个对象的字段： Java版本:public class WC &#123; public String word; public int count;&#125;DataStream&lt;WC&gt; words = // [...]KeyedStream&lt;WC&gt; kyed = words.keyBy(new KeySelector&lt;WC, String&gt;() &#123; public String getKey(WC wc) &#123; return wc.word; &#125;&#125;); Scala版本:case class WC(word: String, count: Int)val words: DataStream[WC] = // [...]val keyed = words.keyBy( _.word ) 备注:Flink版本:1.4 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 Flink程序剖析]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-anatomy-of-a-flink-program%2F</url>
    <content type="text"><![CDATA[Flink程序程序看起来像转换数据集合的普通程序。每个程序都由相同的基本部分组成： 获得一个执行环境 加载/创建初始数据 指定在这些数据上的转换操作 指定计算结果存放位置 触发程序执行 现在我们将对每一步进行一个简要的概述。请注意，Java DataSet API的所有核心类都可以在org.apache.flink.api.java包中找到，而Java DataStream API的类可以在org.apache.flink.streaming.api中找到。Scala DataSet API的所有核心类都可以在org.apache.flink.api.scala包中找到，而Scala DataStream API的类可以在org.apache.flink.streaming.api.scala中找到。 StreamExecutionEnvironment是所有Flink程序的基础。你可以使用StreamExecutionEnvironment上的如下静态方法获取：Java版本:getExecutionEnvironment()createLocalEnvironment()createRemoteEnvironment(String host, int port, String... jarFiles) Scala版本:getExecutionEnvironment()createLocalEnvironment()createRemoteEnvironment(host: String, port: Int, jarFiles: String*) 通常情况下，我们只需要使用getExecutionEnvironment()即可，因为这会根据上下文做正确的选择：如果你在IDE内执行程序或作为常规的Java程序，将创建一个本地环境，在你的本地机器上执行你的程序。如果使用程序创建JAR文件并通过命令行调用它，那么Flink集群管理器将执行你的main方法，并且getExecutionEnvironment()返回一个用于在集群上执行你程序的执行环境。 对于指定数据源，执行环境有多种方法可以从文件中读取数据：可以逐行读取，以CSV格式文件读取或使用完全自定义的数据输入格式。只要将文本文件作为一系列行读取，就可以使用： Java版本:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;String&gt; text = env.readTextFile("file:///path/to/file"); Scala版本:val env = StreamExecutionEnvironment.getExecutionEnvironment()val text: DataStream[String] = env.readTextFile(&quot;file:///path/to/file&quot;) 这将为你提供一个DataStream，然后就可以应用转换函数来创建新的派生DataStream。 通过调用DataStream上的转换函数来应用转换操作。例如，一个map转换函数看起来像这样： Java版本:DataStream&lt;String&gt; input = ...;DataStream&lt;Integer&gt; parsed = input.map(new MapFunction&lt;String, Integer&gt;() &#123; @Override public Integer map(String value) &#123; return Integer.parseInt(value); &#125;&#125;); Scala版本:val input: DataSet[String] = ...val mapped = input.map &#123; x =&gt; x.toInt &#125; 这将通过将原始集合中的每个String转换为Integer来创建一个新的DataStream。 一旦获得了包含最终结果的DataStream，就可以通过创建接收器(sink)将其写入外部系统中。下面是创建接收器的一些示例方法： Java版本:writeAsText(String path)print() Scala版本:writeAsText(path: String)print() 一旦你指定的完整程序需要触发程序执行，可以通过调用StreamExecutionEnvironment的execute()方法来触发程序的执行。根据执行环境的类型，执行将在你的本地机器上触发，或提交程序在集群上执行。 execute()方法返回一个JobExecutionResult，它包含执行时间和累加器结果。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink1.4 安装与启动]]></title>
    <url>%2F2018%2F01%2F04%2Fflink-how-to-install-and-run%2F</url>
    <content type="text"><![CDATA[1. 下载Flink 可以运行在 Linux, Mac OS X和Windows上。为了运行Flink, 唯一的要求是必须在Java 7.x (或者更高版本)上安装。Windows 用户, 请查看 Flink在Windows上的安装指南。 你可以使用以下命令检查Java当前运行的版本：java -version 如果你安装的是Java 8，输出结果类似于如下:java version &quot;1.8.0_91&quot;Java(TM) SE Runtime Environment (build 1.8.0_91-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode) 从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合方式。如果你只是打算使用本地文件系统，那么可以使用任何版本的Hadoop。进入下载目录，解压下载的压缩包:xiaosi@yoona:~$ tar -zxvf flink-1.3.2-bin-hadoop27-scala_2.11.tgz -C opt/flink-1.3.2/flink-1.3.2/opt/flink-1.3.2/opt/flink-cep_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-datadog-1.3.2.jarflink-1.3.2/opt/flink-metrics-statsd-1.3.2.jarflink-1.3.2/opt/flink-gelly_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-dropwizard-1.3.2.jarflink-1.3.2/opt/flink-gelly-scala_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-ganglia-1.3.2.jarflink-1.3.2/opt/flink-cep-scala_2.11-1.3.2.jarflink-1.3.2/opt/flink-table_2.11-1.3.2.jarflink-1.3.2/opt/flink-ml_2.11-1.3.2.jarflink-1.3.2/opt/flink-metrics-graphite-1.3.2.jarflink-1.3.2/lib/... 2. 启动本地集群使用如下命令启动Flink：xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/start-local.shStarting jobmanager daemon on host yoona. 通过访问 http://localhost:8081 检查JobManager网页,确保所有组件都启动并已运行。网页会显示一个有效的TaskManager实例。 你也可以通过检查日志目录里的日志文件来验证系统是否已经运行:xiaosi@yoona:~/opt/flink-1.3.2/log$ cat flink-xiaosi-jobmanager-0-yoona.log | less2017-10-16 14:42:10,972 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager (Version: 1.3.2, Rev:0399bee, Date:03.08.2017 @ 10:23:11 UTC)...2017-10-16 14:42:11,109 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager without high-availability2017-10-16 14:42:11,111 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager on localhost:6123 with execution mode LOCAL...2017-10-16 14:42:11,915 INFO org.apache.flink.runtime.jobmanager.JobManager - Starting JobManager web frontend...2017-10-16 14:42:13,941 INFO org.apache.flink.runtime.instance.InstanceManager - Registered TaskManager at localhost (akka://flink/user/taskmanager) as 0df4d4ebd25ffec4878906726c29f88c. Current number of registered hosts is 1. Current number of alive task slots is 1.... 3. Example Code你可以在GitHub上找到SocketWindowWordCount例子的完整代码，有Java和Scala两个版本。 Scala:package org.apache.flink.streaming.scala.examples.socketimport org.apache.flink.api.java.utils.ParameterToolimport org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Time/** * Implements a streaming windowed version of the "WordCount" program. * * This program connects to a server socket and reads strings from the socket. * The easiest way to try this out is to open a text sever (at port 12345) * using the ''netcat'' tool via * &#123;&#123;&#123; * nc -l 12345 * &#125;&#125;&#125; * and run this example with the hostname and the port as arguments.. */object SocketWindowWordCount &#123; /** Main program method */ def main(args: Array[String]) : Unit = &#123; // the host and the port to connect to var hostname: String = "localhost" var port: Int = 0 try &#123; val params = ParameterTool.fromArgs(args) hostname = if (params.has("hostname")) params.get("hostname") else "localhost" port = params.getInt("port") &#125; catch &#123; case e: Exception =&gt; &#123; System.err.println("No port specified. Please run 'SocketWindowWordCount " + "--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) and port " + "is the address of the text server") System.err.println("To start a simple text server, run 'netcat -l &lt;port&gt;' " + "and type the input text into the command line") return &#125; &#125; // get the execution environment val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // get input data by connecting to the socket val text: DataStream[String] = env.socketTextStream(hostname, port, '\n') // parse the data, group it, window it, and aggregate the counts val windowCounts = text .flatMap &#123; w =&gt; w.split("\\s") &#125; .map &#123; w =&gt; WordWithCount(w, 1) &#125; .keyBy("word") .timeWindow(Time.seconds(5)) .sum("count") // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1) env.execute("Socket Window WordCount") &#125; /** Data type for words with count */ case class WordWithCount(word: String, count: Long)&#125; Java版本:package org.apache.flink.streaming.examples.socket;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.ReduceFunction;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;/** * Implements a streaming windowed version of the "WordCount" program. * * &lt;p&gt;This program connects to a server socket and reads strings from the socket. * The easiest way to try this out is to open a text server (at port 12345) * using the &lt;i&gt;netcat&lt;/i&gt; tool via * &lt;pre&gt; * nc -l 12345 * &lt;/pre&gt; * and run this example with the hostname and the port as arguments. */@SuppressWarnings("serial")public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the host and the port to connect to final String hostname; final int port; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); hostname = params.has("hostname") ? params.get("hostname") : "localhost"; port = params.getInt("port"); &#125; catch (Exception e) &#123; System.err.println("No port specified. Please run 'SocketWindowWordCount " + "--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) " + "and port is the address of the text server"); System.err.println("To start a simple text server, run 'netcat -l &lt;port&gt;' and " + "type the input text into the command line"); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(hostname, port, "\n"); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split("\\s")) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy("word") .timeWindow(Time.seconds(5)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute("Socket Window WordCount"); &#125; // ------------------------------------------------------------------------ /** * Data type for words with count. */ public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + " : " + count; &#125; &#125;&#125; 4. 运行Example现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读取一段文本，并且每隔5秒打印之前5秒内每个单词出现的个数。例如：a tumbling window of processing time, as long as words are floating in. (1) 首先,我们可以通过netcat命令来启动本地服务:nc -l 9000 (2) 提交Flink程序:xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000Cluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programSubmitting job with JobID: a963626a1e09f7aeb0dc34412adfb801. Waiting for job completion.Connected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#941160871] with leader session id 00000000-0000-0000-0000-000000000000.10/16/2017 15:12:26 Job execution switched to status RUNNING.10/16/2017 15:12:26 Source: Socket Stream -&gt; Flat Map(1/1) switched to SCHEDULED10/16/2017 15:12:26 TriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to SCHEDULED10/16/2017 15:12:26 Source: Socket Stream -&gt; Flat Map(1/1) switched to DEPLOYING10/16/2017 15:12:26 TriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to DEPLOYING10/16/2017 15:12:26 Source: Socket Stream -&gt; Flat Map(1/1) switched to RUNNING10/16/2017 15:12:26 TriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to RUNNING 应用程序连接socket并等待输入，你可以通过web界面来验证任务期望的运行结果： 单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) :xiaosi@yoona:~/opt/flink-1.3.2$ nc -l 9000lorem ipsumipsum ipsum ipsumbye .out文件将在每个时间窗口截止之际打印每个单词的个数：xiaosi@yoona:~/opt/flink-1.3.2$ tail -f log/flink-*-jobmanager-*.outlorem : 1bye : 1ipsum : 4 使用以下命令来停止Flink:./bin/stop-local.sh 阅读更多的例子来熟悉Flink的编程API。 当你完成这些，可以继续阅读streaming指南。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之分布式运行环境]]></title>
    <url>%2F2018%2F01%2F03%2Fflink-distributed-runtime%2F</url>
    <content type="text"><![CDATA[1. 任务链与算子链在分布式运行中，Flink将算子(operator) SubTask 连接成 Task。每个 Task 都只由一个线程执行。将算子链接到 Task 是一个很有用处的优化：它降低了线程间切换和缓冲的开销，并增加了整体吞吐量，同时降低了延迟。链接行为可以在API中配置。 下图中的示例数据流由五个子任务执行，因此具有五个并行线程。 2. 作业管理器, 任务管理器, 客户端Flink运行时(runtime)由两种类型的进程组成： (1) 作业管理器JobManagers(也称为masters)协调分布式运行。主要功能是调度任务，协调检查点，协调故障恢复等。 至少有一个JobManager。高可用配置下将有多个JobManagers，其中一个始终是领导者，其他都是备份。 (2) 任务管理器TaskManagers(也称为workers)执行数据流中的任务(更具体地说是子任务)，并对数据流进行缓冲和交换。 跟JobManager一样，也是至少有一个TaskManager。 JobManagers和TaskManagers可以以不同方式启动：直接在机器上，在容器中，或者由像YARN这样的资源框架来管理。TaskManagers与JobManagers进行连接，来报告自己可用，并分配工作。 客户端不是运行时和程序执行的一部分，而是用来准备数据流并将其发送到JobManager。之后，客户端可以断开连接或保持连接来接收进度报告。客户端作为触发执行的Java/Scala程序的一部分运行，或者在命令行中运行./bin/flink命令来运行…. 3. 任务槽与资源每个worker(TaskManager)都是一个JVM进程，可以在不同的线程中执行一个或多个子任务(译者注:一个任务有一个线程执行)。worker使用任务槽(至少一个)来控制worker能接受多少任务。 每个任务槽代表TaskManager的一个固定资源子集。例如，一个拥有三个任务槽的TaskManager将为每个任务槽分配1/3的内存。资源任务槽化意味着子任务不会与其他作业中的子任务争夺内存，而是任务具有一定数量的保留托管内存。请注意，这里不会对CPU进行隔离。目前任务槽只分离任务的托管内存。 通过调整任务槽的数量，用户可以定义子任务与其他子任务进行隔离。如果每个TaskManager只拥有一个任务槽意味着每个任务组都会在独立的JVM中运行(例如，可以在单独的容器中启动)。如果拥有多个任务槽意味着多个子任务共享同一个JVM。同一JVM中的任务共享TCP连接(通过多路复用)和心跳消息，他们也可以共享数据集和数据结构，从而降低单个任务的开销。 默认情况下，Flink允许子任务共享任务槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个任务槽可能会是一个完整的作业管道。允许任务槽共享有两个主要好处： (1) Flink集群所需的任务槽数与作业中使用的最高并行度数保持一致。不需要计算一个程序总共包含多少个任务(不同任务具有不同的并行度)。 (2) 提高资源利用率。如果没有使用任务槽共享机制，那么非密集的sour/map()子任务就会与资源密集型window子任务阻塞一样多的资源。在我们的示例中，通过任务槽共享，将基本并行度从两个增加到六个，可以充分利用已分配的资源，同时确保繁重的子任务在TaskManager之间公平分配。 这些API还包括一个资源组机制，可以避免不合理的任务槽共享。 根据经验来说，默认任务槽数应该设置为CPU核的数量。如果使用超线程技术，每个任务槽需要2个或更多的硬件线程上下文(With hyper-threading, each slot then takes 2 or more hardware thread contexts)。 4. 后端状态键/值索引存储的确切数据结构取决于所选的后端状态。一个后端状态将数据存储在内存中hash map中，另一个后端状态使用RocksDB存储键/值。除了定义保存状态的数据结构之外，后端状态还实现了获取键/值状态的时间点快照逻辑并将该快照存储为检查点的一部分。 5. 保存点用Data Stream API编写的程序可以从保存点恢复执行。保存点允许更新你的程序和你的Flink集群，而不会丢失任何状态。 保存点是手动触发的检查点，它会捕获程序的快照并将其写入后端状态。他们依赖于常规检查点机制。在执行期间的程序定期在工作节点上生成快照并生成检查点。为了恢复，只需要最后完成的检查点，一旦新的检查点完成，可以安全地丢弃较旧的检查点。 保存点与这些定期检查点类似，只不过它们是由用户触发的，不会在新检查点完成时自动失效。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 工作过程]]></title>
    <url>%2F2017%2F12%2F30%2Fhadoop-mapreduce-working-process%2F</url>
    <content type="text"><![CDATA[1. 从输入到输出一个MapReducer作业经过了input，map，combine，reduce，output五个阶段，其中combine阶段并不一定发生，map输出的中间结果被分到reduce的过程成为shuffle（数据清洗）。在shuffle阶段还会发生copy（复制）和sort（排序）。 在MapReduce的过程中，一个作业被分成Map和Reducer两个计算阶段，它们由一个或者多个Map任务和Reduce任务组成。如下图所示，一个MapReduce作业从数据的流向可以分为Map任务和Reduce任务。当用户向Hadoop提交一个MapReduce作业时，JobTracker则会根据各个TaskTracker周期性发送过来的心跳信息综合考虑TaskTracker的资源剩余量，作业优先级，作业提交时间等因素，为TaskTracker分配合适的任务。Reduce任务默认会在Map任务数量完成5%后才开始启动。 Map任务的执行过程可以概括为：首先通过用户指定的InputFormat类中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。然后map函数经过处理之后将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。最后map函数将中间结果保存到本地。 Reduce任务的执行过程可以概括为：首先需要将已经完成Map任务的中间结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。 2. input如果使用HDFS上的文件作为MapReduce的输入，MapReduce计算框架首先会用 org.apache.hadoop.mapreduce.InputFomat 类的子类FileInputFormat类将作为输入HDFS上的文件切分形成输入分片(InputSplit)，每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MaoReduce作业的性能有非常大的影响。 InputSplit 只是逻辑上对输入数据进行分片，并不会将文件在磁盘上分成分片进行存储。InputSplit 只是记录了分片的元数据节点信息，例如起始位置，长度以及所在的节点列表等。数据切分的算法需要确定 InputSplit 的个数，对于 HDFS 上的文件，FileInputFormat 类使用 computeSplitSize 方法计算出 InputSplit 的大小，代码如下：protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123; return Math.max(minSize, Math.min(maxSize, blockSize));&#125; 其中 minSize 由 mapred-site.xml 文件中的配置项 mapred.min.split.size 决定，默认为1；maxSize 由 mapred-site.xml 文件中的配置项 mapred.max.split.size 决定，默认为9223 372 036 854 775 807；而 blockSize 是由 hdfs-site.xml 文件中的配置项 dfs.block.size 决定，默认为67 108 864字节（64M）。所以InputSplit的大小确定公式为：max(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size)); 一般来说，dfs.block.size 的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size 和 mapred.max.split.size 的大小即可。InputSplit的数量为文件大小除以InputSplitSize。InputSplit的原数据信息会通过一下代码取得：splits.add(new FileSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts())); 从上面的代码可以发现，元数据的信息由四部分组成：文件路径，文件开始位置，文件结束位置，数据块所在的host。 对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念，InputSplit所包含的数据是仍然存储在HDFS的块里面，它们之间的关系如下图所示： 当输入文件切分为InputSplit后，由FileInputFormat的子类（如TextInputFormat）的createRecordReader方法将InputSplit解析为键值对，代码如下：public RecordReader&lt;LongWritable, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) &#123; String delimiter = context.getConfiguration().get( "textinputformat.record.delimiter"); byte[] recordDelimiterBytes = null; if (null != delimiter) recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8); return new LineRecordReader(recordDelimiterBytes);&#125; 此处默认是将行号作为键。解析出来的键值对将被用来作为map函数的输入。至此input阶段结束。 3. map及中间结果的输出InputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和 sort（排序）的操作。 map函数产生输出时，并不是简单的刷写磁盘。为了保证I/O效率，采取了先写到内存的环形内存缓冲区，并做一次预排序，如下图所示： 每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲区大小是100M，该值可以通过mapred-site.xml文件中的 io.sort.mb 的配置项配置。一旦缓冲区内容达到阈值（由mapred-site.xml文件的 io.sort.spill.percent 的值决定，默认为0.80 或者 80%），一个后台线程便会将缓冲区的内容溢写到磁盘中。再写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到 mapred.local.dir（mapred-site.xml文件的配置项）配置的作业特定目录下。 在写磁盘之前，线程会根据数据最终要传入到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程按照建进行内排序，此时如果有一个Combiner，它会在排序后的输出上运行。 一旦内存缓冲区达到溢出的阈值，就会新建一个溢出写文件，因此在Map任务完成最后一个输出记录之后，会有若干个溢出写文件。在Map任务完成之前，溢出写文件被合并成一个已分区且已排序的输出文件作为map输出的中间结果，这也是Map任务的输出结果。 如果已经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner的意义在于使map输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。 为了提高磁盘IO性能，可以考虑压缩map的输出，这样会写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将 mapred-site.xml文件的配置项 mapred.compress.map.output 设为true即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项 mapred.map.output.compression.codec。 指定，如下列出了Hadoop支持的常见压缩格式： map输出的中间结果存储的格式为IFile，IFile是一种支持航压缩的存储格式，支持上述压缩算法。 Reducer通过Http方式得到输出文件的分区。将map输出的中间结果发送到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threds配置项决定，此配置针对每个节点，而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。 4. shuffleshuffle，也叫数据清洗。在某些语境下，代表map函数产生输出到reduce的消化输入的整个过程。 4.1 copy阶段Map任务输出的结果位于Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是，Reduce任务可能需要多个Map任务的输出作为其特殊的分区文件。每个Map任务的完成时间可能不同，当只要有一个任务完成，Reduce任务就开始复制其输出。这就是shuffle的copy阶段。如下图所示，Reduce任务有少量复制线程，可以并行取得Map任务的输出，默认值为5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies的配置项来改变。 如果map输出相当小，则会被复制到Reduce所在TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffle.input.buffer.percent配置项指定。否则，map输出将会被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapred-site.xml文件mapred.job.shuffle.merge.percent配置项决定）或缓冲区的文件数达到阈值大小（由mapred-site.xml文件mapred.inmem.merge.threshold配置项决定），则合并后溢写到磁盘中。 4.2 sort阶段随着溢写到磁盘的文件增多，shuffle进行sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的 io.sort.factor 配置项决定，默认为10）为10，合并操作将进行5次，每次将10个文件合并成一个文件，最后有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合并，将会直接把5个文件交给Reduce函数处理。到此shuffle阶段完成。 从shuffle的过程可以看出，Map任务处理的是一个InputSplit，而Reduce任务处理的是所有Map任务同一个分区的中间结果。 5. reduce及最后结果的输出reduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。 与map的中间结果不同的是，reduce的输出一般为HDFS。 6. sort排序贯穿于Map任务和Reduce任务，排序操作属于MapReduce计算框架的默认行为，不管流程是否需要，都会进行排序。在MapReduce计算框架中，主要用到了两种排序算法：快速排序和归并排序。 在Map任务和Reduce任务的过程中，一共发生了3次排序操作。 （1）当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阈值，在刷写磁盘之前，后台线程会将缓冲区的数据划分相应的分区。在每个分区中，后台线程按键进行内排序。如下图所示。 （2）在Map任务完成之前，磁盘上存在多个已经分好区，并排好序，大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。如下图所示。 （3）在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需在做一次排序就可以使输出文件整体有序: 在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序；第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。 7. 作业的进度组成一个MapReduce作业在Hadoop上运行时，客户端的屏幕通常会打印作业日志，如下： 对于一个大型的MapReduce作业来说，执行时间可能会比较比较长，通过日志了解作业的运行状态和作业进度是非常重要的。对于Map来说，进度代表实际处理输入所占比例，例如 map 60% reduce 0% 表示Map任务已经处理了作业输入文件的60%，而Reduce任务还没有开始。而对于Reduce的进度来说，情况比较复杂，从前面得知，reduce阶段分为copy，sort 和 reduce，这三个步骤共同组成了reduce的进度，各占1/3。如果reduce已经处理了2/3的输入，那么整个reduce的进度应该为 1/3 + 1/3 + 1/3 * (2/3) = 5/9 ，因为reduce开始处理时，copy和sort已经完成。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 配置远程登录]]></title>
    <url>%2F2017%2F12%2F29%2FMySQL%2F%5BMySQL%5DMySQL%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[1. 修改配置修改/etc/mysql/mysql.conf.d目录下的mysqld.cnf配置文件:# Instead of skip-networking the default is now to listen only on# localhost which is more compatible and is not less secure.#bind-address = 127.0.0.1 在bind-address前面加个#进行注释，允许任意IP访问。或者指定自己需要远程访问的IP地址。然后重启mysql:ubuntu@VM-0-7-ubuntu:/etc/mysql/mysql.conf.d$ sudo /etc/init.d/mysql restartRestarting mysql (via systemctl): mysql.service. 2. 授权用户我们先看一下当前能登录到我们数据的用户以及允许连接的IP:mysql&gt; USE mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select User,Host from user;+------------------+-----------+| User | Host |+------------------+-----------+| debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost || root | localhost |+------------------+-----------+4 rows in set (0.00 sec) 我们可以看到只有一个默认的root用户，且只允许使用localhost连接。下面我们另外添加一个新的root用户在指定IP下使用指定密码来访问数据库:mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;Query OK, 0 rows affected, 1 warning (0.00 sec) *.*中第一个*代表数据库名称，第二个*代表表名。在这里我们设置的是所有数据库里的所有表都授权给用户，如果只想授权某数据库或某些数据库下某些表，可以把*替换成你所需的数据库名和表明即可:mysql&gt; GRANT ALL PRIVILEGES ON test_db.user TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;dev&apos; WITH GRANT OPTION; 上述表示是把test_db数据下的user数据表授权给用户。 root表示授予root用户可以登录数据库。%表示授权的用户使用哪些IP可以登录，这里表示可以使用用户root在任意IP地址来访问数据库。dev表示分配root用户对应的密码。 当然我们也可以直接用UPDATE更新root用户Host, 但不推荐：UPDATE user SET Host=&apos;%&apos; WHERE User=&apos;root&apos; AND Host=&apos;localhost&apos;; 授权用户之后，执行如下命令刷新一下权限:mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 至此我们已经完成了配置远程访问数据的所有操作，我们在看一下当前能访问我们数据库的用户:mysql&gt; select User,Host from user;+------------------+-----------+| User | Host |+------------------+-----------+| root | % || debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost || root | localhost |+------------------+-----------+5 rows in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Airflow使用指南一 安装与启动]]></title>
    <url>%2F2017%2F12%2F29%2FAirflow%2F%5BAirFlow%5DAirFlow%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%E4%B8%80%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[1. 安装通过pip安装:xiaosi@yoona:~$ pip install airflow 如果速度比较慢，可以使用下面提供的源进行安装:xiaosi@yoona:~$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple airflow 如果出现下面提示，表示你的airflow安装成功了:Successfully installed airflow alembic croniter dill flask flask-admin flask-cache flask-login flask-swagger flask-wtf funcsigs future gitpython gunicorn jinja2 lxml markdown pandas psutil pygments python-daemon python-dateutil python-nvd3 requests setproctitle sqlalchemy tabulate thrift zope.deprecation Mako python-editor click itsdangerous Werkzeug wtforms PyYAML ordereddict gitdb2 MarkupSafe pytz numpy docutils setuptools lockfile six python-slugify idna urllib3 certifi chardet smmap2 UnidecodeCleaning up... 安装完成之后我的默认安装在~/.local/bin目录下 2. 配置如果不修改路径，默认的配置为~/airflow 永久修改环境变量echo &quot;export AIRFLOW_HOME=/home/xiaosi/opt/airflow&quot; &gt;&gt; /etc/profilesource /etc/profile 为了便于操作方便，进行如下配置:echo &quot;export PATH=/home/xiaosi/.local/bin:$PATH&quot; &gt;&gt; /etc/profilesource /etc/profile 3. 初始化初始化数据库:xiaosi@yoona:~$ airflow initdb[2017-08-02 16:39:22,319] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 16:39:22,432] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 16:39:22,451] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txtDB: sqlite:////home/xiaosi/opt/airflow/airflow.db[2017-08-02 16:39:22,708] &#123;db.py:287&#125; INFO - Creating tablesINFO [alembic.runtime.migration] Context impl SQLiteImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; e3a246e0dc1, current schemaINFO [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encrypted/home/xiaosi/.local/lib/python2.7/site-packages/alembic/util/messaging.py:69: UserWarning: Skipping unsupported ALTER for creation of implicit constraint warnings.warn(msg)INFO [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrationsINFO [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntanceINFO [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indicesINFO [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to LogINFO [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrunINFO [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_durationINFO [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_configINFO [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to userINFO [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start endINFO [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_missINFO [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connectionINFO [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable tableINFO [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user tableINFO [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state indexINFO [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal tableINFO [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats tableINFO [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tablesINFO [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indicesINFO [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstanceINFO [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run tableDone. 运行上述命令之后，会在$AIRFLOW_HOME目录下生成如下文件:xiaosi@yoona:~/opt/airflow$ ll总用量 88drwxrwxr-x 2 xiaosi xiaosi 4096 8月 2 16:39 ./drwxrwxr-x 26 xiaosi xiaosi 4096 7月 31 13:56 ../-rw-rw-r-- 1 xiaosi xiaosi 11424 8月 2 16:38 airflow.cfg-rw-r--r-- 1 xiaosi xiaosi 58368 8月 2 16:39 airflow.db-rw-rw-r-- 1 xiaosi xiaosi 1554 8月 2 16:38 unittests.cfg 4. 修改默认数据库找到$AIRFLOW_HOME/airflow.cfg配置文件，进行如下修改:sql_alchemy_conn = mysql://root:root@localhost:3306/airflow 备注 数据库用户名与密码均为root，airflow使用的数据库为airflow．使用如下命令创建对应的数据库:mysql&gt; create database airflow;Query OK, 1 row affected (0.00 sec) 重新初始化服务器数据库:xiaosi@yoona:~$ airflow initdb 出现了如下错误:xiaosi@yoona:~$ airflow initdbTraceback (most recent call last): File &quot;/home/xiaosi/.local/bin/airflow&quot;, line 17, in &lt;module&gt; from airflow import configuration File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/__init__.py&quot;, line 30, in &lt;module&gt; from airflow import settings File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 159, in &lt;module&gt; configure_orm() File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 147, in configure_orm engine = create_engine(SQL_ALCHEMY_CONN, **engine_args) File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py&quot;, line 387, in create_engine return strategy.create(*args, **kwargs) File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py&quot;, line 80, in create dbapi = dialect_cls.dbapi(**dbapi_args) File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py&quot;, line 110, in dbapi return __import__(&apos;MySQLdb&apos;)ImportError: No module named MySQLdb 解决方案: MySQL是最流行的开源数据库之一，但在Python标准库中并没有集成MySQL接口程序，MySQLdb是一个第三方包，需独立下载并安装。sudo apt-get install python-mysqldb 再次初始化:xiaosi@yoona:~$ airflow initdb[2017-08-02 17:22:21,169] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 17:22:21,282] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 17:22:21,302] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txtDB: mysql://root:***@localhost:3306/airflow[2017-08-02 17:22:21,553] &#123;db.py:287&#125; INFO - Creating tablesINFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; e3a246e0dc1, current schemaINFO [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encryptedINFO [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrationsINFO [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntanceINFO [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indicesINFO [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to LogINFO [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrunINFO [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_durationINFO [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_configINFO [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to userINFO [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start endINFO [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_missINFO [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connectionINFO [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable tableINFO [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user tableINFO [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state indexINFO [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal tableINFO [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats tableINFO [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tablesINFO [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indicesINFO [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstanceINFO [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run tableDone. 查看一下airflow数据库中做了哪些操作:mysql&gt; use airflow;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+-------------------+| Tables_in_airflow |+-------------------+| alembic_version || chart || connection || dag || dag_pickle || dag_run || dag_stats || import_error || job || known_event || known_event_type || log || sla_miss || slot_pool || task_fail || task_instance || users || variable || xcom |+-------------------+19 rows in set (0.00 sec) 5. 启动通过如下命令就可以启动后台管理界面，默认访问localhost:8080即可:xiaosi@yoona:~$ airflow webserver[2017-08-02 17:25:31,961] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 17:25:32,075] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 17:25:32,095] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ ______ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / /___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__//home/xiaosi/.local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead. .format(x=modname), ExtDeprecationWarning[2017-08-02 17:25:32,469] [9703] &#123;models.py:167&#125; INFO - Filling up the DagBag from /home/xiaosi/opt/airflow/dagsRunning the Gunicorn Server with:Workers: 4 syncHost: 0.0.0.0:8080Timeout: 120Logfiles: - -================================================================= [2017-08-02 17:25:33,052] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor[2017-08-02 17:25:33,156] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt[2017-08-02 17:25:33,179] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt[2017-08-02 17:25:33 +0000] [9706] [INFO] Starting gunicorn 19.3.0[2017-08-02 17:25:33 +0000] [9706] [INFO] Listening at: http://0.0.0.0:8080 (9706)[2017-08-02 17:25:33 +0000] [9706] [INFO] Using worker: sync... 可以访问 http://localhost:8080/admin/ , 呈现出的主界面如下:]]></content>
      <categories>
        <category>Airflow</category>
      </categories>
      <tags>
        <tag>Airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 内部原理之编程模型]]></title>
    <url>%2F2017%2F12%2F29%2Fflink-programming-model%2F</url>
    <content type="text"><![CDATA[1. 抽象层次Flink提供不同级别的抽象层次来开发流处理和批处理应用程序。 (1) 最低级别的抽象只是提供有状态的数据流。通过Process Function集成到DataStream API中。它允许用户不受限制的处理来自一个或多个数据流的事件，并可以使用一致的容错状态(consistent fault tolerant state)。另外，用户可以注册事件时间和处理时间的回调函数，允许程序实现复杂的计算。 (2) 在实际中，大多数应用程序不需要上述描述的低级抽象，而是使用如DataStream API(有界/无界流)和DataSet API(有界数据集)的核心API进行编程。这些核心API提供了用于数据处理的通用构建模块，如用户指定的各种转换，连接，聚集，窗口，状态等。在这些API中处理的数据类型被表示为对应编程语言中的类。 低级别的Process Function与DataStream API集成在一起，使得可以对特定操作使用较低级别的抽象接口。DataSet API为有限数据集提供了额外的原语(primitives)，如循环/迭代。 (3) Table API是以表为核心的声明式DSL，可以动态地改变表(当表表示流数据时)。Table API遵循(扩展的)关系模型：每个表都有一个schema(类似于关系数据库中的表)，对应的API提供了类似的操作(offers comparable operations)，如select，project，join，group-by，aggregate等。Table API程序声明性地定义了如何在逻辑上实现操作，而不是明确指定操作实现的具体代码。尽管Table API可以通过各种类型的用户自定义函数进行扩展，它比核心API表达性要差一些，但使用上更简洁(编写代码更少)。另外，Table API程序也会通过一个优化器，在执行之前应用优化规则。 可以在表和DataStream/DataSet之间进行无缝转换，允许程序混合使用Table API和DataStream和DataSet API。 (4) Flink提供的最高级抽象是SQL。这种抽象在语法和表现力方面与Table API类似，但是是通过SQL查询表达式实现程序。SQL抽象与Table API紧密交互，SQL查询可以在Table API中定义的表上执行。 2. 程序与数据流Flink程序的基本构建块是流和转换操作。 备注:Flink的DataSet API中使用的数据集也是内部的流 - 稍后会介绍这一点。 从概念上讲，流是数据记录(可能是永无止境的)流，而转换是将一个或多个流作为输入，并产生一个或多个输出流。 执行时，Flink程序被映射到由流和转换算子组成的流式数据流(streaming dataflows)。每个数据流从一个或多个source开始，并在一个或多个sink中结束。数据流类似于有向无环图(DAG)。尽管通过迭代构造允许特殊形式的环，但是为了简单起见，大部分我们都会这样描述。 程序中的转换与数据流中的算子通常是一一对应的。然而，有时候，一个转换可能由多个转换算子组成。 3. 并行数据流图Flink中的程序本质上是分布式并发执行的。在执行过程中，一个流有一个或多个流分区，每个算子有一个或多个算子子任务。算子子任务之间相互独立，并且在不同的线程中执行，甚至有可能在不同的机器或容器上执行。 算子子任务的数量是该特定算子的并发数。流的并发数总是产生它的算子的并发数。同一程序的不同算子可能具有不同的并发级别。 在两个算子之间的流可以以一对一模式或重新分发模式传输数据: (1) 一对一流(例如上图中的Source和map()算子之间的流)保留了元素的分区和排序。这意味着将会在map()算子的子任务[1]中看到在Source算子的子任务[1]中产生的相同元素，并且具有相同的顺序。 (2) 重分发流(例如上图的的map()和keyBy()/window()/apply()之间，以及在keyBy()/window()/apply()和Sink之间的数据流)改变了流的分区。每个算子子任务根据所选的转换操作将数据发送到不同的目标子任务。比如keyBy()(根据key的哈希值重新分区)，broadcast()，或者rebalance()(随机重新分区)。在重新分配交换中，只会在每对发送与接受子任务(比如，map()的子任务[1]与keyBy()/window()/apply()的子任务[2])中保留元素间的顺序。在上图的例子中，尽管在子任务之间每个 key 的顺序都是确定的，但是由于程序的并发引入了不确定性，最终到达Sink的元素顺序就不能保证与一开始的元素顺序完全一致。 关于配置并发的更多信息可以参阅并发执行文档。 4. 窗口聚合事件(比如计数、求和)在流上的工作方式与批处理不同。比如，不可能对流中的所有元素进行计数，因为通常流是无限的(无界的)。相反，流上的聚合(计数，求和等)需要由窗口来划定范围，比如在最近5分钟内计算，或者对最近100个元素求和。 窗口可以是时间驱动的(比如：每30秒）或者数据驱动的(比如：每100个元素)。窗口通常被区分为不同的类型，比如滚动窗口(没有重叠)，滑动窗口(有重叠)，以及会话窗口(由不活动的间隙所打断) 更多的窗口示例可以在这篇博客中找到。更多详细信息在窗口文档。 5. 时间当提到流程序(例如定义窗口)中的时间时，你可以参考不同的时间概念： (1) 事件时间是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过时间戳分配器访问事件时间戳。 (2) 摄入时间是事件进入Flink数据流源(source)算子的时间。 (3) 处理事件是每一个执行基于时间操作算子的本地时间。 更多关于如何处理时间的详细信息可以查看事件时间文档. 6. 有状态操作尽管数据流中的很多操作一次只查看一个独立的事件(比如事件解析器)，但是有些操作会记录多个事件间的信息(比如窗口算子)。这些操作被称为有状态的 。 有状态操作的状态保存在一个可被视为嵌入式键值对存储中。状态与由有状态算子读取的流一起被严格地分区与分布(distributed)。因此，只有在应用keyBy()函数之后，才能访问keyed streams上的键/值对状态，并且仅限于与当前事件key相关联的值(access to the key/value state is only possible on keyed streams, after a keyBy() function, and is restricted to the values associated with the current event’s key. )。对齐流和状态的key(Aligning the keys of streams and state)确保了所有状态更新都是本地操作，保证一致性，而没有事务开销(guaranteeing consistency without transaction overhead)。这种对齐还使得Flink可以透明地重新分配状态与调整流的分区。 7. 容错性检查点Flink组合使用流重放与检查点实现了容错。检查点与每一个输入流以及每一个算子对应的状态所在的特定点相关联(A checkpoint is related to a specific point in each of the input streams along with the corresponding state for each of the operators.)。一个流数据流可以可以从一个检查点恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性(一次处理语义) 检查点时间间隔是在恢复时间(需要重放的事件数量)内消除执行过程中容错开销的一种手段。 更多关于检查点与容错的详细信息可以查看容错文档。 8. 批处理操作Flink将批处理程序作为流处理程序的一种特殊情况来执行，只是流是有界的(有限个元素)。在内部DataSet被视为数据流(A DataSet is treated internally as a stream of data)。因此上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外： (1) 批处理程序的容错不使用检查点。通过重放全部流来恢复。这是可能的，因为输入是有限的。这使恢复的成本更高(This pushes the cost more towards the recovery)，但是使常规处理更便宜，因为它避免了检查点。 (2) DataSet API中的有状态操作使用简化的in-memory/out-of-core数据结构，而不是键/值索引。 (3) DataSet API引入了特殊的同步(基于superstep的)迭代，而这种迭代仅仅能在有界流上执行。详细信息可以查看迭代文档。 原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.x HDFS架构]]></title>
    <url>%2F2017%2F12%2F20%2Fhadoop-2.x-hdfs-architecture%2F</url>
    <content type="text"><![CDATA[1. 概述Hadoop分布式文件系统(HDFS)是一个分布式文件系统，设计初衷是可以在商用硬件上运行。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的也有显著的差异。HDFS具有高容错能力，可以部署在低成本的硬件上。HDFS提供对应用程序数据的高吞吐量访问，适用于具有大数据集的应用程序。HDFS放宽了一些POSIX要求，以便对文件系统数据进行流式访问。HDFS最初是作为Apache Nutch网络搜索引擎项目的基础架构构建的。HDFS是Apache Hadoop Core项目的一部分。项目URL为: http://hadoop.apache.org/ 2. 设想与目标2.1 硬件故障 硬件故障很常见不要感到意外。HDFS实例可能由成百上千台服务器机器组成，每台机器存储部分文件系统的数据。事实上，有大量的组件，并且每个组件具有不一定的故障概率，这意味着可能HDFS的某些组件总是不起作用的。因此，故障检测和快速自动恢复是HDFS的核心架构。 2.2 流式数据访问运行在HDFS上的应用程序需要流式访问其数据集。HDFS不是运行在通用文件系统上通用应用程序。HDFS设计是为了更多的批量处理，而不是与用户进行交互。重点是数据访问的高吞吐量，而不是数据访问的低延迟。 2.3 大数据集运行在HDFS上的应用程序具有较大的数据集。HDFS中的文件大小一般为几GB或几TB。因此，HDFS需要支持大文件。它需要提供高数据聚合带宽并可以在单个集群中扩展到的数百个节点。它需要在一个实例中支持数千万个文件。 2.4 简单一致性模型HDFS数据访问模式为一次写入多次读取。文件一旦创建、写入和关闭后，除了追加和截断外，文件不能更改。可以支持将内容追加到文件末尾，但不能在随意位置更新文件内容。该假设简化了数据一致性问题，并实现了数据访问的高吞吐量。MapReduce应用程序或Web爬虫程序应用程序与此模型完美匹配。 2.5 ‘移动计算比移动数据便宜’如果应用程序能够在其操作的数据附近执行，那么应用程序所请求的计算效率会更高一些。当数据集很大时，这一点更能体现。这样可以最大限度地减少网络拥塞并提高系统的整体吞吐量。我们假设将计算迁移到更靠近数据的位置比将数据转移到应用程序运行的位置更好。HDFS为应用程序提供接口，使其更靠近数据所在的位置。 2.6 跨越异构硬件和软件平台的可移植性HDFS被设计为可以从一个平台轻松地移植到另一个平台。这有助于HDFS作为大型应用程序的首选平台。 3. NameNode and DataNodesHDFS是一个主/从结构。一个HDFS集群包含一个NameNode，管理文件系统命名空间以及管理客户端对文件访问的主服务。除此之外，还有一些DataNode，通常集群中的每个节点都有一个DataNode，用于管理它们所运行节点相关的存储。HDFS公开文件系统命名空间，并允许用户数据存储在文件中。在内部，一个文件被分成一个或多个数据块，这些数据块被存储在一组DataNode中。NameNode执行文件系统命名空间操作，例如打开，关闭和重命名文件和目录等。它也决定数据块到DataNode的映射。DataNode负责为文件系统客户端的读写请求提供服务。DataNode还根据来自NameNode的指令执行数据块的创建，删除和复制。 NameNode和DataNode是设计用于在商业机器上运行的软件。这些机器通常运行GNU/Linux操作系统(OS)。HDFS是使用Java语言构建的; 任何支持Java的机器都可以运行NameNode或DataNode。使用高可移植性的Java语言意味着HDFS可以部署在各种机器上。一个典型的部署是有一台专用机器来运行NameNode。集群中的其他机器运行DataNode实例。该体系结构并不排除在同一台计算机上运行多个DataNode，但在实际部署中很少出现这种情况。 集群中NameNode的存在大大简化了系统的体系结构。NameNode是所有HDFS元数据的决策者和存储仓库。系统的这种设计方式可以允许用户数据不会经过NameNode，直接与DataNode进行连接。 4. 文件系统命名空间HDFS支持传统的分层文件组织方式。用户或应用程序可以创建目录以及在这些目录内存储文件。文件系统命名空间层次结构与大多数其他文件系统类似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。HDFS支持用户配额和访问权限。HDFS不支持硬链接或软链接。但是，HDFS体系结构并不排除实现这些功能。 NameNode维护文件系统的命名空间。对文件系统命名空间或其属性的任何更改都会在NameNode中记录。应用程序可以指定HDFS应该维护的文件的副本数量。文件的副本数称为该文件的复制因子。这个信息由NameNode存储。 5. 数据复制HDFS旨在大型集群多台机器上可靠地存储非常大的文件。将每个文件存储为一系列的数据块。文件的数据块被复制多份以实现容错。数据块大小和副本因子是可以通过配置文件进行配置。 一个文件的数据块除最后一个块以外的所有其他块的大小都相同，在添加对可变长度块和hsync的支持后，用户可以不用填充最后一个块到配置大小而启动一个新块。 应用程序可以指定文件的副本数量。复制因子可以在文件创建时指定，也可以在以后更改。HDFS中的文件是一次性编写的(追加和截断除外)，并且严格限定在任何时候都只能有一个编写者。 NameNode做出关于块复制的所有决定。它周期性的从集群中的每个DataNode接收Heartbeat和Blockreport。收到Heartbeat意味着DataNode运行正常。Blockreport包含DataNode上所有块的列表。 5.1 副本安置副本的放置对HDFS的可靠性和性能至关重要。优化副本放置能将HDFS与大多数其他分布式文件系统区分开来。这是一个需要大量调整和体验的功能。机架感知副本放置策略的目的是提高数据可靠性，可用性和网络带宽利用率。副本放置策略的目前实现是朝这个方向迈进的第一步。实施这一策略的短期目标是在生产环境上进行验证，更多地了解其行为，并为测试和研究更复杂的策略奠定基础。 大型HDFS实例运行在通常分布在多个机架上的一组计算机上。不同机架中的两个节点之间的通信必须经过交换机。在大多数情况下，同一机架中的机器之间的网络带宽大于不同机架中的机器之间的网络带宽。 NameNode通过Hadoop机架感知中概述的过程确定每个DataNode所属的机架Id。一个简单但不是最佳的策略是将副本放在不同的机架上。这可以防止整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在集群中均匀分配副本，以便轻松平衡组件故障的负载(This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure)。但是，此策略会增加写入成本，因为写入需要将数据块传输到多个机架。 正常情况下，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架的同一个节点上，另一个放在本地机架的不同节点上，最后放在另一个机架的不同节点上。这个政策降低了机架间写入流量，这通常会提高写入性能。机架故障的几率远远小于节点故障的几率;此策略不会影响数据可靠性和可用性的保证。但是，它降低了读取数据时使用的总体网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本在同一个节点上，三分之二的副本在同一个机架上，另外三分之一在其它机架上均匀分布。此策略可提高写入性能，而不会影响数据可靠性或读取性能。 这里描述的就是当前默认副本放置策略。 5.2 副本选择为了尽量减少全局带宽消耗和读取延迟，HDFS会尝试将读取请求发送到离读取者最近的副本上(HDFS tries to satisfy a read request from a replica that is closest to the reader.)。 如果在与读取者节点相同的机架上存在副本，则该副本优选满足读取请求。如果HDFS进群跨越多个数据中心，则保存在本地数据中心的副本优先于任何远程副本。 5.3 安全模式在启动时，NameNode进入一个称为Safemode(安全模式)的特殊状态。当NameNode处于安全模式状态时，不会发生数据块的复制。NameNode接收来自DataNode的Heartbeat和Blockreport消息。Blockreport包含DataNode托管的数据块列表。每个块都有指定的最小数量的副本。当该数据块的最小副本数与NameNode签入时，将认为该块被安全地复制。在安全复制数据块的可配置百分比检入NameNode（再加上30秒）之后，NameNode退出安全模式状态。然后确定仍然少于指定副本数量的数据块列表（如果有的话）。NameNode然后将这些块复制到其他DataNode。 6. 文件系统元数据持久化HDFS命名空间存储在NameNode中。NameNode使用称之为EditLog编辑日志的事务日志来持久化存储在文件系统元数据上发生的每一个变化。例如，在HDFS中创建一个新文件会导致NameNode向EditLog编辑日志中插入一条记录。同样，更改文件的复制因子也会导致将新记录插入到EditLog编辑日志中。NameNode使用其本地主机OS文件系统中的文件来存储EditLog编辑日志。整个文件系统命名空间，包括数据块到文件的映射以及文件系统属性，都存储在一个名为FsImage的文件中。FsImage作为文件存储在NameNode的本地文件系统中。 NameNode将整个文件系统命名空间和文件Blockmap的快照(image)保存在内存中。这个关键的元数据被设计得很紧凑，这样一个具有4GB内存的NameNode足以支持大量的文件和目录。当NameNode启动时，它会从磁盘中读取FsImage和EditLog编辑日志，将EditLog编辑日志中的所有事务应用到内存中的FsImage(applies all the transactions from the EditLog to the in-memory representation of the FsImage)，并将这个新版本刷新到磁盘上生成一个新FsImage。它可以截断旧的EditLog编辑日志，因为它的事务已经被应用到持久化的FsImage上。这个过程被称为检查点。在目前的实现中，只有在NameNode启动时才会出现检查点。在未来版本中正在进行工作的NameNode也会支持周期性的检查点。 DataNode将HDFS数据存储在本地文件系统的文件中。DataNode不了解HDFS文件(The DataNode has no knowledge about HDFS files)。它将每个HDFS数据块存储在本地文件系统中的单个文件中。DataNode不会在同一目录中创建所有文件。相反，它使用启发式来确定每个目录的最佳文件数量并适当地创建子目录。由于本地文件系统可能无法有效地支持单个目录中的大量文件，因此在同一目录中创建所有本地文件并不是最佳选择。当DataNode启动时，它会扫描其本地文件系统，生成一个包含所有HDFS数据块(与每个本地文件相对应)的列表，并将此报告发送给NameNode：这是Blockreport。 7. 通信协议所有的HDFS通信协议都是基于TCP/IP协议的。客户端建立到NameNode机器上的可配置TCP端口的连接。它使用ClientProtocol与NameNode交谈。DataNode使用DataNode协议与NameNode进行通信。远程过程调用(RPC)抽象包装客户端协议和数据节点协议。根据设计，NameNode永远不会启动任何RPC。而是只响应由DataNode或客户端发出的RPC请求。 8. 稳定性HDFS的主要目标是即使在出现故障时也能可靠地存储数据。三种常见的故障类型是NameNode故障，DataNode故障和网络分裂(network partitions)。 8.1 数据磁盘故障，心跳和重新复制每个DataNode定期向NameNode发送一个Heartbeat消息。网络分裂可能导致一组DataNode与NameNode失去联系。NameNode通过丢失Heartbeat消息来检测这种情况。NameNode将最近没有Heartbeats的DataNode标记为死亡，并且不会将任何新的IO请求转发给它们。任何注册在标记为死亡的DataNode中的数据不再可用。DataNode死亡可能导致某些块的复制因子降到其指定值以下。NameNode不断跟踪哪些块需要复制，并在需要时启动复制。重新复制可能由于许多原因而产生：DataNode可能变得不可用，副本可能被破坏，DataNode上的硬盘可能出现故障，或者文件的复制因子可能需要增加。 为了避免由于DataNode的状态震荡而导致的复制风暴，标记DataNode死亡的超时时间设置的比较保守(The time-out to mark DataNodes dead is conservatively long)(默认超过10分钟)。用户可以设置较短的时间间隔以将DataNode标记为陈旧，并避免陈旧节点在读取或按配置写入时性能出现负载(Users can set shorter interval to mark DataNodes as stale and avoid stale nodes on reading and/or writing by configuration for performance sensitive workloads)。 8.2 集群重新平衡HDFS体系结构与数据重新平衡方案兼容。如果某个DataNode上的可用空间低于某个阈值，那么会自动将数据从一个DataNode移动到另一个DataNode。对于特定文件突然高需求(sudden high demand)的情况下，可能会动态创建额外的副本并重新平衡集群中的其他数据。这些类型的数据重新平衡方案尚未实现。 8.3 数据完整性从DataNode上获取的数据块可能会损坏。发生损坏可能是由存储设备故障，网络故障或软件错误引起。HDFS客户端实现了对HDFS上文件内容进行校验和检查。当客户端创建一个HDFS文件时，它会计算每个文件的对应数据块的校验和，并将这些校验和存储在同一个HDFS命名空间中的单独隐藏文件中。当客户端检索文件内容时，它会验证从每个DataNode收到的数据是否与存储在相关校验和文件中的校验和相匹配。如果不匹配，那么客户端可以选择从另一个具有该数据块副本的DataNode中检索该数据块。 8.4 元数据磁盘故障FsImage和EditLog编辑日志是HDFS中的中心数据结构。这些文件的损坏可能会导致HDFS实例无法正常运行。为此，NameNode可以配置为支持维护FsImage和EditLog编辑日志的多个副本。任何对FsImage或EditLog编辑日志的更新都会引起每个FsImages和EditLogs编辑日志同步更新。同步更新FsImage和EditLog编辑日志的多个副本可能会降低NameNode支持的每秒的命名空间事务的速度(degrade the rate of namespace transactions per second)。但是，这种降低是可以接受的，因为尽管HDFS应用程序实质上是非常密集的数据，但是它们也不是元数据密集型的。当NameNode重新启动时，它会选择最新的一致的FsImage和EditLog编辑日志来使用。 另一个增强防御故障的方法是使用多个NameNode以启用高可用性，或者使用NFS上的共享存储或使用分布式编辑日志(称为Journal)。后者是推荐的方法。 8.5 快照快照支持在特定时刻存储数据副本。快照功能的一种用法是将损坏的HDFS实例回滚到先前已知的良好时间点。 9. 数据组织9.1 数据块HDFS为支持大文件而设计的。与HDFS兼容的应用程序是处理大型数据集的应用程序。这些应用程序只写入数据一次，但是读取一次或多次，并读取速度要求满足流式处理速度。HDFS支持在文件上一次写入多次读取语义。HDFS使用的一般块大小为128 MB。因此，一个HDFS文件被分成多个128MB的块，如果可能的话，每个块将保存在不同的DataNode上。 9.2 分阶段客户端创建文件的请求不会立即到达NameNode。事实上，最初HDFS客户端将文件数据缓存到本地缓冲区。应用程序写入重定向到本地缓冲区。当本地文件累积超过一个块大小的数据时，客户端才会联系NameNode。NameNode将文件名插入到文件系统层次结构中，并为其分配一个数据块。NameNode将DataNode和目标数据块的标识和返回给客户请求。然后，客户端将本地缓冲区中的数据块保存到指定的DataNode上。当文件关闭时，本地缓冲区中剩余的未保存数据也被传输到DataNode。客户端然后告诉NameNode该文件已关闭。此时，NameNode将文件创建操作提交到持久化存储中。如果NameNode在文件关闭之前崩溃，那么文件会丢失。 在仔细考虑在HDFS上运行的目标应用程序之后，采用了上述方法。这些应用程序需要流式写入文件。如果客户端直接写入远程文件目录而没有在客户端进行任何缓冲，那么网络速度和网络拥塞会大大影响吞吐量。这种方法并非没有先例。较早的分布式文件系统，例如AFS，已经使用客户端缓存来提高性能。POSIX的要求已经放宽，以实现更高的数据传输性能。 9.3 副本流水线当客户端将数据写入HDFS文件时，首先将数据写入本地缓冲区，如上一节所述。假设HDFS文件复制因子为3。当本地缓冲区累积了一个块的用户数据时，客户端从NameNode中检索DataNode列表。该列表包含保存数据的数据块副本的DataNode。客户端然后将数据块刷新到第一个DataNode。第一个DataNode开始接收一小部分数据，将这一小部分数据写入其本地存储库，然后传输到列表中的第二个DataNode。第二个DataNode依次接收数据块的每一部分数据，将其写入存储库，然后再将刷新到第三个DataNode。最后，第三个DataNode将数据写入其本地存储库。因此，DataNode可以以流水线的方式从前一个DataNode接收数据，同时将数据转发到流水线中的下一个DataNode。因此，数据从一个DataNode流到下一个。 10. 访问应用程序可以以多种不同的方式访问HDFS。HDFS为应用程序提供了一个FileSystem Java API。Java API和REST API的C语言包装器也可以使用。另外还有一个HTTP浏览器(HTTP browser)，也可以用来浏览HDFS实例的文件。通过使用NFS网关，可以将HDFS作为客户端本地文件系统的一部分。 10.1 FS ShellHDFS将用户数据以文件和目录的形式进行组织。它提供了一个名为FS shell的命令行接口，让用户可以与HDFS中的数据进行交互。这个命令集的语法类似于用户已经熟悉的其他shell(例如bash，csh)。 以下是一些示例操作/命令对： 操作 命令 创建/foodir目录 bin/hadoop dfs -mkdir /foodir 删除目录/foodir bin/hadoop fs -rm -R /foodir 查看/foodir/myfile.txt中内容 bin/hadoop dfs -cat /foodir/myfile.txt FS shell针对需要脚本语言与存储数据进行交互的应用程序。 10.2 DFSAdminDFSAdmin命令集用于管理HDFS集群。这些是仅能由HDFS管理员使用的命令。以下是一些示例操作/命令对： 操作 命令 使集群处于安全模式 bin/hdfs dfsadmin -safemode enter 生成DataNode列表 bin/hdfs dfsadmin -report 重新投放或停用DataNode(s) bin/hdfs dfsadmin -refreshNodes 10.3 浏览器接口一个典型的HDFS安装会配置一个Web服务器，通过一个可配置的TCP端口公开HDFS命名空间。这允许用户使用Web浏览器浏览HDFS命名空间并查看其文件的内容。 备注:Hadoop版本: 2.7.3 原文:http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 1.x Secondary NameNode]]></title>
    <url>%2F2017%2F12%2F20%2Fhadoop-mapReduce1.x-secondary-nameNode%2F</url>
    <content type="text"><![CDATA[Secondary NameNode是Hadoop中命名不当的其中一个组件。不当命名很容易造成歧义，通过Secondary NameNode这个名字，我们很容易理解为是一个备份NameNode，但实际上它不是。很多Hadoop的初学者对Secondary NameNode究竟做了什么以及为什么存在于HDFS中感到困惑。因此，在这篇博文中，我试图解释HDFS中Secondary NameNode的作用。 通过它的名字，你可能会认为它和NameNode有关，它确实是NameNode相关。所以在我们深入研究Secondary NameNode之前，让我们看看NameNode究竟做了什么。 1. NameNodeNameNode保存HDFS的元数据，如名称空间信息，块信息等。使用时，所有这些信息都存储在主存储器中。 但是这些信息也存储在磁盘中用于持久性存储。 上图显示了NameNode如何将信息存储在磁盘中。上图中两个不同的文件是： fsimage - 它是NameNode启动时文件系统元数据的快照 编辑日志 - 它是在NameNode启动之后对文件系统进行更改的序列 只有在重新启动NameNode时，编辑日志才会合并到fsimage以获取文件系统元数据的最新快照。但是在线上集群上，NameNode重启并不是很常见，这就意味对于NameNode长时间运行的集群来说编辑日志可能会变得非常大(可能会无限增长)。在这种情况下我们会遇到以下问题： 编辑日志变得非常大，对于管理来说是一个挑战 NameNode重启需要很长时间，因为很多更改需要合并(译者注:需要恢复编辑日志中的各项操作，导致NameNode重启会比较慢) 在崩溃的情况下，我们将丢失大量的元数据，因为fsimage是比较旧的(译者注:生成最新fsimage之后的各项操作都保存在编辑日志中，而不是fsimage，还未合并) 所以为了解决这个问题，我们需要一个机制来帮助我们减少编辑日志的大小，并且得到一个最新的fsimage，这样NameNode上的负载就会降低一些。这与Windows恢复点非常相似，它可以让我们获得操作系统的快照，以便在出现问题时退回到上一个恢复点。 现在我们理解了NameNode的功能，以及保持最新元数据的挑战。所以这一切都与Seconadary NameNode有关？ 2. Seconadary NameNode通过Secondary NameNode实现编辑日志与fsimage的合来解决上述问题。 上图显示了Secondary NameNode的工作原理： 它定期从NameNode获取编辑日志，并与fsimage合并成新的fsimage 一旦生成新的fsimage，就会复制回NameNode NameNode下次重新启动时将使用这个fsimage进行重新启动，从而减少启动时间 Secondary NameNode的整个目的就是在HDFS中提供一个检查点。它只是NameNode的一个帮助节点。这也是它在社区内被认为是检查点节点的原因。 所以我们现在明白所有的Secondary NameNode都会在文件系统中设置一个检查点，这将有助于NameNode更好地运行。它不可以替换NameNode或也不是NameNode的备份。所以从现在开始习惯把它叫做检查点节点。 原文:http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 图解HDFS工作原理]]></title>
    <url>%2F2017%2F12%2F19%2Fhadoop-illustrate-how-hdfs-works%2F</url>
    <content type="text"><![CDATA[结合Maneesh Varshney的漫画改编，为大家分析HDFS存储机制与运行原理。 1. HDFS角色如下图所示，HDFS存储相关角色与功能如下： Client：客户端，系统使用者，调用HDFS API操作文件；与NameNode交互获取文件元数据；与DataNode交互进行数据读写。 Namenode：元数据节点，是系统唯一的管理者。负责元数据的管理；与client交互进行提供元数据查询；分配数据存储节点等。 Datanode：数据存储节点，负责数据块的存储与冗余备份；执行数据块的读写操作等。 2. HDFS写文件2.1 发送写数据请求 HDFS中的存储单元是block。文件通常被分成64或128M一块的数据块进行存储。与普通文件系统不同的是，在HDFS中，如果一个文件大小小于一个数据块的大小，它是不需要占用整个数据块的存储空间的。 2.2 文件切分 2.3 DataNode分配 2.4 数据写入 2.5 完成写入 2.6 角色定位 2.7 写操作分析通过写过程，我们可以了解到： HDFS属于Master与Slave结构。一个集群中只有一个NameNode，可以有多个DataNode； HDFS存储机制保存了多个副本，当写入1T文件时，我们需要3T的存储，3T的网络流量带宽；系统提供容错机制，副本丢失或宕机可自动恢复，保证系统高可用性。 HDFS默认会将文件分割成block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，会导致内存的负担很重。 3. HDFS读文件3.1 用户需求 HDFS采用的是一次写入多次读取的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。 3.2 联系元数据节点 3.3 下载数据 前文提到在写数据过程中，数据存储已经按照客户端与DataNode节点之间的距离进行了排序，距客户端越近的DataNode节点被放在最前面，客户端会优先从本地读取该数据块。 3.4 思考 4. HDFS容错机制一4.1 三类故障4.1.1 节点失败 4.1.2 网络故障 4.1.3 数据损坏(脏数据) 4.2 故障监测机制4.2.1 节点失败监测机制 4.2.2 通信故障监测机制 4.2.3 数据错误监测机制 4.3 心跳信息与数据块报告 HDFS存储理念是以最少的钱买最烂的机器并实现最安全、难度高的分布式文件系统（高容错性低成本），从上可以看出，HDFS认为机器故障是种常态，所以在设计时充分考虑到单个机器故障，单个磁盘故障，单个文件丢失等情况。 5. HDFS容错机制二5.1 写容错 5.2 读容错 6. HDFS容错机制三6.1 数据节点(DN)失效 7. 备份规则 7.1 机架与数据节点 7.2 副本放置策略 数据块的第一个副本优先放在写入数据块的客户端所在的节点上，但是如果这个客户端上的数据节点空间不足或者是当前负载过重，则应该从该数据节点所在的机架中选择一个合适的数据节点作为本地节点。 如果客户端上没有一个数据节点的话，则从整个集群中随机选择一个合适的数据节点作为此时这个数据块的本地节点。 HDFS的存放策略是将一个副本存放在本地机架节点上，另外两个副本放在不同机架的不同节点上。 这样集群可在完全失去某一机架的情况下还能存活。同时，这种策略减少了机架间的数据传输，提高了写操作的效率，因为数据块只存放在两个不同的机架上，减少了读取数据时需要的网络传输总带宽。这样在一定程度上兼顾了数据安全和网络传输的开销。 来源于: 京东大数据专家公众号]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 2.x 工作原理]]></title>
    <url>%2F2017%2F12%2F14%2Fhadoop-mapReduce2.x-working-principle%2F</url>
    <content type="text"><![CDATA[1. 概述对于节点数超出4000的大型集群，MapReduce 1的系统开始面领着扩展性的瓶颈。在2010年雅虎的一个团队开始设计下一代MapReduce.由此，YARN(Yet Another Resource Negotiator的缩写或者为YARN Application Resource Neforiator的缩写)应运而生。 YARN 将 Jobtracker 的职能划分为多个独立的实体，从而改善了’经典的’MapReduce面临的扩展瓶颈问题。Jobtracker负责作业调度和任务进度监视、追踪任务、重启失败或过慢的任务和进行任务登记，例如维护计数器总数。 YARN将这两种角色划分为两个独立的守护进程：管理集群上资源使用的资源管理器(ResourceManager)和管理集群上运行任务生命周期的应用管理器(ApplicationMaster)。基本思路是：应用服务器与资源管理器协商集群的计算资源：容器(每个容器都有特定的内存上限)，在这些容器上运行特定应用程序的进程。容器由集群节点上运行的节点管理器(NodeManager)监视，以确保应用程序使用的资源不会超过分配给它的资源。 与jobtracker不同，应用的每个实例（这里指一个MapReduce作业）有一个专用的应用master(ApplicationMaster)，它运行在应用的运行期间。这种方式实际上和最初的Google的MapReduce论文里介绍的方法很相似，该论文描述了master进程如何协调在一组worker上运行的map任务和reduce任务。 如前所述，YARN比MapReduce更具一般性，实际上MapReduce只是YARN应用的一种形式。有很多其他的YARN应用(例如能够在集群中的一组节点上运行脚本的分布式shell)以及其他正在开发的程序。YARN设计的精妙之处在于不同的YARN应用可以在同一个集群上共存。例如，一个MapReduce应用可以同时作为MPI应用运行，这大大提高了可管理性和集群的利用率。 此外，用户甚至有可能在同一个YARN集群上运行多个不同版本的MapReduce，这使得MapReduce升级过程更容易管理。注意，MapReduce的某些部分(比如作业历史服务器和shuffle处理器)以及YARN本身仍然需要在整个集群上升级。 YARN上的MapReduce比经典的MapReduce包括更多的实体： 提交MapReduce作业的客户端 YARN资源管理器(ResourceManager)，负责协调集群上计算资源的分配 YARN节点管理器(NodeManager)，负责启动和监视集群中机器上的计算容器(container) MapReduce应用程序master(ApplicationMaster)，负责协调运行MapReduce作业的任务。它和MapReduce任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理 分布式文件系统（一般为HDFS），用来与其他实体见共享作业文件 作业运行过程如下图所示: 2. 作业提交MapReduce 2 中的作业提交是使用与MapReduce 1相同的用户API(参见上图步骤1)。MapReduce 2实现了ClientProtocol，当mapreduce.framework.name设置为yarn时启动。提交的过程与经典的非常相似。从资源管理器ResourceManager(而不是jobtracker)获取新的作业ID，在YARN命名法中它是一个应用程序ID(参见上图步骤2)。作业客户端检查作业的输出说明，计算输入分片(虽然有选项yarn.app.mapreduce.am.compute-splits-in-cluster在集群上来产生分片，这可以使具有多个分片的作业从中受益)并将作业资源(包括作业JAR、配置和分片信息)复制到HDFS(参见上图步骤3）。最后，通过调用资源管理器上的submitApplication()方法提交作业(参见上图步骤4)。 3. 作业初始化资源管理器收到调用它的submitApplication()消息后，便将请求传递给调度器(scheduler)。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动应用程序的master进程(ApplicationMaster)(参见上图步骤5a和5b)。 MapReduce作业的ApplicationMaster是一个Java应用程序,它的主类是MRAppMaster。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告(参见上图步骤6)。接下来，它接受来自共享文件系统的在客户端计算的输入分片(参见上图步骤7)。对每一个分片创建一个map任务对象以及由mapreduce.job.reduces属性确定的多个reduce任务对象。 接下来，ApplicationMaster决定如何运行构成MapReduce作业的各个任务。如果作业很小，就选择在与它同一个JVM上运行任务。 相对于在一个节点上顺序运行它们，判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这不同于MapReduce 1，MapReduce 1从不在单个tasktracker上运行小作业。这样的作业称为uberized，或者作为uber任务运行。 哪些任务是小任务呢？ 默认情况下，小任务就是小于10个mapper且只有1个reducer且输入大小小于一个HDFS块的任务。(通过设置mapreduce.job.ubertask.maxmaps、mapreduce.job.ubertask.maxreduces和mapreduce.job.ubertask.maxbytes可以改变一个作业的上述值。)将mapreduce.job.ubertask.enable设置为false也可以完全使uber任务不可用。 在任何任务运行之前，作业的setup方法为了设置作业的OutputCommitter被调用来建立作业的输出目录。在MapReduce 1中，它在一个由tasktracker运行的特殊任务中被调用，而在YARN执行框架中，该方法由应用程序master直接调用。 4. 任务分配如果作业不适合作为uber任务运行，那么ApplicationMaster就会为该作业中的所有map任务和reduce任务向资源管理器请求容器(参见上图步骤8)。附着心跳信息的请求包括每个map任务的数据本地化信息，特别是输入分片所在的主机和相应机架信息。调度器使用这些信息来做调度策略(像jobtracker的调度器一样)。理想情况下，它将任务分配到数据本地化的节点，但如果不可能这样做，调度器就会相对于非本地化的分配有限使用机架本地化的分配。 请求也为任务指定了内存需求。在默认情况下，map任务和reduce任务都分配到1024MB的内存，但这可以通过mapreduce.map.memory.mb和mapreduce.reduce.memory.mb来设置。 内存的分配方式不同于MapReduce 1，后者中tasktrackers有在集群配置时设置的固定数量的槽，每个任务在一个槽上运行。槽有最大内存分配限制，这对集群是固定的，导致当任务使用较少内存时无法充分利用内存(因为其他等待的任务不能使用这些未使用的内存)以及由于任务不能获取足够内存而导致作业失败。 在YARN中，资源划分更细的粒度，所以可以避免上述问题。具体而言，应用程序可以请求最小到最大限制范围内的任意最小值倍数的内存容量。默认的内存分配容量是调度器特定的，对于容量调度器，它的默认值最小值是1024MB(由 yarn.sheduler.capacity.minimum-allocation-mb设置)，默认的最大值是10240MB(由yarn.sheduler.capacity.maximum-allocation-mb设置)。因此，任务可以通过适当设置mapreduce.map.memory.mb和 mapreduce.reduce.memory.mb来请求1GB到10GB间的任务1GB倍数的内存容量(调度器在需要的时候使用最接近的倍数)。 5. 任务执行一旦资源管理器的调度器为任务分配了容器，ApplicationMaster就通过与节点管理器通信来启动容器(参见上图步骤9a和9b)。该任务由主类YarnChild的Java应用程序执行，在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、JAR文件和所有来自分布式缓存的文件(参见上图步骤10)。最后，运行map任务或reduce任务(参见上图步骤11)。 6. 进度和状态更新在YARN下运行时，任务每三秒钟通过umbilical接口向ApplicationMaster汇报进度和状态(包含计数器)，作为作业的汇聚视图(aggregate view)。相比之下，MapReduce 1通过tasktracker到jobtracker来实现进度更新。 客户端每秒钟(通过mapreduce.client.progressmonitor.pollinterval设置)查询一次ApplicationMaster以接收进度更新，通常都会向用户显示。 7. 作业完成除了向ApplicationMaster查询进度外，客户端每5秒钟通过调用Job的waitForCompletion()来检查作业是否完成。查询的间隔可以通过mapreduce.client.completion.pollinterval属性进行设置。 作业完成后，ApplicationMaster和任务容器清理其工作状态，OutputCommitter的作业清理方法会被调用。作业历史服务器保存作业的信息供用户需要时查询。 来源于: Hadoop 权威指南]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce 1.x 工作原理]]></title>
    <url>%2F2017%2F12%2F14%2Fhadoop-mapReduce1.x-working-principle%2F</url>
    <content type="text"><![CDATA[下面解释一下作业在经典的MapReduce 1.0中运行的工作原理。最顶层包含4个独立的实体: 客户端，提交MapReduce作业。 JobTracker，协调作业的运行。JobTracker是一个Java应用程序，它的主类是JobTracker。 TaskTracker，运行作业划分后的任务。TaskTracker是一个Java应用程序，它的主类是TaskTracker。 分布式文件系统(一般为HDFS)，用来在其他实体间共享作业文件。 1. 作业提交Job的submit()方法创建一个内部的JobSunmmiter实例，并且调用其submitJobInternal()方法。提交作业后，waitForCompletion()每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功，就显示作业计数器。如果失败，导致作业失败的错误被记录到控制台。 JobSunmmiter所实现的作业提交过程如下: (1) 向jobtracker请求一个新的作业ID(通过调用JobTracker的getNewJobId()方法获取）。参见上图步骤2。 (2) 检查作业的输出说明。例如，如果没有指定输出目录或输出目录已经存在，作业就不提交，错误抛回给MapReduce程序。 (3) 计算作业的输入分片。如果分片无法计算，比如因为输入路径不存在，作业不提交，错误返回给MapReduce程序。 (4) 将运行作业所需要的资源(包括作业JAR文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的目录下jobtracker的文件系统中。作业JAR中副本较多(由mapred.submit.replication属性控制，默认值为10)，因此在运行作业的任务时，集群中有很多个副本可供tasktracker访问。参见上图步骤3. (5) 告知jobtracker作业准备执行(通过调用JobTracker的submitJob()方法实现)。参见上图步骤4. 2. 作业初始化当JobTracker接收到其submitJob()方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度，并对其进行初始化。初始化包括创建一个表示正在运行作业的对象，用于封装任务和记录信息，以便跟踪任务的状态和进程(参见上图步骤 5)。 为了创建任务运行列表，作业调度器首先从共享文件系统中获取JobClient已计算好的输入分片信息(参见上图步骤6)。然后为每个分片创建一个Map任务。创建的Reduce任务的数量由JobConf的mapred.reduce.task属性决定，它是用setNumReduceTasks()方法来设置的，然后调度器创建相应数量的要运行的reduce任务。任务在此时被指定ID。 除了Map任务和Reduce任务，还会创建两个任务:作业创建和作业清理。这两个任务在TaskTracker中执行，在Map任务之前运行代码来创建作业，并且在所有Reduce任务完成之后完成清理工作。配置项OutputCommitter属性能设置运行的代码。默认值是FileOutputCommitter。作业创建为作业创建输出路径和临时工作空间。作业清理清除作业运行过程中的临时目录。 3. 任务分配tasktracker运行一个简单的循环来定期发送心跳(heartbeat)给jobtracker。心跳向jobtracker表明tasktracker是否还存活，同时也充当两者之间的消息通道。作为心跳的一部分，tasktracker会指明它是否已经准备好运行新的任务，如果是，jobtracker会为它分配一个任务，并使用心跳的返回值与tasktracker进行通信(参见上图步骤7)。 在jobtracker为tasktracker选择任务之前，jobtracker必须先选定任务所在的作业。一旦选择好作业，jobtracker就可以为该作业选定一个任务。 对于Map任务和Reduce任务，tasktracker有固定数据的任务槽。例如，一个tasktracker可能可以同时运行两个Map任务和两个Reduce任务。准确数据由tasktracker核的数据和内存大小来决定。默认调度器在处理Reduce任务槽之前，会填满空闲的Map任务槽，因此，如果tasktracker至少有一个空闲的Map任务槽，jobtracker会为它选择一个Map任务，否则选择一个Reduce任务。 为了选择一个Reduce任务，jobtracker简单地从待运行的Reduce任务列表中选取下一个来执行，用不着考虑数据的本地化。然而，对于一个Map任务，jobtracker会考虑tasktracker的网络设置，并选取一个距离其输入分片最近的tasktracker。在最理想的情况下， 任务是数据本地化的(data-local)，也就是任务运行在输入分片所在的节点上。同样，任务也可能是机架本地化的(rack-local)：任务和输入分片在所同一机架，但不在同一节点上。一些任务即不是数据本地化的，也不是机架本地化的，而是从与它们自身运行的不同机架上检索数据。可以通过查看作业的计数器得知每类任务的比例。 4. 任务执行现在，tasktracker已经被分配了一个任务，下一步是运行该任务。第一步，通过从共享文件系统把作业的JAR文件复制到tasktracker所在的文件系统，从而实现作业的JAR文件本地化。同时，tasktracker将应用程序所需要的全部文件从分布式缓存复制到本地磁盘(参见上图步骤8)。第二步，tasktracker为任务新建一个本地工作目录，并把JAR文件中的内容解压到这个文件夹下。第三步，tasktracker新建一个TaskRunner实例来运行该任务。 TaskRunner启动一个新的JVM(参见上图步骤9)来运行每个任务(参见上图步骤10)，以便用户定义的map和reduce函数的任务软件问题都不会影响到tasktracker(例如导致崩溃或挂起等）。但在不同的任务之前重用JVM还是可能的。 5. 进度和状态的更新MapReduce作业是长时间运行的批处理作业，运行时间范围从数分钟到数小时。这是一个很长的时间段，所以对于用户而言，能够的制作也进展是很重要的。一个作业和它的每个任务都有一个状态(status)，包括:作业或任务的状态(比如，运行状态，成功完成，失败状态)，map和reduce的进度，作业计数器的值，状态消息或描述。 6. 作业完成当jobtracker收到作业最后一个任务已完成的通知后(这是一个特定的作业清理任务)，便把作业的状态设置为’成功’。然后，在Job查询状态时，便知道任务以成功完成，于是Job打印一条消息告知用户，然后从waitForCompletion()方法返回。Job的统计信息和计数值也在这是输出到控制台。 最后，jobtracker清空作业的工作状态，只是tasktracker也清空作业的工作状态(如删除中间输出)。 来源于: Hadoop 权威指南]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 内部表与外部表]]></title>
    <url>%2F2017%2F12%2F08%2Fhive-managed-table-external-table%2F</url>
    <content type="text"><![CDATA[托管表(内部表)和外部表是Hive中的两种不同类型的表，在这篇文章中，我们将讨论Hive中表的类型以及它们之间的差异以及如何创建这些表以及何时将这些表用于特定的数据集。 1. 内部表托管表(Managed TABLE)也称为内部表(Internal TABLE)。这是Hive中的默认表。当我们在Hive中创建一个表，没有指定为外部表时，默认情况下我们创建的是一个内部表。如果我们创建一个内部表，那么表将在HDFS中的特定位置创建。默认情况下，表数据将在HDFS的/usr/hive/warehouse目录中创建。如果我们删除了一个内部表，那么这个表的表数据和元数据都将从HDFS中删除。 1.1 创建表我们可以用下面的语句在Hive里面创建一个内部表：CREATE TABLE IF NOT EXISTS tb_station_coordinate( station string, lon string, lat string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;,&apos;; 我们已经成功创建了表并使用如下命令检查表的详细信息：hive&gt; describe formatted tb_station_coordinate;OK# col_name data_type comment station string lon string lat string # Detailed Table Information Database: default Owner: xiaosi CreateTime: Tue Dec 12 17:42:09 CST 2017 LastAccessTime: UNKNOWN Retention: 0 Location: hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE &#123;\&quot;BASIC_STATS\&quot;:\&quot;true\&quot;&#125; numFiles 0 numRows 0 rawDataSize 0 totalSize 0 transient_lastDdlTime 1513071729 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim , serialization.format , Time taken: 0.16 seconds, Fetched: 33 row(s) 从上面我们可以看到表的类型Table Type为MANAGED_TABLE，即我们创建了一个托管表(内部表)。 1.2 导入数据我们使用如下命令将一个样本数据集导入到表中：hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;Loading data to table default.tb_station_coordinateOKTime taken: 2.418 seconds 如果我们在HDFS的目录/user/hive/warehouse/tb_station_coordinate查看，我们可以得到表中的内容：xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinateFound 1 items-rwxr-xr-x 1 xiaosi supergroup 374 2017-12-12 17:50 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txtxiaosi@yoona:~$xiaosi@yoona:~$xiaosi@yoona:~$ hadoop fs -text /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt桂林北站,110.302159,25.329024杭州东站,120.213116,30.290998山海关站,119.767555,40.000793武昌站,114.317576,30.528401北京南站,116.378875,39.865052... &gt;/home/xiaosi/station_coordinate.txt是本地文件系统路径。从上面的输出我们可以看到数据是从本地的这个路径复制到HDFS上的/user/hive/warehouse/tb_station_coordinate/目录下。为什么会自动复制到HDFS这个目录下呢？这个是由Hive的配置文件设置的。在Hive的${HIVE_HOME}/conf/hive-site.xml配置文件中指定，hive.metastore.warehouse.dir属性指向的就是Hive表数据存放的路径(在这配置的是/user/hive/warehouse/)。Hive每创建一个表都会在hive.metastore.warehouse.dir指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面/user/hive/warehouse/tb_station_coordinate。 1.3 删除表现在让我们使用如下命令删除上面创建的表:hive&gt; drop table tb_station_coordinate;Moved: &apos;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&apos; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/CurrentOKTime taken: 1.327 seconds 从上面的输出我们可以得知，原来属于tb_station_coordinate表的数据被移到hdfs://localhost:9000/user/xiaosi/.Trash/Current文件夹中(如果你的Hadoop没有采用回收站机制，那么删除操作将会把属于该表的所有数据全部删除)(回收站机制请参阅:Hadoop Trash回收站使用指南)。 如果我们在HDFS的目录/user/hive/warehouse/tb_station_coordinate查看：xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinatels: `/user/hive/warehouse/tb_station_coordinate&apos;: No such file or directory 你可以看到输出为No such file or directory，因为表及其内容都从HDFS从删除了。 2. 外部表当数据在Hive之外使用时，创建外部表(EXTERNAL TABLE)来在外部使用。无论何时我们想要删除表的元数据，并且想保留表中的数据，我们使用外部表。外部表只删除表的schema。 2.1 外部普通表我们使用如下命令创建一个外部表：CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate( station string, lon string, lat string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;,&apos;; 我们现在已经成功创建了外部表。我们使用如下命令检查关于表的细节：hive&gt; describe formatted tb_station_coordinate;OK# col_name data_type comment station string lon string lat string # Detailed Table Information Database: default Owner: xiaosi CreateTime: Tue Dec 12 18:16:13 CST 2017 LastAccessTime: UNKNOWN Retention: 0 Location: hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate Table Type: EXTERNAL_TABLE Table Parameters: COLUMN_STATS_ACCURATE &#123;\&quot;BASIC_STATS\&quot;:\&quot;true\&quot;&#125; EXTERNAL TRUE numFiles 0 numRows 0 rawDataSize 0 totalSize 0 transient_lastDdlTime 1513073773 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim , serialization.format , Time taken: 0.132 seconds, Fetched: 34 row(s) 从上面我们可以看到表的类型Table Type为EXTERNAL_TABLE，即我们创建了一个外部表。 2.2 导入数据我们使用如下命令将一个样本数据集导入到表中：hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;Loading data to table default.tb_station_coordinateOKTime taken: 2.418 seconds 如果我们在HDFS的目录/user/hive/warehouse/tb_station_coordinate查看，我们可以得到表中的内容：xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinateFound 1 items-rwxr-xr-x 1 xiaosi supergroup 374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txtxiaosi@yoona:~$xiaosi@yoona:~$xiaosi@yoona:~$ hadoop fs -text /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt桂林北站,110.302159,25.329024杭州东站,120.213116,30.290998山海关站,119.767555,40.000793武昌站,114.317576,30.528401... 2.3 删除表现在让我们使用如下命令删除上面创建的表:hive&gt; drop table tb_station_coordinate;OKTime taken: 0.174 secondshive&gt; 我们的Hadoop已经开启了回收站机制，但是删除操作并没有将数据进行删除，不像删除内部表一样，输出Moved: &#39;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&#39; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current(回收站机制请参阅:Hadoop Trash回收站使用指南)。为了验证我们真的没有删除数据，我们在HDFS目录下查看数据:xiaosi@yoona:~$ hadoop fs -ls /user/hive/warehouse/tb_station_coordinateFound 1 items-rwxr-xr-x 1 xiaosi supergroup 374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txtxiaosi@yoona:~$xiaosi@yoona:~$ hadoop fs -text /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt桂林北站,110.302159,25.329024杭州东站,120.213116,30.290998山海关站,119.767555,40.000793武昌站,114.317576,30.528401北京南站,116.378875,39.865052... 你可以看到表中的数据仍然在HDFS中。所以我们得知如果我们创建一个外部表，在删除表之后，只有与表相关的元数据被删除，而不会删除表的内容。 2.4 创建表指定外部目录只有当你的数据在/user/hive/warehouse目录中时，上述方法才能有效。但是，如果你的数据在另一个位置，如果你删除该表，数据也将被删除。所以在这种情况下，你需要在创建表时设置数据的外部位置，如下所示：CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate( station string, lon string, lat string)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;,&apos;LOCATION &apos;/user/xiaosi/test/coordinate/&apos;; 备注:你也可以通过在创建表时设置数据存储位置来创建一个内部表。但是，如果删除表，数据将被删除。 如果你想要创建外部表，需要在创建表的时候加上 EXTERNAL 关键字，同时指定外部表存放数据的路径(例如2.4所示)，也可以不指定外部表的存放路径(例如2.3所示)，这样Hive将在HDFS上的/user/hive/warehouse/目录下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里。 3. 使用场景3.1 内部表 数据是临时的 希望使用Hive来管理表和数据的生命周期 删除后不想要数据 3.2 外部表 这些数据也在Hive之外使用。 Hive不管理数据和权限设置以及目录等，需要你有另一个程序或过程来做这些事情 不是基于现有表(AS SELECT)来创建的表 可以创建表并使用相同的模式并指向数据的位置 参考:https://acadgild.com/blog/managed-and-external-tables-in-hive/ https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce2.0 架构详解]]></title>
    <url>%2F2017%2F12%2F08%2Fhadoop-mapReduce2.0-architecture-detail%2F</url>
    <content type="text"><![CDATA[1. MapReduce 2.0 概述Apache Hadoop 0.23比以前的版本有了很大的改进。以下是MapReduce的一些亮点；请注意，HDFS也有一些主要的改进，这些都不在本文的讨论范围之内。 MapReduce 2.0(又名MRv2或YARN)。新的架构将JobTracker的两个主要功能 - 资源管理和作业生命周期管理 - 分解成单独的组件： 管理计算资源到应用程序的全局分配的ResourceManager(RM)。 每个应用程序的ApplicationMaster(AM)，用于管理应用程序的生命周期。 在Hadoop 0.23中，MapReduce应用程序是由MapReduce ApplicationMaster执行的MapReduce传统意义上的单一作业。 每个节点上还有一个NodeManager(NM)，用于管理该节点上的用户进程。RM和NM构成集群的计算框架。该设计还允许在NM中运行长时间的辅助服务(The design also allows plugging long-running auxiliary services to the NM)；这些都是特定于应用程序的服务，作为配置的一部分进行指定，并在启动期间由NM加载。对于YARN上的MapReduce应用，shuffle是由NM加载的典型的辅助服务。请注意，在Hadoop 0.23版本之前，shuffle是TaskTracker的一部分。 每个应用程序的ApplicationMaster是一个特定框架库，负责与ResourceManager协商资源，并与NodeManager一起来执行和监视这些任务。在YARN设计中，MapReduce只是一个应用程序框架， 该设计也可以允许使用其他框架来构建和部署分布式应用程序。例如，Hadoop 0.23附带了一个分布式Shell应用程序，允许在YARN集群上的多个节点上运行一个shell脚本。 2. MapReduce 2.0 设计下图显示了一个YARN集群。只有一个资源管理器，它有两个主要的服务： 可插拔的调度器，用于管理和实施集群中的资源调度策略。请注意，在编写本博文时，Hadoop 0.23中支持两个调度器，默认的FIFO调度器和Capacity调度器; Fair调度器尚未支持(译者注:博文2012编写，现在已经支持)。 Applications Manager(AsM)，负责管理集群中运行的Application Masters，例如，负责启动Application Masters，以及在发生故障时在不同节点上监视和重新启动Application Masters。 上图还显示了在集群上的每个节点上都运行一个NM服务。该图还显示了有两个AM(AM1和AM2)。对于给定的任意YARN集群中，有多少个应用程序(作业)，就运行多少个Application Masters。每个AM管理应用程序的各个任务(启动，监视，以及在发生故障时重新启动任务)。该图还显示了AM1管理三个任务(容器1.1,1.2和1.3)，AM2管理四个任务(容器2.1,2.2,2.3和2.4)。每个任务运行在每个节点的Container中。在AM联系对应的NM来启动应用程序的各个任务之前，从RM的调度器中获取这些容器。这些容器可以大致与以前的Hadoop版本中的Map/Reduce插槽进行比较。然而，从集群利用角度来看，Hadoop-0.23中的资源分配模型更加优化。 3. MapReduce 2.0 资源分配模型在较早的Hadoop版本中，集群中的每个节点都静态分配运行预定义数量的Map插槽和预定义数量的Reduce插槽的资源容量。插槽无法在Map和Reduce之间共享。这种静态分配槽的方式并不是最佳选择，因为在MR作业生命周期中槽的需求是不同的(典型地，当作业开始时对Map槽有需求，相反，对Reduce槽的需求是在最后)。实际上，在一个真正的集群中，作业是随机提交的，每个集群都有自己的Map/Reduce槽需求，对集群充分利用不是不可能，是非常难。 Hadoop 0.23中的资源分配模型通过提供更灵活的资源建模来解决此类缺陷。以容器的形式请求资源，其中每个容器具有许多非静态属性。在写本博客时(2012年)，唯一支持的属性是内存(RAM)。然而，该模型是通用的，并且有意在将来的版本中添加更多的属性(例如CPU和网络带宽)。在这个新的资源管理模型中，每个属性只定义了最小值和最大值，并且AM可以请求具有这些最小值的倍数的属性值的容器。 4. MapReduce 2.0 主要组件我们将详细介绍MapReduce架构的主要组件，以了解这些组件的功能以及它们如何交互的。 2.1 Client – Resource Manager下展示了在YARN集群上运行应用程序的初始步骤。通常，客户端与RM(特别是与RM的Applications Manager组件)通信来开启此步骤。图中标记为(1)的第一步是让客户端告诉Applications Manager我们提交应用程序的意愿，这是通过创建应用程序请求(New Application Request)完成的。标记为(2)的RM响应通常包含一个新生成的唯一应用程序ID，以及有关客户端在请求资源以运行应用程序AM时所需要的集群资源容量的信息。 使用从RM接收到的响应信息，客户端可以构建并提交标记为(3)的应用程序提交上下文(Application Submission Context)，除了RM所需要来启动AM的信息之外，通常还包含诸如调度器队列，优先级和用户信息之类的信息。这些信息包含在容器启动上下文(Container Launch Context)中，还包含应用程序的jar，作业文件，安全令牌和任何需要的资源等。 在提交申请之后，客户端可以向RM查询应用程序报告，并接收返回的报告，并且如果需要，客户端也可以要求RM终止该应用程序。这三个步骤在下图中展示: 2.2 Resource Manager – Application Master当RM从客户端接收到应用程序提交上下文时，会找到一个满足运行AM资源需求的可用容器，并与该容器的NM联系，以便在该节点上启动AM进程。下图描述了AM和RM(特别是与RM的调度器)之间的通信步骤。图中标记为(1)的第一步是AM将自己注册到RM中。这一步由一个握手过程组成，同时还传递了AM将要监听的RPC端口，监视应用程序状态和进度的跟踪URL等信息。 标记为(2)的RM注册响应为AM传递一些基本信息，比如集群的最小和最大资源容量。AM将使用这些信息为各个任务的任何资源请求来计算和请求资源。标记为(3)的从AM到RM的资源分配请求主要包含所请求的容器列表，并且还可能包含该AM所释放的容器列表。心跳和进度信息也可以通过资源分配请求进行传达，如箭头(4)所示。 当RM的调度器接收到资源分配请求时，它基于调度策略计算满足该请求的容器列表，并且返回分配响应，标记为(5)，其中包含分配的资源列表。使用资源列表，AM开始联系相关联的NM(很快就会看到)，最后，如箭头(6)所示，当作业完成时，AM向RM发送应用完成的消息并退出。 2.3 Application Master – Container Manager下图描述了AM和Node Manager之间的通信。AM为每个容器请求NM来启动它，如图中箭头(1)所示。在容器运行时，AM可以分别请求并接收容器状态报告，如步骤(2)和(3)所示。 基于以上讨论，编写YARN应用程序的开发人员应主要关注以下接口： ClientRMProtocol：Client RM(图3)。这是客户端与RM进行通信以启动新的应用程序(即AM)，检查应用程序状态或终止应用程序的协议。 AMRMProtocol：AM RM(图4)。这是AM用来向RM注册或取消注册，以及从RM调度器请求资源来运行任务的协议。 ContainerManager：AM NM(图5)。这是AM用来与NM进行通信以启动或停止容器以及获取容器状态更新的协议。 原文:http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce新一代架构MRv2]]></title>
    <url>%2F2017%2F12%2F08%2Fhadoop-mapReduce-new-generation-architecture-mrv2%2F</url>
    <content type="text"><![CDATA[MapReduce在hadoop-0.23中经历了彻底的改变，现在我们称之为MapReduce 2.0(MRv2)或者YARN。 MRv2的基本思想是将JobTracker的两个主要功能，资源管理和作业调度/监视的功能拆分为独立的守护进程。设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager(RM)和每个应用程序特有的ApplicationMaster(AM)。每个应用程序要么是单个作业，要么是DAG作业。 1. ResourceManagerResourceManager(RM)和每个从节点以及NodeManager(NM)构成了数据计算框架。ResourceManager是系统中所有应用程序资源分配的最终决策者。 ResourceManager有两个主要组件:Scheduler(调度器) 和 ApplicationsManager。 1.1 SchedulerScheduler根据容量，队列等限制条件将资源分配给各种正在运行的应用程序。Scheduler是’纯调度器’，因为它负责监视或跟踪应用程序的状态。此外，它也不保证会重启由于应用程序错误或硬件故障原因导致失败的任务。Scheduler仅根据应用程序的资源请求来执行调度。它基于’资源容器’(Resource Container)这一抽象概念来实现的，资源容器包括如内存，cpu，磁盘，网络等。 Scheduler是一个可插拔的组件，它负责将集群资源分配给不同队列和应用程序。目前Scheduler支持诸如CapacityScheduler和FairScheduler。CapacityScheduler支持分层队列，以便更可预测地共享群集资源 1.2 ApplicationsManagerApplicationsManager(ASM)主要负责接受作业提交，协商获取第一个容器来执行应用程序的ApplicationMaster(negotiating the first container for executing the application specific ApplicationMaster)，并提供在故障时重新启动ApplicationMaster的服务。 2. NodeManagerNodeManager是每个节点上框架代理，主要负责启动应用所需要的容器，监视它们的资源使用情况(cpu，内存，磁盘，网络)，并将其报告给ResourceManager的Scheduler。 3. ApplicationMaster事实上，每一个应用程序的ApplicationMaster是一个框架库，负责与Scheduler协商合适的资源容器以及与NodeManager一起跟踪他们的状态并监视进度。 MRV2保持与以前稳定版本(hadoop-1.x)API的兼容性。这意味着所有的Map-Reduce作业仍然可以在MRv2上运行，只需重新编译即可。 原文:http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 内部原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 推测执行]]></title>
    <url>%2F2017%2F12%2F07%2Fspeculative-execution-in-hadoop-mapreduce%2F</url>
    <content type="text"><![CDATA[1. 概述Hadoop不会去诊断或修复执行慢的任务，相反，它试图检测任务的运行速度是否比预期慢，并启动另一个等效任务作为备份(备份任务称为推测任务)。这个过程在Hadoop中被称为推测执行。 在这篇文章中，我们将讨论推测执行 - Hadoop中提高效率的一个重要功能，我们有必要去了解Hadoop中的推测执行是否总是有帮助的，或者我们需要关闭它时如何禁用。 2. 什么是推测执行在Hadoop中，MapReduce将作业分解为任务，并且这些任务并行而不是顺序地运行，从而缩短了总体执行时间。这种执行模式对缓慢的任务很敏感(即使他们的数量很少)，因为它们减慢了整个工作的执行速度。 任务执行缓慢的原因可能有各种，包括硬件退化或软件错误配置等，尽管花费的时间超过了预期的时间，但是由于任务仍然有可能成功完成，因此很难检测缓慢原因。Hadoop不会尝试诊断和修复运行缓慢的任务，而是尝试检测并为其运行一个备份任务。这在Hadoop中被称为推测执行。这些备份任务在Hadoop中被称为推测任务。 3. 推测执行如何工作现在让我们看看Hadoop的推测执行过程。 首先，在Hadoop MapReduce中启动所有任务。为那些已经运行了一段时间(至少一分钟)且比作业中其他任务平均进度慢的任务启动推测任务。如果原始任务在推测性任务之前完成，那么推测任务将被终止，相反，如果推测性任务在原始任务之前完成，那么原始任务被终止。一个任务成功完成之后，任何正在运行的重复任务都将被终止。 4. 推测执行的优势Hadoop MapReduce推测执行在某些情况下是很有帮助的，因为在具有100个节点的Hadoop集群中，硬件故障或网络拥塞等问题很常见，并行或重复运行任务会更好一些，因为我们不必等到有问题的任务执行之后。 但是如果两个重复的任务同时启动，就会造成集群资源的浪费。 5. 配置推测执行推测执行是Hadoop MapReduce作业中的一种优化技术，默认情况下启用的。你可以在mapred-site.xml中禁用mappers和reducer的推测执行，如下所示：&lt;property&gt; &lt;name&gt;mapred.map.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapred.reduce.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 6. 有没有必要关闭推测执行推测执行的主要目的是减少工作执行时间，但是，由于重复的任务，集群效率受到影响。由于在推测执行中正在执行冗余任务，因此这可能降低整体吞吐量。出于这个原因，一些集群管理员喜欢关闭Hadoop中的推测执行。 对于Reduce任务，关闭推测执行是有益的，因为任意重复的reduce任务都必须将取得map任务输出作为最先的任务，这可能会大幅度的增加集群上的网络传输。 关闭推测执行的另一种情况是考虑到非幂等任务。然而在很多情况下，将任务写成幂等的并使用OutputCommitter来提升任务成功时输出到最后位置的速度，这是可行的。 原文:https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Trash回收站使用指南]]></title>
    <url>%2F2017%2F12%2F07%2Fhadoop-hdfs-trash%2F</url>
    <content type="text"><![CDATA[我们在删除一个文件时，遇到如下问题，提示我们不能删除文件放回回收站:sudo -uxiaosi hadoop fs -rm -r tmp/data_group/test/employee/employee_salary.txt17/12/06 16:34:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.17/12/06 16:34:48 WARN fs.TrashPolicyDefault: Can&apos;t create trash directory: hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employeerm: Failed to move to trash: hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt. Consider using -skipTrash option 去回收站对应目录下观察一下，得出的结论是：无法创建目录employee，因为employee文件已经存在，自然导致employee_salary.txt文件不能放回收回站:-rw-r--r-- 3 xiaosi xiaosi 352 2017-12-06 16:18 hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee 跟如下是同样的道理:xiaosi@yoona:~$ ll employee-rw-rw-r-- 1 xiaosi xiaosi 0 12月 6 16:56 employeexiaosi@yoona:~$xiaosi@yoona:~$xiaosi@yoona:~$ mkdir employeemkdir: 无法创建目录&quot;employee&quot;: 文件已存在 借此机会，详细研究了一下HDFS的Trash回收站机制。 1. 配置HDFS的回收站就像Windows操作系统中的回收站一样。它的目的是防止你无意中删除某些东西。你可以通过设置如下属性来启用此功能(默认是不开启的)：&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;description&gt;Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt;Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.&lt;/description&gt; &lt;/property&gt; 属性 说明 fs.trash.interval 分钟数，当超过这个分钟数后检查点会被删除。如果为零，回收站功能将被禁用。 fs.trash.checkpoint.interval 检查点创建的时间间隔(单位为分钟)。其值应该小于或等于fs.trash.interval。如果为零，则将该值设置为fs.trash.interval的值。 2. Trash启用回收站功能后，使用rm命令从HDFS中删除某些内容时，文件或目录不会立即被清除，它们将被移动到回收站Current目录中(/user/${username}/.Trash/current)。如果检查点已经启用，会定期使用时间戳重命名Current目录。.Trash中的文件在用户可配置的时间延迟后被永久删除。回收站中的文件和目录可以简单地通过将它们移动到.Trash目录之外的位置来恢复:sudo -uxiaosi hadoop fs -rm tmp/data_group/test/employee/employee_salary.txt17/12/06 17:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.Moved: &apos;hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt&apos; to trash at: hdfs://cluster/user/xiaosi/.Trash/Current 说明: Deletion interval表示检查点删除时间间隔(单位为分钟)。这里是fs.trash.interval的值。NameNode运行一个线程来定期从文件系统中删除过期的检查点。 Emptier interval表示在运行线程来管理检查点之前，NameNode需要等待多长时间(以分钟为单位)，即检查点创建时间间隔。NameNode删除超过fs.trash.interval的检查点，并为/user/${username}/.Trash/Current创建一个新的检查点。该频率由fs.trash.checkpoint.interval的值确定，且不得大于Deletion interval。这确保了在emptier窗口内回收站中有一个或多个检查点。 例如，可以设置如下:fs.trash.interval = 360 (deletion interval = 6 hours)fs.trash.checkpoint.interval = 60 (emptier interval = 1 hour) 这导致NameNode为Current目录下的垃圾文件每小时创建一个新的检查点，并删除已经存在超过6个小时的检查点。 在回收站生命周期结束后，NameNode从HDFS命名空间中删除该文件。删除文件会导致与文件关联的块被释放。请注意，用户删除文件的时间与HDFS中相应增加可用空间的时间之间可能存在明显的时间延迟，即用户删除文件，HDFS可用空间不会立马增加，中间有一定的延迟。 3. 检查点检查点仅仅是用户回收站下的一个目录，用于存储在创建检查点之前删除的所有文件或目录。如果你想查看回收站目录，可以在/user/${username}/.Trash/{timestamp_of_checkpoint_creation}处看到:hadoop fs -ls hdfs://cluster/user/xiaosi/.Trash/Found 3 itemsdrwx------ - xiaosi xiaosi 0 2017-12-05 08:00 hdfs://cluster/user/xiaosi/.Trash/171205200038drwx------ - xiaosi xiaosi 0 2017-12-06 01:00 hdfs://cluster/user/xiaosi/.Trash/171206080038drwx------ - xiaosi xiaosi 0 2017-12-06 08:00 hdfs://cluster/user/xiaosi/.Trash/Current 最近删除的文件被移动到回收站Current目录，并且在可配置的时间间隔内，HDFS会为在Current回收站目录下的文件创建检查点/user/${username}/.Trash/&lt;日期&gt;，并在过期时删除旧的检查点。 4. 清空回收站首先想到的是只要删除整个回收站目录，将会清空回收站。诚然，这是一个选择。但是我们有更好的选择。HDFS提供了一个命令行工具来完成这个工作：hadoop fs -expunge 该命令使NameNode永久删除回收站中比阈值更早的文件，而不是等待下一个emptier窗口。它立即从文件系统中删除过期的检查点。 5. 注意点回收站功能默认是禁用的。对于生产环境，建议启用回收站功能以避免意外的删除操作。启用回收站提供了从用户操作删除或用户意外删除中恢复数据的机会。但是为fs.trash.interval和fs.trash.checkpoint.interval设置合适的值也是非常重要的，以使垃圾回收以你期望的方式运作。例如，如果你需要经常从HDFS上传和删除文件，则可能需要将fs.trash.interval设置为较小的值，否则检查点将占用太多空间。 当启用垃圾回收并删除一些文件时，HDFS容量不会增加，因为文件并未真正删除。HDFS不会回收空间，除非文件从回收站中删除，只有在检查点过期后才会发生。 回收站功能默认只适用于使用Hadoop shell删除的文件和目录。使用其他接口(例如WebHDFS或Java API)以编程的方式删除的文件或目录不会移动到回收站，即使已启用回收站，除非程序已经实现了对回收站功能的调用。 有时你可能想要在删除文件时临时禁用回收站，也就是删除的文件或目录不用放在回收站而直接删除，在这种情况下，可以使用-skipTrash选项运行rm命令。例如：sudo -uxiaosi hadoop fs -rm -skipTrash tmp/data_group/test/employee/employee_salary.txt 这会绕过垃圾回收站并立即从文件系统中删除文件。 资料: https://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/ https://my.oschina.net/cloudcoder/blog/179381 http://debugo.com/hdfs-trash/ http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce中的InputSplit]]></title>
    <url>%2F2017%2F12%2F06%2Fhadoop-mapreduce-inputsplit%2F</url>
    <content type="text"><![CDATA[Hadoop的初学者经常会有这样两个问题： Hadoop的一个Block默认是128M(或者64M)，那么对于一条记录来说，会不会造成一条记录被分到两个Block中？ 从Block中读取数据进行切分时，会不会造成一条记录被分到两个InputSplit中？ 对于上面的两个问题，首先要明确两个概念：Block和InputSplit。在Hadoop中，文件由一个一个的记录组成，最终由mapper任务一个一个的处理。例如，示例数据集包含有关1987至2008年间美国境内已完成航班的信息。如果要下载数据集可以打开如下网址： http://stat-computing.org/dataexpo/2009/the-data.html 。每一年都会生成一个大文件（例如：2008年文件大小为108M），在每个文件中每单独的一行都代表一次航班信息。换句话说，一行代表一个记录。HDFS以固定大小的Block为基本单位存储数据，而对于MapReduce而言，其处理单位是InputSplit。 1. Block块是以block size进行划分数据。因此，如果集群的block size为128MB，则数据集的每个块将为128MB，除非最后一个块小于block size（文件大小不能被 block size 完全整除）。例如下图中文件大小为513MB，513%128=1，最后一个块e小于block size，大小为1MB。因此，块是以block size进行切割，并且块甚至可以在到逻辑记录结束之前结束(blocks can end even before a logical record ends)。 假设我们的集群中block size是128MB，每个逻辑记录大约100MB（假设为巨大的记录）。所以第一个记录将完全在一个块中，因为记录大小为100MB小于块大小128 MB。但是，第二个记录不能完全在一个块中，第二条记录将出现在两个块中，从块1开始，溢出到块2中。 2.InputSplit但是如果每个Map任务都处理特定数据块中的所有记录，那怎么处理这种跨越块边界的记录呢？如果分配一个Mapper给块1，在这种情况下，Mapper不能处理第二条记录，因为块1中没有完整的第二条记录。因为HDFS对文件块内部并不清楚，它不知道一个记录会什么时候可能溢出到另一个块(because HDFS has no conception of what’s inside the file blocks, it can’t gauge when a record might spill over into another block)。InputSplit就是解决这种跨越块边界记录问题的，Hadoop使用逻辑表示存储在文件块中的数据，称为输入拆分InputSplit。InputSplit是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等。它的划分方法完全取决于用户自己。但是需要注意的是InputSplit的多少决定了MapTask的数目，因为每个InputSplit会交由一个MapTask处理。 当MapReduce作业客户端计算InputSplit时，它会计算出块中第一个记录的开始位置和最后一个记录的结束位置。在最后一个记录不完整的情况下，InputSplit包括下一个块的位置信息和完成该记录所需的数据的字节偏移（In cases where the last record in a block is incomplete, the input split includes location information for the next block and the byte offset of the data needed to complete the record）。下图显示了数据块和InputSplit之间的关系： 块是磁盘中的数据存储的物理块，其中InputSplit不是物理数据块。它只是一个逻辑概念，并没有对实际文件进行切分，指向块中的开始和结束位置。因此，当Mapper尝试读取数据时，它清楚地知道从何处开始读取以及在哪里停止读取。InputSplit的开始位置可以在一个块中开始，在另一个块中结束。InputSplit代表了逻辑记录边界，在MapReduce执行期间，Hadoop扫描块并创建InputSplits，并且每个InputSplit将被分配给一个Mapper进行处理。 原文：http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/http://hadoopinrealworld.com/inputsplit-vs-block/ http://hadoopinrealworld.com/inputsplit-vs-block/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Reducer总是能复用为Combiner？]]></title>
    <url>%2F2017%2F12%2F06%2Fhadoop-can-reducer-always-be-reused-for-combiner%2F</url>
    <content type="text"><![CDATA[Combiner函数是一个可选的中间函数，发生在Map阶段，Mapper执行完成后立即执行。使用Combiner有如下两个优势： Combiner可以用来减少发送到Reducer的数据量，从而提高网络效率。 Combiner可以用于减少发送到Reducer的数据量，这将提高Reduce端的效率，因为每个reduce函数将处理相比于未使用Combiner之前更少的记录。 Combiner与Reducer结构相同，因为Combiner和Reducer都对Mapper的输出进行处理。这给了我们一个复用Reducer作为Combiner的好机会。但问题是，复用Reducer作为Combiner总是可行的吗？ 1. Reducer作为Combiner的适用场景假设我们正在编写一个MapReduce程序来计算股票数据集中每个股票代码的最大收盘价。Mapper将数据集中每个股票记录的股票代码作为key和收盘价作为value。Reducer然后将循环遍历股票代码对应的所有收盘价，并从收盘价列表中计算最高收盘价。假设Mapper 1处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设Mapper 2处理股票代码为ABC的2个记录，收盘价分别为100和31。那么Reducer将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有收盘价，并将计算最高收盘价为111。 我们可以在每个Mapper之后使用相同的Reducer作为Combiner。Mapper 1上的Combiner将处理3个收盘价格–50，60和111，并且仅输出111，因为它是3个收盘价的最大值。Mapper 2上的Combiner将处理2个收盘价格–100和31，并且仅输出100，因为它是2个收盘价的最大值。现在使用Combiner之后，Reducer仅处理股票代码ABC的2个收盘价(原先需要处理5个收盘价)，即来自Mapper 1的111和来自Mapper 2的100，并且将从这两个值中计算出最大收盘价格为111。 正如我们看到的，使用Combiner情况下Reducer输出与没有使用Combiner的输出结果是相同的，因此在这种情况下复用Reducer作为Combiner是没有问题。 2. Reducer作为Combiner的不适用场景假设我们正在编写一个MapReduce程序来计算股票数据集中每个股票代码的平均交易量（average volume for each symbol）。Mapper将数据集中每个股票记录的股票代码作为key和交易量（volume）作为value。Reducer然后将循环遍历股票代码对应的所有交易量，并从交易量列表中计算出平均交易量（average volume from the list of volumes for that symbol）。假设Mapper 1处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设Mapper 2处理股票代码为ABC的2个记录，收盘价分别为100和31。那么Reducer将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有交易量，并将计算出平均交易量为70.4。permalink: hadoop-setup-and-start50 + 60 + 111 + 100 + 31 / 5 = 352 / 5 = 70.4 让我们看看如果我们在每个Mapper之后复用Reducer作为Combiner会发生什么。Mapper 1上的Combiner将处理3个交易量–50，60和111，并计算出三个交易量的平均交易量为73.66。Mapper 2上的Combiner将处理2个交易量–100和31，并计算出两个交易量的平均交易量为65.5。那么在复用Reducer作为Combiner的情况下，Reducer仅处理股票代码ABC的2个平均交易量，来自Mapper1的73.66和来自Mapper2的65.5，并计算股票代码ABC最终的平均交易量为69.58。73.66 + 65.5 /2 = 69.58 这与我们不复用Reducer作为Combiner得出的结果不一样，因此复用Reducer作为Combiner得出平均交易量是不正确的。 所以我们可以看到Reducer不能总是被用于Combiner。所以，当你决定复用Reducer作为Combiner的时候，你需要问自己这样一个问题:使用Combiner与不使用Combiner的输出结果是否一样？ 3. 区别Combiner需要实现Reducer接口。Combiner只能用于特定情况。 与Reducer不同，Combiner有一个约束，Combiner输入/输出键和值类型必须与Mapper的输出键和值类型相匹配。而Reducer只是输入键和值类型与Mapper的输出键和值类型相匹配。 Combiner只能用于满足交换律（a.b = b.a）和结合律（a.(b.c)= (a.b).c）的情况。这也意味着Combiner可能只能用于键和值的一个子集或者可能不能使用。 Reducer可以从多个Mapper获取数据。Combiner只能从一个Mapper获取其输入。 原文：http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Partitioner使用教程]]></title>
    <url>%2F2017%2F12%2F05%2Fhadoop-mapreduce-partitioner-usage%2F</url>
    <content type="text"><![CDATA[partitioner在处理输入数据集时就像条件表达式(condition)一样工作。分区阶段发生在Map阶段之后，Reduce阶段之前。partitioner的个数等于reducer的个数(The number of partitioners is equal to the number of reducers)。这就意味着一个partitioner将根据reducer的个数来划分数据(That means a partitioner will divide the data according to the number of reducers)。因此，从一个单独partitioner传递过来的数据将会交由一个单独的reducer处理(the data passed from a single partitioner is processed by a single Reducer)。 1. Partitionerpartitioner对Map中间输出结果的键值对进行分区。使用用户自定义的分区条件来对数据进行分区，它的工作方式类似于hash函数。partitioner的总个数与作业的reducer任务的个数相同。下面我们以一个例子来说明partitioner是如何工作的。 2. MapReduce的Partitioner实现为了方便，假设我们有一个Employee表，数据如下。我们使用下面样例数据作为输入数据集来验证partitioner是如何工作的。 Id Name Age Gender Salary 1201 gopal 45 Male 50,000 1202 manisha 40 Female 50,000 1203 khalil 34 Male 30,000 1204 prasanth 30 Male 30,000 1205 kiran 20 Male 40,000 1206 laxmi 25 Female 35,000 1207 bhavya 20 Female 15,000 1208 reshma 19 Female 15,000 1209 kranthi 22 Male 22,000 1210 Satish 24 Male 25,000 1211 Krishna 25 Male 25,000 1212 Arshad 28 Male 20,000 1213 lavanya 18 Female 8,000 我们写一个程序来处理输入数据集，对年龄进行分组(例如：小于20，21-30，大于30)，并找到每个分组中的最高工资的员工。 2.1 输入数据以上数据存储在/home/xiaosi/tmp/partitionerExample/input/目录中的input.txt文件中，数据存储格式如下：1201 gopal 45 Male 500001202 manisha 40 Female 510001203 khaleel 34 Male 300001204 prasanth 30 Male 310001205 kiran 20 Male 400001206 laxmi 25 Female 350001207 bhavya 20 Female 150001208 reshma 19 Female 140001209 kranthi 22 Male 220001210 Satish 24 Male 250001211 Krishna 25 Male 260001212 Arshad 28 Male 200001213 lavanya 18 Female 8000 基于以上输入数据，下面是具体的算法描述。 2.2 Map任务Map任务以键值对作为输入，我们存储文本数据在text文件中。Map任务输入数据如下： 2.2.1 Inputkey以特殊key+文件名+行号的模式表示(例如，key = @input1)，value为一行中的数据(例如，value = 1201\tgopal\t45\tMale\t50000)。 2.2.2 Method读取一行中数据，使用split方法以\t进行分割，取出性别存储在变量中String[] str = value.toString().split("\t", -3);String gender = str[3]; 以性别为key，行记录数据为value作为输出键值对，从Map任务传递到Partition任务：context.write(new Text(gender), new Text(value)); 对text文件中的所有记录重复以上所有步骤。 2.2.3 Output得到性别与记录数据组成的键值对 2.3 Partition任务Partition任务接受来自Map任务的键值对作为输入。Partition意味着将数据分成几个片段。根据给定分区条件规则，基于年龄标准将输入键值对数据划分为三部分。 2.3.1 Input键值对集合中的所有数据。key为记录中性别字段值，value为该性别对应的完整记录数据。 2.3.2 Method从键值对数据中读取年龄字段值String[] str = value.toString().split("\t");int age = Integer.parseInt(str[2]); 根据如下条件校验age值：// age 小于等于20if (age &lt;= 20) &#123; return 0;&#125;// age 大于20 小于等于30else if (age &gt; 20 &amp;&amp; age &lt;= 30) &#123; return 1 % numReduceTask;&#125;// age 大于30else &#123; return 2 % numReduceTask;&#125; 2.3.3 Output键值对所有数据被分割成三个键值对集合。Reducer会处理每一个集合。 2.4 Reduce任务partitioner任务的数量等于reducer任务的数量。这里我们有三个partitioner任务，因此我们有三个reducer任务要执行。 2.4.1 InputReducer将使用不同的键值对集合执行三次。key为记录中性别字段值，value为该性别对应的完整记录数据。 2.4.2 Method读取记录数据中的Salary字段值：String[] str = value.toString().split("\t", -3);int salary = Integer.parseInt(str[4]); 获取salary最大值:if (salary &gt; max) &#123; max = salary;&#125; 对于每个key集合（Male与Female为两个key集合）中的数据重复以上步骤。执行完这三个步骤之后，我们将会分别从女性集合中得到一个最高工资，从男性集合中得到一个最高工资。context.write(new Text(key), new IntWritable(max)); 2.4.3 Output最后，我们将在不同年龄段的三个集合中获得一组键值对数据。它分别包含每个年龄段的男性集合的最高工资和每个年龄段的女性集合的最高工资。 执行Map，Partition和Reduce任务后，键值对数据的三个集合存储在三个不同的文件中作为输出。 所有这三项任务都被视为MapReduce作业。这些作业的以下要求和规范应在配置中指定： 作业名称 keys和values的输入输出格式 Map，Reduce和Partitioner任务的类 Configuration conf = getConf();//Create JobJob job = new Job(conf, "topsal");job.setJarByClass(PartitionerExample.class);// File Input and Output pathsFileInputFormat.setInputPaths(job, new Path(arg[0]));FileOutputFormat.setOutputPath(job,new Path(arg[1]));//Set Mapper class and Output format for key-value pair.job.setMapperClass(MapClass.class);job.setMapOutputKeyClass(Text.class);job.setMapOutputValueClass(Text.class);//set partitioner statementjob.setPartitionerClass(CaderPartitioner.class);//Set Reducer class and Input/Output format for key-value pair.job.setReducerClass(ReduceClass.class);//Number of Reducer tasks.job.setNumReduceTasks(3);//Input and Output format for datajob.setInputFormatClass(TextInputFormat.class);job.setOutputFormatClass(TextOutputFormat.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(Text.class); 3. Examplepackage com.sjf.open.test;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapred.JobPriority;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import com.sjf.open.utils.FileSystemUtil;public class PartitionerExample extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int status = ToolRunner.run(new PartitionerExample(), args); System.exit(status); &#125; private static class mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; try &#123; String[] str = value.toString().split("\t", -3); String gender = str[3]; context.write(new Text(gender), new Text(value)); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125; &#125; private static class reducer extends Reducer&lt;Text, Text, Text, IntWritable&gt; &#123; private int max = Integer.MIN_VALUE; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; String[] str = value.toString().split("\t", -3); int salary = Integer.parseInt(str[4]); if (salary &gt; max) &#123; max = salary; &#125; &#125; context.write(new Text(key), new IntWritable(max)); &#125; &#125; private static class partitioner extends Partitioner&lt;Text, Text&gt; &#123; @Override public int getPartition(Text key, Text value, int numReduceTask) &#123; System.out.println(key.toString() + "------" + value.toString()); String[] str = value.toString().split("\t"); int age = Integer.parseInt(str[2]); if (numReduceTask == 0) &#123; return 0; &#125; if (age &lt;= 20) &#123; return 0; &#125; else if (age &gt; 20 &amp;&amp; age &lt;= 30) &#123; return 1 % numReduceTask; &#125; else &#123; return 2 % numReduceTask; &#125; &#125; &#125; @Override public int run(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("./run &lt;input&gt; &lt;output&gt;"); System.exit(1); &#125; String inputPath = args[0]; String outputPath = args[1]; int numReduceTasks = 3; Configuration conf = this.getConf(); conf.set("mapred.job.queue.name", "test"); conf.set("mapreduce.map.memory.mb", "1024"); conf.set("mapreduce.reduce.memory.mb", "1024"); conf.setBoolean("mapred.output.compress", true); conf.setClass("mapred.output.compression.codec", GzipCodec.class, CompressionCodec.class); Job job = Job.getInstance(conf); job.setJarByClass(PartitionerExample.class); job.setPartitionerClass(partitioner.class); job.setMapperClass(mapper.class); job.setReducerClass(reducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileSystem fileSystem = FileSystem.get(conf); fileSystem.delete(new Path(outputPath), true); FileSystemUtil.filterNoExistsFile(conf, job, inputPath); FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.setNumReduceTasks(numReduceTasks); boolean success = job.waitForCompletion(true); return success ? 0 : 1; &#125;&#125; 4. 集群上执行运行结果:17/01/03 20:22:02 INFO mapreduce.Job: Running job: job_1472052053889_705919817/01/03 20:22:21 INFO mapreduce.Job: Job job_1472052053889_7059198 running in uber mode : false17/01/03 20:22:21 INFO mapreduce.Job: map 0% reduce 0%17/01/03 20:22:37 INFO mapreduce.Job: map 100% reduce 0%17/01/03 20:22:55 INFO mapreduce.Job: map 100% reduce 100%17/01/03 20:22:55 INFO mapreduce.Job: Job job_1472052053889_7059198 completed successfully17/01/03 20:22:56 INFO mapreduce.Job: Counters: 43 File System Counters FILE: Number of bytes read=470 FILE: Number of bytes written=346003 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=485 HDFS: Number of bytes written=109 HDFS: Number of read operations=12 HDFS: Number of large read operations=0 HDFS: Number of write operations=6 Job Counters Launched map tasks=1 Launched reduce tasks=3 Rack-local map tasks=1 Total time spent by all maps in occupied slots (ms)=5559 Total time spent by all reduces in occupied slots (ms)=164768 Map-Reduce Framework Map input records=13 Map output records=13 Map output bytes=426 Map output materialized bytes=470 Input split bytes=134 Combine input records=0 Combine output records=0 Reduce input groups=6 Reduce shuffle bytes=470 Reduce input records=13 Reduce output records=6 Spilled Records=26 Shuffled Maps =3 Failed Shuffles=0 Merged Map outputs=3 GC time elapsed (ms)=31 CPU time spent (ms)=2740 Physical memory (bytes) snapshot=1349193728 Virtual memory (bytes) snapshot=29673148416 Total committed heap usage (bytes)=6888620032 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=351 File Output Format Counters Bytes Written=109 原文:https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫使用Requests获取网页文本内容中文乱码]]></title>
    <url>%2F2017%2F12%2F05%2FPython%2F%5BPython%5D%20%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8Requests%E8%8E%B7%E5%8F%96%E7%BD%91%E9%A1%B5%E6%96%87%E6%9C%AC%E5%86%85%E5%AE%B9%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1. 问题使用Requests去获取网页文本内容时，输出的中文出现乱码。 2. 乱码原因爬取的网页编码与我们爬取编码方式不一致造成的。如果爬取的网页编码方式为utf8，而我们爬取后程序使用ISO-8859-1编码方式进行编码并输出，这会引起乱码。如果我们爬取后程序改用utf8编码方式，就不会造成乱码。 3. 乱码解决方案3.1 Content-Type我们首先确定爬取的网页编码方式，编码方式往往可以从HTTP头(header)的Content-Type得出。 Content-Type，内容类型，一般是指网页中存在的Content-Type，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件，这就是经常看到一些Asp网页点击的结果却是下载到的一个文件或一张图片的原因。如果未指定ContentType，默认为TEXT/HTML。charset决定了网页的编码方式，一般为gb2312、utf-8等 HTML语法格式:&lt;meta content=&quot;text/html; charset=utf-8&quot; http-equiv=&quot;Content-Type&quot;/&gt; station_request = requests.get("http://blog.csdn.net/sunnyyoona")content_type = station_request.headers['content-type']print content_type # text/html; charset=utf-8 3.2 chardet如果上述方式没有编码信息，一般可以采用chardet等第三方网页编码智能识别工具识别:pip install chardet 使用chardet可以很方便的实现文本内容的编码检测。虽然HTML页面有charset标签，但是有些时候并不准确，这时候我们可以使用chardet来进一步的判断:raw_data = urllib.urlopen('http://blog.csdn.net/sunnyyoona').read()print chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;raw_data = urllib.urlopen('http://www.jb51.net').read()print chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'GB2312'&#125; 函数返回值为字典，有2个元素，一个是检测的可信度，另外一个就是检测到的编码。 3.3 猜测编码当你收到一个响应时，Requests会猜测响应(response)的编码方式，用于在你调用Response.text方法时，对响应进行解码。Requests首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 charadet来尝试猜测编码方式。 只有当HTTP头部不存在明确指定的字符集，并且Content-Type头部字段包含text值之时，Requests才不去猜测编码方式。在这种情况下， RFC 2616指定默认字符集必须是ISO-8859-1。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置Response.encoding属性，或使用原始的Response.content。 # 一等火车站url = "https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99"headers = &#123;'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'&#125;r = requests.get(url, headers=headers)print r.headers['Content-Type'] # text/html# 猜测的编码方式print r.encoding # ISO-8859-1print r.text # 出现乱码raw_data = urllib.urlopen(url).read()print chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125; 如上所述，只有当HTTP头部不存在明确指定的字符集，并且Content-Type头部字段包含text值之时，Requests才不去猜测编码方式。直接使用ISO-8859-1编码方式。而使用chardet检测结果来看，网页编码方式与猜测的编码方式不一致，这就造成了结果输出的乱码。 3.4 解决你可以使用r.encoding = xxx来更改编码方式，这样Requests将在你调用r.text时使用r.encoding的新值，使用新的编码方式。下面示例使用chardet检测的编码方式解码网页:# 一等火车站url = "https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99"headers = &#123;'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'&#125;r = requests.get(url, headers=headers)# 检测编码raw_data = urllib.urlopen(url).read()charset = chardet.detect(raw_data) # &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;encoding = charset['encoding']# 更改编码方式r.encoding = encodingprint r.text # 未出现乱码 参考:http://docs.python-requests.org/en/latest/user/quickstart/#response-contenthttp://blog.csdn.net/a491057947/article/details/47292923https://www.cnblogs.com/GUIDAO/p/6679574.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法一 快速排序]]></title>
    <url>%2F2017%2F12%2F03%2Falgorithm-quick-sort%2F</url>
    <content type="text"><![CDATA[1. 分析 2. 伪代码 3. 思路图 4. 运行过程 5. 代码5.1 C++版本/********************************** 日期：2014-04-01* 作者：SJF0115* 题目：快速排序**********************************/#include &lt;iostream&gt;#include &lt;stdio.h&gt;using namespace std;//对子数组array[p...r]就地重排int Partition(int array[],int p,int r)&#123; int j,temp; //定义哨兵 int x = array[r]; //i为小于哨兵元素的最后一个元素下标 int i = p - 1; //j为待排序元素的第一个元素 for(j = p;j &lt; r;j++)&#123; //跟哨兵比较 if(array[j] &lt; x)&#123; i++; //交换array[i] array[j] temp = array[j]; array[j] = array[i]; array[i] = temp; &#125; &#125; //交换array[i+1](大于哨兵元素的第一个元素) array[r] temp = array[i+1]; array[i+1] = array[r]; array[r] = temp; //返回分割下标 return i + 1;&#125;//快排void QuickSort(int array[],int p,int r)&#123; if(p &gt;= r || array == NULL)&#123; return; &#125; int index = Partition(array,p,r); QuickSort(array,p,index-1); QuickSort(array,index+1,r);&#125;int main()&#123; int array[] = &#123;2,8,7,1,3,5,6,4&#125;; QuickSort(array,0,7); for(int i = 0;i &lt;= 7;i++)&#123; printf("%d\n",array[i]); &#125;&#125; 5.2 Java版本package com.sjf.open;/** 快速排序 * @author sjf0115 * @Date Created in 下午5:24 18-3-27 */public class QuickSort &#123; /** * 分割点 * @param array * @param start * @param end * @return */ int partition(int array[], int start, int end)&#123; int x = array[end]; int i = start - 1; int tmp; for(int j = start;j &lt; end;j++)&#123; if(array[j] &lt; x)&#123; i++; tmp = array[j]; array[j] = array[i]; array[i] = tmp; &#125; &#125; tmp = array[i+1]; array[i+1] = array[end]; array[end] = tmp; return i+1; &#125; /** * 快速排序 * @param array * @param start * @param end */ void quickSort(int array[], int start, int end)&#123; if(start &gt; end || array == null)&#123; return; &#125; int index = partition(array, start, end); quickSort(array, start, index-1); quickSort(array, index+1, end); &#125; public static void main(String[] args) &#123; QuickSort quickSort = new QuickSort(); int array[] = &#123;4,1,6,3,9,0&#125;; quickSort.quickSort(array, 0, 5); for(int num : array)&#123; System.out.println(num); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github搭建博客更换皮肤]]></title>
    <url>%2F2017%2F12%2F02%2FHexo%2FHexo%2BGithub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%8D%A2%E7%9A%AE%E8%82%A4%2F</url>
    <content type="text"><![CDATA[1. 更换皮肤主题地址: https://github.com/haojen/hexo-theme-Anisina 2. 添加Disqus评论系统Anisina主题支持Disqus和多说评论，要想使用这两者，需要对其进行使用配置 (1) 首先，你需要注册其中任何一个评论系统的帐号，在这里我们使用Disqus评论系统:https://disqus.com (2) 在Disqus设置页面中点 Add Disqus to your site 添加你的网站地址, 和设置Choose your unique Disqus URL, 这一栏填写的就是hexo中所使用的short_name 3. Tagsxiaosi@yoona:~/qunar/study/hexo-blog$ hexo new page &quot;Tags&quot;INFO Created: /media/xiaosi/司吉峰/study/hexo-blog/source/Tags/index.md]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github为NexT主题添加文章阅读量统计功能]]></title>
    <url>%2F2017%2F12%2F01%2FHexo%2F%5BHexo%5DHexo%2BGithub%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[在注册完成 LeanCloud 帐号并验证邮箱之后，我们就可以登录我们的 LeanCloud 帐号，进行一番配置之后拿到 AppID 以及 AppKey 这两个参数即可正常使用文章阅读量统计的功能了。 1. 创建应用(1) 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示： (2) 在出现的界面点击创建应用： (3) 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的: (4) 创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置： (5) 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建 Class。在弹出的选项中选择创建 Class 来新建 Class 用来专门保存我们博客的文章访问量等数据。 备注:点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter。 由于 LeanCloud 升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。 (6) 创建完成之后，左侧数据栏应该会多出一栏名为 Counter 的栏目，这个时候我们点击左侧的设置，切换到我们创建的应用 smartsi 应用的操作界面。 在弹出的界面中，选择左侧的 应用Key 选项，即可发现我们创建应用的 AppID 以及 AppKey，有了它，我们就有权限能够通过主题中配置好的 Javascript 代码与这个应用的 Counter表进行数据存取操作了: 复制 AppID 以及 AppKey 并在 NexT 主题的 _config.yml 文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:leancloud_visitors: enable: true app_id: 你的app_id app_key: 你的app_key 2. 后台管理当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们 LeanCloud 对应的应用的 Counter 表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的 Counter 表中： 我们可以修改其中的 time 字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。 url 字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。 title 字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。 其他字段皆为自动生成，具体作用请查阅 LeanCloud 官方文档，如果你不知道有什么作用请不要随意修改。 3. Web安全因为AppID以及AppKey是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。 选择应用的设置的安全中心选项卡,在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全: 如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出: 这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。 原文:https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github配置与主题]]></title>
    <url>%2F2017%2F12%2F01%2FHexo%2F%5BHexo%5DHexo%2BGithub%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 基本配置1.1 语言设置每个主题都会配置几种界面显示语言，修改语言只要编辑站点配置文件，找到 language 字段，并将其值更改为你所需要的语言(例如，简体中文)：language: zh-Hans 1.2 网站标题，作者打开站点配置文件，修改这些值：title: SmartSi #博客标题subtitle: #博客副标题description: #博客描述author: sjf0115 #博客作者 注意：配置文件要符合英文标点符号使用规范: 冒号后必须空格，否则会编译错误 1.3 域名与文章链接# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://sjf0115.github.io #你的博客网址root: / #博客跟目录，如果你的博客在网址的二级目录下，在这里填上permalink: :year/:month/:day/:title/ # 文章链接permalink_defaults: 2. 安装与启用主题最简单的安装方法是克隆整个仓库，在这里我们使用的是NexT主题：cd hexogit clone https://github.com/theme-next/hexo-theme-next themes/next 或者你可以看到其他详细的安装说明 安装后，我们要启用我们安装的主题，与所有Hexo主题启用的模式一样。 当克隆/下载完成后，打开站点配置文件， 找到 theme 字段，并将其值更改为 next 。theme: next 3. 主题风格NexT 主题目前提供了3中风格类似，但是又有点不同的主题风格，可以通过修改 主题配置文件 中的 Scheme 值来启用其中一种风格，例如我的博客用的是 Mist 风格，只要把另外两个用#注释掉即可:# Schemes#scheme: Musescheme: Mist#scheme: Pisces 4. 设置 RSSNexT 中 RSS 有三个设置选项，满足特定的使用场景。 更改 主题配置文件，设定 rss 字段的值： false：禁用 RSS，不在页面上显示 RSS 连接。 留空：使用 Hexo 生成的 Feed 链接。 你可以需要先安装 hexo-generator-feed 插件。 具体的链接地址：适用于已经烧制过 Feed 的情形。 5. 导航栏添加标签菜单新建标签页面，并在菜单中显示标签链接。标签页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。 (1) 在终端窗口下，定位到 Hexo 站点目录下。使用如下命令新建一名为 tags 页面:hexo new page &quot;tags&quot; (2) 编辑刚新建的页面，将页面的类型设置为 tags ，主题将自动为这个页面显示标签云。页面内容如下：title: 标签date: 2017-12-22 12:39:04type: &quot;tags&quot; (3) 在菜单中添加链接。编辑 主题配置文件 ，添加 tags 到 menu 中，如下:menu: home: / archives: /archives tags: /tags (4) 使用时在你的文章中添加如下代码：---title: title namedate: 2017-12-12-22 12:39:04tags: - first tag - second tag--- 6. 添加分类页面新建分类页面，并在菜单中显示分类链接。分类页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。 (1) 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 categories ：hexo new page categories (2) 编辑刚新建的页面，将页面的 type 设置为 categories ，主题将自动为这个页面显示分类。页面内容如下：---title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;--- (3) 在菜单中添加链接。编辑 主题配置文件 ， 添加 categories 到 menu 中，如下:menu: home: / archives: /archives categories: /categories (4) 使用时在你的文章中添加如下代码：---title: title namedate: 2017-12-12-22 12:39:04type: &quot;categories&quot;--- 7. 侧边栏社交链接侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件 中。 (1) 链接放置在 social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址 || 图标：social: GitHub: https://github.com/sjf0115 || github E-Mail: mailto:1203745031@qq.com || envelope CSDN: http://blog.csdn.net/sunnyyoona 备注:如果没有指定图标（带或不带分隔符），则会加载默认图标。 Example: (2) 设定链接的图标，对应的字段是 social_icons。其键值格式是: 匹配键: Font Awesome 图标名称， 匹配键与上一步所配置的链接的显示文本相同（大小写严格匹配），图标名称是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，你可以设置成 false 来去掉图标:# Social Iconssocial_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 微博: weibo 8. 友情链接编辑 主题配置文件 添加：友情链接配置示例# Blog rollslinks_icon: linklinks_title: Linkslinks_layout: block#links_layout: inlinelinks: CSDN: http://blog.csdn.net/sunnyyoona Example: 9. 站点建立时间这个时间将在站点的底部显示，例如 © 2017 - 2018。 编辑 主题配置文件，新增字段 since:配置示例since: 2017 10. 腾讯公益404页面腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 http://smartsi.club/404.html (1) 使用方法，新建 404.html 页面，放到主题的 source 目录下，内容如下：&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; (2) 开启404页面功能 在 menu 下添加 commonweal: /404/ || heartbeat：menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat 11. 开启打赏功能越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 主题配置文件 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能：打赏功能配置示例reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 12. 订阅微信公众号备注:此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。 在微信公众号平台下载您的二维码，并将它存放于博客source/uploads/目录下。 然后编辑 主题配置文件，如下：配置示例# Wechat Subscriberwechat_subscriber: enabled: true qcode: /uploads/wechat-qcode.jpg description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！ 13. 设置背景动画NexT 自带两种背景动画效果，编辑 主题配置文件， 搜索 canvas_nest 或 three_waves，根据你的需求设置值为 true 或者 false 即可： 备注:three_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。 canvas_nest 配置示例# canvas_nestcanvas_nest: true //开启动画canvas_nest: false //关闭动画 three_waves 配置示例# three_wavesthree_waves: true //开启动画three_waves: false //关闭动画 14. 设置阅读全文在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法：(1) 在文章中使用 &lt;!-- more --&gt; 手动进行截断，Hexo 提供的方式 推荐(2) 在文章的 front-matter 中添加 description，并提供文章摘录(3) 自动形成摘要，在 主题配置文件 中添加：auto_excerpt: enable: true length: 150 默认截取的长度为 150 字符，可以根据需要自行设定 建议使用 &lt;!-- more --&gt;（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 Hexo 中的插件更好的识别。 15. 站内搜索NexT 支持集成 Swiftype、 微搜索、Local Search 和 Algolia。在这里我使用的是Local Search，下面将介绍如何使用: (1) 添加百度/谷歌/本地 自定义站点内容搜索，安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：npm install hexo-generator-searchdb --save (2) 编辑 站点配置文件，新增以下内容到任意位置：search: path: search.xml field: post format: html limit: 10000 (3) 编辑 主题配置文件，启用本地搜索功能：# Local searchlocal_search: enable: true 其他搜索方式请查看搜索服务 16. 不蒜子统计备注：此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 (1) 全局配置。编辑 主题配置文件 中的 busuanzi_count 的配置项。当 enable: true 时，代表开启全局开关。若 site_uv 、site_pv 、 page_pv 的值均为 false 时，不蒜子仅作记录而不会在页面上显示。 (2) 站点UV配置。当 site_uv: true 时，代表在页面底部显示站点的UV值。site_uv_header 和 site_uv_footer 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为 [site_uv_header]UV值[site_uv_footer]。# 效果：本站访客数12345人次site_uv: truesite_uv_header: 本站访客数site_uv_footer: 人次 (3) 站点PV配置。当 site_pv: true 时，代表在页面底部显示站点的PV值。site_pv_header 和 site_pv_footer 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为 [site_pv_header]PV值[site_pv_footer]。# 效果：本站总访问量12345次site_pv: truesite_pv_header: 本站总访问量site_pv_footer: 次 (4) Example:busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 本站访客数 site_uv_footer: 次 # custom pv span for the whole site site_pv: true site_pv_header: 本站总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: false page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 17. 开启about自我介绍页面(1) 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 about ：cd your-hexo-sitehexo new page abhout (2) 编辑刚新建的页面，将页面的类型设置为 about。页面内容如下：---title: aboutdate: 2014-12-22 12:39:04type: &quot;about&quot;--- (3) 在菜单中添加链接。编辑 主题配置文件 ， 添加 about 到 menu 中，如下:menu: home: / || home about: /about/ || user 参考: http://theme-next.iissnan.com/theme-settings.html http://theme-next.iissnan.com/third-party-services.html]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 搭建静态博客]]></title>
    <url>%2F2017%2F12%2F01%2Fhexo_generate_blog%2F</url>
    <content type="text"><![CDATA[1.简介Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章（经常玩CSDN上的人都知道），在几秒内，即可利用靓丽的主题生成静态网页。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，在这里我是基于github搭建的个人博客站点。 2. 环境配置安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js Git 2.1 GitGit安装参考博文：http://blog.csdn.net/sunnyyoona/article/details/51453880 2.2 Node.js安装 Node.js 的最佳方式是使用 nvm:wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh 安装完成后，重启终端并执行下列命令即可安装 Node.js:nvm install 4 2.3 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。一般情况下我们机器上没有安装npm，首先要安装npm：sudo apt-get install npm 下面使用npm安装Hexo，安装过程中我们可能会遇到下面的问题: 我们需要运行下面的命令，才能安装成功： 再重新安装hexo: 3. 建站3.1 目录和文件安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。hexo init blog cd blognpm install 备注:在我这我初始化的目录名称为blog 新建完成后，指定目录blog文件如下：xiaosi@yoona:~/blog$ tree -L 2.├── _config.yml├── package.json├── scaffolds│ ├── draft.md│ ├── page.md│ └── post.md├── source│ └── _posts└── themes └── landscape5 directories, 5 files 文件 说明 scaffolds 脚手架，也就是一个工具模板 source 存放博客正文内容 _posts 文件箱 themes 存放皮肤的目录 themes/landscape 默认的皮肤 _config.yml 全局的配置文件 备注：我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。_posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。 3.2 全局配置_config.yml配置信息：（网站的配置信息，可以在此配置大部分的参数） 配置 说明 站点信息 定义标题，作者，语言 URL URL访问路径 文件目录 正文的存储目录 写博客配置 文章标题，文章类型，外部链接等 目录和标签 默认分类，分类图，标签图 归档设置 归档的类型 服务器设置 IP，访问端口，日志输出 时间和日期格式 时间显示格式，日期显示格式 分页设置 每页显示数量 评论 外挂的Disqus评论系统 插件和皮肤 换皮肤，安装插件 Markdown语言 markdown的标准 CSS的stylus格式 是否允许压缩 部署配置 github发布项目地址 配置_config.yml：# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# 站点信息title: Yoonasubtitle:description: Stay Hungry Stay Foolishauthor: sjf0115language:timezone:# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://sjf0115.club/root: /permalink: :year/:month/:day/:title/permalink_defaults:# Directory 文件目录source_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writing 写博客配置new_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Home page setting# path: Root path for your blogs index page. (default = &apos;&apos;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &apos;&apos; per_page: 10 order_by: -date# Category &amp; Tag 目录和标签default_category: uncategorizedcategory_map:tag_map:# Date / Time format 时间和日期格式## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination 分页设置## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions 插件与皮肤## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape# Deployment 部署配置## Docs: https://hexo.io/docs/deployment.htmldeploy: type:git repo:git@github.com:sjf0115/hexo-blog.git 3.3 创建新文章接下来，我们开始新博客了，创建第一博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。 下面我们创建一篇名为hexo的文章： 在_post目录下，就会生成文件：”hexo.md”:xiaosi@yoona:~/blog/source/_posts$ ll总用量 5drwxrwxrwx 1 xiaosi xiaosi 256 12月 1 10:17 ./drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 09:59 ../-rwxrwxrwx 1 xiaosi xiaosi 826 12月 1 09:59 hello-wor 然后，我们编辑文件：hexo.md，以markdown语法写文章，然后保存。在命令行，启动服务器进行保存:xiaosi@yoona:~/blog/source/posts$ hexo sNative thread-sleep not available.This will result in much slower performance, but it will still work.You should re-install spawn-sync or upgrade to the lastest version of node if possible.Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more detailsINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 通过浏览器打开， http://localhost:4000/ ，就出现了我们新写的文章。 4. 发布项目到github4.1 静态化处理写完文章之后，可以发布到github上面。hexo是一个静态博客框架。静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的：xiaosi@yoona:~/blog$ hexo generateNative thread-sleep not available.This will result in much slower performance, but it will still work.You should re-install spawn-sync or upgrade to the lastest version of node if possible.Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more detailsINFO Start processingINFO Files loaded in 143 msINFO Generated: index.htmlINFO Generated: archives/index.htmlINFO Generated: archives/2016/index.htmlINFO Generated: categories/diary/index.htmlINFO Generated: archives/2016/05/index.htmlINFO Generated: fancybox/blank.gifINFO Generated: archives/2017/12/index.htmlINFO Generated: fancybox/fancybox_loading.gifINFO Generated: fancybox/fancybox_overlay.pngINFO Generated: fancybox/fancybox_sprite@2x.pngINFO Generated: archives/2017/index.htmlINFO Generated: tags/hexo/index.htmlINFO Generated: fancybox/fancybox_sprite.pngINFO Generated: js/script.jsINFO Generated: fancybox/jquery.fancybox.cssINFO Generated: css/style.cssINFO Generated: fancybox/jquery.fancybox.pack.jsINFO Generated: fancybox/helpers/jquery.fancybox-buttons.jsINFO Generated: fancybox/helpers/jquery.fancybox-media.jsINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.cssINFO Generated: fancybox/helpers/jquery.fancybox-buttons.cssINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.jsINFO Generated: css/fonts/fontawesome-webfont.woffINFO Generated: css/fonts/fontawesome-webfont.eotINFO Generated: css/fonts/FontAwesome.otfINFO Generated: fancybox/helpers/fancybox_buttons.pngINFO Generated: fancybox/fancybox_loading@2x.gifINFO Generated: fancybox/jquery.fancybox.jsINFO Generated: 2017/12/01/hello-world/index.htmlINFO Generated: css/fonts/fontawesome-webfont.ttfINFO Generated: 2016/05/17/hexo/index.htmlINFO Generated: css/fonts/fontawesome-webfont.svgINFO Generated: css/images/banner.jpgINFO 33 files generated in 1.19 s 在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件：xiaosi@yoona:~/blog/public$ ll总用量 24drwxrwxrwx 1 xiaosi xiaosi 4096 12月 1 10:26 ./drwxrwxrwx 1 xiaosi xiaosi 4096 12月 1 10:24 ../drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 2017/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 archives/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 categories/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 css/drwxrwxrwx 1 xiaosi xiaosi 4096 12月 1 10:24 fancybox/-rwxrwxrwx 1 xiaosi xiaosi 9841 12月 1 10:24 index.html*drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 js/drwxrwxrwx 1 xiaosi xiaosi 0 12月 1 10:24 tags/ 4.2 发布到github接下来，我们把这个博客发布到github。 在github中创建一个项目hexo-blog，项目地址：https://github.com/sjf0115/hexo-blog 编辑全局配置文件：_config.yml，找到deploy的部分，设置github的项目地址：# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:sjf0115/hexo-blog.git 然后，通过如下命令进行部署: 出现上述问题，可以使用配置ssh秘钥解决。如果出现deployer找不到git: ERROR Deployer not found: git错误，使用下面方式解决： 再来一次hexo deploy： 到目前为止这个静态的web网站就被部署到了github，检查一下分支是gh-pages。gh-pages是github为了web项目特别设置的分支: 然后，点击”Settings”，找到GitHub Pages，提示“Your site is published at http://sjf0115.github.io/hexo-blog: 打开网页，就是我们刚刚发布站点： 可以看到网页样式出现问题，不用担心，我们设置域名之后就OK了。 4.3 设置域名 在dnspod控制台，设置主机记录@，类型A，到IP 23.235.37.133（github地址）: 对域名判断是否生效，对域名执行ping： 在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名sjf0115.club。通过浏览器，访问http://sjf0115.club ， 就打开了我们建好的博客站点:]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop SSH免密码登录]]></title>
    <url>%2F2016%2F12%2F29%2Fhadoop-ssh-password-free-login%2F</url>
    <content type="text"><![CDATA[1. 创建ssh-key这里我们采用rsa方式，使用如下命令：xiaosi@xiaosi:~$ ssh-keygen -t rsa -f ~/.ssh/id_rsaGenerating public/private rsa key pair.Created directory &apos;/home/xiaosi/.ssh&apos;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/xiaosi/.ssh/id_rsa.Your public key has been saved in /home/xiaosi/.ssh/id_rsa.pub.The key fingerprint is:SHA256:n/sFaAT94A/xxxxxxxxxxxxxxxxxxxxxxx xiaosi@xiaosiThe key&apos;s randomart image is:+---[xxxxx]----+| o= .. .. || o.= .. .|| *.* o .|| +.4.=E+..|| .SBo=. h+ || ogo..oo. || or +j..|| ...+o=.|| ... o=+|+----[xxxxx]-----+ 备注：这里会提示输入pass phrase，不需要输入任何字符，回车即可。 2. 生成authorized_keys文件xiaosi@xiaosi:~$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 记得要把authorized_keys文件放到.ssh目录下，与rsa等文件放在一起，否则免登录失败，debug如下（ssh -vvv localhost进行调试，查找错误原因）：xiaosi@xiaosi:~$ ssh -vvv localhostOpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips 1 Mar 2016debug1: Reading configuration data /etc/ssh/ssh_configdebug1: /etc/ssh/ssh_config line 19: Applying options for *debug2: resolving &quot;localhost&quot; port 22debug2: ssh_connect_direct: needpriv 0debug1: Connecting to localhost [127.0.0.1] port 22.debug1: Connection established.debug1: identity file /home/xiaosi/.ssh/id_rsa type 1...debug2: we sent a publickey packet, wait for replydebug3: receive packet: type 51debug1: Authentications that can continue: publickey,passworddebug1: Trying private key: /home/xiaosi/.ssh/id_dsadebug3: no such identity: /home/xiaosi/.ssh/id_dsa: No such file or directorydebug1: Trying private key: /home/xiaosi/.ssh/id_ecdsadebug3: no such identity: /home/xiaosi/.ssh/id_ecdsa: No such file or directorydebug1: Trying private key: /home/xiaosi/.ssh/id_ed25519debug3: no such identity: /home/xiaosi/.ssh/id_ed25519: No such file or directorydebug2: we did not send a packet, disable methoddebug3: authmethod_lookup passworddebug3: remaining preferred: ,passworddebug3: authmethod_is_enabled passworddebug1: Next authentication method: passwordxiaosi@localhost&apos;s password: 3. 验证xiaosi@xiaosi:~$ ssh localhostThe authenticity of host &apos;localhost (127.0.0.1)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:378enl3ckhdpObP8fnsHr1EXz4d1q2Jde+jUplkub/Y.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;localhost&apos; (ECDSA) to the list of known hosts.sign_and_send_pubkey: signing failed: agent refused operationxiaosi@localhost&apos;s password: 4. authorized_keys权限我们可以看到还是让我输入密码，很大可能是authorized_keys文件权限的问题，我们给该文件赋予一定权限：xiaosi@xiaosi:~$ chmod 600 ~/.ssh/authorized_keys 再次验证：xiaosi@xiaosi:~$ ssh localhostWelcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-24-generic x86_64) * Documentation: https://help.ubuntu.com/0 个可升级软件包。0 个安全更新。Last login: Thu Jun 16 08:05:50 2016 from 127.0.0.1 到此表示OK了。 备注：第一次需要输入密码，以后再次登陆就不需要输入密码了。 有更明白的小伙伴可以指导一下。。。。。。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 安装与启动]]></title>
    <url>%2F2016%2F12%2F29%2Fhadoop-setup-and-start%2F</url>
    <content type="text"><![CDATA[1. SSH参考博文：[Hadoop]SSH免密码登录以及失败解决方案（http://blog.csdn.net/sunnyyoona/article/details/51689041#t1） 2. 下载(1) 直接从官网上下载 http://hadoop.apache.org/releases.html (2) 使用命令行下载：xiaosi@yoona:~$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz--2016-06-16 08:40:07-- http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz正在解析主机 mirrors.hust.edu.cn (mirrors.hust.edu.cn)... 202.114.18.160正在连接 mirrors.hust.edu.cn (mirrors.hust.edu.cn)|202.114.18.160|:80... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度： 196015975 (187M) [application/octet-stream]正在保存至: “hadoop-2.6.4.tar.gz” 3. 解压缩Hadoop包解压位于根目录/文件夹下的hadoop-2.7.3.tar.gz到~/opt文件夹下xiaosi@yoona:~$ tar -zxvf hadoop-2.7.3.tar.gz -C opt/ 4. 配置配置文件都位于安装目录下的/etc/hadoop文件夹下：xiaosi@yoona:~/opt/hadoop-2.7.3/etc/hadoop$ lscapacity-scheduler.xml hadoop-env.sh httpfs-log4j.properties log4j.properties mapred-site.xml.templateconfiguration.xsl hadoop-metrics2.properties httpfs-signature.secret log4j.properties slavescontainer-executor.cfg hadoop-metrics.properties httpfs-site.xml mapred-env.cmd ssl-client.xml.examplecore-site.xml hadoop-policy.xml kms-acls.xml mapred-env.sh ssl-server.xml.examplecore-site.xml hdfs-site.xml kms-env.sh mapred-queues.xml.template yarn-env.cmdhadoop-env.cmd hdfs-site.xml kms-log4j.properties mapred-site.xml yarn-env.shhadoop-env.sh httpfs-env.sh kms-site.xml mapred-site.xml yarn-site.xml Hadoop的各个组件均可利用XML文件进行配置。core-site.xml文件用于配置Common组件的属性，hdfs-site.xml文件用于配置HDFS属性，而mapred-site.xml文件则用于配置MapReduce属性。 备注：Hadoop早期版本采用一个配置文件hadoop-site.xml来配置Common，HDFS和MapReduce组件。从0.20.0版本开始该文件以分为三，各对应一个组件。 4.1 配置core-site.xmlcore-site.xml 配置如下：&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/$&#123;user.name&#125;/tmp/hadoop&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.xiaosi.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.xiaosi.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 4.2 配置hdfs-site.xmlhdfs-site.xml配置如下：&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/xiaosi/tmp/hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/xiaosi/tmp/hadoop/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.3 配置 mapred-site.xmlmapred-site.xml配置如下：&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行Hadoop的时候可能会找不到jdk，需要我们修改hadoop.env.sh脚本文件，唯一需要修改的环境变量就是JAVE_HOME，其他选项都是可选的：export JAVA_HOME=/home/xiaosi/opt/jdk-1.8.0 5. 运行5.1 初始化HDFS系统在配置完成后，运行hadoop前，要初始化HDFS系统，在bin/目录下执行如下命令：xiaosi@yoona:~/opt/hadoop-2.7.3$ ./bin/hdfs namenode -format 5.2 启动开启NameNode和DataNode守护进程：xiaosi@yoona:~/opt/hadoop-2.7.3$ ./sbin/start-dfs.shStarting namenodes on [localhost]localhost: starting namenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-namenode-yoona.outlocalhost: starting datanode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-datanode-yoona.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-secondarynamenode-yoona.out 通过jps命令查看namenode和datanode是否已经启动起来：xiaosi@yoona:~/opt/hadoop-2.7.3$ jps13400 SecondaryNameNode13035 NameNode13197 DataNode13535 Jps 从启动日志我们可以知道，日志信息存储在hadoop-2.7.3/logs/目录下，如果启动过程中有任何问题，可以通过查看日志来确认问题原因。 6. Yarn模式安装6.1 配置修改yarn-site.xml，添加如下配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce.shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml，做如下修改：&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 本地模式下是value值是local，yarn模式下value值是yarn。 6.2 启动yarn启动yarn：xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-resourcemanager-yoona.outlocalhost: starting nodemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-nodemanager-yoona.out 关闭yarn:xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/stop-yarn.shstopping yarn daemonsstopping resourcemanagerlocalhost: stopping nodemanager 6.3 检查是否运行成功打开浏览器，输入：http://localhost:8088/cluster]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hadoop 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch2.x Java API之索引文档]]></title>
    <url>%2F2016%2F07%2F07%2Felasticsearch-java-api-index-doc%2F</url>
    <content type="text"><![CDATA[Index API 允许我们存储一个JSON格式的文档，使数据可以被搜索。文档通过index、type、id唯一确定。我们可以自己提供一个id，或者也使用Index API 为我们自动生成一个。 这里有几种不同的方式来产生JSON格式的文档(document)： 手动方式，使用原生的byte[]或者String 使用Map方式，会自动转换成与之等价的JSON 使用第三方库来序列化beans，如Jackson 使用内置的帮助类 XContentFactory.jsonBuilder() 1. 手动方式// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(json);indexRequestBuilder.setTTL(8000);// 执行IndexResponse indexResponse = indexRequestBuilder.get(); 测试，下面代码存储梅西信息到索引为football-index，类型为football-type，id为1的文档中：String index = "football-index";String type = "football-type";String id = "1";String json = "&#123;" + "\"club\":\"巴萨罗那\"," + "\"country\":\"阿根廷\"," + "\"name\":\"梅西\"" + "&#125;"; 2. Map方式// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(map);indexRequestBuilder.setTTL(8000);// 执行IndexResponse indexResponse = indexRequestBuilder.get(); 测试，下面代码存储穆勒信息到索引为football-index，类型为football-type，id为2的文档中：String index = "football-index";String type = "football-type";String id = "2";Map&lt;String, Object&gt; map = Maps.newHashMap();map.put("name", "穆勒");map.put("club", "拜仁慕尼黑俱乐部");map.put("country", "德国"); 3. 序列化方式// Bean转换为字节ObjectMapper mapper = new ObjectMapper();byte[] json;try &#123; json = mapper.writeValueAsBytes(bean);&#125; catch (JsonProcessingException e) &#123; logger.error("---------- json 转换失败 Bean:&#123;&#125;", bean.toString()); return false;&#125;// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(json);indexRequestBuilder.setTTL(8000);// 执行IndexResponse response = indexRequestBuilder.get(); 测试，下面代码存储卡卡信息到索引为football-index，类型为football-type，id为3的文档中：String index = "football-index";String type = "football-type";String id = "3";FootballPlayer footballPlayer = new FootballPlayer();footballPlayer.setName("卡卡");footballPlayer.setClub("奥兰多城俱乐部");footballPlayer.setCountry("巴西"); 4. XContentBuilder帮助类方式ElasticSearch提供了一个内置的帮助类XContentBuilder来产生JSON文档// IndexIndexRequestBuilder indexRequestBuilder = client.prepareIndex();indexRequestBuilder.setIndex(index);indexRequestBuilder.setType(type);indexRequestBuilder.setId(id);indexRequestBuilder.setSource(xContentBuilder);indexRequestBuilder.setTTL(8000);// 执行IndexResponse response = indexRequestBuilder.get(); 测试，下面代码存储托雷斯信息到索引为football-index，类型为football-type，id为4的文档中：String index = "football-index";String type = "football-type";String id = "4";XContentBuilder xContentBuilder;try &#123; xContentBuilder = XContentFactory.jsonBuilder(); xContentBuilder .startObject() .field("name", "托雷斯") .field("club", "马德里竞技俱乐部") .field("country", "西班牙") .endObject();&#125; catch (IOException e) &#123; logger.error("----------indexDocByXContentBuilder create xContentBuilder failed", e); return;&#125; 备注:你还可以通过startArray(string)和endArray()方法添加数组。.field()方法可以接受多种对象类型。你可以给它传递数字、日期、甚至其他XContentBuilder对象。 ElasticSearch版本:2.x 参考：https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.x/java-docs-index.html#java-docs-index-generate]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch安装与启动]]></title>
    <url>%2F2016%2F06%2F23%2Felasticsearch-setup-and-run%2F</url>
    <content type="text"><![CDATA[1. 检查JDK版本使用如下命令检验JDK版本：xiaosi@Qunar:~$ java -versionjava version &quot;1.7.0_40&quot;Java(TM) SE Runtime Environment (build 1.7.0_40-b43)Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)xiaosi@Qunar:~$ 如果你的JDK版本为1.7，有可能会遇到如下问题：xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearchException in thread &quot;main&quot; java.lang.RuntimeException: Java version: Oracle Corporation 1.7.0_40 [Java HotSpot(TM) 64-Bit Server VM 24.0-b56] suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JAVA_OPTS environment variable.Upgrading is preferred, this workaround will result in degraded performance. at org.elasticsearch.bootstrap.JVMCheck.check(JVMCheck.java:123) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:268) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)Refer to the log for complete error details. 主要原因是Elasticsearch至少需要Java 8. 2. 安装ElasticSearch2.1 下载检查JDK版本之后，我们可以下载并运行Elasticsearch。 二进制文件可以从 www.elastic.co/downloads 获取，过去版本也可以从中获取。 对于每个版本，您可以选择zip或tar存档，或DEB或RPM软件包。 为了简单起见，我们使用tar文件。 我们下载Elasticsearch 2.3.3 为例：xiaosi@Qunar:~$ curl -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.3/elasticsearch-2.3.3.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 26.2M 100 26.2M 0 0 190k 0 0:02:21 0:02:21 --:--:-- 211k 具体的下载版本可以查看：https://www.elastic.co/downloads/elasticsearch 2.2 解压xiaosi@Qunar:~$ tar -zxvf elasticsearch-2.3.3.tar.gz -C /home/xiaosi/optelasticsearch-2.3.3/README.textileelasticsearch-2.3.3/LICENSE.txtelasticsearch-2.3.3/NOTICE.txtelasticsearch-2.3.3/modules/elasticsearch-2.3.3/modules/lang-groovy/elasticsearch-2.3.3/modules/reindex/elasticsearch-2.3.3/modules/lang-expression/elasticsearch-2.3.3/modules/lang-groovy/plugin-security.policy 解压完之后如下：xiaosi@Qunar:~/opt$ cd elasticsearch-2.3.3/xiaosi@Qunar:~/opt/elasticsearch-2.3.3$ lsbin config lib LICENSE.txt modules NOTICE.txt README.textile 2.3 启动2.3.1 启动帮助xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearch -helpNAME start - Start ElasticsearchSYNOPSIS elasticsearch startDESCRIPTION This command starts Elasticsearch. You can configure it to run in the foreground, write a pid file and configure arbitrary options that override file-based configuration.OPTIONS -h,--help Shows this message -p,--pidfile &lt;pidfile&gt; Creates a pid file in the specified path on start -d,--daemonize Starts Elasticsearch in the background -Dproperty=value Configures an Elasticsearch specific property, like -Dnetwork.host=127.0.0.1 --property=value Configures an elasticsearch specific property, like --network.host 127.0.0.1 --property value NOTE: The -d, -p, and -D arguments must appear before any --property arguments.xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ 2.3.2 启动xiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearch[2016-06-23 22:06:36,034][INFO ][node ] [Venom] version[2.3.3], pid[21245], build[218bdf1/2016-05-17T15:40:04Z][2016-06-23 22:06:36,035][INFO ][node ] [Venom] initializing ...[2016-06-23 22:06:36,475][INFO ][plugins ] [Venom] modules [reindex, lang-expression, lang-groovy], plugins [], sites [][2016-06-23 22:06:36,493][INFO ][env ] [Venom] using [1] data paths, mounts [[/ (/dev/sda7)]], net usable_space [9.6gb], net total_space [41.5gb], spins? [no], types [ext4][2016-06-23 22:06:36,493][INFO ][env ] [Venom] heap size [990.7mb], compressed ordinary object pointers [true][2016-06-23 22:06:38,041][INFO ][node ] [Venom] initialized[2016-06-23 22:06:38,041][INFO ][node ] [Venom] starting ...[2016-06-23 22:06:38,097][INFO ][transport ] [Venom] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[2016-06-23 22:06:38,102][INFO ][discovery ] [Venom] elasticsearch/CCQnbPSBQQmVK3c8f4CbHg[2016-06-23 22:06:41,167][INFO ][cluster.service ] [Venom] new_master &#123;Venom&#125;&#123;CCQnbPSBQQmVK3c8f4CbHg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: zen-disco-join(elected_as_master, [0] joins received)[2016-06-23 22:06:41,190][INFO ][http ] [Venom] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2016-06-23 22:06:41,190][INFO ][node ] [Venom] started[2016-06-23 22:06:41,232][INFO ][gateway ] [Venom] recovered [0] indices into cluster_state 2.3.3 验证在浏览器中输入：http://localhost:9200/ （elasticsearch默认端口号为9200）&#123;name: &quot;Ulysses&quot;,cluster_name: &quot;elasticsearch&quot;,version: &#123;number: &quot;2.3.3&quot;,build_hash: &quot;218bdf10790eef486ff2c41a3df5cfa32dadcfde&quot;,build_timestamp: &quot;2016-05-17T15:40:04Z&quot;,build_snapshot: false,lucene_version: &quot;5.5.0&quot;&#125;,tagline: &quot;You Know, for Search&quot;&#125; health状况：xiaosi@Qunar:~$ curl &apos;localhost:9200/_cat/health?v&apos;epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1466691011 22:10:11 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0%xiaosi@Qunar:~$ 备注：（status）绿色表示一切是好的(集群功能齐全)黄色意味着所有数据是可用的，但是一些副本尚未分配(集群功能齐全)红色意味着一些数据不可用即使一个集群是红色的,它仍然是部分功能(即它将继续搜索请求从服务可用的碎片)但是你可能需要尽快修复它,因为你有缺失的数据。 2.3.5 说明刚开始安装在/opt目录下，普通用户是没有权限的，在启动elasticsearch时会告诉你权限不够：xiaosi@Qunar:~$ cd /opt/elasticsearch-2.3.3/xiaosi@Qunar:/opt/elasticsearch-2.3.3$ cd bin/xiaosi@Qunar:/opt/elasticsearch-2.3.3/bin$ ./elasticsearchlog4j:ERROR setFile(null,true) call failed.java.io.FileNotFoundException: /opt/elasticsearch-2.3.3/logs/elasticsearch.log (权限不够) at java.io.FileOutputStream.open0(Native Method) at java.io.FileOutputStream.open(FileOutputStream.java:270) 权限不够，那就用root用户去启动elasticsearch，会告诉你不能使用root用户来启动elasticsearch：xiaosi@Qunar:/opt/elasticsearch-2.3.3/bin$ sudo ./elasticsearchException in thread &quot;main&quot; java.lang.RuntimeException: don&apos;t run elasticsearch as root. at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:93) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270) at org.elasticsearch.bootstrap. 所以只好安装在自己的目录下。对此不知道有什么解决之道？ 3. 安装Kibana3.1 下载地址https://www.elastic.co/downloads/kibana 备注：Kibana 4.5.x requires Elasticsearch 2.3.x 3.2 下载xiaosi@Qunar:~/opt$ curl -L -O https://download.elastic.co/kibana/kibana/kibana-4.5.1-linux-x64.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 31.5M 100 31.5M 0 0 136k 0 0:03:56 0:03:56 --:--:-- 214k 3.3 解压tar -zxvf kibana-4.5.1-linux-x64.tar.gz 4. 安装MarvelMarvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了一个叫做Sense的交互式控制台，使用户方便的通过浏览器直接与Elasticsearch进行交互。Elasticsearch线上文档中的很多示例代码都附带一个View in Sense的链接。点击进去，就会在Sense控制台打开相应的实例。安装Marvel不是必须的。 4.1 失败方法 （已丢弃）Marvel是一个插件，可在Elasticsearch目录中运行以下命令来下载和安装：./bin/plugin -i elasticsearch/marvel/latest 但是在运行时遇到如下问题，没有找到 -i 命令：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ ./bin/plugin -i elasticsearch/marvel/latestERROR: unknown command [-i]. Use [-h] option to list available commands 提示可以使用-h 参数去查看可以使用的命令：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ ./bin/plugin -hNAME plugin - Manages pluginsSYNOPSIS plugin &lt;command&gt;DESCRIPTION Manage pluginsCOMMANDS install Install a plugin remove Remove a plugin list List installed pluginsNOTES [*] For usage help on specific commands please type &quot;plugin &lt;command&gt; -h&quot; 我们可以看到我们可以使用install命令来代替-i参数命令：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install elasticsearch/marvel/latest-&gt; Installing elasticsearch/marvel/latest...Trying https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip ...Trying https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip ...Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip ...Trying https://github.com/elasticsearch/marvel/archive/latest.zip ...Trying https://github.com/elasticsearch/marvel/archive/master.zip ...ERROR: failed to download out of all possible locations..., use --verbose to get detailed information 按照提示使用–verbose，查看报错原因：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install elasticsearch/marvel/latest --verbose-&gt; Installing elasticsearch/marvel/latest...Trying https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip ...Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip];Trying https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip ...Failed: FileNotFoundException[https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip]; nested: FileNotFoundException[https://search.maven.org/remotecontent?filepath=elasticsearch/marvel/latest/marvel-latest.zip];Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip ...Failed: FileNotFoundException[https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip]; nested: FileNotFoundException[https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/marvel/latest/marvel-latest.zip];Trying https://github.com/elasticsearch/marvel/archive/latest.zip ...Failed: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/latest.zip]; nested: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/latest.zip];Trying https://github.com/elasticsearch/marvel/archive/master.zip ...Failed: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/master.zip]; nested: FileNotFoundException[https://github.com/elasticsearch/marvel/archive/master.zip];ERROR: failed to download out of all possible locations..., use --verbose to get detailed information 4.2 正确方法上面方法已经不在适用了（不知道还有没有解决方法？），现在marvel用Kibana管理，所以第二步先安装Kibana。安装完进行如下操作：xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install license-&gt; Installing license...Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.3.3/license-2.3.3.zip ...Downloading .......DONEVerifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.3.3/license-2.3.3.zip checksums if available ...Downloading .DONEInstalled license into /opt/elasticsearch-2.3.3/plugins/license xiaosi@Qunar:/opt/elasticsearch-2.3.3$ sudo ./bin/plugin install marvel-agent-&gt; Installing marvel-agent...Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.3.3/marvel-agent-2.3.3.zip ...Downloading ..........DONEVerifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.3.3/marvel-agent-2.3.3.zip checksums if available ...Downloading .DONE@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.lang.RuntimePermission setFactory* javax.net.ssl.SSLPermission setHostnameVerifierSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]yInstalled marvel-agent into /opt/elasticsearch-2.3.3/plugins/marvel-agent 使用Kibana安装最新版本的marvel：xiaosi@Qunar:~/opt/kibana-4.5.1-linux-x64/bin$ ./kibana plugin --install elasticsearch/marvel/latestInstalling marvelAttempting to transfer from https://download.elastic.co/elasticsearch/marvel/marvel-latest.tar.gzTransferring 1597693 bytes....................Transfer completeExtracting plugin archiveExtraction completeOptimizing and caching browser bundles...Plugin installation complete 5. 安装总结 步骤 命令 Step 1: 安装Marvel into Elasticsearch: bin/plugin install license和bin/plugin install marvel-agent Step 2: 安装Marvel into Kibana bin/kibana plugin –install elasticsearch/marvel/latest Step 3: 启动Elasticsearch and Kibana bin/elasticsearch和bin/kibana Step 4: 跳转到http://localhost:5601/app/marvel Step 5: 进入Getting Started Guide. 在没网的情况下运行集群? !imgoffline installation instructions. 6. 启动elasticsearch，kibana和Marvel6.1 启动elasticsearchxiaosi@Qunar:~/opt/elasticsearch-2.3.3/bin$ ./elasticsearch[2016-06-23 22:36:56,431][INFO ][node ] [Suprema] version[2.3.3], pid[21803], build[218bdf1/2016-05-17T15:40:04Z][2016-06-23 22:36:56,431][INFO ][node ] [Suprema] initializing ...[2016-06-23 22:36:56,895][INFO ][plugins ] [Suprema] modules [reindex, lang-expression, lang-groovy], plugins [], sites [][2016-06-23 22:36:56,913][INFO ][env ] [Suprema] using [1] data paths, mounts [[/ (/dev/sda7)]], net usable_space [9.6gb], net total_space [41.5gb], spins? [no], types [ext4][2016-06-23 22:36:56,913][INFO ][env ] [Suprema] heap size [990.7mb], compressed ordinary object pointers [true][2016-06-23 22:36:58,387][INFO ][node ] [Suprema] initialized[2016-06-23 22:36:58,387][INFO ][node ] [Suprema] starting ...[2016-06-23 22:36:58,464][INFO ][transport ] [Suprema] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[2016-06-23 22:36:58,468][INFO ][discovery ] [Suprema] elasticsearch/enfSRI_1RoORbk9eIqogQw[2016-06-23 22:37:01,546][INFO ][cluster.service ] [Suprema] new_master &#123;Suprema&#125;&#123;enfSRI_1RoORbk9eIqogQw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: zen-disco-join(elected_as_master, [0] joins received)[2016-06-23 22:37:01,569][INFO ][http ] [Suprema] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2016-06-23 22:37:01,572][INFO ][node ] [Suprema] started[2016-06-23 22:37:01,611][INFO ][gateway ] [Suprema] recovered [0] indices into cluster_state[2016-06-23 22:37:42,722][INFO ][cluster.metadata ] [Suprema] [.kibana] creating index, cause [api], templates [], shards [1]/[1], mappings [config][2016-06-23 22:37:43,056][INFO ][cluster.routing.allocation] [Suprema] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).[2016-06-23 22:37:45,803][INFO ][cluster.metadata ] [Suprema] [.kibana] create_mapping [index-pattern] 在浏览器中输入：http://localhost:9200/&#123; name: &quot;Suprema&quot;, cluster_name: &quot;elasticsearch&quot;, version: &#123; number: &quot;2.3.3&quot;, build_hash: &quot;218bdf10790eef486ff2c41a3df5cfa32dadcfde&quot;, build_timestamp: &quot;2016-05-17T15:40:04Z&quot;, build_snapshot: false, lucene_version: &quot;5.5.0&quot; &#125;, tagline: &quot;You Know, for Search&quot;&#125; 6.2 启动kibanaxiaosi@Qunar:~$ cd ~/opt/kibana-4.5.1-linux-x64/bin/xiaosi@Qunar:~/opt/kibana-4.5.1-linux-x64/bin$ ./kibana log [22:37:36.150] [info][status][plugin:kibana] Status changed from uninitialized to green - Ready log [22:37:36.177] [info][status][plugin:elasticsearch] Status changed from uninitialized to yellow - Waiting for Elasticsearch log [22:37:36.181] [info][status][plugin:marvel] Status changed from uninitialized to yellow - Waiting for Elasticsearch log [22:37:36.198] [info][status][plugin:kbn_vislib_vis_types] Status changed from uninitialized to green - Ready log [22:37:36.202] [info][status][plugin:markdown_vis] Status changed from uninitialized to green - Ready log [22:37:36.210] [info][status][plugin:metric_vis] Status changed from uninitialized to green - Ready log [22:37:36.218] [info][status][plugin:spyModes] Status changed from uninitialized to green - Ready log [22:37:37.455] [info][status][plugin:statusPage] Status changed from uninitialized to green - Ready log [22:37:37.462] [info][status][plugin:table_vis] Status changed from uninitialized to green - Ready log [22:37:37.468] [info][listening] Server running at http://0.0.0.0:5601 log [22:37:42.513] [info][status][plugin:elasticsearch] Status changed from yellow to yellow - No existing Kibana index found log [22:37:45.762] [info][status][plugin:elasticsearch] Status changed from yellow to green - Kibana index ready log [22:37:45.876] [info][status][plugin:marvel] Status changed from yellow to green - Marvel index ready 浏览器中输入： http://0.0.0.0:5601 6.3 启动Marvel浏览器中输入：http://localhost:5601/app/marvel 初步安装完毕，以后还需配置一些东西。。。。]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 字符串操作]]></title>
    <url>%2F2016%2F05%2F17%2FPython%2FPython%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[字符串是 Python 中最常用的数据类型。我们可以使用引号(‘或”)来创建字符串。创建字符串很简单，只要为变量分配一个值即可。例如：s = &quot;Hello World&quot;print s # Hello World 1. 大小写转换函数大小写转换函数返回原始字符串s的一个副本： 函数 说明 lower() 将所有字符转换为小写 upper() 将所有字符转换为大写 capitalize() 将第一个字符转换为大写，同时将其他所有字符转换为小写 这些函数不会影响非字母字符。大小写转换函数是规范化的一个重要元素。 Example:s = "Hello World"print s # Hello World# 转为大写us = s.upper()print us # HELLO WORLD# 转为小写ls = s.lower()print ls # hello world# 首字母大写 其余小写cs = s.capitalize()print cs # Hello world 2. 判定函数判断函数根据字符串s是否属于适当的类而返回True或False： 函数 说明 islower() 检查所有字母字符是否为小写 isupper() 检查所有字母字符是否为大写 isspace() 检查所有字符是否为空格 isdigit() 检查所有字符是否为范围0～9中的十进制数字 isalpha() 检查所有字符是否为a～z或A～Z范围内的字母字符 使用这些函数，你可以识别有效的单词、非负整数、标点符号等。 Example:# 是否为大写ius = "HELLO".isupper()print ius # True# 是否为小写ils = "hello".islower()print ils # True# 是否为空格iss = " ".isspace()print iss # True# 是否为范围0～9中的十进制数字ids = "232".isdigit()print ids # True# 是否为a～z或A～Z范围内的字母字符ias = "a2".isalpha()print ias # False 3. 解码函数Python有时会将字符串数据表示为原始的二进制数组，而非字符串，尤其是当数据来自外部源（外部文件、数据库或Web）时。Python使用符号b来标识二进制数组。例如:# 二进制数组bin = b"Hello"# 字符串 s = "Hello"print bin[0]print s[0] s[0]和bin[0]分别是’H’和72，其中72是字符’H’的ASCII码。 解码函数将二进制数组转换为字符串或反之： 函数 说明 decode() 将二进制数组转换为字符串 encode() 将字符串转换为二进制数组 许多Python函数都需要将二进制数据转换为字符串，然后再做处理。 4. 去除空白函数字符串处理的第一步是去除不需要的空白(包括换行符和制表符)。 函数 说明 lstrip() left strip 在字符串的开始处删除所有空格 rstrip() right strip 在字符串的结束处删除所有空格 strip() 对整个字符串删除所有空格(不删除字符串内部空格) 经过这些删除操作后，得到的可能会是一个空字符串！ Example:ls = " Hello world ".lstrip()print ls + "," + str(len(ls)) # Hello world ,12rs = " Hello World ".rstrip()print rs + "," + str(len(rs)) # Hello World,12ss = " Hello World ".strip()print ss + "," + str(len(ss)) # Hello World,1 5. 分割函数字符串通常包含多个标记符，用空格、冒号和逗号这样的分隔符分割。函数split(delim=’’)使用delim作为分隔符，将字符串s分割为子字符串组成的一个列表。如果未指定分隔符，Python会使用空白字符来分割字符串，并将所有连续的空白合并：ss = "Hello World".split()print ss # ['Hello', 'World']ss = "Hello,World".split(",")print ss # ['Hello', 'World'] 6. 连接函数连接函数join(ls)，将字符串列表ls连接在一起，形成一个字符串，并使用特定的对象字符串作为连接符：s = ",".join("b")print s # bs = ",".join(["a", "b", "c", "d"])print s # a,b,c,d 备注:join()函数仅在字符串之间插入连接符，而在第一个字符串前或最后一个字符串后都不插入连接符。 7. 查找函数find(needle)函数返回对象字符串中子字符串needle第一次出现的索引值(下标从0开始)，当子字符串不存在时，返回-1。该函数区分大小写。index = &quot;Hello World&quot;.find(&quot;o&quot;)print index # 4 来自于]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 数据库操作]]></title>
    <url>%2F2016%2F05%2F17%2FPython%2FPython%20%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. MySQLPython使用数据库驱动模块与MySQL通信。诸如pymysql等许多数据库驱动都是免费的。这里我们将使用pymysql，它是Anaconda的一部分。驱动程序经过激活后与数据库服务器相连，然后将Python的函数调用转换为数据库查询，反过来，将数据库结果转换为Python数据结构。 connect()函数需要以下信息：数据库(名称)、数据库服务器的位置(主机和端口号)和数据库用户(名称和密码)。如果数据库成功连接，则返回连接标识符。接下来，创建与数据库连接相关联的数据库游标：import pymysql# 连接数据库conn = pymysql.connect(host="localhost", port=3306, user="root", passwd="root", db="test")cur = conn.cursor() 备注:使用pymysql需要导入pymysql库：import pymysql。 游标的execute()函数向数据服务器提交要执行的查询命令，并返回受影响的行数(如果查询是非破坏性的，则返回零)。与命令行MySQL查询不同，pymysql查询语句不需要在结尾加上分号。# 查询query = ''' SELECT first_name, last_name FROM People ORDER BY dob LIMIT 3'''n_rows = cur.execute(query)print n_rows # 3 如果提交非破坏性查询(比如SELECT)，需要使用游标函数fetchall()获取所有匹配的记录。该函数返回一个生成器，可以将其转换为列字段的元组构成的列表：results = list(cur.fetchall())print results # [('gztAQV', 'aLhko'), ('ZXMtHd', 'cgwjI'), ('yHwDRF', 'NgBkY')] 如果查询是破坏性的(例如UPDATE、DELETE或INSERT)，则必须执行commit操作:# 修改update_query = ''' UPDATE People SET first_name = 'yoona' WHERE last_name = 'aLhko''''n_rows = cur.execute(update_query)print n_rows # 1conn.commit()select_query = ''' SELECT first_name, last_name FROM People WHERE last_name = 'aLhko''''cur.execute(select_query)results = list(cur.fetchall())print results # [('yoona', 'aLhko')] 备注:提供commit()函数的是连接本身(conn)，而不是游标(cur)。 2. MongoDB在Python中，我们用pymongo模块中MongoClient类的实例来实现MongoDB客户端。首先安装pymongo模块(ubuntu15.10):sudo pip install pymongo 下面就可以创建一个无参数的客户端(适用于典型的安装了本地服务器的情况)，也可以用服务器的主机名和端口号作为参数创建客户端，或使用服务器的统一资源标识符(URI)作为参数创建客户端：# 使用默认的初始化方式client1 = pymongo.MongoClient()# 指定主机和端口号client2 = pymongo.MongoClient("localhost", 27017)# 用URI方式指定主机和端口号client3 = pymongo.MongoClient("mongodb://localhost:27017/") 客户一旦端建立了与数据库服务器的连接，就可以选择当前激活的数据库，进而选择激活的集合。可以使用面向对象(“.”)或字典样式的符号。如果所选的数据库或集合不存在，服务器会立即创建它们：# 创建并选择活动数据库的两种方法db = client1.test_dbdb = client1["test_db"]# 创建并选择活动集合的两种方法people = db.peoplepeople = db["people"] pymongo模块用字典变量来表示MongoDB文档。表示对象的每个字典必须具有_id这个键。如果该键不存在，服务器会自动生成它。 集合对象提供用于在文档集合中插入、搜索、删除、更新、替换和聚合文档以及创建索引的功能。 函数insert_one(doc)和insert_many(docs)将文档或文档列表插入集合。它们分别返回对象InsertOneResult或InsertManyResult，这两个对象分别提供inserted_id和inserted_ids属性。当文档没有提供明确的唯一键时，就需要使用这两个属性值作为文档的唯一键。如果指定了_id键，就是用该值作为唯一键：# 插入person1 = &#123;"name": "John", "dob": "2017-11-24"&#125;person_id1 = people.insert_one(person1).inserted_idprint person_id1 # 5a1d4ba92317d71bb605f8ceperson2 = &#123;"_id": "XVT162", "name": "Jane", "dob": "2017-11-27"&#125;person_id2 = people.insert_one(person2).inserted_idprint person_id2 # XVT162persons = [&#123;"name": "Lucy", "dob": "2017-11-12"&#125;, &#123;"name": "Tom"&#125;]result = people.insert_many(persons)print result.inserted_ids # [ObjectId('5a1d4c832317d71c2c4e284f'), ObjectId('5a1d4c832317d71c2c4e2850')] 函数find_one()和find()分别给出匹配可选属性的一个或多个文档，其中find_one()返回文档，而find()返回一个游标(一个生成器)，可以使用list()函数将该游标转换为列表，或者在for循环中将其用作迭代器。如果将字典作为参数传递给这些函数中的任意一个，函数将给出与字典的所有键值相等的文档：# 查找everyone = people.find()print list(everyone) # [&#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;]print list(people.find(&#123;"dob": "2017-11-27"&#125;)) # [&#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;]one_people = people.find_one()print one_people # &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;one_people = people.find_one(&#123;"name": "Lucy"&#125;)print one_people # &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;one_people = people.find_one(&#123;"_id": "XVT162"&#125;)print one_people # &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125; 下面介绍几个实现数据聚合和排序的分组和排序函数。函数sort()对查询的结果进行排序。当以无参数的方式调用它时，该函数按键_id的升序进行排序。函数count()返回查询结果中或整个集合中的文档数量：# 聚合count = people.count()print count # 5count = people.find(&#123;"dob": "2017-11-27"&#125;).count()print count # 1people_list = list(people.find().sort("dob"))print people_list # [&#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;] 函数delete_one(doc)和delete_many(docs)从集合中删除字典doc所标识的一个或多个文档。如果要在删除所有文档的同时保留集合，需使用空字典作为参数调用函数delete_many({})：# 删除result = people.delete_many(&#123;"dob": "2017-11-27"&#125;)print result.deleted_count # 1 来自于:Python数据科学入门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 文件处理]]></title>
    <url>%2F2016%2F05%2F17%2FPython%2FPython%20%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. csv文件处理记录中的字段通常由逗号分隔，但其他分隔符也是比较常见的，例如制表符（制表符分隔值，TSV）、冒号、分号和竖直条等。建议在自己创建的文件中坚持使用逗号作为分隔符，同时保证编写的处理程序能正确处理使用其他分隔符的CSV文件。 备注:有时看起来像分隔符的字符并不是分隔符。通过将字段包含在双引号中，可确保字段中的分隔符只是作为变量值的一部分，不参与分割字段(如...,&quot;Hello, world&quot;,...)。 Python的csv模块提供了一个CSV读取器和一个CSV写入器。两个对象的第一个参数都是已打开的文本文件句柄(在下面的示例中，使用newline=’’选项打开文件，从而避免删除行的操作)。必要时可以通过可选参数delimiter和quotechar，提供默认的分隔符和引用字符。Python还提供了控制转义字符、行终止符等定界符的可选参数。with open("somefile.csv", newline='') as infile: reader = csv.reader(infile, delimiter=',', quotechar='"') CSV文件的第一条记录通常包含列标题，可能与文件的其余部分有所不同。这只是一个常见的做法，并非CSV格式本身的特性。 CSV读取器提供了一个可以在for循环中使用的迭代器接口。迭代器将下一条记录作为一个字符串字段列表返回。读取器不会将字段转换为任何数值数据类型，另外，除非传递可选参数skipinitialspace=True，否则不会删除前导的空白。 如果事先不知道CSV文件的大小，而且文件可能很大，则不宜一次性读取所有记录，而应使用增量的、迭代的、逐行的处理方式：读出一行，处理一行，再获取另一行。 CSV写入器提供writerow()和writerows()两个函数。writerow()将一个字符串或数字序列作为一条记录写入文件。该函数将数字转换成字符串，因此不必担心数值表示的问题。类似地，writerows()将字符串或数字序列的列表作为记录集写入文件。 在下面的示例中，使用csv模块从CSV文件中提取Answer.Age列。假设此列肯定存在，但列的索引未知。一旦获得数值，借助statistics模块就能得到年龄的平均值和标准偏差。 首先，打开文件并读取数据：with open("demographics.csv", newline='') as infile: data = list(csv.reader(infile)) 检查文件中的第一个记录 data[0] ，它必须包含感兴趣的列标题：ageIndex = data[0].index("Answer.Age") 最后，访问剩余记录中感兴趣的字段，并计算和显示统计数据：ages = [int(row[ageIndex]) for row in data[1:]]print(statistics.mean(ages), statistics.stdev(ages)) csv和statistics模块是底层的、快速而粗糙的工具。在第6章，你将了解如何在更为复杂的项目中使用pandas的数据frame，完成那些比对几列数据进行琐碎的检索要高端得多的任务。 2. Json文件处理需要注意的一点就是某些Python数据类型和结构(比如集合和复数)无法存储在JSON文件中。因此，要在导出到JSON之前，将它们转换为JSON可表示的数据类型。例如，将复数存储为两个double类型的数字组成的数组，将集合存储为一个由集合的各项所组成的数组。 将复杂数据存储到JSON文件中的操作称为JSON序列化，相应的反向操作则称为JSON反序列化。Python通过json模块中的函数，实现JSON序列化和反序列化。 函数 说明 dump() 将Python对象导出到文件中 dumps() 将Python对象编码成JSON字符串 load() 将文件导出为Python对象 loads() 将已编码的JSON字符串解码为Python对象 备注:把多个对象存储在一个JSON文件中是一种错误的做法，但如果已有的文件包含多个对象，则可将其以文本的方式读入，进而将文本转换为对象数组（在文本中各个对象之间添加方括号和逗号分隔符），并使用loads()将文本反序列化为对象列表。 Example: 以下代码片段实现了将任意（可序列化的）对象按先序列化、后反序列化的顺序进行处理：# 将Python对象编码成JSON字符串data = [&#123;'apple': 23, 'bear': 11, 'banana': 54&#125;]s = json.dumps(data)print type(s) # &lt;type 'str'&gt;print s # [&#123;"apple": 23, "bear": 11, "banana": 54&#125;]# 将Python对象编码成JSON字符串并格式化输出format_str = json.dumps(data, sort_keys=True, indent=4, separators=(',', ': '))print format_str'''[ &#123; "apple": 23, "banana": 54, "bear": 11 &#125;]'''# 将已编码的JSON字符串解码为Python对象data = '[&#123;"apple": 23, "bear": 11, "banana": 54&#125;]'o = json.loads(data)print type(o) # &lt;type 'list'&gt;print o[0].get('apple', 0) # 23# 将Python对象导出到文件中data = [&#123;'apple': 23, 'bear': 11, 'banana': 54&#125;]with open("/home/xiaosi/data.json", "w") as f_dump: s_dump = json.dump(data, f_dump, ensure_ascii=False)# 将文件导出为Python对象with open("/home/xiaosi/data.json", 'r') as f_load: ob = json.load(f_load)print type(ob) # &lt;type 'list'&gt;print ob[0].get('banana') # 54 备注:使用JSON函数需要导入json库：import json。 JSON 类型转换到 python 的类型对照表： JSON Python object dict array list string unicode number (int) int, long number (real) float true True false False null None 来自于:]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 列表,元组与集合]]></title>
    <url>%2F2016%2F05%2F17%2FPython%2FPython%20%E5%88%97%E8%A1%A8%2C%E5%85%83%E7%BB%84%E4%B8%8E%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[1. 选择合适的数据结构列表、元组、集合和字典是Python中最常用的复合数据结构，它们都属于容器类的数据结构。 (1) 列表 Python用数组的方式实现列表。列表的搜索时间是线性的，因此用列表来存储大量可搜索的数据是不切实际的。 (2) 元组 元组是不可变的列表，创建后就无法再更改。元组的搜索时间也是线性的。 (3) 集合 与列表和元组不同，集合不是序列：集合项不存在索引。集合最多只能存储一个项的副本，具有次线性O(log(N))的搜索时间。集合非常适合于成员查找和消除重复项。 在下面的例子中，展示了列表与集合查询速度对比，bigList是以十进制字符串表示的前1000万个整数的列表： bigList = [str(i) for i in range(10000000)]"abc" in bigList # 耗时0.2秒bigSet = set(bigList)"abc" in bigSet # 耗时15~30微秒——快了10 000倍！ (4) 字典 字典构建了从键到值的映射。任何可哈希的数据类型（数字、布尔、字符串、元组）的对象都可以作为键，且同一字典中的不同键可以属于不同的数据类型。Python对字典值的数据类型也没有限制。字典具有次线性O(log(N))搜索时间，它非常适合用于键值的查找。 你可以通过(键, 值)这样的元组列表创建字典，也可以使用内置类构造函数enumerate(seq)创建字典，这样得到的字典，其键为各项在seq中的序列号： seq = ["alpha", "bravo", "charlie", "delta"]d = dict(enumerate(seq))print d # &#123;0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'&#125; 另一种从键序列和值序列创建字典变量的巧妙方法，是使用内置类构造函数zip(kseq, vseq)(两个序列必须具有相同的长度): kseq = ["apple", "bear", "banana"]vseq = [12, 21, 3]d = dict(zip(kseq, vseq))print d # &#123;'apple': 12, 'bear': 21, 'banana': 3&#125; Python将enumerate(seq)和zip(kseq, vseq)实现为列表生成器。列表生成器提供了一个迭代器接口，这使得我们可以在for循环中使用它们。与真正的列表不同的是，列表生成器只在需要时才生成下一个元素，这可以说是一种巧妙的偷懒方式。列表生成器便于处理大型列表，甚至允许“无限”的列表。你可以通过调用list()函数，将列表生成器显式强制转换为列表。 总结： 数据结构 查询时间复杂度 说明 列表 O(N) 数组实现，不适合大数据搜索 元组 O(N) 不可变的列表，不可更改，不适合大数据搜索 集合 O(log(N)) 不是序列，不存在重复项，适合成员查找 字典 O(log(N)) 键到值的映射，键和值数据类型无限制，适合键值查找 2. 列表推导式列表推导式是一个将数据集(不一定是列表)转换为列表的表达式。通过列表推导式，可以实现对所有或某些列表元素应用相同的操作，例如将所有元素转换为大写或对其中大于0的元素进行运算等。 列表表达式的转换过程如下： (1) 表达式遍历数据集并访问集合中的每一项。 (2) 为每一项计算布尔表达式(可选，不选默认为True)。 (3) 如果布尔表达式为True，则计算当前项目的循环表达式，并将其值附加到结果列表中。 (4) 如果布尔表达式为False，则跳过该项。 Example：# 复制myList；等同于myList.copy()或者myList[:]，但二者的效率都没有列表推导式高[x for x in myList]# 提取非负项[x for x in myList if x &gt;= 0]# 用Mylist非零项的倒数构建一个新列表[1/x for x in myList if x != 0]# 从打开的infile文件中选出所有的非空行，并删除这些行开头和结尾的空格[l.strip() for l in infile if l.strip()] 如果列表推导式被包含在圆括号中，而不是在方括号中，则程序将返回一个列表生成器对象：(x**2 for x in myList) # 结果为：&lt;generator object &lt;genexpr&gt; at 0x...&gt; 3. 计数器计数器是一种字典式集合，用于给集合项目计数。计数器定义在collections模块中。你可以将要计数的集合传递给构造函数Counter，然后使用函数most_common(n)来获取n个出现频率最高的项及对应频率的列表(如果没有提供参数n，则函数返回的将是一个针对所有项目的列表)。 phrase = "a boy eat a banana"cntr = Counter(phrase.split())print cntr.most_common() # [('a', 2), ('boy', 1), ('eat', 1), ('banana', 1)] 来源于:]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Numpy 数组]]></title>
    <url>%2F2016%2F05%2F17%2FPython%2FPython%20Numpy%20%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[NumPy（Numeric Python，以numpy导入）是一系列高效的、可并行的、执行高性能数值运算的函数的接口。numpy模块提供了一种新的Python数据结构——数组(array)，以及特定于该结构的函数工具箱。该模块还支持随机数、数据聚合、线性代数和傅里叶变换等非常实用的数值计算工具。 下面将学习如何创建不同形状的numpy数组，基于不同的源创建numpy数组，数组的重排和切片操作，添加数组索引，以及对某些或所有数组元素进行算术运算、逻辑运算和聚合运算。 1. 创建数组numpy数组比原生的Python列表更为紧凑和高效，尤其是在多维的情况下。但与列表不同的是，数组的语法要求更为严格：数组必须是同构的。这意味着数组项不能混合使用不同的数据类型，而且不能对不同数据类型的数组项进行匹配操作。 创建numpy数组的方法很多。可以使用函数array()，基于类数组(array-like)数据创建数组。numpy基于数据本身推断出数组元素的类型，当然，你也可以给array()传递确定的dtype参数。numpy支持的数据类型接近二十种，例如bool_、int64、uint64、float64和&lt;U32（针对Unicode字符串）。 备注:所谓的类数组数据可以是列表、元组或另一个数组。 为获得较高的效率，numpy在创建一个数组时，不会将数据从源复制到新数组，而是建立起数据间的连接。也就是说，在默认情况下，numpy数组相当于是其底层数据的视图，而不是其副本。如果底层数据对象发生改变，则相应的数组数据也会随之改变。如果你不喜欢这种方式（这是默认的处理方式，除非复制的数据量过大），可以给构造函数传递copy=True。 备注:创建数组，不会将数据从源复制到新数组，相当于是其底层数据的视图，而不是其副本。 实际上，Python的”列表”(list)是以数组的方式实现的，而并非列表的方式，这与”列表”(list)的字面含义并不一致。由于未使用前向指针，所以Python并没有给列表预留前向指针的存储空间。Python的大型列表只比”真正的”numpy数组多使用约13%的存储空间，但对于一些简单的内置操作，比如sum()，使用列表则要比数组快五到十倍。因此在使用numpy之前，应该问问自己是否真的需要用到某些numpy特有的功能。 我们来创建第一个数组——前10个正整数组成的简单数组：import numpy as np# 简单数组numbers = np.array(range(1, 11), copy=True)print numbers # [ 1 2 3 4 5 6 7 8 9 10] 函数ones()、zeros()和empty()分别构造全1数组、全零数组和尚未初始化的数组。这些函数必须有数组的形状参数，该参数用一个与数组的维度相同的列表或元组来表征:# 给定数组形状shape与数据类型type 全1数组ones = np.ones([2, 4], dtype=np.float64)print ones'''[ [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]]'''# 给定数组形状shape与数据类型type 全0数组zeros = np.zeros([2, 4], dtype=np.float64)print zeros'''[ [ 0. 0. 0. 0.] [ 0. 0. 0. 0.]]'''# 给定数组形状shape与数据类型type 尚未初始化数组 其元素值不一定为零empty = np.empty([2, 4], dtype=np.float64)print empty'''[ [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]]''' numpy使用数组的ndim、shape和dtype属性分别存储数组的维数、形状和数据类型:# 只要没有经过变形(reshape) 该属性给出的就是数组的原始形状print ones.shape # (2, 4)# 等价于len(numbers.shape)print ones.ndim # 2# 数据类型print ones.dtype # float64 函数eye(N, M=None, k=0, dtype=np.float)用于构造一个N×M的眼形单位矩阵，其第k对角线上的值为1，其他地方的值为零。当k为正数时，对应的对角线位于主对角线上方的第k条。M为None（默认值）等价于M=N:# N×M的眼形单位矩阵eye = np.eye(3, k=1)print eye'''[ [ 0. 1. 0.] [ 0. 0. 1.] [ 0. 0. 0.]]''' 当需要将几个矩阵相乘时，可以使用单位矩阵作为乘法链累积器中的初始值。 除了经典的内置函数range()外，numpy有其独有的、更高效的生成等间隔数值数组的方式：函数arange([start,] stop [, step,], dtype=None)：# 等间隔数值数组double_numbers = np.arange(2, 5, 0.25)print double_numbers # [ 2. 2.25 2.5 2.75 3. 3.25 3.5 3.75 4. 4.25 4.5 4.75] numpy在创建数组时记录每一项的数据类型，不过该数据类型并非不可变的。可在数组创建后，调用函数astype(dtype, casting =&quot;unsafe&quot;, copy=True)来改变它。对于类型缩小的情况(将较抽象的数据类型转换为更具体的数据类型)，可能会丢失一些信息。这并非numpy特有的，任何缩小变换都可能会丢失信息:# 改变数组数据类型int_numbers = double_numbers.astype(np.int)print int_numbers # [2 2 2 2 3 3 3 3 4 4 4 4] 大多数numpy操作返回的是一个视图，而非原始数组的副本。为了保留原始数据，可使用copy()函数创建现有数组的副本。这样一来，对原始数组的任何更改都不会影响到副本。但如果数组较为庞大，比如有十亿个数组项，那就不要轻易进行复制:# 数组的副本double_numbers_copy = double_numbers.copy() 2. 转置和重排借助numpy可以很容易地改变数组的形状和方向，我们再也不用像“瞎猫踫到死耗子”那样看运气了。下面我们用几个标准普尔（S&amp;P）股票代码组成一个一维数组，然后用所有可能的方式改变它的形状： 来源于: Python数据科学入门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
