{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":1,"renderable":0},{"_id":"source/img/header.png","path":"img/header.png","modified":1,"renderable":0},{"_id":"source/img/header_16.jpg","path":"img/header_16.jpg","modified":1,"renderable":0},{"_id":"source/img/header_32.jpg","path":"img/header_32.jpg","modified":1,"renderable":0},{"_id":"source/img/post-default.jpg","path":"img/post-default.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/404.html","path":"404.html","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/alipay.png","path":"images/alipay.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/header.png","path":"images/header.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/wechatpay.png","path":"images/wechatpay.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"59de02e999399baa6e2803832337b8d493f54e5d","modified":1517218619660},{"_id":"source/robots.txt","hash":"5bd1453bce44724c8b3c5b6c5fa6af3ad1fb8268","modified":1517218619607},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1517218619607},{"_id":"themes/next/README.md","hash":"e935757f7afd24eb9414692c8c0850a14a0264c3","modified":1517218619606},{"_id":"themes/next/_config.yml","hash":"389108a22cf691dfe79eb52daf7ebf22b4c691f5","modified":1517218619606},{"_id":"themes/next/bower.json","hash":"74115626b419f40126e07bb0bbfc5e6ab163f222","modified":1517218619605},{"_id":"themes/next/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1517218619597},{"_id":"themes/next/package.json","hash":"edcba128994bb34491334db3a49d5c3a50976cf3","modified":1517218619539},{"_id":"source/about/index.md","hash":"75da5509c3eca5adc115fb2a6699e5cd7bd75358","modified":1517218619612},{"_id":"source/archives/index.md","hash":"c8d9edc353f9b86b711f3a6c6833820e6d1c8af9","modified":1517218619612},{"_id":"source/img/header.png","hash":"db8e718a1ff056adffe3688599442c42aa14149b","modified":1517218619611},{"_id":"source/img/header_16.jpg","hash":"679ec05839ea6d58371fe39d76d908bc8ad0de40","modified":1517218619609},{"_id":"source/img/header_32.jpg","hash":"5522f74309b0e2468d82a50771c379b385d6e01e","modified":1517218619609},{"_id":"source/tags/index.md","hash":"d0acb818cb29aee665997d9b80a61421c4a725a5","modified":1517218619607},{"_id":"source/img/post-default.jpg","hash":"303f34e43017037cc09a753e1b4ae035c6ee9983","modified":1517218619608},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1517218619604},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1517218619605},{"_id":"themes/next/docs/DATA-FILES.md","hash":"de63aa8466ee8c4d4b418dfbe4e8f27fa117751d","modified":1517218619604},{"_id":"themes/next/docs/INSTALLATION.md","hash":"7696f11eb52eaf793bc593d9fb55f7c8f4f33ca7","modified":1517218619603},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1517218619602},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1517218619602},{"_id":"source/categories/index.md","hash":"33a2f58955eecfd69facbd7588d8ebba3fe48cf8","modified":1517218619611},{"_id":"themes/next/layout/_layout.swig","hash":"cf8c9aaf1997005b5bf41efc78ecdc2483a226b1","modified":1517218619581},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1517218619541},{"_id":"themes/next/layout/archive.swig","hash":"833a2213d31be45a12b8e4e5e609073978bd251b","modified":1517218619542},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1517218619541},{"_id":"themes/next/layout/post.swig","hash":"8e2d079b46076996cc9343213d5bf7da8178d32d","modified":1517218619540},{"_id":"themes/next/layout/page.swig","hash":"e17757096eb878f1e9c34c567d7898a37305c302","modified":1517218619540},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1517218619539},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1517218619539},{"_id":"themes/next/languages/default.yml","hash":"b39706b2d22696eed6b036f4c90aa5362e331090","modified":1517218619596},{"_id":"themes/next/languages/en.yml","hash":"df63017548589b2e567647e08d736c2a7f342b12","modified":1517218619596},{"_id":"themes/next/languages/fr-FR.yml","hash":"6ee34c8103a95839207dac1201fef7a8f727d2fc","modified":1517218619595},{"_id":"themes/next/languages/de.yml","hash":"92964c8ed184fa959a5e2001e7c6d9a07d7a4344","modified":1517218619596},{"_id":"themes/next/languages/id.yml","hash":"60473cc81a871ceb868c97cd3291d602fda1b338","modified":1517218619594},{"_id":"themes/next/languages/it.yml","hash":"5a8a29d145dd2cd882d52b3fbb1203c1a3540cbd","modified":1517218619594},{"_id":"themes/next/languages/ja.yml","hash":"8a3acfb56dc783f261b640dca662c0ec431fea6f","modified":1517218619593},{"_id":"themes/next/languages/ko.yml","hash":"73253087d7754a0213e2ad72de16ab5138b9ba54","modified":1517218619586},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1517218619586},{"_id":"themes/next/languages/pt-BR.yml","hash":"4d017b7b9542050b87a99931dff98889090fc781","modified":1517218619585},{"_id":"themes/next/languages/pt.yml","hash":"95585897cd108d5ff15e998cd0acdc0fba5d572c","modified":1517218619585},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1517218619584},{"_id":"themes/next/languages/zh-Hans.yml","hash":"d9c17f3ccc5382ff37bb45f2da1bc02dd2d33136","modified":1517218619584},{"_id":"themes/next/languages/zh-hk.yml","hash":"54e4aa1d04ccad1e13cf08124fe5f70a930592dd","modified":1517218619583},{"_id":"themes/next/languages/zh-tw.yml","hash":"280dd00495e90b8a8c4d9351bab8ae65c78bbe87","modified":1517218619583},{"_id":"themes/next/scripts/merge-configs.js","hash":"ca9845dc76f5710b4c6fba5fe25ff0d2fcf0adaa","modified":1517218619538},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1517218619538},{"_id":"themes/next/languages/ru.yml","hash":"4ba9f4971115bce0213c437158424428e0e13d5a","modified":1517218619584},{"_id":"themes/next/source/404.html","hash":"a5991bb3598c5d934bd97e3bc4ed14b71ebf1e62","modified":1517218619532},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1517218619410},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1517218619410},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1517218619409},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619457},{"_id":"source/_posts/Airflow/[AirFlow]AirFlow使用指南一 安装与启动.md","hash":"faa209806329bd421bb338f0ec9cdccb6bf768b2","modified":1517218619660},{"_id":"source/_posts/Flink/[Flink]Flink Flink程序剖析.md","hash":"59b6e6dec4eb9c0078cde88dd31cb69e840fc435","modified":1517218619659},{"_id":"source/_posts/Flink/[Flink]Flink 分布式运行环境.md","hash":"7356315c341c1a06be0baecb3d9ee01f9c753651","modified":1517218619659},{"_id":"source/_posts/Flink/[Flink]Flink 四种优化Flink应用程序的方法.md","hash":"ec0254f47afe8a129a1a1dfa3b89f8a4ea47d465","modified":1518423579132},{"_id":"source/_posts/Flink/[Flink]Flink 安装与启动.md","hash":"a81aa9a2e76adc761aa98fb9f2fc3480e9ead881","modified":1517218619658},{"_id":"source/_posts/Flink/[Flink]Flink 编程模型.md","hash":"72ebe366e29ddea5fdd6316853ca28d17915396b","modified":1517218619657},{"_id":"source/_posts/Flink/[Flink]Flink1.4 事件时间与Watermarks.md","hash":"8828174fb206804142fe0998af045384d8b33666","modified":1517218619657},{"_id":"source/_posts/Flink/[Flink]Flink 动态表的持续查询.md","hash":"3f6eed4718076a0eb530f0c2e7d6169db3af3eeb","modified":1517561270237},{"_id":"source/_posts/Flink/[Flink]Flink1.4 事件时间与处理时间.md","hash":"053d27d613104f55813d0823ec0283c5c0a70283","modified":1517218619656},{"_id":"source/_posts/Flink/[Flink]Flink1.4 内置时间戳提取器和Watermark生成器.md","hash":"674d38d8458df8ed57e9f55a80f089c2bbab7f37","modified":1517218619656},{"_id":"source/_posts/Flink/[Flink]Flink1.4 使用状态.md","hash":"a354aadf6aad212e25b671dc813b20cec1cea26d","modified":1517466148468},{"_id":"source/_posts/Flink/[Flink]Flink1.4 内部原理之作业与调度.md","hash":"0f709c21c8bc79fc0576170ac11bef2025385d3b","modified":1517285019799},{"_id":"source/_posts/Flink/[Flink]Flink1.4 内部原理之数据流容错.md","hash":"89239bb22bed7de8e590481bfe2e35e15cfdf6ea","modified":1517218121682},{"_id":"source/_posts/Flink/[Flink]Flink1.4 命令行接口.md","hash":"c4c4d30689c6b0d0cdfb8478d7d3ebd89f6248f4","modified":1517308328441},{"_id":"source/_posts/Flink/[Flink]Flink1.4 保存点之回溯时间.md","hash":"54f15f49697740662d9a714117bfa13768aa93f1","modified":1517390672826},{"_id":"source/_posts/Flink/[Flink]Flink1.4 图解Watermark.md","hash":"e552f9a03f06db9825b6d6339d970f9b8963f9f5","modified":1517218619655},{"_id":"source/_posts/Flink/[Flink]Flink1.4 定义keys的几种方法.md","hash":"2ea5a0f2139963e67113ed5006e7f6426051bc85","modified":1517218619655},{"_id":"source/_posts/Flink/[Flink]Flink1.4 外部检查点.md","hash":"fce3685755cfb3bf2b818c2239152abfe6acaf67","modified":1517297185425},{"_id":"source/_posts/Flink/[Flink]Flink1.4 处理背压.md","hash":"bad07f938a60b2815a071c4e865f995b995450a0","modified":1518407244113},{"_id":"source/_posts/Flink/[Flink]Flink1.4 数据流类型和转换关系.md","hash":"1d19fc4800c0f58a1c5180a6487df390258b1fc4","modified":1517218619648},{"_id":"source/_posts/Flink/[Flink]Flink1.4 执行计划.md","hash":"8317877a79d0e827b2e5b42629922f767a5bc911","modified":1517218619649},{"_id":"source/_posts/Flink/[Flink]Flink1.4 状态概述.md","hash":"42f3aa4b1940cfc5d00fe06dfbd00ded8981a72e","modified":1517466078771},{"_id":"source/_posts/Flink/[Flink]Flink1.4 生成时间戳与Watermarks.md","hash":"99e6056eed399903a8d967c283b366e1245dfa32","modified":1517218619648},{"_id":"source/_posts/Flink/[Flink]Flink1.4 累加器与计数器.md","hash":"70ba59853202cd9f1421e68b39d981ae97c25ae7","modified":1517218619647},{"_id":"source/_posts/Hadoop/Hadoop Hadoop中的推测执行.md","hash":"c3640d24905b3e505128e3749b814e8219efd5a7","modified":1517218619638},{"_id":"source/_posts/Flink/[Flink]Flink1.4 重启策略.md","hash":"65b2cc23752a8d3da8b306c2556dbbe389fe604b","modified":1517218619638},{"_id":"source/_posts/Hadoop/Hadoop MapReduce 2 工作原理.md","hash":"ba957e8c79a0d888529accc58c52a183e9903f6a","modified":1517218619637},{"_id":"source/_posts/Hadoop/Hadoop MapReduce1.0 工作原理.md","hash":"ce43c3d42de95e11cf2f6c490d5bdb47b1d7c540","modified":1517218619637},{"_id":"source/_posts/Hadoop/Hadoop MapReduce2.0 架构详解.md","hash":"7a36121b7eafd78ff78cf101d4c384877858375d","modified":1517218619636},{"_id":"source/_posts/Hadoop/Hadoop MapReduce中的InputSplit.md","hash":"a09f60e625bfcf0e34b7d0181346a460a2a7d41f","modified":1517218619635},{"_id":"source/_posts/Hadoop/Hadoop MapReducer工作过程.md","hash":"11b47382aaf814a9bbbf01b2e38b00f17f8545a4","modified":1517218619635},{"_id":"source/_posts/Hadoop/Hadoop MapReduce新一代架构MRv2.md","hash":"881372ef01c29945edc7a228b190b00ad7678190","modified":1517218619634},{"_id":"source/_posts/Hadoop/Hadoop Partitioner使用教程.md","hash":"07bbeeb9e5b8e9a54ffaca6a090f301b4802ee36","modified":1517218619634},{"_id":"source/_posts/Hadoop/Hadoop Reducer总是能复用为Combiner？.md","hash":"b5301e709086fb137cbf88f1d2970ae969e13178","modified":1517218619633},{"_id":"source/_posts/Hadoop/Hadoop 图解HDFS工作原理.md","hash":"c536c5f3ca362f626afc5c0a01484d4a788aff67","modified":1517218619632},{"_id":"source/_posts/Hadoop/Hadoop1.x Secondary NameNode的作用.md","hash":"3b45fa652a5bd3010a0a06503410b4f9a093b557","modified":1517218619631},{"_id":"source/_posts/Hadoop/Hadoop Trash回收站使用指南.md","hash":"bcce128a532d0be4fb4fc6bf97abefe9e0310318","modified":1517218619632},{"_id":"source/_posts/Hadoop/Hadoop2.x HDFS架构.md","hash":"369e20fd54cbbbfb87c504a9c5333606d83ddc80","modified":1517218619630},{"_id":"source/_posts/Hadoop/[Hadoop]Hadoop SSH免密码登录.md","hash":"26ec091b62c978930a6b6e56cda71dbf783b830b","modified":1517218619630},{"_id":"source/_posts/Hadoop/[Hadoop]Hadoop 安装与启动.md","hash":"f365f811935666bef1c906521a2e3aca22fc97bf","modified":1517218619629},{"_id":"source/_posts/Hexo/Hexo+Github搭建博客更换皮肤.md","hash":"2187aa57471b5e6c733bf10d6eec751fe2035673","modified":1517218619628},{"_id":"source/_posts/Hadoop/[Hadoop]脱离JVM？ Hadoop生态圈的挣扎与演化.md","hash":"efab3631b4cfd872ae4d23a7256d2c68cb19b013","modified":1517802471139},{"_id":"source/_posts/Hexo/Hexo+Github搭建静态博客.md","hash":"4e85368e084446798a2c6e6cf9bf489ae9f7445f","modified":1517224941234},{"_id":"source/_posts/Hexo/[Hexo]Hexo+Github为NexT主题添加文章阅读量统计功能.md","hash":"eded261160c94f11f5a89d083aef1c2d9688b15e","modified":1517218619628},{"_id":"source/_posts/Hive/Hive使用指南 内部表与外部表.md","hash":"be67444c5fa28c83a6784a8a416739bfebfa453b","modified":1517218619627},{"_id":"source/_posts/Hexo/[Hexo]Hexo+Github配置与主题.md","hash":"8ca4f818069b81950563a0f43b77b3170287145c","modified":1516601556603},{"_id":"source/_posts/Java/[JVM]JVM 垃圾收集器.md","hash":"de01477ce34914805bd2153f84f82ae1b3a32f3e","modified":1518316731187},{"_id":"source/_posts/Java/[JVM]JVM 垃圾收集算法.md","hash":"6df99bb31b3c48b8bb3f8a08fe96d7fa772a0cc2","modified":1518338895570},{"_id":"source/_posts/Java/[Java]Java 堆内内存与堆外内存.md","hash":"8b068f8f52ddc0e1d7a1a5468fdeb2ea34002cdd","modified":1517917877880},{"_id":"source/_posts/MySQL/[MySQL]MySQL配置远程登录.md","hash":"529d9ac6f87b670133bbdbdc8893bba5557ba01f","modified":1517218619626},{"_id":"source/_posts/Python/Python 字符串操作.md","hash":"effe4336c32928ea0695bd04aefed08094c31ce5","modified":1517218619619},{"_id":"source/_posts/Other/[Cache]Cache 缓存更新策略.md","hash":"27151ef5106ff88f6bd9cda8310085dfb20d7e62","modified":1517883369857},{"_id":"source/_posts/Python/Python 列表,元组与集合.md","hash":"52e1140596d61bbf4cae90ac0fbe17506c0e7bd0","modified":1517218619619},{"_id":"source/_posts/Python/Python Numpy 数组.md","hash":"1516f3aafb55aa23c9cc5aa9490e5ee1ebba4487","modified":1517218619626},{"_id":"source/_posts/Python/Python 数据库操作.md","hash":"91e1c3d4eaabc00631c94d1032fbfb5e971ee4db","modified":1517218619618},{"_id":"source/_posts/Python/Python 文件处理.md","hash":"2e676deb61da0665c5e6e77eb3d7a0cd20c0e5d5","modified":1517218619617},{"_id":"source/_posts/Hadoop/[Hadoop]Shell中判断HDFS文件是否存在.md","hash":"2e055dcc9bba1c592a5bb7be18eb7aabbfc0a819","modified":1517226963444},{"_id":"source/_posts/Python/[Python] 爬虫使用Requests获取网页文本内容中文乱码.md","hash":"26a5ca795e0e17db5030ee655d30eba49b498057","modified":1517218619617},{"_id":"source/_posts/Scala/[Scala]Scala学习笔记一 基础.md","hash":"07a4ef11c9687a5cba3eee9576e574c804901658","modified":1519696935330},{"_id":"source/_posts/Scala/[Scala]Scala学习笔记三 Map与Tuple.md","hash":"dae2695072079c86a814bd84837fe90eb99ad617","modified":1519699704167},{"_id":"source/_posts/Spark/[Spark]Spark 性能优化之基础篇.md","hash":"dffbfbaa8087b506cec4efa320965ff32f4d26cd","modified":1517536225412},{"_id":"source/_posts/Stream/Exactly once未必严格一次.md","hash":"9a06bd4ecbcaf2e339ca6682a69030c4b67c3b20","modified":1517218619616},{"_id":"source/_posts/Scala/[Scala]Scala学习笔记五 Object.md","hash":"3cf98846ccbc239550b12741ec542e00db7b287c","modified":1519708407687},{"_id":"source/_posts/Scala/[Scala]Scala学习笔记四 类.md","hash":"1a7aceda80ff9021d1898819da7b808f104af76f","modified":1519711158557},{"_id":"source/_posts/Scala/[Scala]Scala学习笔记二 数组.md","hash":"025669d6bd8c6f032ed7432a02e75c836d526667","modified":1519698723477},{"_id":"source/_posts/Stream/[Stream]主流流处理框架比较(1).md","hash":"13309ee50c136ccbf902c65ab76a2c04655e6026","modified":1517218619615},{"_id":"source/_posts/Stream/[Stream]主流流处理框架比较(2).md","hash":"a7ebd22624029ced173e515f006ed4286bf7f4f6","modified":1517218619614},{"_id":"source/_posts/Stream/[Stream]对于流处理技术的谬见.md","hash":"f4c073786115459cdf7d20ac4645776933469e64","modified":1517218619613},{"_id":"themes/next/docs/cn/DATA-FILES.md","hash":"58c58c7d98365395dba904a87f9b5f5e0b1e99cb","modified":1517218619601},{"_id":"themes/next/docs/cn/INSTALLATION.md","hash":"f60778b75adada4a98479dd52af7c03a1f3cabf8","modified":1517218619601},{"_id":"themes/next/docs/cn/README.md","hash":"fca120e7425058f1204c68ffdecfaec641f195a1","modified":1517218619600},{"_id":"themes/next/docs/cn/UPDATE-FROM-5.1.X.md","hash":"b6422e0e1bbb02ddd29a2f9969fc3ff709555560","modified":1517218619600},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1517218619582},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1517218619582},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"065108bb77f37dec89c533952f5a2991e2c53ead","modified":1517218619599},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"1f15b876c106bae74148fb526bc3b914721b8ff2","modified":1517218619599},{"_id":"themes/next/docs/ru/README.md","hash":"a19b2fc3920d241e10dc6ec17dfa2a9ad247c03f","modified":1517218619598},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1517218619597},{"_id":"themes/next/layout/_partials/comments.swig","hash":"5df32b286a8265ba82a4ef5e1439ff34751545ad","modified":1517218619577},{"_id":"themes/next/layout/_partials/footer.swig","hash":"73835db2ea30a9824f1d90351f975f6011f42d27","modified":1517218619577},{"_id":"themes/next/layout/_partials/header.swig","hash":"203fabaf17acfafbaf1a97d0015a6613ec71d2b0","modified":1517218619574},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1517218619573},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1517218619573},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1517218619572},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1517218619581},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1517218619580},{"_id":"themes/next/layout/_macro/post.swig","hash":"92e715e1aba163c416964fb33e4ca4ce4bf6aa9b","modified":1517218619579},{"_id":"themes/next/layout/_macro/reward.swig","hash":"aa620c582143f43ba1cb1a5e59240041a911185b","modified":1517218619579},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"43f13d75cfb37ec4ed5386bee0f737641977200b","modified":1517218619578},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"fea45ab314b9ea23edab25c2b8620f909d856b1d","modified":1517218619578},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1517218619568},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1517218619568},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1517218619567},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1517218619564},{"_id":"source/_posts/Flink/[Flink]Flink1.4 并发执行.md","hash":"b58436bdc71921e9dfd3c4be645ec526b9768ca6","modified":1517218619649},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1517218619553},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"4d0253223a10d2c6823deacc6622387f7f1e7dcc","modified":1517218619552},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1517218619552},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1517218619551},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1517218619550},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1517218619545},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1517218619537},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1517218619536},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1517218619536},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1517218619535},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1517218619534},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1517218619533},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1517218619551},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1517218619534},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1517218619535},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1517218619533},{"_id":"themes/next/source/css/main.styl","hash":"fe94a9beca4b6d33a1af69b92064c90ade7dbd6d","modified":1517218619458},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1517218619457},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1517218619455},{"_id":"themes/next/source/images/alipay.png","hash":"c4ccdb8b06b311da4f350732f2a80fa2e3dd448e","modified":1517218619456},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1517218619455},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1517218619454},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1517218619454},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1517218619453},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1517218619452},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1517218619452},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1517218619451},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1517218619450},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1517218619449},{"_id":"themes/next/source/images/header.png","hash":"db8e718a1ff056adffe3688599442c42aa14149b","modified":1517218619449},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1517218619448},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1517218619447},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1517218619447},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1517218619449},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1517218619446},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1517218619446},{"_id":"themes/next/source/images/wechatpay.png","hash":"d504c5042cbaaf125077a61c9526bf58dd025151","modified":1517218619445},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1517218619448},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619566},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619565},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619477},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619476},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619475},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619460},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1517218619458},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1517218619576},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1517218619576},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"1a1692fa03a2d7dd9f5dd7feb5bfcc4b7e769f45","modified":1517218619575},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"52a112fd6582c48e46967c6d68fa4f192578c40c","modified":1517218619575},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1517218619572},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1517218619572},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1517218619571},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1517218619570},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1517218619570},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1517218619569},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1517218619569},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1517218619566},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1517218619565},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1517218619567},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1517218619564},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1517218619563},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1517218619563},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1517218619562},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1517218619561},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1517218619561},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1517218619561},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1517218619560},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1517218619560},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1517218619559},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1517218619559},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1517218619558},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1517218619558},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1517218619544},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1517218619543},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1517218619543},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1517218619542},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1517218619557},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"2fbee33a4ccd7c8217c73e85a9a1488170e05629","modified":1517218619557},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1517218619556},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"17a54796f6e03fc834880a58efca45c286e40e40","modified":1517218619556},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"40e3cacbd5fa5f2948d0179eff6dd88053e8648e","modified":1517218619555},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"6f340d122a9816ccdf4b45b662880a4b2d087671","modified":1517218619555},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1517218619554},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"42f62695029834d45934705c619035733762309e","modified":1517218619553},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1517218619478},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1517218619477},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1517218619476},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1517218619476},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1517218619461},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1517218619460},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1517218619460},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1517218619459},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1517218619429},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1517218619428},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1517218619428},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1517218619427},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1517218619427},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1517218619413},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1517218619412},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1517218619411},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"531cdedd7fbe8cb1dab2e4328774a9f6b15b59da","modified":1517218619443},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1517218619444},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1517218619442},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1517218619444},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1517218619442},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1517218619433},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1517218619432},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1517218619431},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1517218619430},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1517218619430},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1517218619418},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1517218619545},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1517218619544},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1517218619487},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1517218619532},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1517218619531},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1517218619531},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1517218619530},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1517218619530},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1517218619519},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1517218619500},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1517218619487},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1517218619486},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1517218619486},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1517218619485},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1517218619479},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1517218619479},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1517218619475},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1517218619474},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1517218619473},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1517218619473},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"dbff05af5e3a7c4f0889cdcc9027f882a246aa68","modified":1517218619472},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1517218619472},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1517218619471},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1517218619470},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1517218619468},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1517218619468},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1517218619467},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1517218619467},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1517218619466},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1517218619465},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1517218619464},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"d204ef41e5f59aa102baf02e1751075a041ae7f4","modified":1517218619464},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1517218619463},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1517218619463},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1517218619462},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1517218619427},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1517218619426},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1517218619425},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1517218619417},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1517218619416},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1517218619432},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1517218619420},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1517218619415},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1517218619421},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1517218619529},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ce77b4ff598c10aca5687e9a65387ae74684632e","modified":1517218619528},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1517218619528},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1517218619527},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1517218619526},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1517218619526},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1517218619525},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1517218619524},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1517218619524},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1517218619523},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1517218619522},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1517218619521},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1517218619521},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1517218619520},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1517218619519},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1517218619518},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1517218619517},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1517218619517},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1517218619516},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1517218619515},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1517218619514},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"0abb074afb6ab7242a22fa6dc3ac688251df708a","modified":1517218619514},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1517218619513},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1517218619512},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1517218619512},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1517218619511},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1517218619510},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1517218619510},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1517218619509},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1517218619509},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1517218619508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1517218619507},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1517218619506},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1517218619505},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1517218619505},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1517218619504},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1517218619504},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1517218619503},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1517218619502},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1517218619501},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1517218619501},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1517218619494},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1517218619493},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1517218619492},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1517218619491},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1517218619491},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1517218619490},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1517218619490},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1517218619489},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"96326454eb7c7a3ec38d20294c94c4cc63f16ebe","modified":1517218619488},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1517218619500},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1517218619499},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1517218619498},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1517218619498},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1517218619497},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1517218619496},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1517218619496},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1517218619495},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1517218619494},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1517218619470},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1517218619424},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1517218619469},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1517218619466},{"_id":"public/baidusitemap.xml","hash":"5ebc841d9dc9e6e6037012fbe8c16f04fed059bb","modified":1519711224223},{"_id":"public/search.xml","hash":"7487b6e4ad21085cb8f2e2aca021f82b08b1a0c0","modified":1519711224266},{"_id":"public/sitemap.xml","hash":"d67356d519049868cbc2cfe9988ac091686ccd6d","modified":1519711224267},{"_id":"public/about/index.html","hash":"57dac4df8f68a4ecac806309793d141d6f114058","modified":1519711225982},{"_id":"public/archives/index.html","hash":"4072e912648b0ca092c25c1a429b775152e3a967","modified":1519711225997},{"_id":"public/tags/index.html","hash":"4905ff5691d574bdd097982bc57e9fd3763c0438","modified":1519711225997},{"_id":"public/categories/index.html","hash":"a37cf4ede276e4c9c11151b85bed0227a8a004cc","modified":1519711226006},{"_id":"public/2018/02/27/scala-notes-object/index.html","hash":"29f66b22bf114a9965f4a0f8fe2ef889d9d23b59","modified":1519711226006},{"_id":"public/2018/02/27/scala-notes-class/index.html","hash":"a503fde77e0382f341763eb0a9979947b48b844e","modified":1519711226006},{"_id":"public/2018/02/27/scala-notes-map-and-tuple/index.html","hash":"5577a738c56cd40eda32bfd8e15800c4a965e80a","modified":1519711226006},{"_id":"public/2018/02/27/scala-notes-array/index.html","hash":"ebfcad27ca47ffe67f430199d2417c4eda9777dc","modified":1519711226006},{"_id":"public/2018/02/27/scala-notes-basis/index.html","hash":"0f8de74553372471b8fa86670e27c4a71727b042","modified":1519711226007},{"_id":"public/2018/02/11/how-flink-handles-backpressure/index.html","hash":"99a13b41e7f03a3b95b0c60d9b37e5bfb1b74183","modified":1519711226007},{"_id":"public/2018/02/11/four-ways-to-optimize-your-flink-applications/index.html","hash":"58cdf84493a5fb49a1f6ea63691cf57dd7221e52","modified":1519711226007},{"_id":"public/2018/02/10/jvm-common-garbage-collector/index.html","hash":"bbe0ce3e36f19b45d9306772bf7a5d7ca0dca001","modified":1519711226007},{"_id":"public/2018/02/04/java-on-off-heap-memory/index.html","hash":"cea29f208bc952234c138e13d722e3c53ec92569","modified":1519711226007},{"_id":"public/2018/02/04/jvm-garbage-collection-algorithm/index.html","hash":"16f2a45ba7a303efe23fc00ac70bb2b865bd219c","modified":1519711226007},{"_id":"public/2018/02/03/cache-update-policy/index.html","hash":"647ada8806ac8f20f735a87c630427edcde63d69","modified":1519711226007},{"_id":"public/2018/02/02/flink-sql-persistent-query-of-dynamic-table/index.html","hash":"1c6f4c0a8e42ddefa3166b0b97543a70433b2581","modified":1519711226007},{"_id":"public/2018/02/02/hadoop-ecosystem-break-away-jvm/index.html","hash":"f9b61967437f0993de8ffa6bce5f364d95463328","modified":1519711226007},{"_id":"public/2018/02/01/spark-performance-optimization-basic/index.html","hash":"7b12746ab381e787f98501f6e901935c192368d6","modified":1519711226007},{"_id":"public/2018/01/31/flink_stream_turning_back_time_savepoints/index.html","hash":"3035b2f49c96cc89aba56f4ff9c75fd4a365ab6c","modified":1519711226007},{"_id":"public/2018/01/30/flink_basic_command_line_interface/index.html","hash":"81b796ddb7d38918efc3dcc95d59657d91066918","modified":1519711226007},{"_id":"public/2018/01/30/flink_stream_deployment_externalized_checkpoints/index.html","hash":"692df8e6350ea3b881047e0e913efea2bcc48996","modified":1519711226007},{"_id":"public/2018/01/29/flink_internals_job_scheduling/index.html","hash":"4ccc4cec68d377fb32596b6bdf00d77bcf09e91d","modified":1519711226007},{"_id":"public/2018/01/25/Hadoop/[Hadoop]Shell中判断HDFS文件是否存在/index.html","hash":"68be5dbd3455f93d360ae22b6d34cb4c44a66a8c","modified":1519711226007},{"_id":"public/2018/01/24/flink_data_streaming_fault_tolerance/index.html","hash":"a74901a9416f41eb231bcc878901f394ac146288","modified":1519711226007},{"_id":"public/2018/01/16/flink_stream_working_with_state/index.html","hash":"00f230251a89f3efcaff24b4bb1ebe7f487ccb23","modified":1519711226007},{"_id":"public/2018/01/16/flink_stream_state_overview/index.html","hash":"c311d719b69bca3e9945f62b9c124921bedeef20","modified":1519711226007},{"_id":"public/2018/01/16/Flink/[Flink]Flink1.4 内置时间戳提取器和Watermark生成器/index.html","hash":"1a3fcac95b5116b7c25338716898330cb437e184","modified":1519711226008},{"_id":"public/2018/01/15/Flink/[Flink]Flink1.4 图解Watermark/index.html","hash":"b0d1dd136a809c731953a0b2f03cf6dfb6fb8a8d","modified":1519711226008},{"_id":"public/2018/01/15/Flink/[Flink]Flink1.4 生成时间戳与Watermarks/index.html","hash":"a525b50d862a66a3ad9eee8b5f2334a97687f700","modified":1519711226008},{"_id":"public/2018/01/11/Stream/Exactly once未必严格一次/index.html","hash":"e42402fea774b1dc691d2215270e5b6586c98d8f","modified":1519711226009},{"_id":"public/2018/01/11/Stream/[Stream]对于流处理技术的谬见/index.html","hash":"920302cad5263fc46edc0c1c65915ebc698675d5","modified":1519711226009},{"_id":"public/2018/01/10/Stream/[Stream]主流流处理框架比较(2)/index.html","hash":"e694a32a5a031785667b7f0c24f9cc009851ae2d","modified":1519711226009},{"_id":"public/2018/01/10/Stream/[Stream]主流流处理框架比较(1)/index.html","hash":"cf6e5f7d262de00e066d035dbd9e11fd686558d2","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 事件时间与Watermarks/index.html","hash":"f7a8a33313a59ab40f9325ad53f32b918363b3a0","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 重启策略/index.html","hash":"2f6271c660dd6d737e03446d78dcd705b0b2aa6b","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 事件时间与处理时间/index.html","hash":"07a7f98f53d9b0b86b51af88494a1e9b04bae5b2","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 数据流类型和转换关系/index.html","hash":"03cf87e051fa08071416ce4f40001ef4d6274b38","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 执行计划/index.html","hash":"0db97a0117b0c8484f1b5ba6389482cf33ce291f","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 并发执行/index.html","hash":"439208bcd24e5682abf40278fcbb75495586345e","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 累加器与计数器/index.html","hash":"8683a2507cab592b1373fb228a7e25227d521c50","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink1.4 定义keys的几种方法/index.html","hash":"5c8c35df81418086b894b8277c98d1a3d98bb687","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink Flink程序剖析/index.html","hash":"b9b0b85431aa49134669da89ce2fcc1646aecf39","modified":1519711226009},{"_id":"public/2018/01/04/Flink/[Flink]Flink 安装与启动/index.html","hash":"f04d9821727d292f49486c753030d7a640f36286","modified":1519711226009},{"_id":"public/2018/01/03/Flink/[Flink]Flink 分布式运行环境/index.html","hash":"9c2c0ca3f7330d8e0051c387b7c761d6d0b411c9","modified":1519711226009},{"_id":"public/2017/12/29/MySQL/[MySQL]MySQL配置远程登录/index.html","hash":"2dccdcbf0b1230705daf359d81d90b66343434fb","modified":1519711226010},{"_id":"public/2017/12/29/Airflow/[AirFlow]AirFlow使用指南一 安装与启动/index.html","hash":"4bb77a4c526005e0b60ff402fde8f6441804b7cd","modified":1519711226010},{"_id":"public/2017/12/29/Flink/[Flink]Flink 编程模型/index.html","hash":"f368c5cf6a7cb382c13196d89bbd7ec3a25d4866","modified":1519711226010},{"_id":"public/2017/12/20/Hadoop/Hadoop2.x HDFS架构/index.html","hash":"05c51a41fe8dedeba46befc828fb295e646cef78","modified":1519711226010},{"_id":"public/2017/12/20/Hadoop/Hadoop1.x Secondary NameNode的作用/index.html","hash":"005b0d86b88d980071212e37aeb5a1212a3bacab","modified":1519711226010},{"_id":"public/2017/12/19/Hadoop/Hadoop 图解HDFS工作原理/index.html","hash":"28bbbfd9084b952d166bd7cf32d9405dfefad7c3","modified":1519711226010},{"_id":"public/2017/12/14/Hadoop/Hadoop MapReduce 2 工作原理/index.html","hash":"c9feb4e52c2bce6198547e623594384a0081b072","modified":1519711226010},{"_id":"public/2017/12/14/Hadoop/Hadoop MapReduce1.0 工作原理/index.html","hash":"68c57af0ba118f6304b619a9981e698618d1ef16","modified":1519711226010},{"_id":"public/2017/12/12/Hadoop/Hadoop MapReduce2.0 架构详解/index.html","hash":"af145e2627a6349e19c5305a5d1944dd59adfec7","modified":1519711226010},{"_id":"public/2017/12/12/Hive/Hive使用指南 内部表与外部表/index.html","hash":"4450ca5df002aa85fbb46bdc26c631f44bc5fcae","modified":1519711226010},{"_id":"public/2017/12/08/Hadoop/Hadoop MapReduce新一代架构MRv2/index.html","hash":"e0585446f45239badff867b74559e47d601082a5","modified":1519711226010},{"_id":"public/2017/12/07/Hadoop/Hadoop Hadoop中的推测执行/index.html","hash":"a5d82a33489cb25495d211cbf7f0123de6fb7179","modified":1519711226010},{"_id":"public/2017/12/07/Hadoop/Hadoop Trash回收站使用指南/index.html","hash":"37e09fc11de247b1765a2106f5e6e26fee4727b7","modified":1519711226010},{"_id":"public/2017/12/06/Hadoop/Hadoop MapReduce中的InputSplit/index.html","hash":"0b2c487ea8cfd428948c5eb2fac5845b2b14ed0b","modified":1519711226010},{"_id":"public/2017/12/06/Hadoop/Hadoop Reducer总是能复用为Combiner？/index.html","hash":"fa8f2ec475b72f44f000ba6f41d4377fcadf3600","modified":1519711226010},{"_id":"public/2017/12/05/Hadoop/Hadoop Partitioner使用教程/index.html","hash":"898ab5b8d1d100252b275a8d002e407e65d61a40","modified":1519711226010},{"_id":"public/2017/12/05/Python/[Python] 爬虫使用Requests获取网页文本内容中文乱码/index.html","hash":"d649c72323d1a0a5a357ba0ade746d99ce4d17ec","modified":1519711226011},{"_id":"public/2017/12/02/Hexo/Hexo+Github搭建博客更换皮肤/index.html","hash":"8927e2dd5745a32f6009740236b682831214d68c","modified":1519711226011},{"_id":"public/2017/12/01/Hexo/[Hexo]Hexo+Github为NexT主题添加文章阅读量统计功能/index.html","hash":"1048b53bcd54ae4587dea55e5ee644d62b552cd0","modified":1519711226011},{"_id":"public/2017/12/01/Hexo/[Hexo]Hexo+Github配置与主题/index.html","hash":"f46b75bf9a409df1c54608757d1152b867f6df78","modified":1519711226011},{"_id":"public/2017/12/01/hexo_generate_blog/index.html","hash":"e2100cc409fce4c035bf48dd5aed65f8266271a2","modified":1519711226011},{"_id":"public/2016/12/30/Hadoop/Hadoop MapReducer工作过程/index.html","hash":"3f43ce3cc033a0563df1853cba5871e7a0693054","modified":1519711226011},{"_id":"public/2016/12/29/Hadoop/[Hadoop]Hadoop SSH免密码登录/index.html","hash":"907b692d79801f58ce2615bdac3c784f275ef336","modified":1519711226011},{"_id":"public/2016/12/29/Hadoop/[Hadoop]Hadoop 安装与启动/index.html","hash":"a12c79512567e8a80943ae21f4fb796302caeca2","modified":1519711226011},{"_id":"public/2016/05/17/Python/Python 文件处理/index.html","hash":"3f498684dbf382d3ceac7260b80dd430297914b9","modified":1519711226011},{"_id":"public/2016/05/17/Python/Python 数据库操作/index.html","hash":"c8934323c489d4f50eca4f2771b954e3c63045e3","modified":1519711226011},{"_id":"public/2016/05/17/Python/Python 列表,元组与集合/index.html","hash":"9ee9ae77b029656c3caf2e30a6c325a423a017c5","modified":1519711226012},{"_id":"public/2016/05/17/Python/Python 字符串操作/index.html","hash":"4ff79c5fbfbebd1a3c3ef766a2a39159a92e9f60","modified":1519711226012},{"_id":"public/2016/05/17/Python/Python Numpy 数组/index.html","hash":"802112711cfdd5ddba558185ab3c5269fa600c36","modified":1519711226012},{"_id":"public/archives/page/2/index.html","hash":"bc36adfc1a511451495489fd62781559270ebf09","modified":1519711226012},{"_id":"public/archives/page/3/index.html","hash":"f7bf772a7ca524cc23b583d60ac72a9677391e16","modified":1519711226012},{"_id":"public/archives/2016/index.html","hash":"f13354aa9230c5779a8cc6d7668139110862fbfc","modified":1519711226012},{"_id":"public/archives/2016/05/index.html","hash":"61274826bbb9b13fcda7158388fb76d70d8a88b9","modified":1519711226012},{"_id":"public/archives/2016/12/index.html","hash":"aaf86bf30c351e0b00a06b3e34a7c810a52cb969","modified":1519711226012},{"_id":"public/archives/2017/index.html","hash":"d46b7d9f9982092a25781f7d7b7830f97049a80e","modified":1519711226012},{"_id":"public/archives/2017/12/index.html","hash":"c16062e793c64614e21ff905092eeb22afb78651","modified":1519711226012},{"_id":"public/archives/2018/index.html","hash":"43757480ec6b1e896e428b7ef7c6c581f2ea3afe","modified":1519711226012},{"_id":"public/archives/2018/page/2/index.html","hash":"c88389a45b01cf8c15db866090e61c66ed2bc72d","modified":1519711226012},{"_id":"public/archives/2018/01/index.html","hash":"0100cbcfa7ea8cd8c546e3d47e9c6a4f7b9c5bd5","modified":1519711226012},{"_id":"public/archives/2018/02/index.html","hash":"12e01d2c59835e6eae9a3c06327003aaef097194","modified":1519711226012},{"_id":"public/categories/Flink/index.html","hash":"5bbab9f7369602392ed60a553eb5fe36a0ad7111","modified":1519711226012},{"_id":"public/categories/Airflow/index.html","hash":"4460ad92d39869767a735cfd28bf165a800c7c8b","modified":1519711226012},{"_id":"public/categories/Hadoop/index.html","hash":"48c2be879a5acdd0e234922cae8b2d620393aea0","modified":1519711226012},{"_id":"public/categories/Hive/index.html","hash":"b2c1dcdfdf0c06a2a8fa6c7d6986635c83314bff","modified":1519711226012},{"_id":"public/categories/Java/index.html","hash":"6a45c752e3675e190c3bb95a37aa50c28422b646","modified":1519711226013},{"_id":"public/categories/MySQL/index.html","hash":"1c02a7ed3924e5fea51311215e0a95b813f0162c","modified":1519711226013},{"_id":"public/categories/Python/index.html","hash":"66a75ad3fcf499f812dc4c526b7065308985d8db","modified":1519711226013},{"_id":"public/categories/other/index.html","hash":"43e2c69b74b3dfd5e24c912abd7a31570655cf4e","modified":1519711226029},{"_id":"public/categories/Scala/index.html","hash":"42552c38045a7fd156b8a0905503efc99541d125","modified":1519711226029},{"_id":"public/categories/Spark/index.html","hash":"f51a29df319df387f43c7992107a6486e750e3b6","modified":1519711226029},{"_id":"public/categories/Stream/index.html","hash":"0739ca51c408e68a6c1252a5495aa98a9b97f021","modified":1519711226029},{"_id":"public/index.html","hash":"b786bf3cbd6fd89ecd8053f38f14886f8a4ff0de","modified":1519711226029},{"_id":"public/page/2/index.html","hash":"19d597276f24f815ed2b517a8bd760377e28f094","modified":1519711226029},{"_id":"public/page/3/index.html","hash":"1f5a9e5a607241b90e2954648c09fd31b86667bd","modified":1519711226029},{"_id":"public/tags/Flink/index.html","hash":"a62f89054d48b3835452cd17f56a0c175572bf5c","modified":1519711226029},{"_id":"public/tags/Airflow/index.html","hash":"56ee6535087bf124291fac18f1d6f3dae1ce3585","modified":1519711226030},{"_id":"public/tags/Flink-优化/index.html","hash":"2ffe0cc5acc4df1936231b6ccd42fd3940d44c28","modified":1519711226030},{"_id":"public/tags/Flink-SQL/index.html","hash":"d8cca9772be17ecd48569e2d6180a53224d4a46d","modified":1519711226030},{"_id":"public/tags/Flink-容错/index.html","hash":"a1b4feb83994040bf8b4f8715d0f0c5e3e6389d8","modified":1519711226030},{"_id":"public/tags/Flink内部原理/index.html","hash":"42239e0a2890152bb1eb860b32f9bdac55ba994d","modified":1519711226030},{"_id":"public/tags/Flink-基础/index.html","hash":"4684a41c3b44b4a3ea5ea03104d952155cdd271c","modified":1519711226030},{"_id":"public/tags/Hadoop/index.html","hash":"48a1c6bf7dd942dda9f1bdd812ad7ad0955ef3c6","modified":1519711226030},{"_id":"public/tags/Hexo/index.html","hash":"92aa6011c5042609136a28b46b15d8163553dd1c","modified":1519711226030},{"_id":"public/tags/Hive/index.html","hash":"116dfaae23ec392e6b7b4048dd2da13c20c30e3a","modified":1519711226030},{"_id":"public/tags/Java/index.html","hash":"cafb117b68dae8b8d11045087fe407f5d9f3ad20","modified":1519711226030},{"_id":"public/tags/JVM/index.html","hash":"205fe0959b76390e9b81554486066e7b451c4895","modified":1519711226030},{"_id":"public/tags/MySQL/index.html","hash":"fc4858f4331d1a48a5b8410c06c55b94c2abeb55","modified":1519711226030},{"_id":"public/tags/Python/index.html","hash":"6ff146ab42fa79ddff3ee5f13c5951a979292612","modified":1519711226030},{"_id":"public/tags/CaChe/index.html","hash":"31486723937a7beca88d25822e16538500830008","modified":1519711226030},{"_id":"public/tags/Scala/index.html","hash":"fb633cd85be3ec118ec69ddf10d412f3b39ef6f1","modified":1519711226030},{"_id":"public/tags/Spark/index.html","hash":"0ddd91b8e312499d0aa4e8db467c5c55237f2f5b","modified":1519711226030},{"_id":"public/tags/Spark-优化/index.html","hash":"e5a0304e40f91e4fd53be5bd5ba348c67d2b9f79","modified":1519711226031},{"_id":"public/tags/Stream/index.html","hash":"b7d5d9e24a27f771ce1bca3d091216e6cfd972a9","modified":1519711226031},{"_id":"public/robots.txt","hash":"5bd1453bce44724c8b3c5b6c5fa6af3ad1fb8268","modified":1519711226031},{"_id":"public/img/header.png","hash":"db8e718a1ff056adffe3688599442c42aa14149b","modified":1519711226031},{"_id":"public/img/header_16.jpg","hash":"679ec05839ea6d58371fe39d76d908bc8ad0de40","modified":1519711226031},{"_id":"public/img/header_32.jpg","hash":"5522f74309b0e2468d82a50771c379b385d6e01e","modified":1519711226031},{"_id":"public/img/post-default.jpg","hash":"303f34e43017037cc09a753e1b4ae035c6ee9983","modified":1519711226031},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1519711226031},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1519711226031},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1519711226031},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1519711226031},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1519711226031},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1519711226031},{"_id":"public/images/alipay.png","hash":"c4ccdb8b06b311da4f350732f2a80fa2e3dd448e","modified":1519711226031},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1519711226031},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1519711226033},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1519711226033},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1519711226033},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1519711226033},{"_id":"public/images/header.png","hash":"db8e718a1ff056adffe3688599442c42aa14149b","modified":1519711226033},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1519711226033},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1519711226033},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1519711226033},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1519711226033},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1519711226033},{"_id":"public/images/wechatpay.png","hash":"d504c5042cbaaf125077a61c9526bf58dd025151","modified":1519711226033},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1519711226033},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1519711226033},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1519711226033},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1519711226033},{"_id":"public/CNAME","hash":"59de02e999399baa6e2803832337b8d493f54e5d","modified":1519711226033},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1519711226855},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1519711226859},{"_id":"public/404.html","hash":"a5991bb3598c5d934bd97e3bc4ed14b71ebf1e62","modified":1519711226867},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1519711226867},{"_id":"public/js/src/bootstrap.js","hash":"531cdedd7fbe8cb1dab2e4328774a9f6b15b59da","modified":1519711226867},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1519711226867},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1519711226867},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1519711226868},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1519711226868},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1519711226868},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1519711226868},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1519711226868},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1519711226868},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1519711226868},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1519711226868},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1519711226868},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1519711226868},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1519711226868},{"_id":"public/css/main.css","hash":"c6ca4053b148e141ead6554ac2218dd048e843c5","modified":1519711226868},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1519711226868},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1519711226868},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1519711226868},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1519711226868},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1519711226868},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1519711226868},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1519711226868}],"Category":[{"name":"Flink","_id":"cje58tipd0006ordbb65xsuz9"},{"name":"Airflow","_id":"cje58tipp000bordbnpm0gfj6"},{"name":"Hadoop","_id":"cje58tirx002aordb2zggsnwh"},{"name":"Hive","_id":"cje58tits0044ordbfn3s2um8"},{"name":"Java","_id":"cje58tity004dordbbi1m5gmv"},{"name":"MySQL","_id":"cje58tiuo004xordbf3g5lt7c"},{"name":"Python","_id":"cje58tiuu0055ordbbjn8erb4"},{"name":"other","_id":"cje58tiuz005cordbxdyjmmts"},{"name":"Scala","_id":"cje58tivw006cordbrfuow4nm"},{"name":"Spark","_id":"cje58tivz006mordboq1ic8ih"},{"name":"Stream","_id":"cje58tiw0006rordbroq8htmm"}],"Data":[],"Page":[{"title":"about","date":"2018-01-19T02:24:56.000Z","type":"about","_content":"\n# Name\n\nsjf0115  \n\n# I am\n\n大数据领域中的小菜鸟 目前专注于 `Hadoop` `Flink` `Spark` ...\n\n# Contact\n\nCSDN：[sunnyyoona](http://blog.csdn.net/sunnyyoona)\n\nGithub: [sjf0115](https://github.com/sjf0115)\n\nEmail: 1203745031@qq.com\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2018-01-19 10:24:56\ntype: \"about\"\n---\n\n# Name\n\nsjf0115  \n\n# I am\n\n大数据领域中的小菜鸟 目前专注于 `Hadoop` `Flink` `Spark` ...\n\n# Contact\n\nCSDN：[sunnyyoona](http://blog.csdn.net/sunnyyoona)\n\nGithub: [sjf0115](https://github.com/sjf0115)\n\nEmail: 1203745031@qq.com\n","updated":"2018-01-29T09:36:59.612Z","path":"about/index.html","comments":1,"layout":"page","_id":"cje58tiho0000ordbr6lqdph6","content":"<h1 id=\"Name\"><a href=\"#Name\" class=\"headerlink\" title=\"Name\"></a>Name</h1><p>sjf0115  </p>\n<h1 id=\"I-am\"><a href=\"#I-am\" class=\"headerlink\" title=\"I am\"></a>I am</h1><p>大数据领域中的小菜鸟 目前专注于 <code>Hadoop</code> <code>Flink</code> <code>Spark</code> …</p>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>CSDN：<a href=\"http://blog.csdn.net/sunnyyoona\" target=\"_blank\" rel=\"noopener\">sunnyyoona</a></p>\n<p>Github: <a href=\"https://github.com/sjf0115\" target=\"_blank\" rel=\"noopener\">sjf0115</a></p>\n<p>Email: 1203745031@qq.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Name\"><a href=\"#Name\" class=\"headerlink\" title=\"Name\"></a>Name</h1><p>sjf0115  </p>\n<h1 id=\"I-am\"><a href=\"#I-am\" class=\"headerlink\" title=\"I am\"></a>I am</h1><p>大数据领域中的小菜鸟 目前专注于 <code>Hadoop</code> <code>Flink</code> <code>Spark</code> …</p>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>CSDN：<a href=\"http://blog.csdn.net/sunnyyoona\" target=\"_blank\" rel=\"noopener\">sunnyyoona</a></p>\n<p>Github: <a href=\"https://github.com/sjf0115\" target=\"_blank\" rel=\"noopener\">sjf0115</a></p>\n<p>Email: 1203745031@qq.com</p>\n"},{"title":"archives","date":"2017-12-03T09:50:17.000Z","type":"archives","_content":"","source":"archives/index.md","raw":"---\ntitle: archives\ndate: 2017-12-03 17:50:17\ntype: \"archives\"\n---\n","updated":"2018-01-29T09:36:59.612Z","path":"archives/index.html","comments":1,"layout":"page","_id":"cje58tihr0001ordbg98vvx4y","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2017-12-02T10:09:57.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2017-12-02 18:09:57\ntype: \"tags\"\n---\n","updated":"2018-01-29T09:36:59.607Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cje58tihu0002ordbu5z18jhe","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2017-12-03T10:54:00.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-12-03 18:54:00\ntype: \"categories\"\n---\n","updated":"2018-01-29T09:36:59.611Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cje58tihx0003ordbqeqjjgh1","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"layout":"post","author":"sjf0115","title":"Flink1.4 Flink程序剖析","date":"2018-01-04T01:54:01.000Z","_content":"\n`Flink`程序程序看起来像转换数据集合的普通程序。每个程序都由相同的基本部分组成：\n- 获得一个执行环境\n- 加载/创建初始数据\n- 指定在这些数据上的转换操作\n- 指定计算结果存放位置\n- 触发程序执行\n\n\n现在我们将对每一步进行一个简要的概述。请注意，`Java DataSet API`的所有核心类都可以在`org.apache.flink.api.java`包中找到，而`Java DataStream API`的类可以在`org.apache.flink.streaming.api`中找到。`Scala DataSet API`的所有核心类都可以在`org.apache.flink.api.scala`包中找到，而`Scala DataStream API`的类可以在`org.apache.flink.streaming.api.scala`中找到。\n\n`StreamExecutionEnvironment`是所有`Flink`程序的基础。你可以使用`StreamExecutionEnvironment`上的如下静态方法获取：\nJava版本:\n```java\ngetExecutionEnvironment()\n\ncreateLocalEnvironment()\n\ncreateRemoteEnvironment(String host, int port, String... jarFiles)\n```\n\nScala版本:\n```\ngetExecutionEnvironment()\n\ncreateLocalEnvironment()\n\ncreateRemoteEnvironment(host: String, port: Int, jarFiles: String*)\n```\n\n通常情况下，我们只需要使用`getExecutionEnvironment()`即可，因为这会根据上下文做正确的选择：如果你在`IDE`内执行程序或作为常规的`Java`程序，将创建一个本地环境，在你的本地机器上执行你的程序。如果使用程序创建`JAR`文件并通过命令行调用它，那么`Flink`集群管理器将执行你的`main`方法，并且`getExecutionEnvironment()`返回一个用于在集群上执行你程序的执行环境。\n\n对于指定数据源，执行环境有多种方法可以从文件中读取数据：可以逐行读取，以CSV格式文件读取或使用完全自定义的数据输入格式。只要将文本文件作为一系列行读取，就可以使用：\n\nJava版本:\n```Java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nDataStream<String> text = env.readTextFile(\"file:///path/to/file\");\n```\n\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\n\nval text: DataStream[String] = env.readTextFile(\"file:///path/to/file\")\n```\n\n这将为你提供一个`DataStream`，然后就可以应用转换函数来创建新的派生`DataStream`。\n\n通过调用`DataStream`上的转换函数来应用转换操作。例如，一个`map`转换函数看起来像这样：\n\nJava版本:\n```java\nDataStream<String> input = ...;\n\nDataStream<Integer> parsed = input.map(new MapFunction<String, Integer>() {\n    @Override\n    public Integer map(String value) {\n        return Integer.parseInt(value);\n    }\n});\n```\n\nScala版本:\n```\nval input: DataSet[String] = ...\n\nval mapped = input.map { x => x.toInt }\n```\n\n这将通过将原始集合中的每个`String`转换为`Integer`来创建一个新的`DataStream`。\n\n一旦获得了包含最终结果的`DataStream`，就可以通过创建接收器(`sink`)将其写入外部系统中。下面是创建接收器的一些示例方法：\n\nJava版本:\n```java\nwriteAsText(String path)\n\nprint()\n```\n\nScala版本:\n```\nwriteAsText(path: String)\n\nprint()\n```\n\n一旦你指定的完整程序需要触发程序执行，可以通过调用`StreamExecutionEnvironment`的`execute()`方法来触发程序的执行。根据执行环境的类型，执行将在你的本地机器上触发，或提交程序在集群上执行。\n\n`execute()`方法返回一个`JobExecutionResult`，它包含执行时间和累加器结果。\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program\n","source":"_posts/Flink/[Flink]Flink Flink程序剖析.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 Flink程序剖析\ndate: 2018-01-04 09:54:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n`Flink`程序程序看起来像转换数据集合的普通程序。每个程序都由相同的基本部分组成：\n- 获得一个执行环境\n- 加载/创建初始数据\n- 指定在这些数据上的转换操作\n- 指定计算结果存放位置\n- 触发程序执行\n\n\n现在我们将对每一步进行一个简要的概述。请注意，`Java DataSet API`的所有核心类都可以在`org.apache.flink.api.java`包中找到，而`Java DataStream API`的类可以在`org.apache.flink.streaming.api`中找到。`Scala DataSet API`的所有核心类都可以在`org.apache.flink.api.scala`包中找到，而`Scala DataStream API`的类可以在`org.apache.flink.streaming.api.scala`中找到。\n\n`StreamExecutionEnvironment`是所有`Flink`程序的基础。你可以使用`StreamExecutionEnvironment`上的如下静态方法获取：\nJava版本:\n```java\ngetExecutionEnvironment()\n\ncreateLocalEnvironment()\n\ncreateRemoteEnvironment(String host, int port, String... jarFiles)\n```\n\nScala版本:\n```\ngetExecutionEnvironment()\n\ncreateLocalEnvironment()\n\ncreateRemoteEnvironment(host: String, port: Int, jarFiles: String*)\n```\n\n通常情况下，我们只需要使用`getExecutionEnvironment()`即可，因为这会根据上下文做正确的选择：如果你在`IDE`内执行程序或作为常规的`Java`程序，将创建一个本地环境，在你的本地机器上执行你的程序。如果使用程序创建`JAR`文件并通过命令行调用它，那么`Flink`集群管理器将执行你的`main`方法，并且`getExecutionEnvironment()`返回一个用于在集群上执行你程序的执行环境。\n\n对于指定数据源，执行环境有多种方法可以从文件中读取数据：可以逐行读取，以CSV格式文件读取或使用完全自定义的数据输入格式。只要将文本文件作为一系列行读取，就可以使用：\n\nJava版本:\n```Java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nDataStream<String> text = env.readTextFile(\"file:///path/to/file\");\n```\n\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\n\nval text: DataStream[String] = env.readTextFile(\"file:///path/to/file\")\n```\n\n这将为你提供一个`DataStream`，然后就可以应用转换函数来创建新的派生`DataStream`。\n\n通过调用`DataStream`上的转换函数来应用转换操作。例如，一个`map`转换函数看起来像这样：\n\nJava版本:\n```java\nDataStream<String> input = ...;\n\nDataStream<Integer> parsed = input.map(new MapFunction<String, Integer>() {\n    @Override\n    public Integer map(String value) {\n        return Integer.parseInt(value);\n    }\n});\n```\n\nScala版本:\n```\nval input: DataSet[String] = ...\n\nval mapped = input.map { x => x.toInt }\n```\n\n这将通过将原始集合中的每个`String`转换为`Integer`来创建一个新的`DataStream`。\n\n一旦获得了包含最终结果的`DataStream`，就可以通过创建接收器(`sink`)将其写入外部系统中。下面是创建接收器的一些示例方法：\n\nJava版本:\n```java\nwriteAsText(String path)\n\nprint()\n```\n\nScala版本:\n```\nwriteAsText(path: String)\n\nprint()\n```\n\n一旦你指定的完整程序需要触发程序执行，可以通过调用`StreamExecutionEnvironment`的`execute()`方法来触发程序的执行。根据执行环境的类型，执行将在你的本地机器上触发，或提交程序在集群上执行。\n\n`execute()`方法返回一个`JobExecutionResult`，它包含执行时间和累加器结果。\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program\n","slug":"Flink/[Flink]Flink Flink程序剖析","published":1,"updated":"2018-01-29T09:36:59.659Z","comments":1,"photos":[],"link":"","_id":"cje58tip10004ordbpy7pt5dk","content":"<p><code>Flink</code>程序程序看起来像转换数据集合的普通程序。每个程序都由相同的基本部分组成：</p>\n<ul>\n<li>获得一个执行环境</li>\n<li>加载/创建初始数据</li>\n<li>指定在这些数据上的转换操作</li>\n<li>指定计算结果存放位置</li>\n<li>触发程序执行</li>\n</ul>\n<p>现在我们将对每一步进行一个简要的概述。请注意，<code>Java DataSet API</code>的所有核心类都可以在<code>org.apache.flink.api.java</code>包中找到，而<code>Java DataStream API</code>的类可以在<code>org.apache.flink.streaming.api</code>中找到。<code>Scala DataSet API</code>的所有核心类都可以在<code>org.apache.flink.api.scala</code>包中找到，而<code>Scala DataStream API</code>的类可以在<code>org.apache.flink.streaming.api.scala</code>中找到。</p>\n<p><code>StreamExecutionEnvironment</code>是所有<code>Flink</code>程序的基础。你可以使用<code>StreamExecutionEnvironment</code>上的如下静态方法获取：<br>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">getExecutionEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createLocalEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createRemoteEnvironment(String host, <span class=\"keyword\">int</span> port, String... jarFiles)</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">getExecutionEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createLocalEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createRemoteEnvironment(host: String, port: Int, jarFiles: String*)</span><br></pre></td></tr></table></figure></p>\n<p>通常情况下，我们只需要使用<code>getExecutionEnvironment()</code>即可，因为这会根据上下文做正确的选择：如果你在<code>IDE</code>内执行程序或作为常规的<code>Java</code>程序，将创建一个本地环境，在你的本地机器上执行你的程序。如果使用程序创建<code>JAR</code>文件并通过命令行调用它，那么<code>Flink</code>集群管理器将执行你的<code>main</code>方法，并且<code>getExecutionEnvironment()</code>返回一个用于在集群上执行你程序的执行环境。</p>\n<p>对于指定数据源，执行环境有多种方法可以从文件中读取数据：可以逐行读取，以CSV格式文件读取或使用完全自定义的数据输入格式。只要将文本文件作为一系列行读取，就可以使用：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;String&gt; text = env.readTextFile(<span class=\"string\">\"file:///path/to/file\"</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">val text: DataStream[String] = env.readTextFile(&quot;file:///path/to/file&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>这将为你提供一个<code>DataStream</code>，然后就可以应用转换函数来创建新的派生<code>DataStream</code>。</p>\n<p>通过调用<code>DataStream</code>上的转换函数来应用转换操作。例如，一个<code>map</code>转换函数看起来像这样：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;String&gt; input = ...;</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;Integer&gt; parsed = input.map(<span class=\"keyword\">new</span> MapFunction&lt;String, Integer&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">map</span><span class=\"params\">(String value)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Integer.parseInt(value);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataSet[String] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val mapped = input.map &#123; x =&gt; x.toInt &#125;</span><br></pre></td></tr></table></figure></p>\n<p>这将通过将原始集合中的每个<code>String</code>转换为<code>Integer</code>来创建一个新的<code>DataStream</code>。</p>\n<p>一旦获得了包含最终结果的<code>DataStream</code>，就可以通过创建接收器(<code>sink</code>)将其写入外部系统中。下面是创建接收器的一些示例方法：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">writeAsText(String path)</span><br><span class=\"line\"></span><br><span class=\"line\">print()</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">writeAsText(path: String)</span><br><span class=\"line\"></span><br><span class=\"line\">print()</span><br></pre></td></tr></table></figure></p>\n<p>一旦你指定的完整程序需要触发程序执行，可以通过调用<code>StreamExecutionEnvironment</code>的<code>execute()</code>方法来触发程序的执行。根据执行环境的类型，执行将在你的本地机器上触发，或提交程序在集群上执行。</p>\n<p><code>execute()</code>方法返回一个<code>JobExecutionResult</code>，它包含执行时间和累加器结果。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>Flink</code>程序程序看起来像转换数据集合的普通程序。每个程序都由相同的基本部分组成：</p>\n<ul>\n<li>获得一个执行环境</li>\n<li>加载/创建初始数据</li>\n<li>指定在这些数据上的转换操作</li>\n<li>指定计算结果存放位置</li>\n<li>触发程序执行</li>\n</ul>\n<p>现在我们将对每一步进行一个简要的概述。请注意，<code>Java DataSet API</code>的所有核心类都可以在<code>org.apache.flink.api.java</code>包中找到，而<code>Java DataStream API</code>的类可以在<code>org.apache.flink.streaming.api</code>中找到。<code>Scala DataSet API</code>的所有核心类都可以在<code>org.apache.flink.api.scala</code>包中找到，而<code>Scala DataStream API</code>的类可以在<code>org.apache.flink.streaming.api.scala</code>中找到。</p>\n<p><code>StreamExecutionEnvironment</code>是所有<code>Flink</code>程序的基础。你可以使用<code>StreamExecutionEnvironment</code>上的如下静态方法获取：<br>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">getExecutionEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createLocalEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createRemoteEnvironment(String host, <span class=\"keyword\">int</span> port, String... jarFiles)</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">getExecutionEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createLocalEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">createRemoteEnvironment(host: String, port: Int, jarFiles: String*)</span><br></pre></td></tr></table></figure></p>\n<p>通常情况下，我们只需要使用<code>getExecutionEnvironment()</code>即可，因为这会根据上下文做正确的选择：如果你在<code>IDE</code>内执行程序或作为常规的<code>Java</code>程序，将创建一个本地环境，在你的本地机器上执行你的程序。如果使用程序创建<code>JAR</code>文件并通过命令行调用它，那么<code>Flink</code>集群管理器将执行你的<code>main</code>方法，并且<code>getExecutionEnvironment()</code>返回一个用于在集群上执行你程序的执行环境。</p>\n<p>对于指定数据源，执行环境有多种方法可以从文件中读取数据：可以逐行读取，以CSV格式文件读取或使用完全自定义的数据输入格式。只要将文本文件作为一系列行读取，就可以使用：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;String&gt; text = env.readTextFile(<span class=\"string\">\"file:///path/to/file\"</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\"></span><br><span class=\"line\">val text: DataStream[String] = env.readTextFile(&quot;file:///path/to/file&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>这将为你提供一个<code>DataStream</code>，然后就可以应用转换函数来创建新的派生<code>DataStream</code>。</p>\n<p>通过调用<code>DataStream</code>上的转换函数来应用转换操作。例如，一个<code>map</code>转换函数看起来像这样：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;String&gt; input = ...;</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;Integer&gt; parsed = input.map(<span class=\"keyword\">new</span> MapFunction&lt;String, Integer&gt;() &#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">map</span><span class=\"params\">(String value)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Integer.parseInt(value);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataSet[String] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val mapped = input.map &#123; x =&gt; x.toInt &#125;</span><br></pre></td></tr></table></figure></p>\n<p>这将通过将原始集合中的每个<code>String</code>转换为<code>Integer</code>来创建一个新的<code>DataStream</code>。</p>\n<p>一旦获得了包含最终结果的<code>DataStream</code>，就可以通过创建接收器(<code>sink</code>)将其写入外部系统中。下面是创建接收器的一些示例方法：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">writeAsText(String path)</span><br><span class=\"line\"></span><br><span class=\"line\">print()</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">writeAsText(path: String)</span><br><span class=\"line\"></span><br><span class=\"line\">print()</span><br></pre></td></tr></table></figure></p>\n<p>一旦你指定的完整程序需要触发程序执行，可以通过调用<code>StreamExecutionEnvironment</code>的<code>execute()</code>方法来触发程序的执行。根据执行环境的类型，执行将在你的本地机器上触发，或提交程序在集群上执行。</p>\n<p><code>execute()</code>方法返回一个<code>JobExecutionResult</code>，它包含执行时间和累加器结果。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#anatomy-of-a-flink-program</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Airflow使用指南一 安装与启动","date":"2017-12-29T04:55:01.000Z","_content":"\n### 1. 安装\n\n通过pip安装:\n```\nxiaosi@yoona:~$ pip install airflow\n```\n如果速度比较慢，可以使用下面提供的源进行安装:\n```\nxiaosi@yoona:~$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple airflow\n```\n如果出现下面提示，表示你的airflow安装成功了:\n```\nSuccessfully installed airflow alembic croniter dill flask flask-admin flask-cache flask-login flask-swagger flask-wtf funcsigs future gitpython gunicorn jinja2 lxml markdown pandas psutil pygments python-daemon python-dateutil python-nvd3 requests setproctitle sqlalchemy tabulate thrift zope.deprecation Mako python-editor click itsdangerous Werkzeug wtforms PyYAML ordereddict gitdb2 MarkupSafe pytz numpy docutils setuptools lockfile six python-slugify idna urllib3 certifi chardet smmap2 Unidecode\nCleaning up...\n```\n安装完成之后我的默认安装在`~/.local/bin`目录下\n\n### 2. 配置\n\n如果不修改路径，默认的配置为`~/airflow`\n\n永久修改环境变量\n```\necho \"export AIRFLOW_HOME=/home/xiaosi/opt/airflow\" >> /etc/profile\nsource /etc/profile\n```\n为了便于操作方便，进行如下配置:\n```\necho \"export PATH=/home/xiaosi/.local/bin:$PATH\" >> /etc/profile\nsource /etc/profile\n```\n\n### 3. 初始化\n\n初始化数据库:\n```\nxiaosi@yoona:~$ airflow initdb\n[2017-08-02 16:39:22,319] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 16:39:22,432] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 16:39:22,451] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\nDB: sqlite:////home/xiaosi/opt/airflow/airflow.db\n[2017-08-02 16:39:22,708] {db.py:287} INFO - Creating tables\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nINFO  [alembic.runtime.migration] Running upgrade  -> e3a246e0dc1, current schema\nINFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -> 1507a7289a2f, create is_encrypted\n/home/xiaosi/.local/lib/python2.7/site-packages/alembic/util/messaging.py:69: UserWarning: Skipping unsupported ALTER for creation of implicit constraint\n  warnings.warn(msg)\nINFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -> 13eb55f81627, maintain history for compatibility with earlier migrations\nINFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -> 338e90f54d61, More logging into task_isntance\nINFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -> 52d714495f0, job_id indices\nINFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -> 502898887f84, Adding extra to Log\nINFO  [alembic.runtime.migration] Running upgrade 502898887f84 -> 1b38cef5b76e, add dagrun\nINFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -> 2e541a1dcfed, task_duration\nINFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -> 40e67319e3a9, dagrun_config\nINFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -> 561833c1c74b, add password column to user\nINFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -> 4446e08588, dagrun start end\nINFO  [alembic.runtime.migration] Running upgrade 4446e08588 -> bbc73705a13e, Add notification_sent column to sla_miss\nINFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -> bba5a7cfc896, Add a column to track the encryption state of the 'Extra' field in connection\nINFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -> 1968acfc09e3, add is_encrypted column to variable table\nINFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -> 2e82aab8ef20, rename user table\nINFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -> 211e584da130, add TI state index\nINFO  [alembic.runtime.migration] Running upgrade 211e584da130 -> 64de9cddf6c9, add task fails journal table\nINFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -> f2ca10b85618, add dag_stats table\nINFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -> 4addfa1236f1, Add fractional seconds to mysql tables\nINFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -> 8504051e801b, xcom dag task indices\nINFO  [alembic.runtime.migration] Running upgrade 8504051e801b -> 5e7d17757c7a, add pid field to TaskInstance\nINFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -> 127d2bf2dfa7, Add dag_id/state index on dag_run table\nDone.\n```\n运行上述命令之后，会在$AIRFLOW_HOME目录下生成如下文件:\n```\nxiaosi@yoona:~/opt/airflow$ ll\n总用量 88\ndrwxrwxr-x  2 xiaosi xiaosi  4096  8月  2 16:39 ./\ndrwxrwxr-x 26 xiaosi xiaosi  4096  7月 31 13:56 ../\n-rw-rw-r--  1 xiaosi xiaosi 11424  8月  2 16:38 airflow.cfg\n-rw-r--r--  1 xiaosi xiaosi 58368  8月  2 16:39 airflow.db\n-rw-rw-r--  1 xiaosi xiaosi  1554  8月  2 16:38 unittests.cfg\n```\n### 4. 修改默认数据库\n\n找到`$AIRFLOW_HOME/airflow.cfg`配置文件，进行如下修改:\n```\nsql_alchemy_conn = mysql://root:root@localhost:3306/airflow\n```\n**备注**\n\n数据库用户名与密码均为`root`，airflow使用的数据库为`airflow`．使用如下命令创建对应的数据库:\n```\nmysql> create database airflow;\nQuery OK, 1 row affected (0.00 sec)\n```\n重新初始化服务器数据库:\n```\nxiaosi@yoona:~$ airflow initdb\n```\n出现了如下错误:\n```\nxiaosi@yoona:~$ airflow initdb\nTraceback (most recent call last):\n  File \"/home/xiaosi/.local/bin/airflow\", line 17, in <module>\n    from airflow import configuration\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/airflow/__init__.py\", line 30, in <module>\n    from airflow import settings\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py\", line 159, in <module>\n    configure_orm()\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py\", line 147, in configure_orm\n    engine = create_engine(SQL_ALCHEMY_CONN, **engine_args)\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py\", line 387, in create_engine\n    return strategy.create(*args, **kwargs)\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 80, in create\n    dbapi = dialect_cls.dbapi(**dbapi_args)\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 110, in dbapi\n    return __import__('MySQLdb')\nImportError: No module named MySQLdb\n```\n解决方案:\n\nMySQL是最流行的开源数据库之一，但在Python标准库中并没有集成MySQL接口程序，MySQLdb是一个第三方包，需独立下载并安装。\n```\nsudo apt-get install python-mysqldb\n```\n再次初始化:\n```\nxiaosi@yoona:~$ airflow initdb\n[2017-08-02 17:22:21,169] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 17:22:21,282] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 17:22:21,302] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\nDB: mysql://root:***@localhost:3306/airflow\n[2017-08-02 17:22:21,553] {db.py:287} INFO - Creating tables\nINFO  [alembic.runtime.migration] Context impl MySQLImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nINFO  [alembic.runtime.migration] Running upgrade  -> e3a246e0dc1, current schema\nINFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -> 1507a7289a2f, create is_encrypted\nINFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -> 13eb55f81627, maintain history for compatibility with earlier migrations\nINFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -> 338e90f54d61, More logging into task_isntance\nINFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -> 52d714495f0, job_id indices\nINFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -> 502898887f84, Adding extra to Log\nINFO  [alembic.runtime.migration] Running upgrade 502898887f84 -> 1b38cef5b76e, add dagrun\nINFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -> 2e541a1dcfed, task_duration\nINFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -> 40e67319e3a9, dagrun_config\nINFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -> 561833c1c74b, add password column to user\nINFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -> 4446e08588, dagrun start end\nINFO  [alembic.runtime.migration] Running upgrade 4446e08588 -> bbc73705a13e, Add notification_sent column to sla_miss\nINFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -> bba5a7cfc896, Add a column to track the encryption state of the 'Extra' field in connection\nINFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -> 1968acfc09e3, add is_encrypted column to variable table\nINFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -> 2e82aab8ef20, rename user table\nINFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -> 211e584da130, add TI state index\nINFO  [alembic.runtime.migration] Running upgrade 211e584da130 -> 64de9cddf6c9, add task fails journal table\nINFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -> f2ca10b85618, add dag_stats table\nINFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -> 4addfa1236f1, Add fractional seconds to mysql tables\nINFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -> 8504051e801b, xcom dag task indices\nINFO  [alembic.runtime.migration] Running upgrade 8504051e801b -> 5e7d17757c7a, add pid field to TaskInstance\nINFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -> 127d2bf2dfa7, Add dag_id/state index on dag_run table\nDone.\n```\n查看一下airflow数据库中做了哪些操作:\n```\nmysql> use airflow;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> show tables;\n+-------------------+\n| Tables_in_airflow |\n+-------------------+\n| alembic_version   |\n| chart             |\n| connection        |\n| dag               |\n| dag_pickle        |\n| dag_run           |\n| dag_stats         |\n| import_error      |\n| job               |\n| known_event       |\n| known_event_type  |\n| log               |\n| sla_miss          |\n| slot_pool         |\n| task_fail         |\n| task_instance     |\n| users             |\n| variable          |\n| xcom              |\n+-------------------+\n19 rows in set (0.00 sec)\n```\n### 5. 启动\n通过如下命令就可以启动后台管理界面，默认访问localhost:8080即可:\n```\nxiaosi@yoona:~$ airflow webserver\n[2017-08-02 17:25:31,961] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 17:25:32,075] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 17:25:32,095] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\n  ____________       _____________\n ____    |__( )_________  __/__  /________      __\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n\n/home/xiaosi/.local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.\n  .format(x=modname), ExtDeprecationWarning\n[2017-08-02 17:25:32,469] [9703] {models.py:167} INFO - Filling up the DagBag from /home/xiaosi/opt/airflow/dags\nRunning the Gunicorn Server with:\nWorkers: 4 sync\nHost: 0.0.0.0:8080\nTimeout: 120\nLogfiles: - -\n=================================================================            \n[2017-08-02 17:25:33,052] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 17:25:33,156] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 17:25:33,179] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\n[2017-08-02 17:25:33 +0000] [9706] [INFO] Starting gunicorn 19.3.0\n[2017-08-02 17:25:33 +0000] [9706] [INFO] Listening at: http://0.0.0.0:8080 (9706)\n[2017-08-02 17:25:33 +0000] [9706] [INFO] Using worker: sync\n...\n```\n\n可以访问 http://localhost:8080/admin/ , 呈现出的主界面如下:\n![img](http://img.blog.csdn.net/20170802173129848?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n","source":"_posts/Airflow/[AirFlow]AirFlow使用指南一 安装与启动.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Airflow使用指南一 安装与启动\ndate: 2017-12-29 12:55:01\ntags:\n  - Airflow\n\ncategories: Airflow\n---\n\n### 1. 安装\n\n通过pip安装:\n```\nxiaosi@yoona:~$ pip install airflow\n```\n如果速度比较慢，可以使用下面提供的源进行安装:\n```\nxiaosi@yoona:~$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple airflow\n```\n如果出现下面提示，表示你的airflow安装成功了:\n```\nSuccessfully installed airflow alembic croniter dill flask flask-admin flask-cache flask-login flask-swagger flask-wtf funcsigs future gitpython gunicorn jinja2 lxml markdown pandas psutil pygments python-daemon python-dateutil python-nvd3 requests setproctitle sqlalchemy tabulate thrift zope.deprecation Mako python-editor click itsdangerous Werkzeug wtforms PyYAML ordereddict gitdb2 MarkupSafe pytz numpy docutils setuptools lockfile six python-slugify idna urllib3 certifi chardet smmap2 Unidecode\nCleaning up...\n```\n安装完成之后我的默认安装在`~/.local/bin`目录下\n\n### 2. 配置\n\n如果不修改路径，默认的配置为`~/airflow`\n\n永久修改环境变量\n```\necho \"export AIRFLOW_HOME=/home/xiaosi/opt/airflow\" >> /etc/profile\nsource /etc/profile\n```\n为了便于操作方便，进行如下配置:\n```\necho \"export PATH=/home/xiaosi/.local/bin:$PATH\" >> /etc/profile\nsource /etc/profile\n```\n\n### 3. 初始化\n\n初始化数据库:\n```\nxiaosi@yoona:~$ airflow initdb\n[2017-08-02 16:39:22,319] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 16:39:22,432] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 16:39:22,451] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\nDB: sqlite:////home/xiaosi/opt/airflow/airflow.db\n[2017-08-02 16:39:22,708] {db.py:287} INFO - Creating tables\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nINFO  [alembic.runtime.migration] Running upgrade  -> e3a246e0dc1, current schema\nINFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -> 1507a7289a2f, create is_encrypted\n/home/xiaosi/.local/lib/python2.7/site-packages/alembic/util/messaging.py:69: UserWarning: Skipping unsupported ALTER for creation of implicit constraint\n  warnings.warn(msg)\nINFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -> 13eb55f81627, maintain history for compatibility with earlier migrations\nINFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -> 338e90f54d61, More logging into task_isntance\nINFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -> 52d714495f0, job_id indices\nINFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -> 502898887f84, Adding extra to Log\nINFO  [alembic.runtime.migration] Running upgrade 502898887f84 -> 1b38cef5b76e, add dagrun\nINFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -> 2e541a1dcfed, task_duration\nINFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -> 40e67319e3a9, dagrun_config\nINFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -> 561833c1c74b, add password column to user\nINFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -> 4446e08588, dagrun start end\nINFO  [alembic.runtime.migration] Running upgrade 4446e08588 -> bbc73705a13e, Add notification_sent column to sla_miss\nINFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -> bba5a7cfc896, Add a column to track the encryption state of the 'Extra' field in connection\nINFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -> 1968acfc09e3, add is_encrypted column to variable table\nINFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -> 2e82aab8ef20, rename user table\nINFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -> 211e584da130, add TI state index\nINFO  [alembic.runtime.migration] Running upgrade 211e584da130 -> 64de9cddf6c9, add task fails journal table\nINFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -> f2ca10b85618, add dag_stats table\nINFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -> 4addfa1236f1, Add fractional seconds to mysql tables\nINFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -> 8504051e801b, xcom dag task indices\nINFO  [alembic.runtime.migration] Running upgrade 8504051e801b -> 5e7d17757c7a, add pid field to TaskInstance\nINFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -> 127d2bf2dfa7, Add dag_id/state index on dag_run table\nDone.\n```\n运行上述命令之后，会在$AIRFLOW_HOME目录下生成如下文件:\n```\nxiaosi@yoona:~/opt/airflow$ ll\n总用量 88\ndrwxrwxr-x  2 xiaosi xiaosi  4096  8月  2 16:39 ./\ndrwxrwxr-x 26 xiaosi xiaosi  4096  7月 31 13:56 ../\n-rw-rw-r--  1 xiaosi xiaosi 11424  8月  2 16:38 airflow.cfg\n-rw-r--r--  1 xiaosi xiaosi 58368  8月  2 16:39 airflow.db\n-rw-rw-r--  1 xiaosi xiaosi  1554  8月  2 16:38 unittests.cfg\n```\n### 4. 修改默认数据库\n\n找到`$AIRFLOW_HOME/airflow.cfg`配置文件，进行如下修改:\n```\nsql_alchemy_conn = mysql://root:root@localhost:3306/airflow\n```\n**备注**\n\n数据库用户名与密码均为`root`，airflow使用的数据库为`airflow`．使用如下命令创建对应的数据库:\n```\nmysql> create database airflow;\nQuery OK, 1 row affected (0.00 sec)\n```\n重新初始化服务器数据库:\n```\nxiaosi@yoona:~$ airflow initdb\n```\n出现了如下错误:\n```\nxiaosi@yoona:~$ airflow initdb\nTraceback (most recent call last):\n  File \"/home/xiaosi/.local/bin/airflow\", line 17, in <module>\n    from airflow import configuration\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/airflow/__init__.py\", line 30, in <module>\n    from airflow import settings\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py\", line 159, in <module>\n    configure_orm()\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py\", line 147, in configure_orm\n    engine = create_engine(SQL_ALCHEMY_CONN, **engine_args)\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py\", line 387, in create_engine\n    return strategy.create(*args, **kwargs)\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 80, in create\n    dbapi = dialect_cls.dbapi(**dbapi_args)\n  File \"/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 110, in dbapi\n    return __import__('MySQLdb')\nImportError: No module named MySQLdb\n```\n解决方案:\n\nMySQL是最流行的开源数据库之一，但在Python标准库中并没有集成MySQL接口程序，MySQLdb是一个第三方包，需独立下载并安装。\n```\nsudo apt-get install python-mysqldb\n```\n再次初始化:\n```\nxiaosi@yoona:~$ airflow initdb\n[2017-08-02 17:22:21,169] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 17:22:21,282] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 17:22:21,302] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\nDB: mysql://root:***@localhost:3306/airflow\n[2017-08-02 17:22:21,553] {db.py:287} INFO - Creating tables\nINFO  [alembic.runtime.migration] Context impl MySQLImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nINFO  [alembic.runtime.migration] Running upgrade  -> e3a246e0dc1, current schema\nINFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -> 1507a7289a2f, create is_encrypted\nINFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -> 13eb55f81627, maintain history for compatibility with earlier migrations\nINFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -> 338e90f54d61, More logging into task_isntance\nINFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -> 52d714495f0, job_id indices\nINFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -> 502898887f84, Adding extra to Log\nINFO  [alembic.runtime.migration] Running upgrade 502898887f84 -> 1b38cef5b76e, add dagrun\nINFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -> 2e541a1dcfed, task_duration\nINFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -> 40e67319e3a9, dagrun_config\nINFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -> 561833c1c74b, add password column to user\nINFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -> 4446e08588, dagrun start end\nINFO  [alembic.runtime.migration] Running upgrade 4446e08588 -> bbc73705a13e, Add notification_sent column to sla_miss\nINFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -> bba5a7cfc896, Add a column to track the encryption state of the 'Extra' field in connection\nINFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -> 1968acfc09e3, add is_encrypted column to variable table\nINFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -> 2e82aab8ef20, rename user table\nINFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -> 211e584da130, add TI state index\nINFO  [alembic.runtime.migration] Running upgrade 211e584da130 -> 64de9cddf6c9, add task fails journal table\nINFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -> f2ca10b85618, add dag_stats table\nINFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -> 4addfa1236f1, Add fractional seconds to mysql tables\nINFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -> 8504051e801b, xcom dag task indices\nINFO  [alembic.runtime.migration] Running upgrade 8504051e801b -> 5e7d17757c7a, add pid field to TaskInstance\nINFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -> 127d2bf2dfa7, Add dag_id/state index on dag_run table\nDone.\n```\n查看一下airflow数据库中做了哪些操作:\n```\nmysql> use airflow;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> show tables;\n+-------------------+\n| Tables_in_airflow |\n+-------------------+\n| alembic_version   |\n| chart             |\n| connection        |\n| dag               |\n| dag_pickle        |\n| dag_run           |\n| dag_stats         |\n| import_error      |\n| job               |\n| known_event       |\n| known_event_type  |\n| log               |\n| sla_miss          |\n| slot_pool         |\n| task_fail         |\n| task_instance     |\n| users             |\n| variable          |\n| xcom              |\n+-------------------+\n19 rows in set (0.00 sec)\n```\n### 5. 启动\n通过如下命令就可以启动后台管理界面，默认访问localhost:8080即可:\n```\nxiaosi@yoona:~$ airflow webserver\n[2017-08-02 17:25:31,961] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 17:25:32,075] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 17:25:32,095] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\n  ____________       _____________\n ____    |__( )_________  __/__  /________      __\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n\n/home/xiaosi/.local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.\n  .format(x=modname), ExtDeprecationWarning\n[2017-08-02 17:25:32,469] [9703] {models.py:167} INFO - Filling up the DagBag from /home/xiaosi/opt/airflow/dags\nRunning the Gunicorn Server with:\nWorkers: 4 sync\nHost: 0.0.0.0:8080\nTimeout: 120\nLogfiles: - -\n=================================================================            \n[2017-08-02 17:25:33,052] {__init__.py:57} INFO - Using executor SequentialExecutor\n[2017-08-02 17:25:33,156] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt\n[2017-08-02 17:25:33,179] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt\n[2017-08-02 17:25:33 +0000] [9706] [INFO] Starting gunicorn 19.3.0\n[2017-08-02 17:25:33 +0000] [9706] [INFO] Listening at: http://0.0.0.0:8080 (9706)\n[2017-08-02 17:25:33 +0000] [9706] [INFO] Using worker: sync\n...\n```\n\n可以访问 http://localhost:8080/admin/ , 呈现出的主界面如下:\n![img](http://img.blog.csdn.net/20170802173129848?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n","slug":"Airflow/[AirFlow]AirFlow使用指南一 安装与启动","published":1,"updated":"2018-01-29T09:36:59.660Z","comments":1,"photos":[],"link":"","_id":"cje58tipa0005ordby9rg5od4","content":"<h3 id=\"1-安装\"><a href=\"#1-安装\" class=\"headerlink\" title=\"1. 安装\"></a>1. 安装</h3><p>通过pip安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ pip install airflow</span><br></pre></td></tr></table></figure></p>\n<p>如果速度比较慢，可以使用下面提供的源进行安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple airflow</span><br></pre></td></tr></table></figure></p>\n<p>如果出现下面提示，表示你的airflow安装成功了:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Successfully installed airflow alembic croniter dill flask flask-admin flask-cache flask-login flask-swagger flask-wtf funcsigs future gitpython gunicorn jinja2 lxml markdown pandas psutil pygments python-daemon python-dateutil python-nvd3 requests setproctitle sqlalchemy tabulate thrift zope.deprecation Mako python-editor click itsdangerous Werkzeug wtforms PyYAML ordereddict gitdb2 MarkupSafe pytz numpy docutils setuptools lockfile six python-slugify idna urllib3 certifi chardet smmap2 Unidecode</span><br><span class=\"line\">Cleaning up...</span><br></pre></td></tr></table></figure></p>\n<p>安装完成之后我的默认安装在<code>~/.local/bin</code>目录下</p>\n<h3 id=\"2-配置\"><a href=\"#2-配置\" class=\"headerlink\" title=\"2. 配置\"></a>2. 配置</h3><p>如果不修改路径，默认的配置为<code>~/airflow</code></p>\n<p>永久修改环境变量<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">echo &quot;export AIRFLOW_HOME=/home/xiaosi/opt/airflow&quot; &gt;&gt; /etc/profile</span><br><span class=\"line\">source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p>为了便于操作方便，进行如下配置:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">echo &quot;export PATH=/home/xiaosi/.local/bin:$PATH&quot; &gt;&gt; /etc/profile</span><br><span class=\"line\">source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-初始化\"><a href=\"#3-初始化\" class=\"headerlink\" title=\"3. 初始化\"></a>3. 初始化</h3><p>初始化数据库:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br><span class=\"line\">[2017-08-02 16:39:22,319] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 16:39:22,432] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 16:39:22,451] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">DB: sqlite:////home/xiaosi/opt/airflow/airflow.db</span><br><span class=\"line\">[2017-08-02 16:39:22,708] &#123;db.py:287&#125; INFO - Creating tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Context impl SQLiteImpl.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Will assume non-transactional DDL.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade  -&gt; e3a246e0dc1, current schema</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encrypted</span><br><span class=\"line\">/home/xiaosi/.local/lib/python2.7/site-packages/alembic/util/messaging.py:69: UserWarning: Skipping unsupported ALTER for creation of implicit constraint</span><br><span class=\"line\">  warnings.warn(msg)</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrations</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to Log</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrun</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_duration</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_config</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to user</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start end</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_miss</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connection</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state index</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run table</span><br><span class=\"line\">Done.</span><br></pre></td></tr></table></figure></p>\n<p>运行上述命令之后，会在$AIRFLOW_HOME目录下生成如下文件:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/airflow$ ll</span><br><span class=\"line\">总用量 88</span><br><span class=\"line\">drwxrwxr-x  2 xiaosi xiaosi  4096  8月  2 16:39 ./</span><br><span class=\"line\">drwxrwxr-x 26 xiaosi xiaosi  4096  7月 31 13:56 ../</span><br><span class=\"line\">-rw-rw-r--  1 xiaosi xiaosi 11424  8月  2 16:38 airflow.cfg</span><br><span class=\"line\">-rw-r--r--  1 xiaosi xiaosi 58368  8月  2 16:39 airflow.db</span><br><span class=\"line\">-rw-rw-r--  1 xiaosi xiaosi  1554  8月  2 16:38 unittests.cfg</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-修改默认数据库\"><a href=\"#4-修改默认数据库\" class=\"headerlink\" title=\"4. 修改默认数据库\"></a>4. 修改默认数据库</h3><p>找到<code>$AIRFLOW_HOME/airflow.cfg</code>配置文件，进行如下修改:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sql_alchemy_conn = mysql://root:root@localhost:3306/airflow</span><br></pre></td></tr></table></figure></p>\n<p><strong>备注</strong></p>\n<p>数据库用户名与密码均为<code>root</code>，airflow使用的数据库为<code>airflow</code>．使用如下命令创建对应的数据库:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; create database airflow;</span><br><span class=\"line\">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>重新初始化服务器数据库:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br></pre></td></tr></table></figure></p>\n<p>出现了如下错误:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/bin/airflow&quot;, line 17, in &lt;module&gt;</span><br><span class=\"line\">    from airflow import configuration</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/__init__.py&quot;, line 30, in &lt;module&gt;</span><br><span class=\"line\">    from airflow import settings</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 159, in &lt;module&gt;</span><br><span class=\"line\">    configure_orm()</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 147, in configure_orm</span><br><span class=\"line\">    engine = create_engine(SQL_ALCHEMY_CONN, **engine_args)</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py&quot;, line 387, in create_engine</span><br><span class=\"line\">    return strategy.create(*args, **kwargs)</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py&quot;, line 80, in create</span><br><span class=\"line\">    dbapi = dialect_cls.dbapi(**dbapi_args)</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py&quot;, line 110, in dbapi</span><br><span class=\"line\">    return __import__(&apos;MySQLdb&apos;)</span><br><span class=\"line\">ImportError: No module named MySQLdb</span><br></pre></td></tr></table></figure></p>\n<p>解决方案:</p>\n<p>MySQL是最流行的开源数据库之一，但在Python标准库中并没有集成MySQL接口程序，MySQLdb是一个第三方包，需独立下载并安装。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo apt-get install python-mysqldb</span><br></pre></td></tr></table></figure></p>\n<p>再次初始化:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br><span class=\"line\">[2017-08-02 17:22:21,169] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 17:22:21,282] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 17:22:21,302] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">DB: mysql://root:***@localhost:3306/airflow</span><br><span class=\"line\">[2017-08-02 17:22:21,553] &#123;db.py:287&#125; INFO - Creating tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Context impl MySQLImpl.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Will assume non-transactional DDL.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade  -&gt; e3a246e0dc1, current schema</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encrypted</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrations</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to Log</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrun</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_duration</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_config</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to user</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start end</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_miss</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connection</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state index</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run table</span><br><span class=\"line\">Done.</span><br></pre></td></tr></table></figure></p>\n<p>查看一下airflow数据库中做了哪些操作:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; use airflow;</span><br><span class=\"line\">Reading table information for completion of table and column names</span><br><span class=\"line\">You can turn off this feature to get a quicker startup with -A</span><br><span class=\"line\"></span><br><span class=\"line\">Database changed</span><br><span class=\"line\">mysql&gt; show tables;</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| Tables_in_airflow |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| alembic_version   |</span><br><span class=\"line\">| chart             |</span><br><span class=\"line\">| connection        |</span><br><span class=\"line\">| dag               |</span><br><span class=\"line\">| dag_pickle        |</span><br><span class=\"line\">| dag_run           |</span><br><span class=\"line\">| dag_stats         |</span><br><span class=\"line\">| import_error      |</span><br><span class=\"line\">| job               |</span><br><span class=\"line\">| known_event       |</span><br><span class=\"line\">| known_event_type  |</span><br><span class=\"line\">| log               |</span><br><span class=\"line\">| sla_miss          |</span><br><span class=\"line\">| slot_pool         |</span><br><span class=\"line\">| task_fail         |</span><br><span class=\"line\">| task_instance     |</span><br><span class=\"line\">| users             |</span><br><span class=\"line\">| variable          |</span><br><span class=\"line\">| xcom              |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">19 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-启动\"><a href=\"#5-启动\" class=\"headerlink\" title=\"5. 启动\"></a>5. 启动</h3><p>通过如下命令就可以启动后台管理界面，默认访问localhost:8080即可:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow webserver</span><br><span class=\"line\">[2017-08-02 17:25:31,961] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 17:25:32,075] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 17:25:32,095] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">  ____________       _____________</span><br><span class=\"line\"> ____    |__( )_________  __/__  /________      __</span><br><span class=\"line\">____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /</span><br><span class=\"line\">___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /</span><br><span class=\"line\"> _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/</span><br><span class=\"line\"></span><br><span class=\"line\">/home/xiaosi/.local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.</span><br><span class=\"line\">  .format(x=modname), ExtDeprecationWarning</span><br><span class=\"line\">[2017-08-02 17:25:32,469] [9703] &#123;models.py:167&#125; INFO - Filling up the DagBag from /home/xiaosi/opt/airflow/dags</span><br><span class=\"line\">Running the Gunicorn Server with:</span><br><span class=\"line\">Workers: 4 sync</span><br><span class=\"line\">Host: 0.0.0.0:8080</span><br><span class=\"line\">Timeout: 120</span><br><span class=\"line\">Logfiles: - -</span><br><span class=\"line\">=================================================================            </span><br><span class=\"line\">[2017-08-02 17:25:33,052] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 17:25:33,156] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 17:25:33,179] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">[2017-08-02 17:25:33 +0000] [9706] [INFO] Starting gunicorn 19.3.0</span><br><span class=\"line\">[2017-08-02 17:25:33 +0000] [9706] [INFO] Listening at: http://0.0.0.0:8080 (9706)</span><br><span class=\"line\">[2017-08-02 17:25:33 +0000] [9706] [INFO] Using worker: sync</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>可以访问 <a href=\"http://localhost:8080/admin/\" target=\"_blank\" rel=\"noopener\">http://localhost:8080/admin/</a> , 呈现出的主界面如下:<br><img src=\"http://img.blog.csdn.net/20170802173129848?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"img\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-安装\"><a href=\"#1-安装\" class=\"headerlink\" title=\"1. 安装\"></a>1. 安装</h3><p>通过pip安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ pip install airflow</span><br></pre></td></tr></table></figure></p>\n<p>如果速度比较慢，可以使用下面提供的源进行安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple airflow</span><br></pre></td></tr></table></figure></p>\n<p>如果出现下面提示，表示你的airflow安装成功了:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Successfully installed airflow alembic croniter dill flask flask-admin flask-cache flask-login flask-swagger flask-wtf funcsigs future gitpython gunicorn jinja2 lxml markdown pandas psutil pygments python-daemon python-dateutil python-nvd3 requests setproctitle sqlalchemy tabulate thrift zope.deprecation Mako python-editor click itsdangerous Werkzeug wtforms PyYAML ordereddict gitdb2 MarkupSafe pytz numpy docutils setuptools lockfile six python-slugify idna urllib3 certifi chardet smmap2 Unidecode</span><br><span class=\"line\">Cleaning up...</span><br></pre></td></tr></table></figure></p>\n<p>安装完成之后我的默认安装在<code>~/.local/bin</code>目录下</p>\n<h3 id=\"2-配置\"><a href=\"#2-配置\" class=\"headerlink\" title=\"2. 配置\"></a>2. 配置</h3><p>如果不修改路径，默认的配置为<code>~/airflow</code></p>\n<p>永久修改环境变量<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">echo &quot;export AIRFLOW_HOME=/home/xiaosi/opt/airflow&quot; &gt;&gt; /etc/profile</span><br><span class=\"line\">source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p>为了便于操作方便，进行如下配置:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">echo &quot;export PATH=/home/xiaosi/.local/bin:$PATH&quot; &gt;&gt; /etc/profile</span><br><span class=\"line\">source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-初始化\"><a href=\"#3-初始化\" class=\"headerlink\" title=\"3. 初始化\"></a>3. 初始化</h3><p>初始化数据库:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br><span class=\"line\">[2017-08-02 16:39:22,319] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 16:39:22,432] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 16:39:22,451] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">DB: sqlite:////home/xiaosi/opt/airflow/airflow.db</span><br><span class=\"line\">[2017-08-02 16:39:22,708] &#123;db.py:287&#125; INFO - Creating tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Context impl SQLiteImpl.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Will assume non-transactional DDL.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade  -&gt; e3a246e0dc1, current schema</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encrypted</span><br><span class=\"line\">/home/xiaosi/.local/lib/python2.7/site-packages/alembic/util/messaging.py:69: UserWarning: Skipping unsupported ALTER for creation of implicit constraint</span><br><span class=\"line\">  warnings.warn(msg)</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrations</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to Log</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrun</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_duration</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_config</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to user</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start end</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_miss</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connection</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state index</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run table</span><br><span class=\"line\">Done.</span><br></pre></td></tr></table></figure></p>\n<p>运行上述命令之后，会在$AIRFLOW_HOME目录下生成如下文件:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/airflow$ ll</span><br><span class=\"line\">总用量 88</span><br><span class=\"line\">drwxrwxr-x  2 xiaosi xiaosi  4096  8月  2 16:39 ./</span><br><span class=\"line\">drwxrwxr-x 26 xiaosi xiaosi  4096  7月 31 13:56 ../</span><br><span class=\"line\">-rw-rw-r--  1 xiaosi xiaosi 11424  8月  2 16:38 airflow.cfg</span><br><span class=\"line\">-rw-r--r--  1 xiaosi xiaosi 58368  8月  2 16:39 airflow.db</span><br><span class=\"line\">-rw-rw-r--  1 xiaosi xiaosi  1554  8月  2 16:38 unittests.cfg</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-修改默认数据库\"><a href=\"#4-修改默认数据库\" class=\"headerlink\" title=\"4. 修改默认数据库\"></a>4. 修改默认数据库</h3><p>找到<code>$AIRFLOW_HOME/airflow.cfg</code>配置文件，进行如下修改:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sql_alchemy_conn = mysql://root:root@localhost:3306/airflow</span><br></pre></td></tr></table></figure></p>\n<p><strong>备注</strong></p>\n<p>数据库用户名与密码均为<code>root</code>，airflow使用的数据库为<code>airflow</code>．使用如下命令创建对应的数据库:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; create database airflow;</span><br><span class=\"line\">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>重新初始化服务器数据库:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br></pre></td></tr></table></figure></p>\n<p>出现了如下错误:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/bin/airflow&quot;, line 17, in &lt;module&gt;</span><br><span class=\"line\">    from airflow import configuration</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/__init__.py&quot;, line 30, in &lt;module&gt;</span><br><span class=\"line\">    from airflow import settings</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 159, in &lt;module&gt;</span><br><span class=\"line\">    configure_orm()</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/airflow/settings.py&quot;, line 147, in configure_orm</span><br><span class=\"line\">    engine = create_engine(SQL_ALCHEMY_CONN, **engine_args)</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py&quot;, line 387, in create_engine</span><br><span class=\"line\">    return strategy.create(*args, **kwargs)</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py&quot;, line 80, in create</span><br><span class=\"line\">    dbapi = dialect_cls.dbapi(**dbapi_args)</span><br><span class=\"line\">  File &quot;/home/xiaosi/.local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py&quot;, line 110, in dbapi</span><br><span class=\"line\">    return __import__(&apos;MySQLdb&apos;)</span><br><span class=\"line\">ImportError: No module named MySQLdb</span><br></pre></td></tr></table></figure></p>\n<p>解决方案:</p>\n<p>MySQL是最流行的开源数据库之一，但在Python标准库中并没有集成MySQL接口程序，MySQLdb是一个第三方包，需独立下载并安装。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo apt-get install python-mysqldb</span><br></pre></td></tr></table></figure></p>\n<p>再次初始化:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow initdb</span><br><span class=\"line\">[2017-08-02 17:22:21,169] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 17:22:21,282] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 17:22:21,302] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">DB: mysql://root:***@localhost:3306/airflow</span><br><span class=\"line\">[2017-08-02 17:22:21,553] &#123;db.py:287&#125; INFO - Creating tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Context impl MySQLImpl.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Will assume non-transactional DDL.</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade  -&gt; e3a246e0dc1, current schema</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -&gt; 1507a7289a2f, create is_encrypted</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -&gt; 13eb55f81627, maintain history for compatibility with earlier migrations</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -&gt; 338e90f54d61, More logging into task_isntance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -&gt; 52d714495f0, job_id indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -&gt; 502898887f84, Adding extra to Log</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 502898887f84 -&gt; 1b38cef5b76e, add dagrun</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -&gt; 2e541a1dcfed, task_duration</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -&gt; 40e67319e3a9, dagrun_config</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -&gt; 561833c1c74b, add password column to user</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -&gt; 4446e08588, dagrun start end</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4446e08588 -&gt; bbc73705a13e, Add notification_sent column to sla_miss</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -&gt; bba5a7cfc896, Add a column to track the encryption state of the &apos;Extra&apos; field in connection</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -&gt; 1968acfc09e3, add is_encrypted column to variable table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -&gt; 2e82aab8ef20, rename user table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -&gt; 211e584da130, add TI state index</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 211e584da130 -&gt; 64de9cddf6c9, add task fails journal table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -&gt; f2ca10b85618, add dag_stats table</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -&gt; 4addfa1236f1, Add fractional seconds to mysql tables</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -&gt; 8504051e801b, xcom dag task indices</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 8504051e801b -&gt; 5e7d17757c7a, add pid field to TaskInstance</span><br><span class=\"line\">INFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -&gt; 127d2bf2dfa7, Add dag_id/state index on dag_run table</span><br><span class=\"line\">Done.</span><br></pre></td></tr></table></figure></p>\n<p>查看一下airflow数据库中做了哪些操作:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; use airflow;</span><br><span class=\"line\">Reading table information for completion of table and column names</span><br><span class=\"line\">You can turn off this feature to get a quicker startup with -A</span><br><span class=\"line\"></span><br><span class=\"line\">Database changed</span><br><span class=\"line\">mysql&gt; show tables;</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| Tables_in_airflow |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">| alembic_version   |</span><br><span class=\"line\">| chart             |</span><br><span class=\"line\">| connection        |</span><br><span class=\"line\">| dag               |</span><br><span class=\"line\">| dag_pickle        |</span><br><span class=\"line\">| dag_run           |</span><br><span class=\"line\">| dag_stats         |</span><br><span class=\"line\">| import_error      |</span><br><span class=\"line\">| job               |</span><br><span class=\"line\">| known_event       |</span><br><span class=\"line\">| known_event_type  |</span><br><span class=\"line\">| log               |</span><br><span class=\"line\">| sla_miss          |</span><br><span class=\"line\">| slot_pool         |</span><br><span class=\"line\">| task_fail         |</span><br><span class=\"line\">| task_instance     |</span><br><span class=\"line\">| users             |</span><br><span class=\"line\">| variable          |</span><br><span class=\"line\">| xcom              |</span><br><span class=\"line\">+-------------------+</span><br><span class=\"line\">19 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-启动\"><a href=\"#5-启动\" class=\"headerlink\" title=\"5. 启动\"></a>5. 启动</h3><p>通过如下命令就可以启动后台管理界面，默认访问localhost:8080即可:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ airflow webserver</span><br><span class=\"line\">[2017-08-02 17:25:31,961] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 17:25:32,075] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 17:25:32,095] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">  ____________       _____________</span><br><span class=\"line\"> ____    |__( )_________  __/__  /________      __</span><br><span class=\"line\">____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /</span><br><span class=\"line\">___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /</span><br><span class=\"line\"> _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/</span><br><span class=\"line\"></span><br><span class=\"line\">/home/xiaosi/.local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.</span><br><span class=\"line\">  .format(x=modname), ExtDeprecationWarning</span><br><span class=\"line\">[2017-08-02 17:25:32,469] [9703] &#123;models.py:167&#125; INFO - Filling up the DagBag from /home/xiaosi/opt/airflow/dags</span><br><span class=\"line\">Running the Gunicorn Server with:</span><br><span class=\"line\">Workers: 4 sync</span><br><span class=\"line\">Host: 0.0.0.0:8080</span><br><span class=\"line\">Timeout: 120</span><br><span class=\"line\">Logfiles: - -</span><br><span class=\"line\">=================================================================            </span><br><span class=\"line\">[2017-08-02 17:25:33,052] &#123;__init__.py:57&#125; INFO - Using executor SequentialExecutor</span><br><span class=\"line\">[2017-08-02 17:25:33,156] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt</span><br><span class=\"line\">[2017-08-02 17:25:33,179] &#123;driver.py:120&#125; INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt</span><br><span class=\"line\">[2017-08-02 17:25:33 +0000] [9706] [INFO] Starting gunicorn 19.3.0</span><br><span class=\"line\">[2017-08-02 17:25:33 +0000] [9706] [INFO] Listening at: http://0.0.0.0:8080 (9706)</span><br><span class=\"line\">[2017-08-02 17:25:33 +0000] [9706] [INFO] Using worker: sync</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>可以访问 <a href=\"http://localhost:8080/admin/\" target=\"_blank\" rel=\"noopener\">http://localhost:8080/admin/</a> , 呈现出的主界面如下:<br><img src=\"http://img.blog.csdn.net/20170802173129848?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"img\"></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink 1.4 分布式运行环境","date":"2018-01-03T03:03:01.000Z","_content":"\n### 1. 任务链与算子链\n\n在分布式运行中，`Flink`将算子(operator)子任务连接成任务。每个任务都只由一个线程执行。将运算符链接到任务中是一个很有用处的优化：它降低了线程间切换和缓冲的开销，并增加了整体吞吐量，同时降低了延迟。链接行为可以在API中配置。\n\n下图中的示例数据流由五个子任务执行，因此具有五个并行线程。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-1.png?raw=true)\n\n### 2. 作业管理器, 任务管理器, 客户端\n\nFlink运行时(`runtime`)由两种类型的进程组成：\n\n(1) 作业管理器`JobManagers`(也称为`masters`)协调分布式运行。主要功能是调度任务，协调检查点，协调故障恢复等。\n\n至少有一个`JobManager`。高可用配置下将有多个`JobManagers`，其中一个始终是领导者，其他都是备份。\n\n(2) 任务管理器`TaskManagers`(也称为`workers`)执行数据流中的任务(更具体地说是子任务)，并对数据流进行缓冲和交换。\n\n跟`JobManager`一样，也是至少有一个`TaskManager`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-2.png?raw=true)\n\n`JobManagers`和`TaskManagers`可以以不同方式启动：直接在机器上，在容器中，或者由像`YARN`这样的资源框架来管理。`TaskManagers`与`JobManagers`进行连接，来报告自己可用，并分配工作。\n\n客户端不是运行时和程序执行的一部分，而是用来准备数据流并将其发送到`JobManager`。之后，客户端可以断开连接或保持连接来接收进度报告。客户端作为触发执行的`Java`/`Scala`程序的一部分运行，或者在命令行中运行`./bin/flink`命令来运行....\n\n### 3. 任务槽与资源\n\n每个`worker`(`TaskManager`)都是一个`JVM`进程，可以在不同的线程中执行一个或多个子任务(译者注:一个任务有一个线程执行)。`worker`使用任务槽(至少一个)来控制`worker`能接受多少任务。\n\n每个任务槽代表`TaskManager`的一个固定资源子集。例如，一个拥有三个任务槽的`TaskManager`将为每个任务槽分配`1/3`的内存。资源任务槽化意味着子任务不会与其他作业中的子任务争夺内存，而是任务具有一定数量的保留托管内存。请注意，这里不会对`CPU`进行隔离。目前任务槽只分离任务的托管内存。\n\n通过调整任务槽的数量，用户可以定义子任务与其他子任务进行隔离。如果每个`TaskManager`只拥有一个任务槽意味着每个任务组都会在独立的`JVM`中运行(例如，可以在单独的容器中启动)。如果拥有多个任务槽意味着多个子任务共享同一个`JVM`。同一`JVM`中的任务共享`TCP`连接(通过多路复用)和心跳消息，他们也可以共享数据集和数据结构，从而降低单个任务的开销。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-3.png?raw=true)\n\n默认情况下，`Flink`允许子任务共享任务槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个任务槽可能会是一个完整的作业管道。允许任务槽共享有两个主要好处：\n\n(1) Flink集群所需的任务槽数与作业中使用的最高并行度数保持一致。不需要计算一个程序总共包含多少个任务(不同任务具有不同的并行度)。\n\n(2) 提高资源利用率。如果没有使用任务槽共享机制，那么非密集的`sour/map()`子任务就会与资源密集型`window`子任务阻塞一样多的资源。在我们的示例中，通过任务槽共享，将基本并行度从两个增加到六个，可以充分利用已分配的资源，同时确保繁重的子任务在`TaskManager`之间公平分配。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-4.png?raw=true)\n\n这些API还包括一个资源组机制，可以避免不合理的任务槽共享。\n\n根据经验来说，默认任务槽数应该设置为`CPU`核的数量。如果使用超线程技术，每个任务槽需要2个或更多的硬件线程上下文(With hyper-threading, each slot then takes 2 or more hardware thread contexts)。\n\n### 4. 后端状态\n\n键/值索引存储的确切数据结构取决于所选的后端状态。一个后端状态将数据存储在内存中`hash map`中，另一个后端状态使用`RocksDB`存储键/值。除了定义保存状态的数据结构之外，后端状态还实现了获取键/值状态的时间点快照逻辑并将该快照存储为检查点的一部分。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-5.png?raw=true)\n\n### 5. 保存点\n\n用`Data Stream API`编写的程序可以从保存点恢复执行。保存点允许更新你的程序和你的Flink集群，而不会丢失任何状态。\n\n保存点是手动触发的检查点，它会捕获程序的快照并将其写入后端状态。他们依赖于常规检查点机制。在执行期间的程序定期在工作节点上生成快照并生成检查点。为了恢复，只需要最后完成的检查点，一旦新的检查点完成，可以安全地丢弃较旧的检查点。\n\n保存点与这些定期检查点类似，只不过它们是由用户触发的，不会在新检查点完成时自动失效。\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html\n","source":"_posts/Flink/[Flink]Flink 分布式运行环境.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink 1.4 分布式运行环境\ndate: 2018-01-03 11:03:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n### 1. 任务链与算子链\n\n在分布式运行中，`Flink`将算子(operator)子任务连接成任务。每个任务都只由一个线程执行。将运算符链接到任务中是一个很有用处的优化：它降低了线程间切换和缓冲的开销，并增加了整体吞吐量，同时降低了延迟。链接行为可以在API中配置。\n\n下图中的示例数据流由五个子任务执行，因此具有五个并行线程。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-1.png?raw=true)\n\n### 2. 作业管理器, 任务管理器, 客户端\n\nFlink运行时(`runtime`)由两种类型的进程组成：\n\n(1) 作业管理器`JobManagers`(也称为`masters`)协调分布式运行。主要功能是调度任务，协调检查点，协调故障恢复等。\n\n至少有一个`JobManager`。高可用配置下将有多个`JobManagers`，其中一个始终是领导者，其他都是备份。\n\n(2) 任务管理器`TaskManagers`(也称为`workers`)执行数据流中的任务(更具体地说是子任务)，并对数据流进行缓冲和交换。\n\n跟`JobManager`一样，也是至少有一个`TaskManager`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-2.png?raw=true)\n\n`JobManagers`和`TaskManagers`可以以不同方式启动：直接在机器上，在容器中，或者由像`YARN`这样的资源框架来管理。`TaskManagers`与`JobManagers`进行连接，来报告自己可用，并分配工作。\n\n客户端不是运行时和程序执行的一部分，而是用来准备数据流并将其发送到`JobManager`。之后，客户端可以断开连接或保持连接来接收进度报告。客户端作为触发执行的`Java`/`Scala`程序的一部分运行，或者在命令行中运行`./bin/flink`命令来运行....\n\n### 3. 任务槽与资源\n\n每个`worker`(`TaskManager`)都是一个`JVM`进程，可以在不同的线程中执行一个或多个子任务(译者注:一个任务有一个线程执行)。`worker`使用任务槽(至少一个)来控制`worker`能接受多少任务。\n\n每个任务槽代表`TaskManager`的一个固定资源子集。例如，一个拥有三个任务槽的`TaskManager`将为每个任务槽分配`1/3`的内存。资源任务槽化意味着子任务不会与其他作业中的子任务争夺内存，而是任务具有一定数量的保留托管内存。请注意，这里不会对`CPU`进行隔离。目前任务槽只分离任务的托管内存。\n\n通过调整任务槽的数量，用户可以定义子任务与其他子任务进行隔离。如果每个`TaskManager`只拥有一个任务槽意味着每个任务组都会在独立的`JVM`中运行(例如，可以在单独的容器中启动)。如果拥有多个任务槽意味着多个子任务共享同一个`JVM`。同一`JVM`中的任务共享`TCP`连接(通过多路复用)和心跳消息，他们也可以共享数据集和数据结构，从而降低单个任务的开销。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-3.png?raw=true)\n\n默认情况下，`Flink`允许子任务共享任务槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个任务槽可能会是一个完整的作业管道。允许任务槽共享有两个主要好处：\n\n(1) Flink集群所需的任务槽数与作业中使用的最高并行度数保持一致。不需要计算一个程序总共包含多少个任务(不同任务具有不同的并行度)。\n\n(2) 提高资源利用率。如果没有使用任务槽共享机制，那么非密集的`sour/map()`子任务就会与资源密集型`window`子任务阻塞一样多的资源。在我们的示例中，通过任务槽共享，将基本并行度从两个增加到六个，可以充分利用已分配的资源，同时确保繁重的子任务在`TaskManager`之间公平分配。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-4.png?raw=true)\n\n这些API还包括一个资源组机制，可以避免不合理的任务槽共享。\n\n根据经验来说，默认任务槽数应该设置为`CPU`核的数量。如果使用超线程技术，每个任务槽需要2个或更多的硬件线程上下文(With hyper-threading, each slot then takes 2 or more hardware thread contexts)。\n\n### 4. 后端状态\n\n键/值索引存储的确切数据结构取决于所选的后端状态。一个后端状态将数据存储在内存中`hash map`中，另一个后端状态使用`RocksDB`存储键/值。除了定义保存状态的数据结构之外，后端状态还实现了获取键/值状态的时间点快照逻辑并将该快照存储为检查点的一部分。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-5.png?raw=true)\n\n### 5. 保存点\n\n用`Data Stream API`编写的程序可以从保存点恢复执行。保存点允许更新你的程序和你的Flink集群，而不会丢失任何状态。\n\n保存点是手动触发的检查点，它会捕获程序的快照并将其写入后端状态。他们依赖于常规检查点机制。在执行期间的程序定期在工作节点上生成快照并生成检查点。为了恢复，只需要最后完成的检查点，一旦新的检查点完成，可以安全地丢弃较旧的检查点。\n\n保存点与这些定期检查点类似，只不过它们是由用户触发的，不会在新检查点完成时自动失效。\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html\n","slug":"Flink/[Flink]Flink 分布式运行环境","published":1,"updated":"2018-01-29T09:36:59.659Z","comments":1,"photos":[],"link":"","_id":"cje58tipg0008ordbq03d3sc9","content":"<h3 id=\"1-任务链与算子链\"><a href=\"#1-任务链与算子链\" class=\"headerlink\" title=\"1. 任务链与算子链\"></a>1. 任务链与算子链</h3><p>在分布式运行中，<code>Flink</code>将算子(operator)子任务连接成任务。每个任务都只由一个线程执行。将运算符链接到任务中是一个很有用处的优化：它降低了线程间切换和缓冲的开销，并增加了整体吞吐量，同时降低了延迟。链接行为可以在API中配置。</p>\n<p>下图中的示例数据流由五个子任务执行，因此具有五个并行线程。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-作业管理器-任务管理器-客户端\"><a href=\"#2-作业管理器-任务管理器-客户端\" class=\"headerlink\" title=\"2. 作业管理器, 任务管理器, 客户端\"></a>2. 作业管理器, 任务管理器, 客户端</h3><p>Flink运行时(<code>runtime</code>)由两种类型的进程组成：</p>\n<p>(1) 作业管理器<code>JobManagers</code>(也称为<code>masters</code>)协调分布式运行。主要功能是调度任务，协调检查点，协调故障恢复等。</p>\n<p>至少有一个<code>JobManager</code>。高可用配置下将有多个<code>JobManagers</code>，其中一个始终是领导者，其他都是备份。</p>\n<p>(2) 任务管理器<code>TaskManagers</code>(也称为<code>workers</code>)执行数据流中的任务(更具体地说是子任务)，并对数据流进行缓冲和交换。</p>\n<p>跟<code>JobManager</code>一样，也是至少有一个<code>TaskManager</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-2.png?raw=true\" alt=\"\"></p>\n<p><code>JobManagers</code>和<code>TaskManagers</code>可以以不同方式启动：直接在机器上，在容器中，或者由像<code>YARN</code>这样的资源框架来管理。<code>TaskManagers</code>与<code>JobManagers</code>进行连接，来报告自己可用，并分配工作。</p>\n<p>客户端不是运行时和程序执行的一部分，而是用来准备数据流并将其发送到<code>JobManager</code>。之后，客户端可以断开连接或保持连接来接收进度报告。客户端作为触发执行的<code>Java</code>/<code>Scala</code>程序的一部分运行，或者在命令行中运行<code>./bin/flink</code>命令来运行….</p>\n<h3 id=\"3-任务槽与资源\"><a href=\"#3-任务槽与资源\" class=\"headerlink\" title=\"3. 任务槽与资源\"></a>3. 任务槽与资源</h3><p>每个<code>worker</code>(<code>TaskManager</code>)都是一个<code>JVM</code>进程，可以在不同的线程中执行一个或多个子任务(译者注:一个任务有一个线程执行)。<code>worker</code>使用任务槽(至少一个)来控制<code>worker</code>能接受多少任务。</p>\n<p>每个任务槽代表<code>TaskManager</code>的一个固定资源子集。例如，一个拥有三个任务槽的<code>TaskManager</code>将为每个任务槽分配<code>1/3</code>的内存。资源任务槽化意味着子任务不会与其他作业中的子任务争夺内存，而是任务具有一定数量的保留托管内存。请注意，这里不会对<code>CPU</code>进行隔离。目前任务槽只分离任务的托管内存。</p>\n<p>通过调整任务槽的数量，用户可以定义子任务与其他子任务进行隔离。如果每个<code>TaskManager</code>只拥有一个任务槽意味着每个任务组都会在独立的<code>JVM</code>中运行(例如，可以在单独的容器中启动)。如果拥有多个任务槽意味着多个子任务共享同一个<code>JVM</code>。同一<code>JVM</code>中的任务共享<code>TCP</code>连接(通过多路复用)和心跳消息，他们也可以共享数据集和数据结构，从而降低单个任务的开销。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-3.png?raw=true\" alt=\"\"></p>\n<p>默认情况下，<code>Flink</code>允许子任务共享任务槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个任务槽可能会是一个完整的作业管道。允许任务槽共享有两个主要好处：</p>\n<p>(1) Flink集群所需的任务槽数与作业中使用的最高并行度数保持一致。不需要计算一个程序总共包含多少个任务(不同任务具有不同的并行度)。</p>\n<p>(2) 提高资源利用率。如果没有使用任务槽共享机制，那么非密集的<code>sour/map()</code>子任务就会与资源密集型<code>window</code>子任务阻塞一样多的资源。在我们的示例中，通过任务槽共享，将基本并行度从两个增加到六个，可以充分利用已分配的资源，同时确保繁重的子任务在<code>TaskManager</code>之间公平分配。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-4.png?raw=true\" alt=\"\"></p>\n<p>这些API还包括一个资源组机制，可以避免不合理的任务槽共享。</p>\n<p>根据经验来说，默认任务槽数应该设置为<code>CPU</code>核的数量。如果使用超线程技术，每个任务槽需要2个或更多的硬件线程上下文(With hyper-threading, each slot then takes 2 or more hardware thread contexts)。</p>\n<h3 id=\"4-后端状态\"><a href=\"#4-后端状态\" class=\"headerlink\" title=\"4. 后端状态\"></a>4. 后端状态</h3><p>键/值索引存储的确切数据结构取决于所选的后端状态。一个后端状态将数据存储在内存中<code>hash map</code>中，另一个后端状态使用<code>RocksDB</code>存储键/值。除了定义保存状态的数据结构之外，后端状态还实现了获取键/值状态的时间点快照逻辑并将该快照存储为检查点的一部分。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"5-保存点\"><a href=\"#5-保存点\" class=\"headerlink\" title=\"5. 保存点\"></a>5. 保存点</h3><p>用<code>Data Stream API</code>编写的程序可以从保存点恢复执行。保存点允许更新你的程序和你的Flink集群，而不会丢失任何状态。</p>\n<p>保存点是手动触发的检查点，它会捕获程序的快照并将其写入后端状态。他们依赖于常规检查点机制。在执行期间的程序定期在工作节点上生成快照并生成检查点。为了恢复，只需要最后完成的检查点，一旦新的检查点完成，可以安全地丢弃较旧的检查点。</p>\n<p>保存点与这些定期检查点类似，只不过它们是由用户触发的，不会在新检查点完成时自动失效。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-任务链与算子链\"><a href=\"#1-任务链与算子链\" class=\"headerlink\" title=\"1. 任务链与算子链\"></a>1. 任务链与算子链</h3><p>在分布式运行中，<code>Flink</code>将算子(operator)子任务连接成任务。每个任务都只由一个线程执行。将运算符链接到任务中是一个很有用处的优化：它降低了线程间切换和缓冲的开销，并增加了整体吞吐量，同时降低了延迟。链接行为可以在API中配置。</p>\n<p>下图中的示例数据流由五个子任务执行，因此具有五个并行线程。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-作业管理器-任务管理器-客户端\"><a href=\"#2-作业管理器-任务管理器-客户端\" class=\"headerlink\" title=\"2. 作业管理器, 任务管理器, 客户端\"></a>2. 作业管理器, 任务管理器, 客户端</h3><p>Flink运行时(<code>runtime</code>)由两种类型的进程组成：</p>\n<p>(1) 作业管理器<code>JobManagers</code>(也称为<code>masters</code>)协调分布式运行。主要功能是调度任务，协调检查点，协调故障恢复等。</p>\n<p>至少有一个<code>JobManager</code>。高可用配置下将有多个<code>JobManagers</code>，其中一个始终是领导者，其他都是备份。</p>\n<p>(2) 任务管理器<code>TaskManagers</code>(也称为<code>workers</code>)执行数据流中的任务(更具体地说是子任务)，并对数据流进行缓冲和交换。</p>\n<p>跟<code>JobManager</code>一样，也是至少有一个<code>TaskManager</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-2.png?raw=true\" alt=\"\"></p>\n<p><code>JobManagers</code>和<code>TaskManagers</code>可以以不同方式启动：直接在机器上，在容器中，或者由像<code>YARN</code>这样的资源框架来管理。<code>TaskManagers</code>与<code>JobManagers</code>进行连接，来报告自己可用，并分配工作。</p>\n<p>客户端不是运行时和程序执行的一部分，而是用来准备数据流并将其发送到<code>JobManager</code>。之后，客户端可以断开连接或保持连接来接收进度报告。客户端作为触发执行的<code>Java</code>/<code>Scala</code>程序的一部分运行，或者在命令行中运行<code>./bin/flink</code>命令来运行….</p>\n<h3 id=\"3-任务槽与资源\"><a href=\"#3-任务槽与资源\" class=\"headerlink\" title=\"3. 任务槽与资源\"></a>3. 任务槽与资源</h3><p>每个<code>worker</code>(<code>TaskManager</code>)都是一个<code>JVM</code>进程，可以在不同的线程中执行一个或多个子任务(译者注:一个任务有一个线程执行)。<code>worker</code>使用任务槽(至少一个)来控制<code>worker</code>能接受多少任务。</p>\n<p>每个任务槽代表<code>TaskManager</code>的一个固定资源子集。例如，一个拥有三个任务槽的<code>TaskManager</code>将为每个任务槽分配<code>1/3</code>的内存。资源任务槽化意味着子任务不会与其他作业中的子任务争夺内存，而是任务具有一定数量的保留托管内存。请注意，这里不会对<code>CPU</code>进行隔离。目前任务槽只分离任务的托管内存。</p>\n<p>通过调整任务槽的数量，用户可以定义子任务与其他子任务进行隔离。如果每个<code>TaskManager</code>只拥有一个任务槽意味着每个任务组都会在独立的<code>JVM</code>中运行(例如，可以在单独的容器中启动)。如果拥有多个任务槽意味着多个子任务共享同一个<code>JVM</code>。同一<code>JVM</code>中的任务共享<code>TCP</code>连接(通过多路复用)和心跳消息，他们也可以共享数据集和数据结构，从而降低单个任务的开销。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-3.png?raw=true\" alt=\"\"></p>\n<p>默认情况下，<code>Flink</code>允许子任务共享任务槽，即使它们是不同任务的子任务，只要它们来自同一个作业。结果是一个任务槽可能会是一个完整的作业管道。允许任务槽共享有两个主要好处：</p>\n<p>(1) Flink集群所需的任务槽数与作业中使用的最高并行度数保持一致。不需要计算一个程序总共包含多少个任务(不同任务具有不同的并行度)。</p>\n<p>(2) 提高资源利用率。如果没有使用任务槽共享机制，那么非密集的<code>sour/map()</code>子任务就会与资源密集型<code>window</code>子任务阻塞一样多的资源。在我们的示例中，通过任务槽共享，将基本并行度从两个增加到六个，可以充分利用已分配的资源，同时确保繁重的子任务在<code>TaskManager</code>之间公平分配。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-4.png?raw=true\" alt=\"\"></p>\n<p>这些API还包括一个资源组机制，可以避免不合理的任务槽共享。</p>\n<p>根据经验来说，默认任务槽数应该设置为<code>CPU</code>核的数量。如果使用超线程技术，每个任务槽需要2个或更多的硬件线程上下文(With hyper-threading, each slot then takes 2 or more hardware thread contexts)。</p>\n<h3 id=\"4-后端状态\"><a href=\"#4-后端状态\" class=\"headerlink\" title=\"4. 后端状态\"></a>4. 后端状态</h3><p>键/值索引存储的确切数据结构取决于所选的后端状态。一个后端状态将数据存储在内存中<code>hash map</code>中，另一个后端状态使用<code>RocksDB</code>存储键/值。除了定义保存状态的数据结构之外，后端状态还实现了获取键/值状态的时间点快照逻辑并将该快照存储为检查点的一部分。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"5-保存点\"><a href=\"#5-保存点\" class=\"headerlink\" title=\"5. 保存点\"></a>5. 保存点</h3><p>用<code>Data Stream API</code>编写的程序可以从保存点恢复执行。保存点允许更新你的程序和你的Flink集群，而不会丢失任何状态。</p>\n<p>保存点是手动触发的检查点，它会捕获程序的快照并将其写入后端状态。他们依赖于常规检查点机制。在执行期间的程序定期在工作节点上生成快照并生成检查点。为了恢复，只需要最后完成的检查点，一旦新的检查点完成，可以安全地丢弃较旧的检查点。</p>\n<p>保存点与这些定期检查点类似，只不过它们是由用户触发的，不会在新检查点完成时自动失效。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/runtime.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink 四种优化Flink应用程序的方法","date":"2018-02-11T02:39:01.000Z","_content":"\n`Flink` 是一个复杂的框架，并提供了许多方法来调整其执行。在本文中，我将展示四种不同的方法来提高 `Flink` 应用程序的性能。如果你不熟悉 `Flink`，你可以阅读其他介绍性的文章，比如[这个](https://brewing.codes/2017/09/25/flink-vs-spark/)，[这个](https://brewing.codes/2017/10/01/start-flink-batch/)和[这个](https://brewing.codes/2017/10/09/start-flink-streaming/)。如果你已经熟悉 `Apache Flink`，本文将帮助你更快地创建应用程序。\n\n### 1. 使用 Flink tuples\n\n当你使用像 `groupBy`，`join` 或 `keyBy` 这样的操作时， `Flink` 提供了多种方式在数据集中选择`key`。你可以使用 `key` 选择器函数：\n```java\n// Join movies and ratings datasets\nmovies.join(ratings)\n      // Use movie id as a key in both cases\n      .where(new KeySelector<Movie, String>() {\n            @Override\n            public String getKey(Movie m) throws Exception {\n                return m.getId();\n            }\n      })\n      .equalTo(new KeySelector<Rating, String>() {\n            @Override\n            public String getKey(Rating r) throws Exception {\n                return r.getMovieId();\n            }\n      })\n```\n或者你可以在 `POJO` 类型中指定一个字段名称：\n```java\nmovies.join(ratings)\n    // Use same fields as in the previous example\n    .where(\"id\")\n    .equalTo(\"movieId\")\n```\n但是，如果你正在使用 `Flink` `tuple` 类型，你可以简单地指定将要作为 `key` 的字段在元组中的位置：\n```java\nDataSet<Tuple2<String, String>> movies ...\nDataSet<Tuple3<String, String, Double>> ratings ...\n\nmovies.join(ratings)\n    // Specify fields positions in tuples\n    .where(0)\n    .equalTo(1)\n```\n\n最后一种方式会给你最好的性能，但可读性呢？ 这是否意味着你的代码现在看起来像这样：\n```java\nDataSet<Tuple3<Integer, String, Double>> result = movies.join(ratings)\n    .where(0)\n    .equalTo(0)\n    .with(new JoinFunction<Tuple2<Integer,String>, Tuple2<Integer,Double>, Tuple3<Integer, String, Double>>() {\n        // What is happening here?\n        @Override\n        public Tuple3<Integer, String, Double> join(Tuple2<Integer, String> first, Tuple2<Integer, Double> second) throws Exception {\n            // Some tuples are joined with some other tuples and some fields are returned???\n            return new Tuple3<>(first.f0, first.f1, second.f1);\n        }\n    });\n```\n在这种情况下，提高可读性的常见方法是创建一个继承自 `TupleX` 类的类，并为这些字段实现 `getter` 和 `setter` 方法。在这里，下面是 `Flink` `Gelly` 库的 `Edge` 类的大体实现，具有三个字段并继承了 `Tuple3` 类：\n```java\npublic class Edge<K, V> extends Tuple3<K, K, V> {\n\n    public Edge(K source, K target, V value) {\n        this.f0 = source;\n        this.f1 = target;\n        this.f2 = value;\n    }\n\n    // Getters and setters for readability\n    public void setSource(K source) {\n        this.f0 = source;\n    }\n\n    public K getSource() {\n        return this.f0;\n    }\n\n    // Also has getters and setters for other fields\n    ...\n}\n```\n\n### 2. 重用 Flink对象\n\n另一个可以用来提高 `Flink` 应用程序性能的方法是当你从自定义函数中返回数据时使用可变对象。看看这个例子：\n```\nstream\n    .apply(new WindowFunction<WikipediaEditEvent, Tuple2<String, Long>, String, TimeWindow>() {\n        @Override\n        public void apply(String userName, TimeWindow timeWindow, Iterable<WikipediaEditEvent> iterable, Collector<Tuple2<String, Long>> collector) throws Exception {\n            long changesCount\n            // A new Tuple instance is created on every execution\n            collector.collect(new Tuple2<>(userName, changesCount));\n        }\n    }\n```\n\n正如你所看到的，在 `apply` 函数的每次执行中，我们都创建一个 `Tuple2` 类型的实例，这会给垃圾收集器造成很大压力。解决这个问题的一种方法是重复使用同一个实例：\n```\nstream\n    .apply(new WindowFunction<WikipediaEditEvent, Tuple2<String, Long>, String, TimeWindow>() {\n        // Create an instance that we will reuse on every call\n        private Tuple2<String, Long> result = new Tuple<>();\n\n        @Override\n        public void apply(String userName, TimeWindow timeWindow, Iterable<WikipediaEditEvent> iterable, Collector<Tuple2<String, Long>> collector) throws Exception {\n            long changesCount = ...\n\n            // Set fields on an existing object instead of creating a new one\n            result.f0 = userName;\n            // Auto-boxing!! A new Long value may be created\n            result.f1 = changesCount;\n\n            // Reuse the same Tuple2 object\n            collector.collect(result);\n        }\n    }\n```\n上述代码会更好些。虽然我们在每次调用的时候只创建了一个 `Tuple2` 实例，但是我们还是间接地创建了 `Long` 类型的实例。为了解决这个问题， `Flink` 提供了很多的值类（value classes），`IntValue`, `LongValue`, `StringValue`, `FloatValue` 等。这些类的目的是为内置类型提供可变版本，所以我们可以在用户自定义函数中重用这些类型，下面就是如何使用的例子：\n```\nstream\n    .apply(new WindowFunction<WikipediaEditEvent, Tuple2<String, Long>, String, TimeWindow>() {\n        // Create a mutable count instance\n        private LongValue count = new IntValue();\n        // Assign mutable count to the tuple\n        private Tuple2<String, LongValue> result = new Tuple<>(\"\", count);\n\n        @Override\n        // Notice that now we have a different return type\n        public void apply(String userName, TimeWindow timeWindow, Iterable<WikipediaEditEvent> iterable, Collector<Tuple2<String, LongValue>> collector) throws Exception {\n            long changesCount = ...\n\n            // Set fields on an existing object instead of creating a new one\n            result.f0 = userName;\n            // Update mutable count value\n            count.setValue(changesCount);\n\n            // Reuse the same tuple and the same LongValue instance\n            collector.collect(result);\n        }\n    }\n```\n上面这些使用习惯在 `Flink` 类库中被普遍使用，比如 `Flink` `Gelly`。\n\n### 3. 使用函数注解\n\n优化 `Flink` 应用程序的另一种方法是提供关于用户自定义函数对输入数据做什么的一些信息。由于 `Flink` 无法解析和理解代码，因此你可以提供关键信息，这将有助于构建更高效的执行计划。有三个注解我们可以使用：\n- `@ForwardedFields` - 指定输入值中的哪些字段保持不变并在输出值中使用。\n- `@NotForwardedFields` - 指定在输出中同一位置不保留的字段。\n- `@ReadFields` - 指定用于计算结果值的字段。你只能指定那些在计算中使用的字段，而不是仅仅将数据拷贝到输出中的字段。\n\n我们来看看如何使用 `ForwardedFields` 注解：\n```java\n// Specify that the first element is copied without any changes\n@ForwardedFields(\"0\")\nclass MyFunction implements MapFunction<Tuple2<Long, Double>, Tuple2<Long, Double>> {\n    @Override\n    public Tuple2<Long, Double> map(Tuple2<Long, Double> value) {\n       // Copy first field without change\n        return new Tuple2<>(value.f0, value.f1 + 123);\n    }\n}\n```\n上述代码意味着输入元组的第一个元素将不会改变，并且在返回时也处于同一个位置（译者注：第一个位置）。\n\n如果你不改变字段，只是简单地将它移到不同的位置上，你同样可以使用 `ForwardedFields` 注解来实现。下面例子中，我们简单地将输入元组的字段进行交换（译者注：第一个字段移到第二个位置，第二个字段移到第一个位置）：\n```java\n// 1st element goes into the 2nd position, and 2nd element goes into the 1st position\n@ForwardedFields(\"0->1; 1->0\")\nclass SwapArguments implements MapFunction<Tuple2<Long, Double>, Tuple2<Double, Long>> {\n    @Override\n    public Tuple2<Double, Long> map(Tuple2<Long, Double> value) {\n       // Swap elements in a tuple\n        return new Tuple2<>(value.f1, value.f0);\n    }\n}\n```\n上面例子中提到的注解只能应用到只有一个输入参数的函数中，比如 `map` 或者 `flatMap`。如果你有两个输入参数的函数，你可以使用 `ForwardedFieldsFirst` 和 `ForwardedFieldsSecond` 注解分别为第一和第二个参数提供信息。\n\n下面我们看一下如何在 `JoinFunction` 接口的实现中使用这些注解（译者注：第一个输入元组的两个字段拷贝到输出元组的第一个和第二个位置，第二个输入元组的第二个字段拷贝到输出元组的第三个位置）：\n```java\n// Two fields from the input tuple are copied to the first and second positions of the output tuple\n@ForwardedFieldsFirst(\"0; 1\")\n// The third field from the input tuple is copied to the third position of the output tuple\n@ForwardedFieldsSecond(\"2\")\nclass MyJoin implements JoinFunction<Tuple2<Integer,String>, Tuple2<Integer,Double>, Tuple3<Integer, String, Double>>() {\n    @Override\n    public Tuple3<Integer, String, Double> join(Tuple2<Integer, String> first, Tuple2<Integer, Double> second) throws Exception {\n        return new Tuple3<>(first.f0, first.f1, second.f1);\n    }\n})\n```\n\n`Flink` 同样提供了 `NotForwardedFieldsFirst`, `NotForwardedFieldsSecond`, `ReadFieldsFirst`, 和 `ReadFirldsSecond` 注解来实现同样的功能。\n\n### 4. 选择 join 类型\n\n如果你告诉 `Flink` 一些信息，可以加快 `join` 的速度，但在讨论它为什么会起作用之前，让我们先来谈谈 `Flink` 是如何执行 `join`的。\n\n当 `Flink` 处理批量数据时，集群中的每台机器都存储了部分数据。要执行 `join` 操作，`Flink` 需要找到两个两个数据集中满足 `join` 条件的所有记录对（译者注：`key` 相同的数据）。 要做到这一点，`Flink` 首先必须将两个数据集中具有相同 `key` 的数据放在集群中的同一台机器上。有两种策略：\n- `Repartition-Repartition` 策略：在这种场景下，根据它们的 `key` 对两个数据集进行重新分区，通过网络发送数据。这就意味着如果数据集非常大，这将花费大量的时间将数据在网络之间进行复制。\n- `Broadcast-Forward` 策略：在这种场景下，一个数据集保持不变，将第二个数据集拷贝到集群上第二个数据集拥有第一个数据集部分数据的所有机器上（译者注：将达尔戈数据集进行分发到对应机器上）。\n\n如果用一个较大的数据集与一个小数据集进行 `join`，你可以使用 `Broadcast-Forward` 策略并避免对第一个数据集进行重分区的昂贵代价。这很容易做到：\n```\nds1.join(ds2, JoinHint.BROADCAST_HASH_FIRST)\n```\n这表示第一个数据集比第二个数据集小得多。\n\n`Flink` 支持的其他 `join` 提示有以下几种：\n- `BROADCAST_HASH_SECOND` – 表示第二个数据集比较小\n- `REPARTITION_HASH_FIRST` – 表示第一个数据集比较小\n- `REPARTITION_HASH_SECOND` – 表示第二个数据集有点小\n- `REPARTITION_SORT_MERGE` – 表示对两个数据集重新分区并使用排序和合并策略\n- `OPTIMIZER_CHOOSES` – `Flink` 优化器将决定如何连接数据集\n\n\n原文： https://brewing.codes/2017/10/17/flink-optimize/\n","source":"_posts/Flink/[Flink]Flink 四种优化Flink应用程序的方法.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink 四种优化Flink应用程序的方法\ndate: 2018-02-11 10:39:01\ntags:\n  - Flink\n  - Flink 优化\n\ncategories: Flink\npermalink: four-ways-to-optimize-your-flink-applications\n---\n\n`Flink` 是一个复杂的框架，并提供了许多方法来调整其执行。在本文中，我将展示四种不同的方法来提高 `Flink` 应用程序的性能。如果你不熟悉 `Flink`，你可以阅读其他介绍性的文章，比如[这个](https://brewing.codes/2017/09/25/flink-vs-spark/)，[这个](https://brewing.codes/2017/10/01/start-flink-batch/)和[这个](https://brewing.codes/2017/10/09/start-flink-streaming/)。如果你已经熟悉 `Apache Flink`，本文将帮助你更快地创建应用程序。\n\n### 1. 使用 Flink tuples\n\n当你使用像 `groupBy`，`join` 或 `keyBy` 这样的操作时， `Flink` 提供了多种方式在数据集中选择`key`。你可以使用 `key` 选择器函数：\n```java\n// Join movies and ratings datasets\nmovies.join(ratings)\n      // Use movie id as a key in both cases\n      .where(new KeySelector<Movie, String>() {\n            @Override\n            public String getKey(Movie m) throws Exception {\n                return m.getId();\n            }\n      })\n      .equalTo(new KeySelector<Rating, String>() {\n            @Override\n            public String getKey(Rating r) throws Exception {\n                return r.getMovieId();\n            }\n      })\n```\n或者你可以在 `POJO` 类型中指定一个字段名称：\n```java\nmovies.join(ratings)\n    // Use same fields as in the previous example\n    .where(\"id\")\n    .equalTo(\"movieId\")\n```\n但是，如果你正在使用 `Flink` `tuple` 类型，你可以简单地指定将要作为 `key` 的字段在元组中的位置：\n```java\nDataSet<Tuple2<String, String>> movies ...\nDataSet<Tuple3<String, String, Double>> ratings ...\n\nmovies.join(ratings)\n    // Specify fields positions in tuples\n    .where(0)\n    .equalTo(1)\n```\n\n最后一种方式会给你最好的性能，但可读性呢？ 这是否意味着你的代码现在看起来像这样：\n```java\nDataSet<Tuple3<Integer, String, Double>> result = movies.join(ratings)\n    .where(0)\n    .equalTo(0)\n    .with(new JoinFunction<Tuple2<Integer,String>, Tuple2<Integer,Double>, Tuple3<Integer, String, Double>>() {\n        // What is happening here?\n        @Override\n        public Tuple3<Integer, String, Double> join(Tuple2<Integer, String> first, Tuple2<Integer, Double> second) throws Exception {\n            // Some tuples are joined with some other tuples and some fields are returned???\n            return new Tuple3<>(first.f0, first.f1, second.f1);\n        }\n    });\n```\n在这种情况下，提高可读性的常见方法是创建一个继承自 `TupleX` 类的类，并为这些字段实现 `getter` 和 `setter` 方法。在这里，下面是 `Flink` `Gelly` 库的 `Edge` 类的大体实现，具有三个字段并继承了 `Tuple3` 类：\n```java\npublic class Edge<K, V> extends Tuple3<K, K, V> {\n\n    public Edge(K source, K target, V value) {\n        this.f0 = source;\n        this.f1 = target;\n        this.f2 = value;\n    }\n\n    // Getters and setters for readability\n    public void setSource(K source) {\n        this.f0 = source;\n    }\n\n    public K getSource() {\n        return this.f0;\n    }\n\n    // Also has getters and setters for other fields\n    ...\n}\n```\n\n### 2. 重用 Flink对象\n\n另一个可以用来提高 `Flink` 应用程序性能的方法是当你从自定义函数中返回数据时使用可变对象。看看这个例子：\n```\nstream\n    .apply(new WindowFunction<WikipediaEditEvent, Tuple2<String, Long>, String, TimeWindow>() {\n        @Override\n        public void apply(String userName, TimeWindow timeWindow, Iterable<WikipediaEditEvent> iterable, Collector<Tuple2<String, Long>> collector) throws Exception {\n            long changesCount\n            // A new Tuple instance is created on every execution\n            collector.collect(new Tuple2<>(userName, changesCount));\n        }\n    }\n```\n\n正如你所看到的，在 `apply` 函数的每次执行中，我们都创建一个 `Tuple2` 类型的实例，这会给垃圾收集器造成很大压力。解决这个问题的一种方法是重复使用同一个实例：\n```\nstream\n    .apply(new WindowFunction<WikipediaEditEvent, Tuple2<String, Long>, String, TimeWindow>() {\n        // Create an instance that we will reuse on every call\n        private Tuple2<String, Long> result = new Tuple<>();\n\n        @Override\n        public void apply(String userName, TimeWindow timeWindow, Iterable<WikipediaEditEvent> iterable, Collector<Tuple2<String, Long>> collector) throws Exception {\n            long changesCount = ...\n\n            // Set fields on an existing object instead of creating a new one\n            result.f0 = userName;\n            // Auto-boxing!! A new Long value may be created\n            result.f1 = changesCount;\n\n            // Reuse the same Tuple2 object\n            collector.collect(result);\n        }\n    }\n```\n上述代码会更好些。虽然我们在每次调用的时候只创建了一个 `Tuple2` 实例，但是我们还是间接地创建了 `Long` 类型的实例。为了解决这个问题， `Flink` 提供了很多的值类（value classes），`IntValue`, `LongValue`, `StringValue`, `FloatValue` 等。这些类的目的是为内置类型提供可变版本，所以我们可以在用户自定义函数中重用这些类型，下面就是如何使用的例子：\n```\nstream\n    .apply(new WindowFunction<WikipediaEditEvent, Tuple2<String, Long>, String, TimeWindow>() {\n        // Create a mutable count instance\n        private LongValue count = new IntValue();\n        // Assign mutable count to the tuple\n        private Tuple2<String, LongValue> result = new Tuple<>(\"\", count);\n\n        @Override\n        // Notice that now we have a different return type\n        public void apply(String userName, TimeWindow timeWindow, Iterable<WikipediaEditEvent> iterable, Collector<Tuple2<String, LongValue>> collector) throws Exception {\n            long changesCount = ...\n\n            // Set fields on an existing object instead of creating a new one\n            result.f0 = userName;\n            // Update mutable count value\n            count.setValue(changesCount);\n\n            // Reuse the same tuple and the same LongValue instance\n            collector.collect(result);\n        }\n    }\n```\n上面这些使用习惯在 `Flink` 类库中被普遍使用，比如 `Flink` `Gelly`。\n\n### 3. 使用函数注解\n\n优化 `Flink` 应用程序的另一种方法是提供关于用户自定义函数对输入数据做什么的一些信息。由于 `Flink` 无法解析和理解代码，因此你可以提供关键信息，这将有助于构建更高效的执行计划。有三个注解我们可以使用：\n- `@ForwardedFields` - 指定输入值中的哪些字段保持不变并在输出值中使用。\n- `@NotForwardedFields` - 指定在输出中同一位置不保留的字段。\n- `@ReadFields` - 指定用于计算结果值的字段。你只能指定那些在计算中使用的字段，而不是仅仅将数据拷贝到输出中的字段。\n\n我们来看看如何使用 `ForwardedFields` 注解：\n```java\n// Specify that the first element is copied without any changes\n@ForwardedFields(\"0\")\nclass MyFunction implements MapFunction<Tuple2<Long, Double>, Tuple2<Long, Double>> {\n    @Override\n    public Tuple2<Long, Double> map(Tuple2<Long, Double> value) {\n       // Copy first field without change\n        return new Tuple2<>(value.f0, value.f1 + 123);\n    }\n}\n```\n上述代码意味着输入元组的第一个元素将不会改变，并且在返回时也处于同一个位置（译者注：第一个位置）。\n\n如果你不改变字段，只是简单地将它移到不同的位置上，你同样可以使用 `ForwardedFields` 注解来实现。下面例子中，我们简单地将输入元组的字段进行交换（译者注：第一个字段移到第二个位置，第二个字段移到第一个位置）：\n```java\n// 1st element goes into the 2nd position, and 2nd element goes into the 1st position\n@ForwardedFields(\"0->1; 1->0\")\nclass SwapArguments implements MapFunction<Tuple2<Long, Double>, Tuple2<Double, Long>> {\n    @Override\n    public Tuple2<Double, Long> map(Tuple2<Long, Double> value) {\n       // Swap elements in a tuple\n        return new Tuple2<>(value.f1, value.f0);\n    }\n}\n```\n上面例子中提到的注解只能应用到只有一个输入参数的函数中，比如 `map` 或者 `flatMap`。如果你有两个输入参数的函数，你可以使用 `ForwardedFieldsFirst` 和 `ForwardedFieldsSecond` 注解分别为第一和第二个参数提供信息。\n\n下面我们看一下如何在 `JoinFunction` 接口的实现中使用这些注解（译者注：第一个输入元组的两个字段拷贝到输出元组的第一个和第二个位置，第二个输入元组的第二个字段拷贝到输出元组的第三个位置）：\n```java\n// Two fields from the input tuple are copied to the first and second positions of the output tuple\n@ForwardedFieldsFirst(\"0; 1\")\n// The third field from the input tuple is copied to the third position of the output tuple\n@ForwardedFieldsSecond(\"2\")\nclass MyJoin implements JoinFunction<Tuple2<Integer,String>, Tuple2<Integer,Double>, Tuple3<Integer, String, Double>>() {\n    @Override\n    public Tuple3<Integer, String, Double> join(Tuple2<Integer, String> first, Tuple2<Integer, Double> second) throws Exception {\n        return new Tuple3<>(first.f0, first.f1, second.f1);\n    }\n})\n```\n\n`Flink` 同样提供了 `NotForwardedFieldsFirst`, `NotForwardedFieldsSecond`, `ReadFieldsFirst`, 和 `ReadFirldsSecond` 注解来实现同样的功能。\n\n### 4. 选择 join 类型\n\n如果你告诉 `Flink` 一些信息，可以加快 `join` 的速度，但在讨论它为什么会起作用之前，让我们先来谈谈 `Flink` 是如何执行 `join`的。\n\n当 `Flink` 处理批量数据时，集群中的每台机器都存储了部分数据。要执行 `join` 操作，`Flink` 需要找到两个两个数据集中满足 `join` 条件的所有记录对（译者注：`key` 相同的数据）。 要做到这一点，`Flink` 首先必须将两个数据集中具有相同 `key` 的数据放在集群中的同一台机器上。有两种策略：\n- `Repartition-Repartition` 策略：在这种场景下，根据它们的 `key` 对两个数据集进行重新分区，通过网络发送数据。这就意味着如果数据集非常大，这将花费大量的时间将数据在网络之间进行复制。\n- `Broadcast-Forward` 策略：在这种场景下，一个数据集保持不变，将第二个数据集拷贝到集群上第二个数据集拥有第一个数据集部分数据的所有机器上（译者注：将达尔戈数据集进行分发到对应机器上）。\n\n如果用一个较大的数据集与一个小数据集进行 `join`，你可以使用 `Broadcast-Forward` 策略并避免对第一个数据集进行重分区的昂贵代价。这很容易做到：\n```\nds1.join(ds2, JoinHint.BROADCAST_HASH_FIRST)\n```\n这表示第一个数据集比第二个数据集小得多。\n\n`Flink` 支持的其他 `join` 提示有以下几种：\n- `BROADCAST_HASH_SECOND` – 表示第二个数据集比较小\n- `REPARTITION_HASH_FIRST` – 表示第一个数据集比较小\n- `REPARTITION_HASH_SECOND` – 表示第二个数据集有点小\n- `REPARTITION_SORT_MERGE` – 表示对两个数据集重新分区并使用排序和合并策略\n- `OPTIMIZER_CHOOSES` – `Flink` 优化器将决定如何连接数据集\n\n\n原文： https://brewing.codes/2017/10/17/flink-optimize/\n","slug":"four-ways-to-optimize-your-flink-applications","published":1,"updated":"2018-02-12T08:19:39.132Z","comments":1,"photos":[],"link":"","_id":"cje58tipj0009ordbuj9qwen5","content":"<p><code>Flink</code> 是一个复杂的框架，并提供了许多方法来调整其执行。在本文中，我将展示四种不同的方法来提高 <code>Flink</code> 应用程序的性能。如果你不熟悉 <code>Flink</code>，你可以阅读其他介绍性的文章，比如<a href=\"https://brewing.codes/2017/09/25/flink-vs-spark/\" target=\"_blank\" rel=\"noopener\">这个</a>，<a href=\"https://brewing.codes/2017/10/01/start-flink-batch/\" target=\"_blank\" rel=\"noopener\">这个</a>和<a href=\"https://brewing.codes/2017/10/09/start-flink-streaming/\" target=\"_blank\" rel=\"noopener\">这个</a>。如果你已经熟悉 <code>Apache Flink</code>，本文将帮助你更快地创建应用程序。</p>\n<h3 id=\"1-使用-Flink-tuples\"><a href=\"#1-使用-Flink-tuples\" class=\"headerlink\" title=\"1. 使用 Flink tuples\"></a>1. 使用 Flink tuples</h3><p>当你使用像 <code>groupBy</code>，<code>join</code> 或 <code>keyBy</code> 这样的操作时， <code>Flink</code> 提供了多种方式在数据集中选择<code>key</code>。你可以使用 <code>key</code> 选择器函数：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Join movies and ratings datasets</span></span><br><span class=\"line\">movies.join(ratings)</span><br><span class=\"line\">      <span class=\"comment\">// Use movie id as a key in both cases</span></span><br><span class=\"line\">      .where(<span class=\"keyword\">new</span> KeySelector&lt;Movie, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getKey</span><span class=\"params\">(Movie m)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> m.getId();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">      &#125;)</span><br><span class=\"line\">      .equalTo(<span class=\"keyword\">new</span> KeySelector&lt;Rating, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getKey</span><span class=\"params\">(Rating r)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> r.getMovieId();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">      &#125;)</span><br></pre></td></tr></table></figure></p>\n<p>或者你可以在 <code>POJO</code> 类型中指定一个字段名称：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">movies.join(ratings)</span><br><span class=\"line\">    <span class=\"comment\">// Use same fields as in the previous example</span></span><br><span class=\"line\">    .where(<span class=\"string\">\"id\"</span>)</span><br><span class=\"line\">    .equalTo(<span class=\"string\">\"movieId\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>但是，如果你正在使用 <code>Flink</code> <code>tuple</code> 类型，你可以简单地指定将要作为 <code>key</code> 的字段在元组中的位置：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataSet&lt;Tuple2&lt;String, String&gt;&gt; movies ...</span><br><span class=\"line\">DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; ratings ...</span><br><span class=\"line\"></span><br><span class=\"line\">movies.join(ratings)</span><br><span class=\"line\">    <span class=\"comment\">// Specify fields positions in tuples</span></span><br><span class=\"line\">    .where(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .equalTo(<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure></p>\n<p>最后一种方式会给你最好的性能，但可读性呢？ 这是否意味着你的代码现在看起来像这样：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; result = movies.join(ratings)</span><br><span class=\"line\">    .where(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .equalTo(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .with(<span class=\"keyword\">new</span> JoinFunction&lt;Tuple2&lt;Integer,String&gt;, Tuple2&lt;Integer,Double&gt;, Tuple3&lt;Integer, String, Double&gt;&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">// What is happening here?</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Tuple3&lt;Integer, String, Double&gt; <span class=\"title\">join</span><span class=\"params\">(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Some tuples are joined with some other tuples and some fields are returned???</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple3&lt;&gt;(first.f0, first.f1, second.f1);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure></p>\n<p>在这种情况下，提高可读性的常见方法是创建一个继承自 <code>TupleX</code> 类的类，并为这些字段实现 <code>getter</code> 和 <code>setter</code> 方法。在这里，下面是 <code>Flink</code> <code>Gelly</code> 库的 <code>Edge</code> 类的大体实现，具有三个字段并继承了 <code>Tuple3</code> 类：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Edge</span>&lt;<span class=\"title\">K</span>, <span class=\"title\">V</span>&gt; <span class=\"keyword\">extends</span> <span class=\"title\">Tuple3</span>&lt;<span class=\"title\">K</span>, <span class=\"title\">K</span>, <span class=\"title\">V</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Edge</span><span class=\"params\">(K source, K target, V value)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f0 = source;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f1 = target;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f2 = value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Getters and setters for readability</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setSource</span><span class=\"params\">(K source)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f0 = source;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> K <span class=\"title\">getSource</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.f0;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Also has getters and setters for other fields</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-重用-Flink对象\"><a href=\"#2-重用-Flink对象\" class=\"headerlink\" title=\"2. 重用 Flink对象\"></a>2. 重用 Flink对象</h3><p>另一个可以用来提高 <code>Flink</code> 应用程序性能的方法是当你从自定义函数中返回数据时使用可变对象。看看这个例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">stream</span><br><span class=\"line\">    .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123;</span><br><span class=\"line\">            long changesCount</span><br><span class=\"line\">            // A new Tuple instance is created on every execution</span><br><span class=\"line\">            collector.collect(new Tuple2&lt;&gt;(userName, changesCount));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>正如你所看到的，在 <code>apply</code> 函数的每次执行中，我们都创建一个 <code>Tuple2</code> 类型的实例，这会给垃圾收集器造成很大压力。解决这个问题的一种方法是重复使用同一个实例：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">stream</span><br><span class=\"line\">    .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        // Create an instance that we will reuse on every call</span><br><span class=\"line\">        private Tuple2&lt;String, Long&gt; result = new Tuple&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123;</span><br><span class=\"line\">            long changesCount = ...</span><br><span class=\"line\"></span><br><span class=\"line\">            // Set fields on an existing object instead of creating a new one</span><br><span class=\"line\">            result.f0 = userName;</span><br><span class=\"line\">            // Auto-boxing!! A new Long value may be created</span><br><span class=\"line\">            result.f1 = changesCount;</span><br><span class=\"line\"></span><br><span class=\"line\">            // Reuse the same Tuple2 object</span><br><span class=\"line\">            collector.collect(result);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>上述代码会更好些。虽然我们在每次调用的时候只创建了一个 <code>Tuple2</code> 实例，但是我们还是间接地创建了 <code>Long</code> 类型的实例。为了解决这个问题， <code>Flink</code> 提供了很多的值类（value classes），<code>IntValue</code>, <code>LongValue</code>, <code>StringValue</code>, <code>FloatValue</code> 等。这些类的目的是为内置类型提供可变版本，所以我们可以在用户自定义函数中重用这些类型，下面就是如何使用的例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">stream</span><br><span class=\"line\">    .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        // Create a mutable count instance</span><br><span class=\"line\">        private LongValue count = new IntValue();</span><br><span class=\"line\">        // Assign mutable count to the tuple</span><br><span class=\"line\">        private Tuple2&lt;String, LongValue&gt; result = new Tuple&lt;&gt;(&quot;&quot;, count);</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        // Notice that now we have a different return type</span><br><span class=\"line\">        public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, LongValue&gt;&gt; collector) throws Exception &#123;</span><br><span class=\"line\">            long changesCount = ...</span><br><span class=\"line\"></span><br><span class=\"line\">            // Set fields on an existing object instead of creating a new one</span><br><span class=\"line\">            result.f0 = userName;</span><br><span class=\"line\">            // Update mutable count value</span><br><span class=\"line\">            count.setValue(changesCount);</span><br><span class=\"line\"></span><br><span class=\"line\">            // Reuse the same tuple and the same LongValue instance</span><br><span class=\"line\">            collector.collect(result);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面这些使用习惯在 <code>Flink</code> 类库中被普遍使用，比如 <code>Flink</code> <code>Gelly</code>。</p>\n<h3 id=\"3-使用函数注解\"><a href=\"#3-使用函数注解\" class=\"headerlink\" title=\"3. 使用函数注解\"></a>3. 使用函数注解</h3><p>优化 <code>Flink</code> 应用程序的另一种方法是提供关于用户自定义函数对输入数据做什么的一些信息。由于 <code>Flink</code> 无法解析和理解代码，因此你可以提供关键信息，这将有助于构建更高效的执行计划。有三个注解我们可以使用：</p>\n<ul>\n<li><code>@ForwardedFields</code> - 指定输入值中的哪些字段保持不变并在输出值中使用。</li>\n<li><code>@NotForwardedFields</code> - 指定在输出中同一位置不保留的字段。</li>\n<li><code>@ReadFields</code> - 指定用于计算结果值的字段。你只能指定那些在计算中使用的字段，而不是仅仅将数据拷贝到输出中的字段。</li>\n</ul>\n<p>我们来看看如何使用 <code>ForwardedFields</code> 注解：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Specify that the first element is copied without any changes</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFields</span>(<span class=\"string\">\"0\"</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyFunction</span> <span class=\"keyword\">implements</span> <span class=\"title\">MapFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Double</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Double</span>&gt;&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Long, Double&gt; <span class=\"title\">map</span><span class=\"params\">(Tuple2&lt;Long, Double&gt; value)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// Copy first field without change</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(value.f0, value.f1 + <span class=\"number\">123</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上述代码意味着输入元组的第一个元素将不会改变，并且在返回时也处于同一个位置（译者注：第一个位置）。</p>\n<p>如果你不改变字段，只是简单地将它移到不同的位置上，你同样可以使用 <code>ForwardedFields</code> 注解来实现。下面例子中，我们简单地将输入元组的字段进行交换（译者注：第一个字段移到第二个位置，第二个字段移到第一个位置）：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 1st element goes into the 2nd position, and 2nd element goes into the 1st position</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFields</span>(<span class=\"string\">\"0-&gt;1; 1-&gt;0\"</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SwapArguments</span> <span class=\"keyword\">implements</span> <span class=\"title\">MapFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Double</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Double</span>, <span class=\"title\">Long</span>&gt;&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Double, Long&gt; <span class=\"title\">map</span><span class=\"params\">(Tuple2&lt;Long, Double&gt; value)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// Swap elements in a tuple</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(value.f1, value.f0);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面例子中提到的注解只能应用到只有一个输入参数的函数中，比如 <code>map</code> 或者 <code>flatMap</code>。如果你有两个输入参数的函数，你可以使用 <code>ForwardedFieldsFirst</code> 和 <code>ForwardedFieldsSecond</code> 注解分别为第一和第二个参数提供信息。</p>\n<p>下面我们看一下如何在 <code>JoinFunction</code> 接口的实现中使用这些注解（译者注：第一个输入元组的两个字段拷贝到输出元组的第一个和第二个位置，第二个输入元组的第二个字段拷贝到输出元组的第三个位置）：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Two fields from the input tuple are copied to the first and second positions of the output tuple</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFieldsFirst</span>(<span class=\"string\">\"0; 1\"</span>)</span><br><span class=\"line\"><span class=\"comment\">// The third field from the input tuple is copied to the third position of the output tuple</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFieldsSecond</span>(<span class=\"string\">\"2\"</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyJoin</span> <span class=\"keyword\">implements</span> <span class=\"title\">JoinFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Integer</span>,<span class=\"title\">String</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Integer</span>,<span class=\"title\">Double</span>&gt;, <span class=\"title\">Tuple3</span>&lt;<span class=\"title\">Integer</span>, <span class=\"title\">String</span>, <span class=\"title\">Double</span>&gt;&gt;() </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple3&lt;Integer, String, Double&gt; <span class=\"title\">join</span><span class=\"params\">(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple3&lt;&gt;(first.f0, first.f1, second.f1);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure></p>\n<p><code>Flink</code> 同样提供了 <code>NotForwardedFieldsFirst</code>, <code>NotForwardedFieldsSecond</code>, <code>ReadFieldsFirst</code>, 和 <code>ReadFirldsSecond</code> 注解来实现同样的功能。</p>\n<h3 id=\"4-选择-join-类型\"><a href=\"#4-选择-join-类型\" class=\"headerlink\" title=\"4. 选择 join 类型\"></a>4. 选择 join 类型</h3><p>如果你告诉 <code>Flink</code> 一些信息，可以加快 <code>join</code> 的速度，但在讨论它为什么会起作用之前，让我们先来谈谈 <code>Flink</code> 是如何执行 <code>join</code>的。</p>\n<p>当 <code>Flink</code> 处理批量数据时，集群中的每台机器都存储了部分数据。要执行 <code>join</code> 操作，<code>Flink</code> 需要找到两个两个数据集中满足 <code>join</code> 条件的所有记录对（译者注：<code>key</code> 相同的数据）。 要做到这一点，<code>Flink</code> 首先必须将两个数据集中具有相同 <code>key</code> 的数据放在集群中的同一台机器上。有两种策略：</p>\n<ul>\n<li><code>Repartition-Repartition</code> 策略：在这种场景下，根据它们的 <code>key</code> 对两个数据集进行重新分区，通过网络发送数据。这就意味着如果数据集非常大，这将花费大量的时间将数据在网络之间进行复制。</li>\n<li><code>Broadcast-Forward</code> 策略：在这种场景下，一个数据集保持不变，将第二个数据集拷贝到集群上第二个数据集拥有第一个数据集部分数据的所有机器上（译者注：将达尔戈数据集进行分发到对应机器上）。</li>\n</ul>\n<p>如果用一个较大的数据集与一个小数据集进行 <code>join</code>，你可以使用 <code>Broadcast-Forward</code> 策略并避免对第一个数据集进行重分区的昂贵代价。这很容易做到：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">ds1.join(ds2, JoinHint.BROADCAST_HASH_FIRST)</span><br></pre></td></tr></table></figure></p>\n<p>这表示第一个数据集比第二个数据集小得多。</p>\n<p><code>Flink</code> 支持的其他 <code>join</code> 提示有以下几种：</p>\n<ul>\n<li><code>BROADCAST_HASH_SECOND</code> – 表示第二个数据集比较小</li>\n<li><code>REPARTITION_HASH_FIRST</code> – 表示第一个数据集比较小</li>\n<li><code>REPARTITION_HASH_SECOND</code> – 表示第二个数据集有点小</li>\n<li><code>REPARTITION_SORT_MERGE</code> – 表示对两个数据集重新分区并使用排序和合并策略</li>\n<li><code>OPTIMIZER_CHOOSES</code> – <code>Flink</code> 优化器将决定如何连接数据集</li>\n</ul>\n<p>原文： <a href=\"https://brewing.codes/2017/10/17/flink-optimize/\" target=\"_blank\" rel=\"noopener\">https://brewing.codes/2017/10/17/flink-optimize/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>Flink</code> 是一个复杂的框架，并提供了许多方法来调整其执行。在本文中，我将展示四种不同的方法来提高 <code>Flink</code> 应用程序的性能。如果你不熟悉 <code>Flink</code>，你可以阅读其他介绍性的文章，比如<a href=\"https://brewing.codes/2017/09/25/flink-vs-spark/\" target=\"_blank\" rel=\"noopener\">这个</a>，<a href=\"https://brewing.codes/2017/10/01/start-flink-batch/\" target=\"_blank\" rel=\"noopener\">这个</a>和<a href=\"https://brewing.codes/2017/10/09/start-flink-streaming/\" target=\"_blank\" rel=\"noopener\">这个</a>。如果你已经熟悉 <code>Apache Flink</code>，本文将帮助你更快地创建应用程序。</p>\n<h3 id=\"1-使用-Flink-tuples\"><a href=\"#1-使用-Flink-tuples\" class=\"headerlink\" title=\"1. 使用 Flink tuples\"></a>1. 使用 Flink tuples</h3><p>当你使用像 <code>groupBy</code>，<code>join</code> 或 <code>keyBy</code> 这样的操作时， <code>Flink</code> 提供了多种方式在数据集中选择<code>key</code>。你可以使用 <code>key</code> 选择器函数：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Join movies and ratings datasets</span></span><br><span class=\"line\">movies.join(ratings)</span><br><span class=\"line\">      <span class=\"comment\">// Use movie id as a key in both cases</span></span><br><span class=\"line\">      .where(<span class=\"keyword\">new</span> KeySelector&lt;Movie, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getKey</span><span class=\"params\">(Movie m)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> m.getId();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">      &#125;)</span><br><span class=\"line\">      .equalTo(<span class=\"keyword\">new</span> KeySelector&lt;Rating, String&gt;() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getKey</span><span class=\"params\">(Rating r)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> r.getMovieId();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">      &#125;)</span><br></pre></td></tr></table></figure></p>\n<p>或者你可以在 <code>POJO</code> 类型中指定一个字段名称：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">movies.join(ratings)</span><br><span class=\"line\">    <span class=\"comment\">// Use same fields as in the previous example</span></span><br><span class=\"line\">    .where(<span class=\"string\">\"id\"</span>)</span><br><span class=\"line\">    .equalTo(<span class=\"string\">\"movieId\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>但是，如果你正在使用 <code>Flink</code> <code>tuple</code> 类型，你可以简单地指定将要作为 <code>key</code> 的字段在元组中的位置：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataSet&lt;Tuple2&lt;String, String&gt;&gt; movies ...</span><br><span class=\"line\">DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; ratings ...</span><br><span class=\"line\"></span><br><span class=\"line\">movies.join(ratings)</span><br><span class=\"line\">    <span class=\"comment\">// Specify fields positions in tuples</span></span><br><span class=\"line\">    .where(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .equalTo(<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure></p>\n<p>最后一种方式会给你最好的性能，但可读性呢？ 这是否意味着你的代码现在看起来像这样：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; result = movies.join(ratings)</span><br><span class=\"line\">    .where(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .equalTo(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .with(<span class=\"keyword\">new</span> JoinFunction&lt;Tuple2&lt;Integer,String&gt;, Tuple2&lt;Integer,Double&gt;, Tuple3&lt;Integer, String, Double&gt;&gt;() &#123;</span><br><span class=\"line\">        <span class=\"comment\">// What is happening here?</span></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Tuple3&lt;Integer, String, Double&gt; <span class=\"title\">join</span><span class=\"params\">(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// Some tuples are joined with some other tuples and some fields are returned???</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple3&lt;&gt;(first.f0, first.f1, second.f1);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure></p>\n<p>在这种情况下，提高可读性的常见方法是创建一个继承自 <code>TupleX</code> 类的类，并为这些字段实现 <code>getter</code> 和 <code>setter</code> 方法。在这里，下面是 <code>Flink</code> <code>Gelly</code> 库的 <code>Edge</code> 类的大体实现，具有三个字段并继承了 <code>Tuple3</code> 类：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Edge</span>&lt;<span class=\"title\">K</span>, <span class=\"title\">V</span>&gt; <span class=\"keyword\">extends</span> <span class=\"title\">Tuple3</span>&lt;<span class=\"title\">K</span>, <span class=\"title\">K</span>, <span class=\"title\">V</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">Edge</span><span class=\"params\">(K source, K target, V value)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f0 = source;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f1 = target;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f2 = value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Getters and setters for readability</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setSource</span><span class=\"params\">(K source)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.f0 = source;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> K <span class=\"title\">getSource</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.f0;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Also has getters and setters for other fields</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-重用-Flink对象\"><a href=\"#2-重用-Flink对象\" class=\"headerlink\" title=\"2. 重用 Flink对象\"></a>2. 重用 Flink对象</h3><p>另一个可以用来提高 <code>Flink</code> 应用程序性能的方法是当你从自定义函数中返回数据时使用可变对象。看看这个例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">stream</span><br><span class=\"line\">    .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123;</span><br><span class=\"line\">            long changesCount</span><br><span class=\"line\">            // A new Tuple instance is created on every execution</span><br><span class=\"line\">            collector.collect(new Tuple2&lt;&gt;(userName, changesCount));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>正如你所看到的，在 <code>apply</code> 函数的每次执行中，我们都创建一个 <code>Tuple2</code> 类型的实例，这会给垃圾收集器造成很大压力。解决这个问题的一种方法是重复使用同一个实例：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">stream</span><br><span class=\"line\">    .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        // Create an instance that we will reuse on every call</span><br><span class=\"line\">        private Tuple2&lt;String, Long&gt; result = new Tuple&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception &#123;</span><br><span class=\"line\">            long changesCount = ...</span><br><span class=\"line\"></span><br><span class=\"line\">            // Set fields on an existing object instead of creating a new one</span><br><span class=\"line\">            result.f0 = userName;</span><br><span class=\"line\">            // Auto-boxing!! A new Long value may be created</span><br><span class=\"line\">            result.f1 = changesCount;</span><br><span class=\"line\"></span><br><span class=\"line\">            // Reuse the same Tuple2 object</span><br><span class=\"line\">            collector.collect(result);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>上述代码会更好些。虽然我们在每次调用的时候只创建了一个 <code>Tuple2</code> 实例，但是我们还是间接地创建了 <code>Long</code> 类型的实例。为了解决这个问题， <code>Flink</code> 提供了很多的值类（value classes），<code>IntValue</code>, <code>LongValue</code>, <code>StringValue</code>, <code>FloatValue</code> 等。这些类的目的是为内置类型提供可变版本，所以我们可以在用户自定义函数中重用这些类型，下面就是如何使用的例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">stream</span><br><span class=\"line\">    .apply(new WindowFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Long&gt;, String, TimeWindow&gt;() &#123;</span><br><span class=\"line\">        // Create a mutable count instance</span><br><span class=\"line\">        private LongValue count = new IntValue();</span><br><span class=\"line\">        // Assign mutable count to the tuple</span><br><span class=\"line\">        private Tuple2&lt;String, LongValue&gt; result = new Tuple&lt;&gt;(&quot;&quot;, count);</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        // Notice that now we have a different return type</span><br><span class=\"line\">        public void apply(String userName, TimeWindow timeWindow, Iterable&lt;WikipediaEditEvent&gt; iterable, Collector&lt;Tuple2&lt;String, LongValue&gt;&gt; collector) throws Exception &#123;</span><br><span class=\"line\">            long changesCount = ...</span><br><span class=\"line\"></span><br><span class=\"line\">            // Set fields on an existing object instead of creating a new one</span><br><span class=\"line\">            result.f0 = userName;</span><br><span class=\"line\">            // Update mutable count value</span><br><span class=\"line\">            count.setValue(changesCount);</span><br><span class=\"line\"></span><br><span class=\"line\">            // Reuse the same tuple and the same LongValue instance</span><br><span class=\"line\">            collector.collect(result);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面这些使用习惯在 <code>Flink</code> 类库中被普遍使用，比如 <code>Flink</code> <code>Gelly</code>。</p>\n<h3 id=\"3-使用函数注解\"><a href=\"#3-使用函数注解\" class=\"headerlink\" title=\"3. 使用函数注解\"></a>3. 使用函数注解</h3><p>优化 <code>Flink</code> 应用程序的另一种方法是提供关于用户自定义函数对输入数据做什么的一些信息。由于 <code>Flink</code> 无法解析和理解代码，因此你可以提供关键信息，这将有助于构建更高效的执行计划。有三个注解我们可以使用：</p>\n<ul>\n<li><code>@ForwardedFields</code> - 指定输入值中的哪些字段保持不变并在输出值中使用。</li>\n<li><code>@NotForwardedFields</code> - 指定在输出中同一位置不保留的字段。</li>\n<li><code>@ReadFields</code> - 指定用于计算结果值的字段。你只能指定那些在计算中使用的字段，而不是仅仅将数据拷贝到输出中的字段。</li>\n</ul>\n<p>我们来看看如何使用 <code>ForwardedFields</code> 注解：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Specify that the first element is copied without any changes</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFields</span>(<span class=\"string\">\"0\"</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyFunction</span> <span class=\"keyword\">implements</span> <span class=\"title\">MapFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Double</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Double</span>&gt;&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Long, Double&gt; <span class=\"title\">map</span><span class=\"params\">(Tuple2&lt;Long, Double&gt; value)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// Copy first field without change</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(value.f0, value.f1 + <span class=\"number\">123</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上述代码意味着输入元组的第一个元素将不会改变，并且在返回时也处于同一个位置（译者注：第一个位置）。</p>\n<p>如果你不改变字段，只是简单地将它移到不同的位置上，你同样可以使用 <code>ForwardedFields</code> 注解来实现。下面例子中，我们简单地将输入元组的字段进行交换（译者注：第一个字段移到第二个位置，第二个字段移到第一个位置）：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 1st element goes into the 2nd position, and 2nd element goes into the 1st position</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFields</span>(<span class=\"string\">\"0-&gt;1; 1-&gt;0\"</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SwapArguments</span> <span class=\"keyword\">implements</span> <span class=\"title\">MapFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Double</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Double</span>, <span class=\"title\">Long</span>&gt;&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Double, Long&gt; <span class=\"title\">map</span><span class=\"params\">(Tuple2&lt;Long, Double&gt; value)</span> </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// Swap elements in a tuple</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(value.f1, value.f0);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上面例子中提到的注解只能应用到只有一个输入参数的函数中，比如 <code>map</code> 或者 <code>flatMap</code>。如果你有两个输入参数的函数，你可以使用 <code>ForwardedFieldsFirst</code> 和 <code>ForwardedFieldsSecond</code> 注解分别为第一和第二个参数提供信息。</p>\n<p>下面我们看一下如何在 <code>JoinFunction</code> 接口的实现中使用这些注解（译者注：第一个输入元组的两个字段拷贝到输出元组的第一个和第二个位置，第二个输入元组的第二个字段拷贝到输出元组的第三个位置）：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Two fields from the input tuple are copied to the first and second positions of the output tuple</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFieldsFirst</span>(<span class=\"string\">\"0; 1\"</span>)</span><br><span class=\"line\"><span class=\"comment\">// The third field from the input tuple is copied to the third position of the output tuple</span></span><br><span class=\"line\"><span class=\"meta\">@ForwardedFieldsSecond</span>(<span class=\"string\">\"2\"</span>)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyJoin</span> <span class=\"keyword\">implements</span> <span class=\"title\">JoinFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Integer</span>,<span class=\"title\">String</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Integer</span>,<span class=\"title\">Double</span>&gt;, <span class=\"title\">Tuple3</span>&lt;<span class=\"title\">Integer</span>, <span class=\"title\">String</span>, <span class=\"title\">Double</span>&gt;&gt;() </span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple3&lt;Integer, String, Double&gt; <span class=\"title\">join</span><span class=\"params\">(Tuple2&lt;Integer, String&gt; first, Tuple2&lt;Integer, Double&gt; second)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple3&lt;&gt;(first.f0, first.f1, second.f1);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure></p>\n<p><code>Flink</code> 同样提供了 <code>NotForwardedFieldsFirst</code>, <code>NotForwardedFieldsSecond</code>, <code>ReadFieldsFirst</code>, 和 <code>ReadFirldsSecond</code> 注解来实现同样的功能。</p>\n<h3 id=\"4-选择-join-类型\"><a href=\"#4-选择-join-类型\" class=\"headerlink\" title=\"4. 选择 join 类型\"></a>4. 选择 join 类型</h3><p>如果你告诉 <code>Flink</code> 一些信息，可以加快 <code>join</code> 的速度，但在讨论它为什么会起作用之前，让我们先来谈谈 <code>Flink</code> 是如何执行 <code>join</code>的。</p>\n<p>当 <code>Flink</code> 处理批量数据时，集群中的每台机器都存储了部分数据。要执行 <code>join</code> 操作，<code>Flink</code> 需要找到两个两个数据集中满足 <code>join</code> 条件的所有记录对（译者注：<code>key</code> 相同的数据）。 要做到这一点，<code>Flink</code> 首先必须将两个数据集中具有相同 <code>key</code> 的数据放在集群中的同一台机器上。有两种策略：</p>\n<ul>\n<li><code>Repartition-Repartition</code> 策略：在这种场景下，根据它们的 <code>key</code> 对两个数据集进行重新分区，通过网络发送数据。这就意味着如果数据集非常大，这将花费大量的时间将数据在网络之间进行复制。</li>\n<li><code>Broadcast-Forward</code> 策略：在这种场景下，一个数据集保持不变，将第二个数据集拷贝到集群上第二个数据集拥有第一个数据集部分数据的所有机器上（译者注：将达尔戈数据集进行分发到对应机器上）。</li>\n</ul>\n<p>如果用一个较大的数据集与一个小数据集进行 <code>join</code>，你可以使用 <code>Broadcast-Forward</code> 策略并避免对第一个数据集进行重分区的昂贵代价。这很容易做到：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">ds1.join(ds2, JoinHint.BROADCAST_HASH_FIRST)</span><br></pre></td></tr></table></figure></p>\n<p>这表示第一个数据集比第二个数据集小得多。</p>\n<p><code>Flink</code> 支持的其他 <code>join</code> 提示有以下几种：</p>\n<ul>\n<li><code>BROADCAST_HASH_SECOND</code> – 表示第二个数据集比较小</li>\n<li><code>REPARTITION_HASH_FIRST</code> – 表示第一个数据集比较小</li>\n<li><code>REPARTITION_HASH_SECOND</code> – 表示第二个数据集有点小</li>\n<li><code>REPARTITION_SORT_MERGE</code> – 表示对两个数据集重新分区并使用排序和合并策略</li>\n<li><code>OPTIMIZER_CHOOSES</code> – <code>Flink</code> 优化器将决定如何连接数据集</li>\n</ul>\n<p>原文： <a href=\"https://brewing.codes/2017/10/17/flink-optimize/\" target=\"_blank\" rel=\"noopener\">https://brewing.codes/2017/10/17/flink-optimize/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 安装与启动","date":"2018-01-04T00:54:01.000Z","_content":"\n### 1. 下载\n\nFlink 可以运行在 Linux, Mac OS X和Windows上。为了运行Flink, 唯一的要求是必须在Java 7.x (或者更高版本)上安装。Windows 用户, 请查看 Flink在Windows上的安装指南。\n\n你可以使用以下命令检查Java当前运行的版本：\n```\njava -version\n```\n如果你安装的是Java 8，输出结果类似于如下:\n```\njava version \"1.8.0_91\"\nJava(TM) SE Runtime Environment (build 1.8.0_91-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\n```\n从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合方式。如果你只是打算使用本地文件系统，那么可以使用任何版本的Hadoop。进入下载目录，解压下载的压缩包:\n```\nxiaosi@yoona:~$ tar -zxvf flink-1.3.2-bin-hadoop27-scala_2.11.tgz -C opt/\nflink-1.3.2/\nflink-1.3.2/opt/\nflink-1.3.2/opt/flink-cep_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-datadog-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-statsd-1.3.2.jar\nflink-1.3.2/opt/flink-gelly_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-dropwizard-1.3.2.jar\nflink-1.3.2/opt/flink-gelly-scala_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-ganglia-1.3.2.jar\nflink-1.3.2/opt/flink-cep-scala_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-table_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-ml_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-graphite-1.3.2.jar\nflink-1.3.2/lib/\n...\n```\n### 2. 启动本地集群\n\n使用如下命令启动Flink：\n```\nxiaosi@yoona:~/opt/flink-1.3.2$ ./bin/start-local.sh\nStarting jobmanager daemon on host yoona.\n```\n通过访问 http://localhost:8081 检查JobManager网页,确保所有组件都启动并已运行。网页会显示一个有效的TaskManager实例。\n\n![img](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-1.png?raw=true)\n\n你也可以通过检查日志目录里的日志文件来验证系统是否已经运行:\n```\nxiaosi@yoona:~/opt/flink-1.3.2/log$ cat flink-xiaosi-jobmanager-0-yoona.log | less\n2017-10-16 14:42:10,972 INFO  org.apache.flink.runtime.jobmanager.JobManager                -  Starting JobManager (Version: 1.3.2, Rev:0399bee, Date:03.08.2017 @ 10:23:11 UTC)\n...\n2017-10-16 14:42:11,109 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager without high-availability\n2017-10-16 14:42:11,111 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager on localhost:6123 with execution mode LOCAL\n...\n2017-10-16 14:42:11,915 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager web frontend\n...\n2017-10-16 14:42:13,941 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at localhost (akka://flink/user/taskmanager) as 0df4d4ebd25ffec4878906726c29f88c. Current number of registered hosts is 1. Current number of alive task slots is 1.\n...\n\n```\n\n### 3. Example Code\n\n你可以在GitHub上找到SocketWindowWordCount例子的完整代码，有[Java](https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java)和[Scala](https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/scala/org/apache/flink/streaming/scala/examples/socket/SocketWindowWordCount.scala)两个版本。\n\nScala:\n```scala\npackage org.apache.flink.streaming.scala.examples.socket\n\nimport org.apache.flink.api.java.utils.ParameterTool\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.time.Time\n\n/**\n * Implements a streaming windowed version of the \"WordCount\" program.\n *\n * This program connects to a server socket and reads strings from the socket.\n * The easiest way to try this out is to open a text sever (at port 12345)\n * using the ''netcat'' tool via\n * {{{\n * nc -l 12345\n * }}}\n * and run this example with the hostname and the port as arguments..\n */\nobject SocketWindowWordCount {\n\n  /** Main program method */\n  def main(args: Array[String]) : Unit = {\n\n    // the host and the port to connect to\n    var hostname: String = \"localhost\"\n    var port: Int = 0\n\n    try {\n      val params = ParameterTool.fromArgs(args)\n      hostname = if (params.has(\"hostname\")) params.get(\"hostname\") else \"localhost\"\n      port = params.getInt(\"port\")\n    } catch {\n      case e: Exception => {\n        System.err.println(\"No port specified. Please run 'SocketWindowWordCount \" +\n          \"--hostname <hostname> --port <port>', where hostname (localhost by default) and port \" +\n          \"is the address of the text server\")\n        System.err.println(\"To start a simple text server, run 'netcat -l <port>' \" +\n          \"and type the input text into the command line\")\n        return\n      }\n    }\n\n    // get the execution environment\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\n    // get input data by connecting to the socket\n    val text: DataStream[String] = env.socketTextStream(hostname, port, '\\n')\n\n    // parse the data, group it, window it, and aggregate the counts\n    val windowCounts = text\n          .flatMap { w => w.split(\"\\\\s\") }\n          .map { w => WordWithCount(w, 1) }\n          .keyBy(\"word\")\n          .timeWindow(Time.seconds(5))\n          .sum(\"count\")\n\n    // print the results with a single thread, rather than in parallel\n    windowCounts.print().setParallelism(1)\n\n    env.execute(\"Socket Window WordCount\")\n  }\n\n  /** Data type for words with count */\n  case class WordWithCount(word: String, count: Long)\n}\n```\nJava版本:\n```java\npackage org.apache.flink.streaming.examples.socket;\n\nimport org.apache.flink.api.common.functions.FlatMapFunction;\nimport org.apache.flink.api.common.functions.ReduceFunction;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.windowing.time.Time;\nimport org.apache.flink.util.Collector;\n\n/**\n * Implements a streaming windowed version of the \"WordCount\" program.\n *\n * <p>This program connects to a server socket and reads strings from the socket.\n * The easiest way to try this out is to open a text server (at port 12345)\n * using the <i>netcat</i> tool via\n * <pre>\n * nc -l 12345\n * </pre>\n * and run this example with the hostname and the port as arguments.\n */\n@SuppressWarnings(\"serial\")\npublic class SocketWindowWordCount {\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// the host and the port to connect to\n\t\tfinal String hostname;\n\t\tfinal int port;\n\t\ttry {\n\t\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\t\thostname = params.has(\"hostname\") ? params.get(\"hostname\") : \"localhost\";\n\t\t\tport = params.getInt(\"port\");\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"No port specified. Please run 'SocketWindowWordCount \" +\n\t\t\t\t\"--hostname <hostname> --port <port>', where hostname (localhost by default) \" +\n\t\t\t\t\"and port is the address of the text server\");\n\t\t\tSystem.err.println(\"To start a simple text server, run 'netcat -l <port>' and \" +\n\t\t\t\t\"type the input text into the command line\");\n\t\t\treturn;\n\t\t}\n\n\t\t// get the execution environment\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// get input data by connecting to the socket\n\t\tDataStream<String> text = env.socketTextStream(hostname, port, \"\\n\");\n\n\t\t// parse the data, group it, window it, and aggregate the counts\n\t\tDataStream<WordWithCount> windowCounts = text\n\n\t\t\t\t.flatMap(new FlatMapFunction<String, WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(String value, Collector<WordWithCount> out) {\n\t\t\t\t\t\tfor (String word : value.split(\"\\\\s\")) {\n\t\t\t\t\t\t\tout.collect(new WordWithCount(word, 1L));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\n\t\t\t\t.keyBy(\"word\")\n\t\t\t\t.timeWindow(Time.seconds(5))\n\n\t\t\t\t.reduce(new ReduceFunction<WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic WordWithCount reduce(WordWithCount a, WordWithCount b) {\n\t\t\t\t\t\treturn new WordWithCount(a.word, a.count + b.count);\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t// print the results with a single thread, rather than in parallel\n\t\twindowCounts.print().setParallelism(1);\n\n\t\tenv.execute(\"Socket Window WordCount\");\n\t}\n\n\t// ------------------------------------------------------------------------\n\n\t/**\n\t * Data type for words with count.\n\t */\n\tpublic static class WordWithCount {\n\n\t\tpublic String word;\n\t\tpublic long count;\n\n\t\tpublic WordWithCount() {}\n\n\t\tpublic WordWithCount(String word, long count) {\n\t\t\tthis.word = word;\n\t\t\tthis.count = count;\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn word + \" : \" + count;\n\t\t}\n\t}\n}\n```\n### 4. 运行Example\n\n现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读取一段文本，并且每隔5秒打印之前5秒内每个单词出现的个数。例如：\n```\na tumbling window of processing time, as long as words are floating in.\n```\n(1) 首先,我们可以通过netcat命令来启动本地服务:\n```\nnc -l 9000\n```\n(2) 提交Flink程序:\n```\nxiaosi@yoona:~/opt/flink-1.3.2$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000\nCluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123\nUsing address localhost:6123 to connect to JobManager.\nJobManager web interface address http://localhost:8081\nStarting execution of program\nSubmitting job with JobID: a963626a1e09f7aeb0dc34412adfb801. Waiting for job completion.\nConnected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#941160871] with leader session id 00000000-0000-0000-0000-000000000000.\n10/16/2017 15:12:26\tJob execution switched to status RUNNING.\n10/16/2017 15:12:26\tSource: Socket Stream -> Flat Map(1/1) switched to SCHEDULED\n10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -> Sink: Unnamed(1/1) switched to SCHEDULED\n10/16/2017 15:12:26\tSource: Socket Stream -> Flat Map(1/1) switched to DEPLOYING\n10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -> Sink: Unnamed(1/1) switched to DEPLOYING\n10/16/2017 15:12:26\tSource: Socket Stream -> Flat Map(1/1) switched to RUNNING\n10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -> Sink: Unnamed(1/1) switched to RUNNING\n```\n应用程序连接socket并等待输入，你可以通过web界面来验证任务期望的运行结果：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-2.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-3.png?raw=true)\n\n单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) :\n```\nxiaosi@yoona:~/opt/flink-1.3.2$  nc -l 9000\nlorem ipsum\nipsum ipsum ipsum\nbye\n```\n.out文件将在每个时间窗口截止之际打印每个单词的个数：\n```\nxiaosi@yoona:~/opt/flink-1.3.2$  tail -f log/flink-*-jobmanager-*.out\nlorem : 1\nbye : 1\nipsum : 4\n```\n使用以下命令来停止Flink:\n```\n ./bin/stop-local.sh\n```\n\n阅读更多的[例子](https://ci.apache.org/projects/flink/flink-docs-release-1.3/examples/)来熟悉Flink的编程API。 当你完成这些，可以继续阅读[streaming指南](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/datastream_api.html)。\n","source":"_posts/Flink/[Flink]Flink 安装与启动.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 安装与启动\ndate: 2018-01-04 08:54:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n### 1. 下载\n\nFlink 可以运行在 Linux, Mac OS X和Windows上。为了运行Flink, 唯一的要求是必须在Java 7.x (或者更高版本)上安装。Windows 用户, 请查看 Flink在Windows上的安装指南。\n\n你可以使用以下命令检查Java当前运行的版本：\n```\njava -version\n```\n如果你安装的是Java 8，输出结果类似于如下:\n```\njava version \"1.8.0_91\"\nJava(TM) SE Runtime Environment (build 1.8.0_91-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\n```\n从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合方式。如果你只是打算使用本地文件系统，那么可以使用任何版本的Hadoop。进入下载目录，解压下载的压缩包:\n```\nxiaosi@yoona:~$ tar -zxvf flink-1.3.2-bin-hadoop27-scala_2.11.tgz -C opt/\nflink-1.3.2/\nflink-1.3.2/opt/\nflink-1.3.2/opt/flink-cep_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-datadog-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-statsd-1.3.2.jar\nflink-1.3.2/opt/flink-gelly_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-dropwizard-1.3.2.jar\nflink-1.3.2/opt/flink-gelly-scala_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-ganglia-1.3.2.jar\nflink-1.3.2/opt/flink-cep-scala_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-table_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-ml_2.11-1.3.2.jar\nflink-1.3.2/opt/flink-metrics-graphite-1.3.2.jar\nflink-1.3.2/lib/\n...\n```\n### 2. 启动本地集群\n\n使用如下命令启动Flink：\n```\nxiaosi@yoona:~/opt/flink-1.3.2$ ./bin/start-local.sh\nStarting jobmanager daemon on host yoona.\n```\n通过访问 http://localhost:8081 检查JobManager网页,确保所有组件都启动并已运行。网页会显示一个有效的TaskManager实例。\n\n![img](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-1.png?raw=true)\n\n你也可以通过检查日志目录里的日志文件来验证系统是否已经运行:\n```\nxiaosi@yoona:~/opt/flink-1.3.2/log$ cat flink-xiaosi-jobmanager-0-yoona.log | less\n2017-10-16 14:42:10,972 INFO  org.apache.flink.runtime.jobmanager.JobManager                -  Starting JobManager (Version: 1.3.2, Rev:0399bee, Date:03.08.2017 @ 10:23:11 UTC)\n...\n2017-10-16 14:42:11,109 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager without high-availability\n2017-10-16 14:42:11,111 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager on localhost:6123 with execution mode LOCAL\n...\n2017-10-16 14:42:11,915 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager web frontend\n...\n2017-10-16 14:42:13,941 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at localhost (akka://flink/user/taskmanager) as 0df4d4ebd25ffec4878906726c29f88c. Current number of registered hosts is 1. Current number of alive task slots is 1.\n...\n\n```\n\n### 3. Example Code\n\n你可以在GitHub上找到SocketWindowWordCount例子的完整代码，有[Java](https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java)和[Scala](https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/scala/org/apache/flink/streaming/scala/examples/socket/SocketWindowWordCount.scala)两个版本。\n\nScala:\n```scala\npackage org.apache.flink.streaming.scala.examples.socket\n\nimport org.apache.flink.api.java.utils.ParameterTool\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.time.Time\n\n/**\n * Implements a streaming windowed version of the \"WordCount\" program.\n *\n * This program connects to a server socket and reads strings from the socket.\n * The easiest way to try this out is to open a text sever (at port 12345)\n * using the ''netcat'' tool via\n * {{{\n * nc -l 12345\n * }}}\n * and run this example with the hostname and the port as arguments..\n */\nobject SocketWindowWordCount {\n\n  /** Main program method */\n  def main(args: Array[String]) : Unit = {\n\n    // the host and the port to connect to\n    var hostname: String = \"localhost\"\n    var port: Int = 0\n\n    try {\n      val params = ParameterTool.fromArgs(args)\n      hostname = if (params.has(\"hostname\")) params.get(\"hostname\") else \"localhost\"\n      port = params.getInt(\"port\")\n    } catch {\n      case e: Exception => {\n        System.err.println(\"No port specified. Please run 'SocketWindowWordCount \" +\n          \"--hostname <hostname> --port <port>', where hostname (localhost by default) and port \" +\n          \"is the address of the text server\")\n        System.err.println(\"To start a simple text server, run 'netcat -l <port>' \" +\n          \"and type the input text into the command line\")\n        return\n      }\n    }\n\n    // get the execution environment\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\n    // get input data by connecting to the socket\n    val text: DataStream[String] = env.socketTextStream(hostname, port, '\\n')\n\n    // parse the data, group it, window it, and aggregate the counts\n    val windowCounts = text\n          .flatMap { w => w.split(\"\\\\s\") }\n          .map { w => WordWithCount(w, 1) }\n          .keyBy(\"word\")\n          .timeWindow(Time.seconds(5))\n          .sum(\"count\")\n\n    // print the results with a single thread, rather than in parallel\n    windowCounts.print().setParallelism(1)\n\n    env.execute(\"Socket Window WordCount\")\n  }\n\n  /** Data type for words with count */\n  case class WordWithCount(word: String, count: Long)\n}\n```\nJava版本:\n```java\npackage org.apache.flink.streaming.examples.socket;\n\nimport org.apache.flink.api.common.functions.FlatMapFunction;\nimport org.apache.flink.api.common.functions.ReduceFunction;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.windowing.time.Time;\nimport org.apache.flink.util.Collector;\n\n/**\n * Implements a streaming windowed version of the \"WordCount\" program.\n *\n * <p>This program connects to a server socket and reads strings from the socket.\n * The easiest way to try this out is to open a text server (at port 12345)\n * using the <i>netcat</i> tool via\n * <pre>\n * nc -l 12345\n * </pre>\n * and run this example with the hostname and the port as arguments.\n */\n@SuppressWarnings(\"serial\")\npublic class SocketWindowWordCount {\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// the host and the port to connect to\n\t\tfinal String hostname;\n\t\tfinal int port;\n\t\ttry {\n\t\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\t\thostname = params.has(\"hostname\") ? params.get(\"hostname\") : \"localhost\";\n\t\t\tport = params.getInt(\"port\");\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"No port specified. Please run 'SocketWindowWordCount \" +\n\t\t\t\t\"--hostname <hostname> --port <port>', where hostname (localhost by default) \" +\n\t\t\t\t\"and port is the address of the text server\");\n\t\t\tSystem.err.println(\"To start a simple text server, run 'netcat -l <port>' and \" +\n\t\t\t\t\"type the input text into the command line\");\n\t\t\treturn;\n\t\t}\n\n\t\t// get the execution environment\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// get input data by connecting to the socket\n\t\tDataStream<String> text = env.socketTextStream(hostname, port, \"\\n\");\n\n\t\t// parse the data, group it, window it, and aggregate the counts\n\t\tDataStream<WordWithCount> windowCounts = text\n\n\t\t\t\t.flatMap(new FlatMapFunction<String, WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(String value, Collector<WordWithCount> out) {\n\t\t\t\t\t\tfor (String word : value.split(\"\\\\s\")) {\n\t\t\t\t\t\t\tout.collect(new WordWithCount(word, 1L));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\n\t\t\t\t.keyBy(\"word\")\n\t\t\t\t.timeWindow(Time.seconds(5))\n\n\t\t\t\t.reduce(new ReduceFunction<WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic WordWithCount reduce(WordWithCount a, WordWithCount b) {\n\t\t\t\t\t\treturn new WordWithCount(a.word, a.count + b.count);\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t// print the results with a single thread, rather than in parallel\n\t\twindowCounts.print().setParallelism(1);\n\n\t\tenv.execute(\"Socket Window WordCount\");\n\t}\n\n\t// ------------------------------------------------------------------------\n\n\t/**\n\t * Data type for words with count.\n\t */\n\tpublic static class WordWithCount {\n\n\t\tpublic String word;\n\t\tpublic long count;\n\n\t\tpublic WordWithCount() {}\n\n\t\tpublic WordWithCount(String word, long count) {\n\t\t\tthis.word = word;\n\t\t\tthis.count = count;\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn word + \" : \" + count;\n\t\t}\n\t}\n}\n```\n### 4. 运行Example\n\n现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读取一段文本，并且每隔5秒打印之前5秒内每个单词出现的个数。例如：\n```\na tumbling window of processing time, as long as words are floating in.\n```\n(1) 首先,我们可以通过netcat命令来启动本地服务:\n```\nnc -l 9000\n```\n(2) 提交Flink程序:\n```\nxiaosi@yoona:~/opt/flink-1.3.2$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000\nCluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123\nUsing address localhost:6123 to connect to JobManager.\nJobManager web interface address http://localhost:8081\nStarting execution of program\nSubmitting job with JobID: a963626a1e09f7aeb0dc34412adfb801. Waiting for job completion.\nConnected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#941160871] with leader session id 00000000-0000-0000-0000-000000000000.\n10/16/2017 15:12:26\tJob execution switched to status RUNNING.\n10/16/2017 15:12:26\tSource: Socket Stream -> Flat Map(1/1) switched to SCHEDULED\n10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -> Sink: Unnamed(1/1) switched to SCHEDULED\n10/16/2017 15:12:26\tSource: Socket Stream -> Flat Map(1/1) switched to DEPLOYING\n10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -> Sink: Unnamed(1/1) switched to DEPLOYING\n10/16/2017 15:12:26\tSource: Socket Stream -> Flat Map(1/1) switched to RUNNING\n10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -> Sink: Unnamed(1/1) switched to RUNNING\n```\n应用程序连接socket并等待输入，你可以通过web界面来验证任务期望的运行结果：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-2.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-3.png?raw=true)\n\n单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) :\n```\nxiaosi@yoona:~/opt/flink-1.3.2$  nc -l 9000\nlorem ipsum\nipsum ipsum ipsum\nbye\n```\n.out文件将在每个时间窗口截止之际打印每个单词的个数：\n```\nxiaosi@yoona:~/opt/flink-1.3.2$  tail -f log/flink-*-jobmanager-*.out\nlorem : 1\nbye : 1\nipsum : 4\n```\n使用以下命令来停止Flink:\n```\n ./bin/stop-local.sh\n```\n\n阅读更多的[例子](https://ci.apache.org/projects/flink/flink-docs-release-1.3/examples/)来熟悉Flink的编程API。 当你完成这些，可以继续阅读[streaming指南](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/datastream_api.html)。\n","slug":"Flink/[Flink]Flink 安装与启动","published":1,"updated":"2018-01-29T09:36:59.658Z","comments":1,"photos":[],"link":"","_id":"cje58tipk000aordbfael9wpz","content":"<h3 id=\"1-下载\"><a href=\"#1-下载\" class=\"headerlink\" title=\"1. 下载\"></a>1. 下载</h3><p>Flink 可以运行在 Linux, Mac OS X和Windows上。为了运行Flink, 唯一的要求是必须在Java 7.x (或者更高版本)上安装。Windows 用户, 请查看 Flink在Windows上的安装指南。</p>\n<p>你可以使用以下命令检查Java当前运行的版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">java -version</span><br></pre></td></tr></table></figure></p>\n<p>如果你安装的是Java 8，输出结果类似于如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">java version &quot;1.8.0_91&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_91-b14)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)</span><br></pre></td></tr></table></figure></p>\n<p>从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合方式。如果你只是打算使用本地文件系统，那么可以使用任何版本的Hadoop。进入下载目录，解压下载的压缩包:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ tar -zxvf flink-1.3.2-bin-hadoop27-scala_2.11.tgz -C opt/</span><br><span class=\"line\">flink-1.3.2/</span><br><span class=\"line\">flink-1.3.2/opt/</span><br><span class=\"line\">flink-1.3.2/opt/flink-cep_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-datadog-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-statsd-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-gelly_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-dropwizard-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-gelly-scala_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-ganglia-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-cep-scala_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-table_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-ml_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-graphite-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/lib/</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-启动本地集群\"><a href=\"#2-启动本地集群\" class=\"headerlink\" title=\"2. 启动本地集群\"></a>2. 启动本地集群</h3><p>使用如下命令启动Flink：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/start-local.sh</span><br><span class=\"line\">Starting jobmanager daemon on host yoona.</span><br></pre></td></tr></table></figure></p>\n<p>通过访问 <a href=\"http://localhost:8081\" target=\"_blank\" rel=\"noopener\">http://localhost:8081</a> 检查JobManager网页,确保所有组件都启动并已运行。网页会显示一个有效的TaskManager实例。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-1.png?raw=true\" alt=\"img\"></p>\n<p>你也可以通过检查日志目录里的日志文件来验证系统是否已经运行:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2/log$ cat flink-xiaosi-jobmanager-0-yoona.log | less</span><br><span class=\"line\">2017-10-16 14:42:10,972 INFO  org.apache.flink.runtime.jobmanager.JobManager                -  Starting JobManager (Version: 1.3.2, Rev:0399bee, Date:03.08.2017 @ 10:23:11 UTC)</span><br><span class=\"line\">...</span><br><span class=\"line\">2017-10-16 14:42:11,109 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager without high-availability</span><br><span class=\"line\">2017-10-16 14:42:11,111 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager on localhost:6123 with execution mode LOCAL</span><br><span class=\"line\">...</span><br><span class=\"line\">2017-10-16 14:42:11,915 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager web frontend</span><br><span class=\"line\">...</span><br><span class=\"line\">2017-10-16 14:42:13,941 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at localhost (akka://flink/user/taskmanager) as 0df4d4ebd25ffec4878906726c29f88c. Current number of registered hosts is 1. Current number of alive task slots is 1.</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-Example-Code\"><a href=\"#3-Example-Code\" class=\"headerlink\" title=\"3. Example Code\"></a>3. Example Code</h3><p>你可以在GitHub上找到SocketWindowWordCount例子的完整代码，有<a href=\"https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java\" target=\"_blank\" rel=\"noopener\">Java</a>和<a href=\"https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/scala/org/apache/flink/streaming/scala/examples/socket/SocketWindowWordCount.scala\" target=\"_blank\" rel=\"noopener\">Scala</a>两个版本。</p>\n<p>Scala:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.apache.flink.streaming.scala.examples.socket</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.utils.<span class=\"type\">ParameterTool</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.scala._</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.windowing.time.<span class=\"type\">Time</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Implements a streaming windowed version of the \"WordCount\" program.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * This program connects to a server socket and reads strings from the socket.</span></span><br><span class=\"line\"><span class=\"comment\"> * The easiest way to try this out is to open a text sever (at port 12345)</span></span><br><span class=\"line\"><span class=\"comment\"> * using the ''netcat'' tool via</span></span><br><span class=\"line\"><span class=\"comment\"> * &#123;&#123;&#123;</span></span><br><span class=\"line\"><span class=\"comment\"> * nc -l 12345</span></span><br><span class=\"line\"><span class=\"comment\"> * &#125;&#125;&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> * and run this example with the hostname and the port as arguments..</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">SocketWindowWordCount</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/** Main program method */</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) : <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// the host and the port to connect to</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> hostname: <span class=\"type\">String</span> = <span class=\"string\">\"localhost\"</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> port: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> params = <span class=\"type\">ParameterTool</span>.fromArgs(args)</span><br><span class=\"line\">      hostname = <span class=\"keyword\">if</span> (params.has(<span class=\"string\">\"hostname\"</span>)) params.get(<span class=\"string\">\"hostname\"</span>) <span class=\"keyword\">else</span> <span class=\"string\">\"localhost\"</span></span><br><span class=\"line\">      port = params.getInt(<span class=\"string\">\"port\"</span>)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</span><br><span class=\"line\">        <span class=\"type\">System</span>.err.println(<span class=\"string\">\"No port specified. Please run 'SocketWindowWordCount \"</span> +</span><br><span class=\"line\">          <span class=\"string\">\"--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) and port \"</span> +</span><br><span class=\"line\">          <span class=\"string\">\"is the address of the text server\"</span>)</span><br><span class=\"line\">        <span class=\"type\">System</span>.err.println(<span class=\"string\">\"To start a simple text server, run 'netcat -l &lt;port&gt;' \"</span> +</span><br><span class=\"line\">          <span class=\"string\">\"and type the input text into the command line\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// get the execution environment</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> env: <span class=\"type\">StreamExecutionEnvironment</span> = <span class=\"type\">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// get input data by connecting to the socket</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> text: <span class=\"type\">DataStream</span>[<span class=\"type\">String</span>] = env.socketTextStream(hostname, port, '\\n')</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> windowCounts = text</span><br><span class=\"line\">          .flatMap &#123; w =&gt; w.split(<span class=\"string\">\"\\\\s\"</span>) &#125;</span><br><span class=\"line\">          .map &#123; w =&gt; <span class=\"type\">WordWithCount</span>(w, <span class=\"number\">1</span>) &#125;</span><br><span class=\"line\">          .keyBy(<span class=\"string\">\"word\"</span>)</span><br><span class=\"line\">          .timeWindow(<span class=\"type\">Time</span>.seconds(<span class=\"number\">5</span>))</span><br><span class=\"line\">          .sum(<span class=\"string\">\"count\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// print the results with a single thread, rather than in parallel</span></span><br><span class=\"line\">    windowCounts.print().setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    env.execute(<span class=\"string\">\"Socket Window WordCount\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/** Data type for words with count */</span></span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordWithCount</span>(<span class=\"params\">word: <span class=\"type\">String</span>, count: <span class=\"type\">Long</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.apache.flink.streaming.examples.socket;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.ReduceFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Implements a streaming windowed version of the \"WordCount\" program.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;This program connects to a server socket and reads strings from the socket.</span></span><br><span class=\"line\"><span class=\"comment\"> * The easiest way to try this out is to open a text server (at port 12345)</span></span><br><span class=\"line\"><span class=\"comment\"> * using the &lt;i&gt;netcat&lt;/i&gt; tool via</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;pre&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> * nc -l 12345</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;/pre&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> * and run this example with the hostname and the port as arguments.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@SuppressWarnings</span>(<span class=\"string\">\"serial\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SocketWindowWordCount</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// the host and the port to connect to</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">final</span> String hostname;</span><br><span class=\"line\">\t\t<span class=\"keyword\">final</span> <span class=\"keyword\">int</span> port;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class=\"line\">\t\t\thostname = params.has(<span class=\"string\">\"hostname\"</span>) ? params.get(<span class=\"string\">\"hostname\"</span>) : <span class=\"string\">\"localhost\"</span>;</span><br><span class=\"line\">\t\t\tport = params.getInt(<span class=\"string\">\"port\"</span>);</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\tSystem.err.println(<span class=\"string\">\"No port specified. Please run 'SocketWindowWordCount \"</span> +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) \"</span> +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"and port is the address of the text server\"</span>);</span><br><span class=\"line\">\t\t\tSystem.err.println(<span class=\"string\">\"To start a simple text server, run 'netcat -l &lt;port&gt;' and \"</span> +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"type the input text into the command line\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// get the execution environment</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// get input data by connecting to the socket</span></span><br><span class=\"line\">\t\tDataStream&lt;String&gt; text = env.socketTextStream(hostname, port, <span class=\"string\">\"\\n\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class=\"line\">\t\tDataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, WordWithCount&gt;() &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">flatMap</span><span class=\"params\">(String value, Collector&lt;WordWithCount&gt; out)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">for</span> (String word : value.split(<span class=\"string\">\"\\\\s\"</span>)) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tout.collect(<span class=\"keyword\">new</span> WordWithCount(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t.keyBy(<span class=\"string\">\"word\"</span>)</span><br><span class=\"line\">\t\t\t\t.timeWindow(Time.seconds(<span class=\"number\">5</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t.reduce(<span class=\"keyword\">new</span> ReduceFunction&lt;WordWithCount&gt;() &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> WordWithCount <span class=\"title\">reduce</span><span class=\"params\">(WordWithCount a, WordWithCount b)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">new</span> WordWithCount(a.word, a.count + b.count);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// print the results with a single thread, rather than in parallel</span></span><br><span class=\"line\">\t\twindowCounts.print().setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tenv.execute(<span class=\"string\">\"Socket Window WordCount\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// ------------------------------------------------------------------------</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Data type for words with count.</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordWithCount</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">public</span> String word;</span><br><span class=\"line\">\t\t<span class=\"keyword\">public</span> <span class=\"keyword\">long</span> count;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">WordWithCount</span><span class=\"params\">()</span> </span>&#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">WordWithCount</span><span class=\"params\">(String word, <span class=\"keyword\">long</span> count)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">this</span>.word = word;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">this</span>.count = count;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> word + <span class=\"string\">\" : \"</span> + count;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-运行Example\"><a href=\"#4-运行Example\" class=\"headerlink\" title=\"4. 运行Example\"></a>4. 运行Example</h3><p>现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读取一段文本，并且每隔5秒打印之前5秒内每个单词出现的个数。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">a tumbling window of processing time, as long as words are floating in.</span><br></pre></td></tr></table></figure></p>\n<p>(1) 首先,我们可以通过netcat命令来启动本地服务:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">nc -l 9000</span><br></pre></td></tr></table></figure></p>\n<p>(2) 提交Flink程序:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000</span><br><span class=\"line\">Cluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123</span><br><span class=\"line\">Using address localhost:6123 to connect to JobManager.</span><br><span class=\"line\">JobManager web interface address http://localhost:8081</span><br><span class=\"line\">Starting execution of program</span><br><span class=\"line\">Submitting job with JobID: a963626a1e09f7aeb0dc34412adfb801. Waiting for job completion.</span><br><span class=\"line\">Connected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#941160871] with leader session id 00000000-0000-0000-0000-000000000000.</span><br><span class=\"line\">10/16/2017 15:12:26\tJob execution switched to status RUNNING.</span><br><span class=\"line\">10/16/2017 15:12:26\tSource: Socket Stream -&gt; Flat Map(1/1) switched to SCHEDULED</span><br><span class=\"line\">10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to SCHEDULED</span><br><span class=\"line\">10/16/2017 15:12:26\tSource: Socket Stream -&gt; Flat Map(1/1) switched to DEPLOYING</span><br><span class=\"line\">10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to DEPLOYING</span><br><span class=\"line\">10/16/2017 15:12:26\tSource: Socket Stream -&gt; Flat Map(1/1) switched to RUNNING</span><br><span class=\"line\">10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to RUNNING</span><br></pre></td></tr></table></figure></p>\n<p>应用程序连接socket并等待输入，你可以通过web界面来验证任务期望的运行结果：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-2.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-3.png?raw=true\" alt=\"\"></p>\n<p>单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) :<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$  nc -l 9000</span><br><span class=\"line\">lorem ipsum</span><br><span class=\"line\">ipsum ipsum ipsum</span><br><span class=\"line\">bye</span><br></pre></td></tr></table></figure></p>\n<p>.out文件将在每个时间窗口截止之际打印每个单词的个数：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$  tail -f log/flink-*-jobmanager-*.out</span><br><span class=\"line\">lorem : 1</span><br><span class=\"line\">bye : 1</span><br><span class=\"line\">ipsum : 4</span><br></pre></td></tr></table></figure></p>\n<p>使用以下命令来停止Flink:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/stop-local.sh</span><br></pre></td></tr></table></figure></p>\n<p>阅读更多的<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/examples/\" target=\"_blank\" rel=\"noopener\">例子</a>来熟悉Flink的编程API。 当你完成这些，可以继续阅读<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/datastream_api.html\" target=\"_blank\" rel=\"noopener\">streaming指南</a>。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-下载\"><a href=\"#1-下载\" class=\"headerlink\" title=\"1. 下载\"></a>1. 下载</h3><p>Flink 可以运行在 Linux, Mac OS X和Windows上。为了运行Flink, 唯一的要求是必须在Java 7.x (或者更高版本)上安装。Windows 用户, 请查看 Flink在Windows上的安装指南。</p>\n<p>你可以使用以下命令检查Java当前运行的版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">java -version</span><br></pre></td></tr></table></figure></p>\n<p>如果你安装的是Java 8，输出结果类似于如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">java version &quot;1.8.0_91&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_91-b14)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)</span><br></pre></td></tr></table></figure></p>\n<p>从下载页下载一个二进制的包，你可以选择任何你喜欢的Hadoop/Scala组合方式。如果你只是打算使用本地文件系统，那么可以使用任何版本的Hadoop。进入下载目录，解压下载的压缩包:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ tar -zxvf flink-1.3.2-bin-hadoop27-scala_2.11.tgz -C opt/</span><br><span class=\"line\">flink-1.3.2/</span><br><span class=\"line\">flink-1.3.2/opt/</span><br><span class=\"line\">flink-1.3.2/opt/flink-cep_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-datadog-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-statsd-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-gelly_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-dropwizard-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-gelly-scala_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-ganglia-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-cep-scala_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-table_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-ml_2.11-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/opt/flink-metrics-graphite-1.3.2.jar</span><br><span class=\"line\">flink-1.3.2/lib/</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-启动本地集群\"><a href=\"#2-启动本地集群\" class=\"headerlink\" title=\"2. 启动本地集群\"></a>2. 启动本地集群</h3><p>使用如下命令启动Flink：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/start-local.sh</span><br><span class=\"line\">Starting jobmanager daemon on host yoona.</span><br></pre></td></tr></table></figure></p>\n<p>通过访问 <a href=\"http://localhost:8081\" target=\"_blank\" rel=\"noopener\">http://localhost:8081</a> 检查JobManager网页,确保所有组件都启动并已运行。网页会显示一个有效的TaskManager实例。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-1.png?raw=true\" alt=\"img\"></p>\n<p>你也可以通过检查日志目录里的日志文件来验证系统是否已经运行:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2/log$ cat flink-xiaosi-jobmanager-0-yoona.log | less</span><br><span class=\"line\">2017-10-16 14:42:10,972 INFO  org.apache.flink.runtime.jobmanager.JobManager                -  Starting JobManager (Version: 1.3.2, Rev:0399bee, Date:03.08.2017 @ 10:23:11 UTC)</span><br><span class=\"line\">...</span><br><span class=\"line\">2017-10-16 14:42:11,109 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager without high-availability</span><br><span class=\"line\">2017-10-16 14:42:11,111 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager on localhost:6123 with execution mode LOCAL</span><br><span class=\"line\">...</span><br><span class=\"line\">2017-10-16 14:42:11,915 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager web frontend</span><br><span class=\"line\">...</span><br><span class=\"line\">2017-10-16 14:42:13,941 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at localhost (akka://flink/user/taskmanager) as 0df4d4ebd25ffec4878906726c29f88c. Current number of registered hosts is 1. Current number of alive task slots is 1.</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-Example-Code\"><a href=\"#3-Example-Code\" class=\"headerlink\" title=\"3. Example Code\"></a>3. Example Code</h3><p>你可以在GitHub上找到SocketWindowWordCount例子的完整代码，有<a href=\"https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java\" target=\"_blank\" rel=\"noopener\">Java</a>和<a href=\"https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/scala/org/apache/flink/streaming/scala/examples/socket/SocketWindowWordCount.scala\" target=\"_blank\" rel=\"noopener\">Scala</a>两个版本。</p>\n<p>Scala:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.apache.flink.streaming.scala.examples.socket</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.utils.<span class=\"type\">ParameterTool</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.scala._</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.windowing.time.<span class=\"type\">Time</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Implements a streaming windowed version of the \"WordCount\" program.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * This program connects to a server socket and reads strings from the socket.</span></span><br><span class=\"line\"><span class=\"comment\"> * The easiest way to try this out is to open a text sever (at port 12345)</span></span><br><span class=\"line\"><span class=\"comment\"> * using the ''netcat'' tool via</span></span><br><span class=\"line\"><span class=\"comment\"> * &#123;&#123;&#123;</span></span><br><span class=\"line\"><span class=\"comment\"> * nc -l 12345</span></span><br><span class=\"line\"><span class=\"comment\"> * &#125;&#125;&#125;</span></span><br><span class=\"line\"><span class=\"comment\"> * and run this example with the hostname and the port as arguments..</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">SocketWindowWordCount</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/** Main program method */</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) : <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// the host and the port to connect to</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> hostname: <span class=\"type\">String</span> = <span class=\"string\">\"localhost\"</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> port: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> params = <span class=\"type\">ParameterTool</span>.fromArgs(args)</span><br><span class=\"line\">      hostname = <span class=\"keyword\">if</span> (params.has(<span class=\"string\">\"hostname\"</span>)) params.get(<span class=\"string\">\"hostname\"</span>) <span class=\"keyword\">else</span> <span class=\"string\">\"localhost\"</span></span><br><span class=\"line\">      port = params.getInt(<span class=\"string\">\"port\"</span>)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</span><br><span class=\"line\">        <span class=\"type\">System</span>.err.println(<span class=\"string\">\"No port specified. Please run 'SocketWindowWordCount \"</span> +</span><br><span class=\"line\">          <span class=\"string\">\"--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) and port \"</span> +</span><br><span class=\"line\">          <span class=\"string\">\"is the address of the text server\"</span>)</span><br><span class=\"line\">        <span class=\"type\">System</span>.err.println(<span class=\"string\">\"To start a simple text server, run 'netcat -l &lt;port&gt;' \"</span> +</span><br><span class=\"line\">          <span class=\"string\">\"and type the input text into the command line\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// get the execution environment</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> env: <span class=\"type\">StreamExecutionEnvironment</span> = <span class=\"type\">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// get input data by connecting to the socket</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> text: <span class=\"type\">DataStream</span>[<span class=\"type\">String</span>] = env.socketTextStream(hostname, port, '\\n')</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> windowCounts = text</span><br><span class=\"line\">          .flatMap &#123; w =&gt; w.split(<span class=\"string\">\"\\\\s\"</span>) &#125;</span><br><span class=\"line\">          .map &#123; w =&gt; <span class=\"type\">WordWithCount</span>(w, <span class=\"number\">1</span>) &#125;</span><br><span class=\"line\">          .keyBy(<span class=\"string\">\"word\"</span>)</span><br><span class=\"line\">          .timeWindow(<span class=\"type\">Time</span>.seconds(<span class=\"number\">5</span>))</span><br><span class=\"line\">          .sum(<span class=\"string\">\"count\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// print the results with a single thread, rather than in parallel</span></span><br><span class=\"line\">    windowCounts.print().setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    env.execute(<span class=\"string\">\"Socket Window WordCount\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/** Data type for words with count */</span></span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordWithCount</span>(<span class=\"params\">word: <span class=\"type\">String</span>, count: <span class=\"type\">Long</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> org.apache.flink.streaming.examples.socket;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.ReduceFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.util.Collector;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Implements a streaming windowed version of the \"WordCount\" program.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;p&gt;This program connects to a server socket and reads strings from the socket.</span></span><br><span class=\"line\"><span class=\"comment\"> * The easiest way to try this out is to open a text server (at port 12345)</span></span><br><span class=\"line\"><span class=\"comment\"> * using the &lt;i&gt;netcat&lt;/i&gt; tool via</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;pre&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> * nc -l 12345</span></span><br><span class=\"line\"><span class=\"comment\"> * &lt;/pre&gt;</span></span><br><span class=\"line\"><span class=\"comment\"> * and run this example with the hostname and the port as arguments.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@SuppressWarnings</span>(<span class=\"string\">\"serial\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SocketWindowWordCount</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// the host and the port to connect to</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">final</span> String hostname;</span><br><span class=\"line\">\t\t<span class=\"keyword\">final</span> <span class=\"keyword\">int</span> port;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class=\"line\">\t\t\thostname = params.has(<span class=\"string\">\"hostname\"</span>) ? params.get(<span class=\"string\">\"hostname\"</span>) : <span class=\"string\">\"localhost\"</span>;</span><br><span class=\"line\">\t\t\tport = params.getInt(<span class=\"string\">\"port\"</span>);</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\tSystem.err.println(<span class=\"string\">\"No port specified. Please run 'SocketWindowWordCount \"</span> +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"--hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) \"</span> +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"and port is the address of the text server\"</span>);</span><br><span class=\"line\">\t\t\tSystem.err.println(<span class=\"string\">\"To start a simple text server, run 'netcat -l &lt;port&gt;' and \"</span> +</span><br><span class=\"line\">\t\t\t\t<span class=\"string\">\"type the input text into the command line\"</span>);</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// get the execution environment</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// get input data by connecting to the socket</span></span><br><span class=\"line\">\t\tDataStream&lt;String&gt; text = env.socketTextStream(hostname, port, <span class=\"string\">\"\\n\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class=\"line\">\t\tDataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, WordWithCount&gt;() &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">flatMap</span><span class=\"params\">(String value, Collector&lt;WordWithCount&gt; out)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">for</span> (String word : value.split(<span class=\"string\">\"\\\\s\"</span>)) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tout.collect(<span class=\"keyword\">new</span> WordWithCount(word, <span class=\"number\">1L</span>));</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t.keyBy(<span class=\"string\">\"word\"</span>)</span><br><span class=\"line\">\t\t\t\t.timeWindow(Time.seconds(<span class=\"number\">5</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t.reduce(<span class=\"keyword\">new</span> ReduceFunction&lt;WordWithCount&gt;() &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> WordWithCount <span class=\"title\">reduce</span><span class=\"params\">(WordWithCount a, WordWithCount b)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">new</span> WordWithCount(a.word, a.count + b.count);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// print the results with a single thread, rather than in parallel</span></span><br><span class=\"line\">\t\twindowCounts.print().setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tenv.execute(<span class=\"string\">\"Socket Window WordCount\"</span>);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// ------------------------------------------------------------------------</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">\t * Data type for words with count.</span></span><br><span class=\"line\"><span class=\"comment\">\t */</span></span><br><span class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordWithCount</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">public</span> String word;</span><br><span class=\"line\">\t\t<span class=\"keyword\">public</span> <span class=\"keyword\">long</span> count;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">WordWithCount</span><span class=\"params\">()</span> </span>&#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">WordWithCount</span><span class=\"params\">(String word, <span class=\"keyword\">long</span> count)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">this</span>.word = word;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">this</span>.count = count;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">toString</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> word + <span class=\"string\">\" : \"</span> + count;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-运行Example\"><a href=\"#4-运行Example\" class=\"headerlink\" title=\"4. 运行Example\"></a>4. 运行Example</h3><p>现在, 我们可以运行Flink 应用程序。 这个例子将会从一个socket中读取一段文本，并且每隔5秒打印之前5秒内每个单词出现的个数。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">a tumbling window of processing time, as long as words are floating in.</span><br></pre></td></tr></table></figure></p>\n<p>(1) 首先,我们可以通过netcat命令来启动本地服务:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">nc -l 9000</span><br></pre></td></tr></table></figure></p>\n<p>(2) 提交Flink程序:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000</span><br><span class=\"line\">Cluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123</span><br><span class=\"line\">Using address localhost:6123 to connect to JobManager.</span><br><span class=\"line\">JobManager web interface address http://localhost:8081</span><br><span class=\"line\">Starting execution of program</span><br><span class=\"line\">Submitting job with JobID: a963626a1e09f7aeb0dc34412adfb801. Waiting for job completion.</span><br><span class=\"line\">Connected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#941160871] with leader session id 00000000-0000-0000-0000-000000000000.</span><br><span class=\"line\">10/16/2017 15:12:26\tJob execution switched to status RUNNING.</span><br><span class=\"line\">10/16/2017 15:12:26\tSource: Socket Stream -&gt; Flat Map(1/1) switched to SCHEDULED</span><br><span class=\"line\">10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to SCHEDULED</span><br><span class=\"line\">10/16/2017 15:12:26\tSource: Socket Stream -&gt; Flat Map(1/1) switched to DEPLOYING</span><br><span class=\"line\">10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to DEPLOYING</span><br><span class=\"line\">10/16/2017 15:12:26\tSource: Socket Stream -&gt; Flat Map(1/1) switched to RUNNING</span><br><span class=\"line\">10/16/2017 15:12:26\tTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor&#123;serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@37ff898e, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@4d15107f&#125;, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:300)) -&gt; Sink: Unnamed(1/1) switched to RUNNING</span><br></pre></td></tr></table></figure></p>\n<p>应用程序连接socket并等待输入，你可以通过web界面来验证任务期望的运行结果：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-2.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8-3.png?raw=true\" alt=\"\"></p>\n<p>单词的数量在5秒的时间窗口中进行累加（使用处理时间和tumbling窗口），并打印在stdout。监控JobManager的输出文件，并在nc写一些文本(回车一行就发送一行输入给Flink) :<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$  nc -l 9000</span><br><span class=\"line\">lorem ipsum</span><br><span class=\"line\">ipsum ipsum ipsum</span><br><span class=\"line\">bye</span><br></pre></td></tr></table></figure></p>\n<p>.out文件将在每个时间窗口截止之际打印每个单词的个数：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/flink-1.3.2$  tail -f log/flink-*-jobmanager-*.out</span><br><span class=\"line\">lorem : 1</span><br><span class=\"line\">bye : 1</span><br><span class=\"line\">ipsum : 4</span><br></pre></td></tr></table></figure></p>\n<p>使用以下命令来停止Flink:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/stop-local.sh</span><br></pre></td></tr></table></figure></p>\n<p>阅读更多的<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/examples/\" target=\"_blank\" rel=\"noopener\">例子</a>来熟悉Flink的编程API。 当你完成这些，可以继续阅读<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/datastream_api.html\" target=\"_blank\" rel=\"noopener\">streaming指南</a>。</p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 Flink编程模型","date":"2017-12-29T04:54:01.000Z","_content":"\n### 1. 抽象层次\n\nFlink提供不同级别的抽象层次来开发流处理和批处理应用程序。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-1.png?raw=true)\n\n(1) 最低级别的抽象只是提供有状态的数据流。通过`Process Function`集成到DataStream API中。它允许用户不受限制的处理来自一个或多个数据流的事件，并可以使用一致的容错状态(consistent fault tolerant state)。另外，用户可以注册事件时间和处理时间的回调函数，允许程序实现复杂的计算。\n\n(2) 在实际中，大多数应用程序不需要上述描述的低级抽象，而是使用如`DataStream API`(有界/无界流)和`DataSet API`(有界数据集)的核心API进行编程。这些核心API提供了用于数据处理的通用构建模块，如用户指定的各种转换，连接，聚集，窗口，状态等。在这些API中处理的数据类型被表示为对应编程语言中的类。\n\n低级别的`Process Function`与`DataStream API`集成在一起，使得可以对特定操作使用较低级别的抽象接口。`DataSet API`为有限数据集提供了额外的原语(primitives)，如循环/迭代。\n\n(3) `Table API`是以表为核心的声明式DSL，可以动态地改变表(当表表示流数据时)。`Table API`遵循(扩展的)关系模型：每个表都有一个schema(类似于关系数据库中的表)，对应的API提供了类似的操作(offers comparable operations)，如`select`，`project`，`join`，`group-by`，`aggregate`等。`Table API`程序声明性地定义了如何在逻辑上实现操作，而不是明确指定操作实现的具体代码。尽管`Table API`可以通过各种类型的用户自定义函数进行扩展，它比核心API表达性要差一些，但使用上更简洁(编写代码更少)。另外，`Table API`程序也会通过一个优化器，在执行之前应用优化规则。\n\n可以在表和`DataStream`/`DataSet`之间进行无缝转换，允许程序混合使用`Table API`和`DataStream`和`DataSet API`。\n\n(4) Flink提供的最高级抽象是SQL。这种抽象在语法和表现力方面与`Table API`类似，但是是通过SQL查询表达式实现程序。SQL抽象与`Table API`紧密交互，SQL查询可以在`Table API`中定义的表上执行。\n\n### 2. 程序与数据流\n\nFlink程序的基本构建块是流和转换操作。\n\n备注:\n```\nFlink的DataSet API中使用的数据集也是内部的流 - 稍后会介绍这一点。\n```\n\n从概念上讲，流是数据记录(可能是永无止境的)流，而转换是将一个或多个流作为输入，并产生一个或多个输出流。\n\n执行时，Flink程序被映射到由流和转换算子组成的流式数据流(streaming dataflows)。每个数据流从一个或多个source开始，并在一个或多个sink中结束。数据流类似于有向无环图(DAG)。尽管通过迭代构造允许特殊形式的环，但是为了简单起见，大部分我们都会这样描述。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-2.png?raw=true)\n\n程序中的转换与数据流中的算子通常是一一对应的。然而，有时候，一个转换可能由多个转换算子组成。\n\n### 3. 并行数据流图\n\nFlink中的程序本质上是分布式并发执行的。在执行过程中，一个流有一个或多个流分区，每个算子有一个或多个算子子任务。算子子任务之间相互独立，并且在不同的线程中执行，甚至有可能在不同的机器或容器上执行。\n\n算子子任务的数量是该特定算子的并发数。流的并发数总是产生它的算子的并发数。同一程序的不同算子可能具有不同的并发级别。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-3.png?raw=true)\n\n在两个算子之间的流可以以一对一模式或重新分发模式传输数据:\n\n(1) 一对一流(例如上图中的Source和map()算子之间的流)保留了元素的分区和排序。这意味着将会在map()算子的子任务[1]中看到在Source算子的子任务[1]中产生的相同元素，并且具有相同的顺序。\n\n(2) 重分发流(例如上图的的`map()`和`keyBy()/window()/apply()`之间，以及在`keyBy()/window()/apply()`和`Sink`之间的数据流)改变了流的分区。每个算子子任务根据所选的转换操作将数据发送到不同的目标子任务。比如`keyBy()`(根据key的哈希值重新分区)，`broadcast()`，或者`rebalance()`(随机重新分区)。在重新分配交换中，只会在每对发送与接受子任务(比如，`map()`的子任务[1]与`keyBy()/window()/apply()`的子任务[2])中保留元素间的顺序。在上图的例子中，尽管在子任务之间每个 key 的顺序都是确定的，但是由于程序的并发引入了不确定性，最终到达`Sink`的元素顺序就不能保证与一开始的元素顺序完全一致。\n\n关于配置并发的更多信息可以参阅[并发执行文档](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/parallel.html)。\n\n### 4. 窗口\n\n聚合事件(比如计数、求和)在流上的工作方式与批处理不同。比如，不可能对流中的所有元素进行计数，因为通常流是无限的(无界的)。相反，流上的聚合(计数，求和等)需要由`窗口`来划定范围，比如`在最近5分钟内计算`，或者`对最近100个元素求和`。\n\n窗口可以是`时间驱动的`(比如：每30秒）或者`数据驱动`的(比如：每100个元素)。窗口通常被区分为不同的类型，比如`滚动窗口`(没有重叠)，`滑动窗口`(有重叠)，以及`会话窗口`(由不活动的间隙所打断)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-4.png?raw=true)\n\n更多的窗口示例可以在这篇[博客](https://flink.apache.org/news/2015/12/04/Introducing-windows.html)中找到。更多详细信息在[窗口](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html)文档。\n\n### 5. 时间\n\n当提到流程序(例如定义窗口)中的时间时，你可以参考不同的时间概念：\n\n(1) `事件时间`是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过[时间戳分配器](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html)访问事件时间戳。\n\n(2) `摄入时间`是事件进入`Flink`数据流源(source)算子的时间。\n\n(3) `处理事件`是每一个执行基于时间操作算子的本地时间。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true)\n\n更多关于如何处理时间的详细信息可以查看[事件时间](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html)文档.\n\n### 6. 有状态操作\n\n尽管数据流中的很多操作一次只查看一个独立的事件(比如事件解析器)，但是有些操作会记录多个事件间的信息(比如窗口算子)。这些操作被称为`有状态`的 。\n\n有状态操作的状态保存在一个可被视为嵌入式键值对存储中。状态与由有状态算子读取的流一起被严格地分区与分布(distributed)。因此，只有在应用`keyBy()`函数之后，才能访问`keyed streams`上的键/值对状态，并且仅限于与当前事件`key`相关联的值(access to the key/value state is only possible on keyed streams, after a keyBy() function, and is restricted to the values associated with the current event’s key. )。对齐流和状态的`key`(Aligning the keys of streams and state)确保了所有状态更新都是本地操作，保证一致性，而没有事务开销(guaranteeing consistency without transaction overhead)。这种对齐还使得`Flink`可以透明地重新分配状态与调整流的分区。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true)\n\n### 7. 容错性检查点\n\n`Flink`组合使用流重放与检查点实现了容错。检查点与每一个输入流以及每一个算子对应的状态所在的特定点相关联(A checkpoint is related to a specific point in each of the input streams along with the corresponding state for each of the operators.)。一个流数据流可以可以从一个检查点恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性(一次处理语义)\n\n检查点时间间隔是在恢复时间(需要重放的事件数量)内消除执行过程中容错开销的一种手段。\n\n更多关于检查点与容错的详细信息可以查看[容错](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html)文档。\n\n### 8. 批处理操作\n\n`Flink`将批处理程序作为流处理程序的一种特殊情况来执行，只是流是有界的(有限个元素)。在内部`DataSet`被视为数据流(A DataSet is treated internally as a stream of data)。因此上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外：\n\n(1) 批处理程序的容错不使用检查点。通过重放全部流来恢复。这是可能的，因为输入是有限的。这使恢复的成本更高(This pushes the cost more towards the recovery)，但是使常规处理更便宜，因为它避免了检查点。\n\n(2) `DataSet API`中的有状态操作使用简化的`in-memory`/`out-of-core`数据结构，而不是键/值索引。\n\n(3) `DataSet API`引入了特殊的同步(基于`superstep`的)迭代，而这种迭代仅仅能在有界流上执行。详细信息可以查看[迭代](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/batch/iterations.html)文档。\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html\n","source":"_posts/Flink/[Flink]Flink 编程模型.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 Flink编程模型\ndate: 2017-12-29 12:54:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n### 1. 抽象层次\n\nFlink提供不同级别的抽象层次来开发流处理和批处理应用程序。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-1.png?raw=true)\n\n(1) 最低级别的抽象只是提供有状态的数据流。通过`Process Function`集成到DataStream API中。它允许用户不受限制的处理来自一个或多个数据流的事件，并可以使用一致的容错状态(consistent fault tolerant state)。另外，用户可以注册事件时间和处理时间的回调函数，允许程序实现复杂的计算。\n\n(2) 在实际中，大多数应用程序不需要上述描述的低级抽象，而是使用如`DataStream API`(有界/无界流)和`DataSet API`(有界数据集)的核心API进行编程。这些核心API提供了用于数据处理的通用构建模块，如用户指定的各种转换，连接，聚集，窗口，状态等。在这些API中处理的数据类型被表示为对应编程语言中的类。\n\n低级别的`Process Function`与`DataStream API`集成在一起，使得可以对特定操作使用较低级别的抽象接口。`DataSet API`为有限数据集提供了额外的原语(primitives)，如循环/迭代。\n\n(3) `Table API`是以表为核心的声明式DSL，可以动态地改变表(当表表示流数据时)。`Table API`遵循(扩展的)关系模型：每个表都有一个schema(类似于关系数据库中的表)，对应的API提供了类似的操作(offers comparable operations)，如`select`，`project`，`join`，`group-by`，`aggregate`等。`Table API`程序声明性地定义了如何在逻辑上实现操作，而不是明确指定操作实现的具体代码。尽管`Table API`可以通过各种类型的用户自定义函数进行扩展，它比核心API表达性要差一些，但使用上更简洁(编写代码更少)。另外，`Table API`程序也会通过一个优化器，在执行之前应用优化规则。\n\n可以在表和`DataStream`/`DataSet`之间进行无缝转换，允许程序混合使用`Table API`和`DataStream`和`DataSet API`。\n\n(4) Flink提供的最高级抽象是SQL。这种抽象在语法和表现力方面与`Table API`类似，但是是通过SQL查询表达式实现程序。SQL抽象与`Table API`紧密交互，SQL查询可以在`Table API`中定义的表上执行。\n\n### 2. 程序与数据流\n\nFlink程序的基本构建块是流和转换操作。\n\n备注:\n```\nFlink的DataSet API中使用的数据集也是内部的流 - 稍后会介绍这一点。\n```\n\n从概念上讲，流是数据记录(可能是永无止境的)流，而转换是将一个或多个流作为输入，并产生一个或多个输出流。\n\n执行时，Flink程序被映射到由流和转换算子组成的流式数据流(streaming dataflows)。每个数据流从一个或多个source开始，并在一个或多个sink中结束。数据流类似于有向无环图(DAG)。尽管通过迭代构造允许特殊形式的环，但是为了简单起见，大部分我们都会这样描述。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-2.png?raw=true)\n\n程序中的转换与数据流中的算子通常是一一对应的。然而，有时候，一个转换可能由多个转换算子组成。\n\n### 3. 并行数据流图\n\nFlink中的程序本质上是分布式并发执行的。在执行过程中，一个流有一个或多个流分区，每个算子有一个或多个算子子任务。算子子任务之间相互独立，并且在不同的线程中执行，甚至有可能在不同的机器或容器上执行。\n\n算子子任务的数量是该特定算子的并发数。流的并发数总是产生它的算子的并发数。同一程序的不同算子可能具有不同的并发级别。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-3.png?raw=true)\n\n在两个算子之间的流可以以一对一模式或重新分发模式传输数据:\n\n(1) 一对一流(例如上图中的Source和map()算子之间的流)保留了元素的分区和排序。这意味着将会在map()算子的子任务[1]中看到在Source算子的子任务[1]中产生的相同元素，并且具有相同的顺序。\n\n(2) 重分发流(例如上图的的`map()`和`keyBy()/window()/apply()`之间，以及在`keyBy()/window()/apply()`和`Sink`之间的数据流)改变了流的分区。每个算子子任务根据所选的转换操作将数据发送到不同的目标子任务。比如`keyBy()`(根据key的哈希值重新分区)，`broadcast()`，或者`rebalance()`(随机重新分区)。在重新分配交换中，只会在每对发送与接受子任务(比如，`map()`的子任务[1]与`keyBy()/window()/apply()`的子任务[2])中保留元素间的顺序。在上图的例子中，尽管在子任务之间每个 key 的顺序都是确定的，但是由于程序的并发引入了不确定性，最终到达`Sink`的元素顺序就不能保证与一开始的元素顺序完全一致。\n\n关于配置并发的更多信息可以参阅[并发执行文档](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/parallel.html)。\n\n### 4. 窗口\n\n聚合事件(比如计数、求和)在流上的工作方式与批处理不同。比如，不可能对流中的所有元素进行计数，因为通常流是无限的(无界的)。相反，流上的聚合(计数，求和等)需要由`窗口`来划定范围，比如`在最近5分钟内计算`，或者`对最近100个元素求和`。\n\n窗口可以是`时间驱动的`(比如：每30秒）或者`数据驱动`的(比如：每100个元素)。窗口通常被区分为不同的类型，比如`滚动窗口`(没有重叠)，`滑动窗口`(有重叠)，以及`会话窗口`(由不活动的间隙所打断)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-4.png?raw=true)\n\n更多的窗口示例可以在这篇[博客](https://flink.apache.org/news/2015/12/04/Introducing-windows.html)中找到。更多详细信息在[窗口](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html)文档。\n\n### 5. 时间\n\n当提到流程序(例如定义窗口)中的时间时，你可以参考不同的时间概念：\n\n(1) `事件时间`是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过[时间戳分配器](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html)访问事件时间戳。\n\n(2) `摄入时间`是事件进入`Flink`数据流源(source)算子的时间。\n\n(3) `处理事件`是每一个执行基于时间操作算子的本地时间。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true)\n\n更多关于如何处理时间的详细信息可以查看[事件时间](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html)文档.\n\n### 6. 有状态操作\n\n尽管数据流中的很多操作一次只查看一个独立的事件(比如事件解析器)，但是有些操作会记录多个事件间的信息(比如窗口算子)。这些操作被称为`有状态`的 。\n\n有状态操作的状态保存在一个可被视为嵌入式键值对存储中。状态与由有状态算子读取的流一起被严格地分区与分布(distributed)。因此，只有在应用`keyBy()`函数之后，才能访问`keyed streams`上的键/值对状态，并且仅限于与当前事件`key`相关联的值(access to the key/value state is only possible on keyed streams, after a keyBy() function, and is restricted to the values associated with the current event’s key. )。对齐流和状态的`key`(Aligning the keys of streams and state)确保了所有状态更新都是本地操作，保证一致性，而没有事务开销(guaranteeing consistency without transaction overhead)。这种对齐还使得`Flink`可以透明地重新分配状态与调整流的分区。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true)\n\n### 7. 容错性检查点\n\n`Flink`组合使用流重放与检查点实现了容错。检查点与每一个输入流以及每一个算子对应的状态所在的特定点相关联(A checkpoint is related to a specific point in each of the input streams along with the corresponding state for each of the operators.)。一个流数据流可以可以从一个检查点恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性(一次处理语义)\n\n检查点时间间隔是在恢复时间(需要重放的事件数量)内消除执行过程中容错开销的一种手段。\n\n更多关于检查点与容错的详细信息可以查看[容错](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html)文档。\n\n### 8. 批处理操作\n\n`Flink`将批处理程序作为流处理程序的一种特殊情况来执行，只是流是有界的(有限个元素)。在内部`DataSet`被视为数据流(A DataSet is treated internally as a stream of data)。因此上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外：\n\n(1) 批处理程序的容错不使用检查点。通过重放全部流来恢复。这是可能的，因为输入是有限的。这使恢复的成本更高(This pushes the cost more towards the recovery)，但是使常规处理更便宜，因为它避免了检查点。\n\n(2) `DataSet API`中的有状态操作使用简化的`in-memory`/`out-of-core`数据结构，而不是键/值索引。\n\n(3) `DataSet API`引入了特殊的同步(基于`superstep`的)迭代，而这种迭代仅仅能在有界流上执行。详细信息可以查看[迭代](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/batch/iterations.html)文档。\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html\n","slug":"Flink/[Flink]Flink 编程模型","published":1,"updated":"2018-01-29T09:36:59.657Z","comments":1,"photos":[],"link":"","_id":"cje58tipq000eordbzcvp4s4q","content":"<h3 id=\"1-抽象层次\"><a href=\"#1-抽象层次\" class=\"headerlink\" title=\"1. 抽象层次\"></a>1. 抽象层次</h3><p>Flink提供不同级别的抽象层次来开发流处理和批处理应用程序。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-1.png?raw=true\" alt=\"\"></p>\n<p>(1) 最低级别的抽象只是提供有状态的数据流。通过<code>Process Function</code>集成到DataStream API中。它允许用户不受限制的处理来自一个或多个数据流的事件，并可以使用一致的容错状态(consistent fault tolerant state)。另外，用户可以注册事件时间和处理时间的回调函数，允许程序实现复杂的计算。</p>\n<p>(2) 在实际中，大多数应用程序不需要上述描述的低级抽象，而是使用如<code>DataStream API</code>(有界/无界流)和<code>DataSet API</code>(有界数据集)的核心API进行编程。这些核心API提供了用于数据处理的通用构建模块，如用户指定的各种转换，连接，聚集，窗口，状态等。在这些API中处理的数据类型被表示为对应编程语言中的类。</p>\n<p>低级别的<code>Process Function</code>与<code>DataStream API</code>集成在一起，使得可以对特定操作使用较低级别的抽象接口。<code>DataSet API</code>为有限数据集提供了额外的原语(primitives)，如循环/迭代。</p>\n<p>(3) <code>Table API</code>是以表为核心的声明式DSL，可以动态地改变表(当表表示流数据时)。<code>Table API</code>遵循(扩展的)关系模型：每个表都有一个schema(类似于关系数据库中的表)，对应的API提供了类似的操作(offers comparable operations)，如<code>select</code>，<code>project</code>，<code>join</code>，<code>group-by</code>，<code>aggregate</code>等。<code>Table API</code>程序声明性地定义了如何在逻辑上实现操作，而不是明确指定操作实现的具体代码。尽管<code>Table API</code>可以通过各种类型的用户自定义函数进行扩展，它比核心API表达性要差一些，但使用上更简洁(编写代码更少)。另外，<code>Table API</code>程序也会通过一个优化器，在执行之前应用优化规则。</p>\n<p>可以在表和<code>DataStream</code>/<code>DataSet</code>之间进行无缝转换，允许程序混合使用<code>Table API</code>和<code>DataStream</code>和<code>DataSet API</code>。</p>\n<p>(4) Flink提供的最高级抽象是SQL。这种抽象在语法和表现力方面与<code>Table API</code>类似，但是是通过SQL查询表达式实现程序。SQL抽象与<code>Table API</code>紧密交互，SQL查询可以在<code>Table API</code>中定义的表上执行。</p>\n<h3 id=\"2-程序与数据流\"><a href=\"#2-程序与数据流\" class=\"headerlink\" title=\"2. 程序与数据流\"></a>2. 程序与数据流</h3><p>Flink程序的基本构建块是流和转换操作。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink的DataSet API中使用的数据集也是内部的流 - 稍后会介绍这一点。</span><br></pre></td></tr></table></figure></p>\n<p>从概念上讲，流是数据记录(可能是永无止境的)流，而转换是将一个或多个流作为输入，并产生一个或多个输出流。</p>\n<p>执行时，Flink程序被映射到由流和转换算子组成的流式数据流(streaming dataflows)。每个数据流从一个或多个source开始，并在一个或多个sink中结束。数据流类似于有向无环图(DAG)。尽管通过迭代构造允许特殊形式的环，但是为了简单起见，大部分我们都会这样描述。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-2.png?raw=true\" alt=\"\"></p>\n<p>程序中的转换与数据流中的算子通常是一一对应的。然而，有时候，一个转换可能由多个转换算子组成。</p>\n<h3 id=\"3-并行数据流图\"><a href=\"#3-并行数据流图\" class=\"headerlink\" title=\"3. 并行数据流图\"></a>3. 并行数据流图</h3><p>Flink中的程序本质上是分布式并发执行的。在执行过程中，一个流有一个或多个流分区，每个算子有一个或多个算子子任务。算子子任务之间相互独立，并且在不同的线程中执行，甚至有可能在不同的机器或容器上执行。</p>\n<p>算子子任务的数量是该特定算子的并发数。流的并发数总是产生它的算子的并发数。同一程序的不同算子可能具有不同的并发级别。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-3.png?raw=true\" alt=\"\"></p>\n<p>在两个算子之间的流可以以一对一模式或重新分发模式传输数据:</p>\n<p>(1) 一对一流(例如上图中的Source和map()算子之间的流)保留了元素的分区和排序。这意味着将会在map()算子的子任务[1]中看到在Source算子的子任务[1]中产生的相同元素，并且具有相同的顺序。</p>\n<p>(2) 重分发流(例如上图的的<code>map()</code>和<code>keyBy()/window()/apply()</code>之间，以及在<code>keyBy()/window()/apply()</code>和<code>Sink</code>之间的数据流)改变了流的分区。每个算子子任务根据所选的转换操作将数据发送到不同的目标子任务。比如<code>keyBy()</code>(根据key的哈希值重新分区)，<code>broadcast()</code>，或者<code>rebalance()</code>(随机重新分区)。在重新分配交换中，只会在每对发送与接受子任务(比如，<code>map()</code>的子任务[1]与<code>keyBy()/window()/apply()</code>的子任务[2])中保留元素间的顺序。在上图的例子中，尽管在子任务之间每个 key 的顺序都是确定的，但是由于程序的并发引入了不确定性，最终到达<code>Sink</code>的元素顺序就不能保证与一开始的元素顺序完全一致。</p>\n<p>关于配置并发的更多信息可以参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/parallel.html\" target=\"_blank\" rel=\"noopener\">并发执行文档</a>。</p>\n<h3 id=\"4-窗口\"><a href=\"#4-窗口\" class=\"headerlink\" title=\"4. 窗口\"></a>4. 窗口</h3><p>聚合事件(比如计数、求和)在流上的工作方式与批处理不同。比如，不可能对流中的所有元素进行计数，因为通常流是无限的(无界的)。相反，流上的聚合(计数，求和等)需要由<code>窗口</code>来划定范围，比如<code>在最近5分钟内计算</code>，或者<code>对最近100个元素求和</code>。</p>\n<p>窗口可以是<code>时间驱动的</code>(比如：每30秒）或者<code>数据驱动</code>的(比如：每100个元素)。窗口通常被区分为不同的类型，比如<code>滚动窗口</code>(没有重叠)，<code>滑动窗口</code>(有重叠)，以及<code>会话窗口</code>(由不活动的间隙所打断)</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-4.png?raw=true\" alt=\"\"></p>\n<p>更多的窗口示例可以在这篇<a href=\"https://flink.apache.org/news/2015/12/04/Introducing-windows.html\" target=\"_blank\" rel=\"noopener\">博客</a>中找到。更多详细信息在<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html\" target=\"_blank\" rel=\"noopener\">窗口</a>文档。</p>\n<h3 id=\"5-时间\"><a href=\"#5-时间\" class=\"headerlink\" title=\"5. 时间\"></a>5. 时间</h3><p>当提到流程序(例如定义窗口)中的时间时，你可以参考不同的时间概念：</p>\n<p>(1) <code>事件时间</code>是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html\" target=\"_blank\" rel=\"noopener\">时间戳分配器</a>访问事件时间戳。</p>\n<p>(2) <code>摄入时间</code>是事件进入<code>Flink</code>数据流源(source)算子的时间。</p>\n<p>(3) <code>处理事件</code>是每一个执行基于时间操作算子的本地时间。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true\" alt=\"\"></p>\n<p>更多关于如何处理时间的详细信息可以查看<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html\" target=\"_blank\" rel=\"noopener\">事件时间</a>文档.</p>\n<h3 id=\"6-有状态操作\"><a href=\"#6-有状态操作\" class=\"headerlink\" title=\"6. 有状态操作\"></a>6. 有状态操作</h3><p>尽管数据流中的很多操作一次只查看一个独立的事件(比如事件解析器)，但是有些操作会记录多个事件间的信息(比如窗口算子)。这些操作被称为<code>有状态</code>的 。</p>\n<p>有状态操作的状态保存在一个可被视为嵌入式键值对存储中。状态与由有状态算子读取的流一起被严格地分区与分布(distributed)。因此，只有在应用<code>keyBy()</code>函数之后，才能访问<code>keyed streams</code>上的键/值对状态，并且仅限于与当前事件<code>key</code>相关联的值(access to the key/value state is only possible on keyed streams, after a keyBy() function, and is restricted to the values associated with the current event’s key. )。对齐流和状态的<code>key</code>(Aligning the keys of streams and state)确保了所有状态更新都是本地操作，保证一致性，而没有事务开销(guaranteeing consistency without transaction overhead)。这种对齐还使得<code>Flink</code>可以透明地重新分配状态与调整流的分区。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"7-容错性检查点\"><a href=\"#7-容错性检查点\" class=\"headerlink\" title=\"7. 容错性检查点\"></a>7. 容错性检查点</h3><p><code>Flink</code>组合使用流重放与检查点实现了容错。检查点与每一个输入流以及每一个算子对应的状态所在的特定点相关联(A checkpoint is related to a specific point in each of the input streams along with the corresponding state for each of the operators.)。一个流数据流可以可以从一个检查点恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性(一次处理语义)</p>\n<p>检查点时间间隔是在恢复时间(需要重放的事件数量)内消除执行过程中容错开销的一种手段。</p>\n<p>更多关于检查点与容错的详细信息可以查看<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html\" target=\"_blank\" rel=\"noopener\">容错</a>文档。</p>\n<h3 id=\"8-批处理操作\"><a href=\"#8-批处理操作\" class=\"headerlink\" title=\"8. 批处理操作\"></a>8. 批处理操作</h3><p><code>Flink</code>将批处理程序作为流处理程序的一种特殊情况来执行，只是流是有界的(有限个元素)。在内部<code>DataSet</code>被视为数据流(A DataSet is treated internally as a stream of data)。因此上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外：</p>\n<p>(1) 批处理程序的容错不使用检查点。通过重放全部流来恢复。这是可能的，因为输入是有限的。这使恢复的成本更高(This pushes the cost more towards the recovery)，但是使常规处理更便宜，因为它避免了检查点。</p>\n<p>(2) <code>DataSet API</code>中的有状态操作使用简化的<code>in-memory</code>/<code>out-of-core</code>数据结构，而不是键/值索引。</p>\n<p>(3) <code>DataSet API</code>引入了特殊的同步(基于<code>superstep</code>的)迭代，而这种迭代仅仅能在有界流上执行。详细信息可以查看<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/batch/iterations.html\" target=\"_blank\" rel=\"noopener\">迭代</a>文档。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-抽象层次\"><a href=\"#1-抽象层次\" class=\"headerlink\" title=\"1. 抽象层次\"></a>1. 抽象层次</h3><p>Flink提供不同级别的抽象层次来开发流处理和批处理应用程序。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-1.png?raw=true\" alt=\"\"></p>\n<p>(1) 最低级别的抽象只是提供有状态的数据流。通过<code>Process Function</code>集成到DataStream API中。它允许用户不受限制的处理来自一个或多个数据流的事件，并可以使用一致的容错状态(consistent fault tolerant state)。另外，用户可以注册事件时间和处理时间的回调函数，允许程序实现复杂的计算。</p>\n<p>(2) 在实际中，大多数应用程序不需要上述描述的低级抽象，而是使用如<code>DataStream API</code>(有界/无界流)和<code>DataSet API</code>(有界数据集)的核心API进行编程。这些核心API提供了用于数据处理的通用构建模块，如用户指定的各种转换，连接，聚集，窗口，状态等。在这些API中处理的数据类型被表示为对应编程语言中的类。</p>\n<p>低级别的<code>Process Function</code>与<code>DataStream API</code>集成在一起，使得可以对特定操作使用较低级别的抽象接口。<code>DataSet API</code>为有限数据集提供了额外的原语(primitives)，如循环/迭代。</p>\n<p>(3) <code>Table API</code>是以表为核心的声明式DSL，可以动态地改变表(当表表示流数据时)。<code>Table API</code>遵循(扩展的)关系模型：每个表都有一个schema(类似于关系数据库中的表)，对应的API提供了类似的操作(offers comparable operations)，如<code>select</code>，<code>project</code>，<code>join</code>，<code>group-by</code>，<code>aggregate</code>等。<code>Table API</code>程序声明性地定义了如何在逻辑上实现操作，而不是明确指定操作实现的具体代码。尽管<code>Table API</code>可以通过各种类型的用户自定义函数进行扩展，它比核心API表达性要差一些，但使用上更简洁(编写代码更少)。另外，<code>Table API</code>程序也会通过一个优化器，在执行之前应用优化规则。</p>\n<p>可以在表和<code>DataStream</code>/<code>DataSet</code>之间进行无缝转换，允许程序混合使用<code>Table API</code>和<code>DataStream</code>和<code>DataSet API</code>。</p>\n<p>(4) Flink提供的最高级抽象是SQL。这种抽象在语法和表现力方面与<code>Table API</code>类似，但是是通过SQL查询表达式实现程序。SQL抽象与<code>Table API</code>紧密交互，SQL查询可以在<code>Table API</code>中定义的表上执行。</p>\n<h3 id=\"2-程序与数据流\"><a href=\"#2-程序与数据流\" class=\"headerlink\" title=\"2. 程序与数据流\"></a>2. 程序与数据流</h3><p>Flink程序的基本构建块是流和转换操作。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink的DataSet API中使用的数据集也是内部的流 - 稍后会介绍这一点。</span><br></pre></td></tr></table></figure></p>\n<p>从概念上讲，流是数据记录(可能是永无止境的)流，而转换是将一个或多个流作为输入，并产生一个或多个输出流。</p>\n<p>执行时，Flink程序被映射到由流和转换算子组成的流式数据流(streaming dataflows)。每个数据流从一个或多个source开始，并在一个或多个sink中结束。数据流类似于有向无环图(DAG)。尽管通过迭代构造允许特殊形式的环，但是为了简单起见，大部分我们都会这样描述。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-2.png?raw=true\" alt=\"\"></p>\n<p>程序中的转换与数据流中的算子通常是一一对应的。然而，有时候，一个转换可能由多个转换算子组成。</p>\n<h3 id=\"3-并行数据流图\"><a href=\"#3-并行数据流图\" class=\"headerlink\" title=\"3. 并行数据流图\"></a>3. 并行数据流图</h3><p>Flink中的程序本质上是分布式并发执行的。在执行过程中，一个流有一个或多个流分区，每个算子有一个或多个算子子任务。算子子任务之间相互独立，并且在不同的线程中执行，甚至有可能在不同的机器或容器上执行。</p>\n<p>算子子任务的数量是该特定算子的并发数。流的并发数总是产生它的算子的并发数。同一程序的不同算子可能具有不同的并发级别。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-3.png?raw=true\" alt=\"\"></p>\n<p>在两个算子之间的流可以以一对一模式或重新分发模式传输数据:</p>\n<p>(1) 一对一流(例如上图中的Source和map()算子之间的流)保留了元素的分区和排序。这意味着将会在map()算子的子任务[1]中看到在Source算子的子任务[1]中产生的相同元素，并且具有相同的顺序。</p>\n<p>(2) 重分发流(例如上图的的<code>map()</code>和<code>keyBy()/window()/apply()</code>之间，以及在<code>keyBy()/window()/apply()</code>和<code>Sink</code>之间的数据流)改变了流的分区。每个算子子任务根据所选的转换操作将数据发送到不同的目标子任务。比如<code>keyBy()</code>(根据key的哈希值重新分区)，<code>broadcast()</code>，或者<code>rebalance()</code>(随机重新分区)。在重新分配交换中，只会在每对发送与接受子任务(比如，<code>map()</code>的子任务[1]与<code>keyBy()/window()/apply()</code>的子任务[2])中保留元素间的顺序。在上图的例子中，尽管在子任务之间每个 key 的顺序都是确定的，但是由于程序的并发引入了不确定性，最终到达<code>Sink</code>的元素顺序就不能保证与一开始的元素顺序完全一致。</p>\n<p>关于配置并发的更多信息可以参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/parallel.html\" target=\"_blank\" rel=\"noopener\">并发执行文档</a>。</p>\n<h3 id=\"4-窗口\"><a href=\"#4-窗口\" class=\"headerlink\" title=\"4. 窗口\"></a>4. 窗口</h3><p>聚合事件(比如计数、求和)在流上的工作方式与批处理不同。比如，不可能对流中的所有元素进行计数，因为通常流是无限的(无界的)。相反，流上的聚合(计数，求和等)需要由<code>窗口</code>来划定范围，比如<code>在最近5分钟内计算</code>，或者<code>对最近100个元素求和</code>。</p>\n<p>窗口可以是<code>时间驱动的</code>(比如：每30秒）或者<code>数据驱动</code>的(比如：每100个元素)。窗口通常被区分为不同的类型，比如<code>滚动窗口</code>(没有重叠)，<code>滑动窗口</code>(有重叠)，以及<code>会话窗口</code>(由不活动的间隙所打断)</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-4.png?raw=true\" alt=\"\"></p>\n<p>更多的窗口示例可以在这篇<a href=\"https://flink.apache.org/news/2015/12/04/Introducing-windows.html\" target=\"_blank\" rel=\"noopener\">博客</a>中找到。更多详细信息在<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html\" target=\"_blank\" rel=\"noopener\">窗口</a>文档。</p>\n<h3 id=\"5-时间\"><a href=\"#5-时间\" class=\"headerlink\" title=\"5. 时间\"></a>5. 时间</h3><p>当提到流程序(例如定义窗口)中的时间时，你可以参考不同的时间概念：</p>\n<p>(1) <code>事件时间</code>是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html\" target=\"_blank\" rel=\"noopener\">时间戳分配器</a>访问事件时间戳。</p>\n<p>(2) <code>摄入时间</code>是事件进入<code>Flink</code>数据流源(source)算子的时间。</p>\n<p>(3) <code>处理事件</code>是每一个执行基于时间操作算子的本地时间。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true\" alt=\"\"></p>\n<p>更多关于如何处理时间的详细信息可以查看<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html\" target=\"_blank\" rel=\"noopener\">事件时间</a>文档.</p>\n<h3 id=\"6-有状态操作\"><a href=\"#6-有状态操作\" class=\"headerlink\" title=\"6. 有状态操作\"></a>6. 有状态操作</h3><p>尽管数据流中的很多操作一次只查看一个独立的事件(比如事件解析器)，但是有些操作会记录多个事件间的信息(比如窗口算子)。这些操作被称为<code>有状态</code>的 。</p>\n<p>有状态操作的状态保存在一个可被视为嵌入式键值对存储中。状态与由有状态算子读取的流一起被严格地分区与分布(distributed)。因此，只有在应用<code>keyBy()</code>函数之后，才能访问<code>keyed streams</code>上的键/值对状态，并且仅限于与当前事件<code>key</code>相关联的值(access to the key/value state is only possible on keyed streams, after a keyBy() function, and is restricted to the values associated with the current event’s key. )。对齐流和状态的<code>key</code>(Aligning the keys of streams and state)确保了所有状态更新都是本地操作，保证一致性，而没有事务开销(guaranteeing consistency without transaction overhead)。这种对齐还使得<code>Flink</code>可以透明地重新分配状态与调整流的分区。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"7-容错性检查点\"><a href=\"#7-容错性检查点\" class=\"headerlink\" title=\"7. 容错性检查点\"></a>7. 容错性检查点</h3><p><code>Flink</code>组合使用流重放与检查点实现了容错。检查点与每一个输入流以及每一个算子对应的状态所在的特定点相关联(A checkpoint is related to a specific point in each of the input streams along with the corresponding state for each of the operators.)。一个流数据流可以可以从一个检查点恢复出来，其中通过恢复算子状态并从检查点重放事件以保持一致性(一次处理语义)</p>\n<p>检查点时间间隔是在恢复时间(需要重放的事件数量)内消除执行过程中容错开销的一种手段。</p>\n<p>更多关于检查点与容错的详细信息可以查看<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html\" target=\"_blank\" rel=\"noopener\">容错</a>文档。</p>\n<h3 id=\"8-批处理操作\"><a href=\"#8-批处理操作\" class=\"headerlink\" title=\"8. 批处理操作\"></a>8. 批处理操作</h3><p><code>Flink</code>将批处理程序作为流处理程序的一种特殊情况来执行，只是流是有界的(有限个元素)。在内部<code>DataSet</code>被视为数据流(A DataSet is treated internally as a stream of data)。因此上述适用于流处理程序的概念同样适用于批处理程序，除了一些例外：</p>\n<p>(1) 批处理程序的容错不使用检查点。通过重放全部流来恢复。这是可能的，因为输入是有限的。这使恢复的成本更高(This pushes the cost more towards the recovery)，但是使常规处理更便宜，因为它避免了检查点。</p>\n<p>(2) <code>DataSet API</code>中的有状态操作使用简化的<code>in-memory</code>/<code>out-of-core</code>数据结构，而不是键/值索引。</p>\n<p>(3) <code>DataSet API</code>引入了特殊的同步(基于<code>superstep</code>的)迭代，而这种迭代仅仅能在有界流上执行。详细信息可以查看<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/batch/iterations.html\" target=\"_blank\" rel=\"noopener\">迭代</a>文档。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/concepts/programming-model.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 事件时间与Watermarks","date":"2018-01-04T09:57:01.000Z","_content":"\n### 1. watermark\n\n`Flink`实现了数据流模型(`Dataflow Model`)中许多技术。如果想对事件时间(`event time`)和`watermarks`更详细的了解，请参阅下面的文章:\n- [The world beyond batch: Streaming 101\n](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101)\n- [The Dataflow Model](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf)\n\n支持事件时间的流处理器需要一种方法来衡量事件时间的进度。例如，一个构建小时窗口的窗口算子(`operator`)，当事件时间超过一小时末尾时需要告知窗口算子，以便算子可以关闭正在进行的窗口。\n\n事件时间可以独立于处理时间来运行。例如，在一个程序中，算子的当前事件时间可以略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度继续运行。另一方面，另一个流式处理程序处理几个星期的事件时间只需几秒钟就可以，通过快速浏览缓存在`Kafka Topic`中历史数据。\n\n`Flink`中测量事件时间进度的机制是`watermarks`。`watermarks`会作为数据流的一部分进行流动，并带有一个时间戳`t`。`Watermark(t)`表示数据流中的事件时间已达到时间`t`，意思就是说数据流之后不再有时间戳`t‘<= t`的元素(即带时间戳的事件老于或等于`watermark`)。\n\n下图显示了具有时间戳(逻辑上)的事件流以及内嵌的`watermark`。在这个例子中，事件是有序的(相对于它们的时间戳)，这意味着`watermark`只是数据流中的周期性标记。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-1.png?raw=true)\n\n`watermark`对于乱序数据流至关重要，如下图所示，事件并未按照时间戳进行排序。通常，`watermark`表示在数据流中那个时刻小于时间戳的所有事件都已经到达。一旦`watermark`到达算子，算子就可以将其内部的事件时间提到`watermark`的那个值。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true)\n\n\n### 2. 数据流中的并行Watermarks\n\n`watermarks`是直接通过数据源函数(source functions)生成的或在数据源函数之后生成的。源函数的每个并行子任务通常独立生成`watermarks`。这些`watermarks`在指定并行数据源上定义事件时间。\n\n`watermarks`贯穿整个流处理程序，他们会在`watermark`到达的算子时将事件时间提前(advance)。每当算子提前事件时间时，它都会为下游的后续算子生成一个新的`watermarks`(Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.)。\n\n一些算子消耗多个输入流；例如，union操作，或者算子后面跟着`keyBy(...)`函数或者`partition(...)函数`。这样的算子的当前事件时间是其输入流的所有事件时间中的最小值。随着输入流更新事件时间，算子也会更新事件。\n\n下图显示了事件和`watermarks`流经并行流的的示例，以及跟踪事件时间的算子:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true)\n\n### 3. 延迟元素\n\n某些元素可能违反`watermarks`条件，这意味着即使出现`watermarks(t)`，但是还是会出现很多的时间戳`t'<= t`的元素。事实上，在现实世界中，某些元素可能被任意地延迟，因此指定一个时间，带有事件时间戳的所有事件在此之前出现是不可能的。此外，即使延迟时间是有限制的，也不希望延迟太多的`watermarks`，因为它会在事件时间窗口的评估中导致太多的延迟。\n\n因此，流处理程序中可能会明确的知道会有延迟元素。延迟元素是那些系统事件时钟(由`watermark`所示)已经超过了延迟元素的时间戳的那些元素。有关如何处理事件时间窗口中的延迟元素的更多信息，请参阅[Allowed Lateness](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#allowed-lateness)。\n\n### 4. 调试Watermarks\n\n请参阅[调试Windows和事件时间](https://ci.apache.org/projects/flink/flink-docs-release-1.3/monitoring/debugging_event_time.html)部分，以便在运行时调试Watermarks。\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks\n","source":"_posts/Flink/[Flink]Flink1.4 事件时间与Watermarks.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 事件时间与Watermarks\ndate: 2018-01-04 17:57:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n### 1. watermark\n\n`Flink`实现了数据流模型(`Dataflow Model`)中许多技术。如果想对事件时间(`event time`)和`watermarks`更详细的了解，请参阅下面的文章:\n- [The world beyond batch: Streaming 101\n](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101)\n- [The Dataflow Model](http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf)\n\n支持事件时间的流处理器需要一种方法来衡量事件时间的进度。例如，一个构建小时窗口的窗口算子(`operator`)，当事件时间超过一小时末尾时需要告知窗口算子，以便算子可以关闭正在进行的窗口。\n\n事件时间可以独立于处理时间来运行。例如，在一个程序中，算子的当前事件时间可以略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度继续运行。另一方面，另一个流式处理程序处理几个星期的事件时间只需几秒钟就可以，通过快速浏览缓存在`Kafka Topic`中历史数据。\n\n`Flink`中测量事件时间进度的机制是`watermarks`。`watermarks`会作为数据流的一部分进行流动，并带有一个时间戳`t`。`Watermark(t)`表示数据流中的事件时间已达到时间`t`，意思就是说数据流之后不再有时间戳`t‘<= t`的元素(即带时间戳的事件老于或等于`watermark`)。\n\n下图显示了具有时间戳(逻辑上)的事件流以及内嵌的`watermark`。在这个例子中，事件是有序的(相对于它们的时间戳)，这意味着`watermark`只是数据流中的周期性标记。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-1.png?raw=true)\n\n`watermark`对于乱序数据流至关重要，如下图所示，事件并未按照时间戳进行排序。通常，`watermark`表示在数据流中那个时刻小于时间戳的所有事件都已经到达。一旦`watermark`到达算子，算子就可以将其内部的事件时间提到`watermark`的那个值。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true)\n\n\n### 2. 数据流中的并行Watermarks\n\n`watermarks`是直接通过数据源函数(source functions)生成的或在数据源函数之后生成的。源函数的每个并行子任务通常独立生成`watermarks`。这些`watermarks`在指定并行数据源上定义事件时间。\n\n`watermarks`贯穿整个流处理程序，他们会在`watermark`到达的算子时将事件时间提前(advance)。每当算子提前事件时间时，它都会为下游的后续算子生成一个新的`watermarks`(Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.)。\n\n一些算子消耗多个输入流；例如，union操作，或者算子后面跟着`keyBy(...)`函数或者`partition(...)函数`。这样的算子的当前事件时间是其输入流的所有事件时间中的最小值。随着输入流更新事件时间，算子也会更新事件。\n\n下图显示了事件和`watermarks`流经并行流的的示例，以及跟踪事件时间的算子:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true)\n\n### 3. 延迟元素\n\n某些元素可能违反`watermarks`条件，这意味着即使出现`watermarks(t)`，但是还是会出现很多的时间戳`t'<= t`的元素。事实上，在现实世界中，某些元素可能被任意地延迟，因此指定一个时间，带有事件时间戳的所有事件在此之前出现是不可能的。此外，即使延迟时间是有限制的，也不希望延迟太多的`watermarks`，因为它会在事件时间窗口的评估中导致太多的延迟。\n\n因此，流处理程序中可能会明确的知道会有延迟元素。延迟元素是那些系统事件时钟(由`watermark`所示)已经超过了延迟元素的时间戳的那些元素。有关如何处理事件时间窗口中的延迟元素的更多信息，请参阅[Allowed Lateness](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#allowed-lateness)。\n\n### 4. 调试Watermarks\n\n请参阅[调试Windows和事件时间](https://ci.apache.org/projects/flink/flink-docs-release-1.3/monitoring/debugging_event_time.html)部分，以便在运行时调试Watermarks。\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks\n","slug":"Flink/[Flink]Flink1.4 事件时间与Watermarks","published":1,"updated":"2018-01-29T09:36:59.657Z","comments":1,"photos":[],"link":"","_id":"cje58tipt000gordbmtolalil","content":"<h3 id=\"1-watermark\"><a href=\"#1-watermark\" class=\"headerlink\" title=\"1. watermark\"></a>1. watermark</h3><p><code>Flink</code>实现了数据流模型(<code>Dataflow Model</code>)中许多技术。如果想对事件时间(<code>event time</code>)和<code>watermarks</code>更详细的了解，请参阅下面的文章:</p>\n<ul>\n<li><a href=\"https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101\" target=\"_blank\" rel=\"noopener\">The world beyond batch: Streaming 101\n</a></li>\n<li><a href=\"http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf\" target=\"_blank\" rel=\"noopener\">The Dataflow Model</a></li>\n</ul>\n<p>支持事件时间的流处理器需要一种方法来衡量事件时间的进度。例如，一个构建小时窗口的窗口算子(<code>operator</code>)，当事件时间超过一小时末尾时需要告知窗口算子，以便算子可以关闭正在进行的窗口。</p>\n<p>事件时间可以独立于处理时间来运行。例如，在一个程序中，算子的当前事件时间可以略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度继续运行。另一方面，另一个流式处理程序处理几个星期的事件时间只需几秒钟就可以，通过快速浏览缓存在<code>Kafka Topic</code>中历史数据。</p>\n<p><code>Flink</code>中测量事件时间进度的机制是<code>watermarks</code>。<code>watermarks</code>会作为数据流的一部分进行流动，并带有一个时间戳<code>t</code>。<code>Watermark(t)</code>表示数据流中的事件时间已达到时间<code>t</code>，意思就是说数据流之后不再有时间戳<code>t‘&lt;= t</code>的元素(即带时间戳的事件老于或等于<code>watermark</code>)。</p>\n<p>下图显示了具有时间戳(逻辑上)的事件流以及内嵌的<code>watermark</code>。在这个例子中，事件是有序的(相对于它们的时间戳)，这意味着<code>watermark</code>只是数据流中的周期性标记。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-1.png?raw=true\" alt=\"\"></p>\n<p><code>watermark</code>对于乱序数据流至关重要，如下图所示，事件并未按照时间戳进行排序。通常，<code>watermark</code>表示在数据流中那个时刻小于时间戳的所有事件都已经到达。一旦<code>watermark</code>到达算子，算子就可以将其内部的事件时间提到<code>watermark</code>的那个值。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-数据流中的并行Watermarks\"><a href=\"#2-数据流中的并行Watermarks\" class=\"headerlink\" title=\"2. 数据流中的并行Watermarks\"></a>2. 数据流中的并行Watermarks</h3><p><code>watermarks</code>是直接通过数据源函数(source functions)生成的或在数据源函数之后生成的。源函数的每个并行子任务通常独立生成<code>watermarks</code>。这些<code>watermarks</code>在指定并行数据源上定义事件时间。</p>\n<p><code>watermarks</code>贯穿整个流处理程序，他们会在<code>watermark</code>到达的算子时将事件时间提前(advance)。每当算子提前事件时间时，它都会为下游的后续算子生成一个新的<code>watermarks</code>(Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.)。</p>\n<p>一些算子消耗多个输入流；例如，union操作，或者算子后面跟着<code>keyBy(...)</code>函数或者<code>partition(...)函数</code>。这样的算子的当前事件时间是其输入流的所有事件时间中的最小值。随着输入流更新事件时间，算子也会更新事件。</p>\n<p>下图显示了事件和<code>watermarks</code>流经并行流的的示例，以及跟踪事件时间的算子:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-延迟元素\"><a href=\"#3-延迟元素\" class=\"headerlink\" title=\"3. 延迟元素\"></a>3. 延迟元素</h3><p>某些元素可能违反<code>watermarks</code>条件，这意味着即使出现<code>watermarks(t)</code>，但是还是会出现很多的时间戳<code>t&#39;&lt;= t</code>的元素。事实上，在现实世界中，某些元素可能被任意地延迟，因此指定一个时间，带有事件时间戳的所有事件在此之前出现是不可能的。此外，即使延迟时间是有限制的，也不希望延迟太多的<code>watermarks</code>，因为它会在事件时间窗口的评估中导致太多的延迟。</p>\n<p>因此，流处理程序中可能会明确的知道会有延迟元素。延迟元素是那些系统事件时钟(由<code>watermark</code>所示)已经超过了延迟元素的时间戳的那些元素。有关如何处理事件时间窗口中的延迟元素的更多信息，请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#allowed-lateness\" target=\"_blank\" rel=\"noopener\">Allowed Lateness</a>。</p>\n<h3 id=\"4-调试Watermarks\"><a href=\"#4-调试Watermarks\" class=\"headerlink\" title=\"4. 调试Watermarks\"></a>4. 调试Watermarks</h3><p>请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/monitoring/debugging_event_time.html\" target=\"_blank\" rel=\"noopener\">调试Windows和事件时间</a>部分，以便在运行时调试Watermarks。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-watermark\"><a href=\"#1-watermark\" class=\"headerlink\" title=\"1. watermark\"></a>1. watermark</h3><p><code>Flink</code>实现了数据流模型(<code>Dataflow Model</code>)中许多技术。如果想对事件时间(<code>event time</code>)和<code>watermarks</code>更详细的了解，请参阅下面的文章:</p>\n<ul>\n<li><a href=\"https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101\" target=\"_blank\" rel=\"noopener\">The world beyond batch: Streaming 101\n</a></li>\n<li><a href=\"http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf\" target=\"_blank\" rel=\"noopener\">The Dataflow Model</a></li>\n</ul>\n<p>支持事件时间的流处理器需要一种方法来衡量事件时间的进度。例如，一个构建小时窗口的窗口算子(<code>operator</code>)，当事件时间超过一小时末尾时需要告知窗口算子，以便算子可以关闭正在进行的窗口。</p>\n<p>事件时间可以独立于处理时间来运行。例如，在一个程序中，算子的当前事件时间可以略微落后于处理时间(考虑到接收事件的延迟)，而两者以相同的速度继续运行。另一方面，另一个流式处理程序处理几个星期的事件时间只需几秒钟就可以，通过快速浏览缓存在<code>Kafka Topic</code>中历史数据。</p>\n<p><code>Flink</code>中测量事件时间进度的机制是<code>watermarks</code>。<code>watermarks</code>会作为数据流的一部分进行流动，并带有一个时间戳<code>t</code>。<code>Watermark(t)</code>表示数据流中的事件时间已达到时间<code>t</code>，意思就是说数据流之后不再有时间戳<code>t‘&lt;= t</code>的元素(即带时间戳的事件老于或等于<code>watermark</code>)。</p>\n<p>下图显示了具有时间戳(逻辑上)的事件流以及内嵌的<code>watermark</code>。在这个例子中，事件是有序的(相对于它们的时间戳)，这意味着<code>watermark</code>只是数据流中的周期性标记。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-1.png?raw=true\" alt=\"\"></p>\n<p><code>watermark</code>对于乱序数据流至关重要，如下图所示，事件并未按照时间戳进行排序。通常，<code>watermark</code>表示在数据流中那个时刻小于时间戳的所有事件都已经到达。一旦<code>watermark</code>到达算子，算子就可以将其内部的事件时间提到<code>watermark</code>的那个值。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-数据流中的并行Watermarks\"><a href=\"#2-数据流中的并行Watermarks\" class=\"headerlink\" title=\"2. 数据流中的并行Watermarks\"></a>2. 数据流中的并行Watermarks</h3><p><code>watermarks</code>是直接通过数据源函数(source functions)生成的或在数据源函数之后生成的。源函数的每个并行子任务通常独立生成<code>watermarks</code>。这些<code>watermarks</code>在指定并行数据源上定义事件时间。</p>\n<p><code>watermarks</code>贯穿整个流处理程序，他们会在<code>watermark</code>到达的算子时将事件时间提前(advance)。每当算子提前事件时间时，它都会为下游的后续算子生成一个新的<code>watermarks</code>(Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.)。</p>\n<p>一些算子消耗多个输入流；例如，union操作，或者算子后面跟着<code>keyBy(...)</code>函数或者<code>partition(...)函数</code>。这样的算子的当前事件时间是其输入流的所有事件时间中的最小值。随着输入流更新事件时间，算子也会更新事件。</p>\n<p>下图显示了事件和<code>watermarks</code>流经并行流的的示例，以及跟踪事件时间的算子:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-延迟元素\"><a href=\"#3-延迟元素\" class=\"headerlink\" title=\"3. 延迟元素\"></a>3. 延迟元素</h3><p>某些元素可能违反<code>watermarks</code>条件，这意味着即使出现<code>watermarks(t)</code>，但是还是会出现很多的时间戳<code>t&#39;&lt;= t</code>的元素。事实上，在现实世界中，某些元素可能被任意地延迟，因此指定一个时间，带有事件时间戳的所有事件在此之前出现是不可能的。此外，即使延迟时间是有限制的，也不希望延迟太多的<code>watermarks</code>，因为它会在事件时间窗口的评估中导致太多的延迟。</p>\n<p>因此，流处理程序中可能会明确的知道会有延迟元素。延迟元素是那些系统事件时钟(由<code>watermark</code>所示)已经超过了延迟元素的时间戳的那些元素。有关如何处理事件时间窗口中的延迟元素的更多信息，请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/windows.html#allowed-lateness\" target=\"_blank\" rel=\"noopener\">Allowed Lateness</a>。</p>\n<h3 id=\"4-调试Watermarks\"><a href=\"#4-调试Watermarks\" class=\"headerlink\" title=\"4. 调试Watermarks\"></a>4. 调试Watermarks</h3><p>请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/monitoring/debugging_event_time.html\" target=\"_blank\" rel=\"noopener\">调试Windows和事件时间</a>部分，以便在运行时调试Watermarks。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time-and-watermarks</a></p>\n"},{"layout":"post","author":"覃璐","title":"Flink 动态表的持续查询","date":"2018-02-02T03:29:01.000Z","_content":"\n越来越多的公司采用流处理，并将现有的批处理应用迁移到流处理，或者对新的用例采用流处理实现的解决方案。其中许多应用集中在流数据分析上，分析的数据流来自各种源，例如数据库事务、点击、传感器测量或IoT 设备。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-1.png?raw=true)\n\nApache Flink 非常适用于流分析应用程序，因为它支持事件时间语义，确保只处理一次，以及同时实现了高吞吐量和低延迟。因为这些特性，Flink 能够近实时对大量的输入数据计算出一个确定和精确的结果，并且在发生故障的时候提供一次性语义。\n\nFlink 的核心流处理API，DataStream API，非常具有表现力，并且为许多常见操作提供了原语。在其他特性中，它提供了高度可定制的窗口逻辑，不同表现特征下的不同状态原语，注册和响应定时器的钩子，以及高效的异步请求外部系统的工具。另一方面，许多流分析应用遵循相似的模式，并不需要DataStream API 提供的表现力级别。他们可以使用领域特定的语言来使用更自然和简洁的方式表达。总所周知，SQL 是数据分析的事实标准。对于流分析，SQL 可以让更多的人在数据流的特定应用中花费更少的时间。然而，目前还没有开源的流处理器提供令人满意的SQL 支持。\n\n### 1. 为什么流中的 SQL 很重要\n\nSQL 是数据分析使用最广泛的语言，有很多原因：\n- SQL 是声明式的：你指定你想要的东西，而不是如何去计算；\n- SQL 可以进行有效的优化：优化器计估算有效的计划来计算结果；\n- SQL 可以进行有效的评估：处理引擎准确的知道计算内容，以及如何有效的执行；\n- 最后，所有人都知道的，许多工具都理解SQL。\n\n因此，使用SQL 处理和分析数据流，可以为更多人提供流处理技术。此外，因为SQL 的声明性质和潜在的自动优化，它可以大大减少定义高效流分析应用的时间和精力。\n\n但是，SQL（以及关系数据模型和代数）并不是为流数据设计的。关系是（多）集合而不是无限序列的元组。当执行SQL 查询时，传统数据库系统和查询引擎读取和处理完整的可用数据集，并产生固定大小的结果。相比之下，数据流持续提供新的记录，使数据随着时间到达。因此，流查询需要不断的处理到达的数据，从来都不是“完整的”。\n\n话虽如此，使用SQL 处理流并不是不可能的。一些关系型数据库系统维护了物化视图，类似于在流数据中评估SQL 查询。物化视图被定义为一个SQL 查询，就像常规（虚拟）视图一样。但是，查询的结果实际上被保存（或者是物化）在内存或硬盘中，这样视图在查询时不需要实时计算。为了防止物化视图的数据过时，数据库系统需要在其基础关系（定义的SQL 查询引用的表）被修改时更新更新视图。如果我们将视图的基础关系修改视作修改流（或者是更改日志流），物化视图的维护和流中的SQL 的关系就变得很明确了。\n\n### 2. Flink 的关系API：Table API 和SQL\n\n从1.1.0版本（2016年8月发布）以来，Flink 提供了两个语义相当的关系API，语言内嵌的Table API（用于Java 和Scala）以及标准SQL。这两种API 被设计用于在线流和遗留的批处理数据API 的统一，这意味着无论输入是静态批处理数据还是流数据，查询产生完全相同的结果。\n\n统一流和批处理的API 非常重要。首先，用户只需要学习一个API 来处理静态和流数据。此外，可以使用同样的查询来分析批处理和流数据，这样可以在同一个查询里面同时分析历史和在线数据。在目前的状况下，我们尚未完全实现批处理和流式语义的统一，但社区在这个目标上取得了很大的进展。\n\n下面的代码片段展示了两个等效的Table API 和SQL 查询，用来在温度传感器测量数据流中计算一个简单的窗口聚合。SQL 查询的语法基于Apache Calcite 的分组窗口函数样式，并将在Flink 1.3.0版本中得到支持。\n\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval tEnv = TableEnvironment.getTableEnvironment(env)\n\n// define a table source to read sensor data (sensorId, time, room, temp)\nval sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...\n// register the table source\ntEnv.registerTableSource(\"sensors\", sensorTable)\n\n// Table API\nval tapiResult: Table = tEnv.scan(\"sensors\")   // scan sensors table\n .window(Tumble over 1.hour on 'rowtime as 'w) // define 1-hour window\n .groupBy('w, 'room)                           // group by window and room\n .select('room, 'w.end, 'temp.avg as 'avgTemp) // compute average temperature\n\n// SQL\nval sqlResult: Table = tEnv.sql(\"\"\"\n |SELECT room, TUMBLE_END(rowtime, INTERVAL '1' HOUR), AVG(temp) AS avgTemp\n |FROM sensors\n |GROUP BY TUMBLE(rowtime, INTERVAL '1' HOUR), room\n |\"\"\".stripMargin)\n```\n\n就像你看到的，两种API 以及Flink 主要的的DataStream 和DataSet API 是紧密结合的。Table 可以和DataSet 或DataStream 相互转换。因此，可以很简单的去扫描一个外部的表，例如数据库或者是Parquet 文件，使用Table API 查询做一些预处理，将结果转换为DataSet，并对其运行Gelly 图形算法。上述示例中定义的查询也可以通过更改执行环境来处理批量数据。\n\n在内部，两种API 都被转换成相同的逻辑表示，由Apache Calcite 进行优化，并被编译成DataStream 或是DataSet 程序。实际上，优化和转换程序并不知道查询是通过Table API 还是SQL 来定义的。如果你对优化过程的细节感兴趣，可以看看我们去年发布的一篇博客文章。由于Table API 和SQL 在语义方面等同，只是在样式上有些区别，在这篇文章中当我们谈论SQL 时我们通常引用这两种API。\n\n在当前的1.2.0版本中，Flink 的关系API 在数据流中，支持有限的关系操作，包括投影、过滤和窗口聚合。所有支持的操作有一个共同点，就是它们永远不会更新已经产生的结果记录。这对于时间记录操作，例如投影和过滤显然不是问题。但是，它会影响收集和处理多条记录的操作，例如窗口聚合。由于产生的结果不能被更新，在Flink 1.2.0中，输入的记录在产生结果之后不得不被丢弃。\n\n当前版本的限制对于将产生的数据发往Kafka 主题、消息队列或者是文件这些存储系统的应用是可以被接受的，因为它们只支持追加操作，没有更新和删除。遵循这种模式的常见用例是持续的ETL 和流存档应用，将流进行持久化存档，或者是准备数据用于进一步的在线（流）或者是离线分析。由于不可能更新之前产生的结果，这一类应用必须确保产生的结果是正确的，并且将来不需要更正。下图说明了这样的应用。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-2.png?raw=true)\n\n虽然只支持追加查询对有些类型的应用和存储系统有用，但是还是有一些流分析的用例需要更新结果。这些流应用包括不能丢弃延迟到达的记录，需要早期的结果用于（长期运行）窗口聚合，或者是需要非窗口的聚合。在每种情况下，之前产生的结果记录都需要被更新。结果更新查询通常将其结果保存在外部数据库或者是键值存储，使其可以让外部应用访问或者是查询。实现这种模式的应用有仪表板、报告应用或者是其他的应用，它们需要及时的访问持续更新的结果。下图说明了这一类应用\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-3.png?raw=true)\n\n### 3. 动态表的持续查询\n\n支持查询更新之前产生的结果是Flink 的关系API 的下一个重要步骤。这个功能非常重要，因为它大大增加了API 支持的用例的范围和种类。此外，一些新的用例可以采用DataStream API 来实现。\n\n因此，当添加对结果更新查询的支持时，我们必须保留之前的流和批处理输入的语义。我们通过动态表的概念来实现。动态表是持续更新，并且能够像常规的静态表一样查询的表。但是，与批处理表查询终止后返回一个静态表作为结果不同的是，动态表中的查询会持续运行，并根据输入表的修改产生一个持续更新的表。因此，结果表也是动态的。这个概念非常类似我们之前讨论的物化视图的维护。\n\n假设我们可以在动态表中运行查询并产生一个新的动态表，那会带来一个问题，流和动态表如何相互关联？答案是流和动态表可以相互转换。下图展示了在流中处理关系查询的概念模型。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-4.png?raw=true)\n\n首先，流被转换为动态表，动态表使用一个持续查询进行查询，产生一个新的动态表。最后，结果表被转换成流。要注意，这个只是逻辑模型，并不意味着查询是如何实际执行的。实际上，持续查询在内部被转换成传统的DataStream 程序。\n\n随后，我们描述了这个模型的不同步骤：\n- 在流中定义动态表\n- 查询动态表\n- 生成动态表\n\n### 3.1 在流中定义动态表\n\n评估动态表上的SQL 查询的第一步是在流中定义一个动态表。这意味着我们必须指定流中的记录如何修改动态表。流携带的记录必须具有映射到表的关系模式的模式。在流中定义动态表有两种模式：附加模式和更新模式。\n\n在附加模式中，流中的每条记录是对动态表的插入修改。因此，流中的所有记录都附加到动态表中，使得它的大小不断增长并且无限大。下图说明了附加模式。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-5.png?raw=true)\n\n在更新模式中，流中的记录可以作为动态表的插入、更新或者删除修改（附加模式实际上是一种特殊的更新模式）。当在流中通过更新模式定义一个动态表时，我们可以在表中指定一个唯一的键属性。在这种情况下，更新和删除操作会带着键属性一起执行。更新模式如下图所示。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-6.png?raw=true)\n\n#### 3.2 查询动态表\n\n一旦我们定义了动态表，我们可以在上面运行查询。由于动态表随着时间进行改变，我们必须定义查询动态表的意义。假定我们有一个特定时间的动态表的快照，这个快照可以作为一个标准的静态批处理表。我们将动态表A 在点t 的快照表示为A[t]，可以使用人意的SQL 查询来查询快照，该查询产生了一个标准的静态表作为结果，我们把在时间t 对动态表A 做的查询q 的结果表示为q(A[t])。如果我们反复在动态表的快照上计算查询结果，以获取进度时间点，我们将获得许多静态结果表，它们随着时间的推移而改变，并且有效的构成一个动态表。我们在动态表的查询中定义如下语义。\n\n查询q 在动态表A 上产生了一个动态表R，它在每个时间点t 等价于在A[t]上执行q 的结果，即R[t]=q(A[t])。该定义意味着在批处理表和流表上执行相同的查询q 会产生相同的结果。在下面的例子中，我们给出了两个例子来说明动态表查询的语义。\n\n在下图中，我们看到左侧的动态输入表A，定义成追加模式。在时间t=8时，A 由6行（标记成蓝色）组成。在时间t=9 和t=12 时，有一行追加到A（分别用绿色和橙色标记）。我们在表A 上运行一个如图中间所示的简单查询，这个查询根据属性k 分组，并统计每组的记录数。在右侧我们看到了t=8（蓝色），t\n=9（绿色）和t=12（橙色）时查询q 的结果。在每个时间点t，结果表等价于在时间t 时再动态表A 上执行批查询。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-7.png?raw=true)\n\n这个例子中的查询是一个简单的分组（但是没有窗口）聚合查询。因此，结果表的大小依赖于输入表的分组键的数量。此外，值得注意的是，这个查询会持续更新之前产生的结果行，而不只是添加新行。\n\n第二个例子展示了一个类似的查询，但是有一个很重要的差异。除了对属性k 分组以外，查询还将记录每5秒钟分组为一个滚动窗口，这意味着它每5秒钟计算一次k 的总数。再一次的，我们使用Calcite 的分组窗口函数来指定这个查询。在图的左侧，我们看到输入表A ，以及它在附加模式下随着时间而改变。在右侧，我们看到结果表，以及它随着时间演变。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-8.png?raw=true)\n\n与第一个例子的结果不同的是，这个结果表随着时间增长，例如每5秒钟计算出新的结果行（考虑到输入表在过去5秒收到更多的记录）。虽然非窗口查询（主要是）更新结果表的行，但是窗口聚合查询只追加新行到结果表中。\n\n虽然这篇博客专注于动态表的SQL 查询的语义，而不是如何有效的处理这样的查询，但是我们要指出的是，无论输入表什么时候更新，都不可能计算查询的完整结果。相反，查询编译成流应用，根据输入的变化持续更新它的结果。这意味着不是所有的有效SQL 都支持，只有那些持续性的、递增的和高效计算的被支持。我们计划在后续的博客文章中讨论关于评估动态表的SQL 查询的详细内容。\n\n#### 3.3 生成动态表\n\n查询动态表生成的动态表，其相当于查询结果。根据查询和它的输入表，结果表会通过插入、更新和删除持续更改，就像普通的数据表一样。它可能是一个不断被更新的单行表，一个只插入不更新的表，或者介于两者之间。\n\n传统的数据库系统在故障和复制的时候，通过日志重建表。有一些不同的日志技术，比如UNDO、REDO和UNDO/REDO日志。简而言之，UNDO 日志记录被修改元素之前的值来回滚不完整的事务，REDO 日志记录元素修改的新值来重做已完成事务丢失的改变，UNDO/REDO 日志同时记录了被修改元素的旧值和新值来撤销未完成的事务，并重做已完成事务丢失的改变。基于这些日志技术的原理，动态表可以转换成两类更改日志流：REDO 流和REDO+UNDO 流。\n\n通过将表中的修改转换为流消息，动态表被转换为redo+undo 流。插入修改生成一条新行的插入消息，删除修改生成一条旧行的删除消息，更新修改生成一条旧行的删除消息以及一条新行的插入消息。行为如下图所示。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-9.png?raw=true)\n\n左侧显示了一个维护在附加模式下的动态表，作为中间查询的输入。查询的结果转换为显示在底部的redo+undo 流。输入表的第一条记录(1,A)作为结果表的一条新纪录，因此插入了一条消息+(A,1)到流中。第二条输入记录k=‘A’(4,A)导致了结果表中 (A,1)记录的更新，从而产生了一条删除消息-(A,1)和一条插入消息+(A,2)。所有的下游操作或数据汇总都需要能够正确处理这两种类型的消息。\n\n在两种情况下，动态表会转换成redo 流：要么它只是一个附加表（即只有插入修改），要么它有一个唯一的键属性。动态表上的每一个插入修改会产生一条新行的插入消息到redo 流。由于redo 流的限制，只有带有唯一键的表能够进行更新和删除修改。如果一个键从动态表中删除，要么是因为行被删除，要么是因为行的键属性值被修改了，所以一条带有被移除键的删除消息发送到redo 流。更新修改生成带有更新的更新消息，比如新行。由于删除和更新修改根据唯一键来定义，下游操作需要能够根据键来访问之前的值。下图展示了如何将上述相同查询的结果表转换为redo 流。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-10.png?raw=true)\n\n插入到动态表的(1,A)产生了+(A,1)插入消息。产生更新的(4,A)生成了*(A,2)的更新消息。\n\nRedo 流的通常做法是将查询结果写到仅附加的存储系统，比如滚动文件或者Kafka 主题，或者是基于键访问的数据存储，比如Cassandra、关系型DBMS以及压缩的Kafka 主题。还可以实现将动态表作为流应用的关键的内嵌部分，来评价持续查询和对外部系统的查询能力，例如一个仪表盘应用。\n\n#### 3.4 切换到动态表发生的改变\n\n在1.2版本中，Flink 关系API 的所有流操作，例如过滤和分组窗口聚合，只会产生新行，并且不能更新先前发布的结果。 相比之下，动态表能够处理更新和删除修改。 现在你可能会问自己，当前版本的处理模式如何与新的动态表模型相关？ API 的语义会完全改变，我们需要从头开始重新实现API，以达到所需的语义？\n\n所有这些问题的答案很简单。当前的处理模型是动态表模型的一个子集。 使用我们在这篇文章中介绍的术语，当前的模型通过附加模式将流转换为动态表，即一个无限增长的表。 由于所有操作仅接受插入更改并在其结果表上生成插入更改（即，产生新行），因此所有在动态附加表上已经支持的查询，将使用重做模型转换回DataStreams，仅用于附加表。 因此，当前模型的语义被新的动态表模型完全覆盖和保留。\n\n### 4. 结论与展望\n\nFlink 的关系API 在任何时候都非常适合用于流分析应用，并在不同的生产环境中使用。在这篇博文中，我们讨论了Table API 和SQL 的未来。 这一努力将使Flink 和流处理更易于访问。 此外，用于查询历史和实时数据的统一语义以及查询和维护动态表的概念，将能够显着简化许多令人兴奋的用例和应用程序的实现。 由于这篇文章专注于流和动态表的关系查询的语义，我们没有讨论查询执行的细节，包括内部执行撤销，处理后期事件，支持结果预览，以及边界空间要求。 我们计划在稍后的时间点发布有关此主题的后续博客文章。\n\n近几个月来，Flink 社区的许多成员一直在讨论和贡献关系API。 到目前为止，我们取得了很大的进步。 虽然大多数工作都专注于以附加模式处理流，但是日程上的下一步是处理动态表以支持更新其结果的查询。 如果您对使用SQL处理流程的想法感到兴奋，并希望为此做出贡献，请提供反馈，加入邮件列表中的讨论或获取JIRA 问题。\n\n\n\n译文: http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table\n\n原文: http://flink.apache.org/news/2017/04/04/dynamic-tables.html\n","source":"_posts/Flink/[Flink]Flink 动态表的持续查询.md","raw":"---\nlayout: post\nauthor: 覃璐\ntitle: Flink 动态表的持续查询\ndate: 2018-02-02 11:29:01\ntags:\n  - Flink\n  - Flink SQL\n\ncategories: Flink\npermalink: flink-sql-persistent-query-of-dynamic-table\n---\n\n越来越多的公司采用流处理，并将现有的批处理应用迁移到流处理，或者对新的用例采用流处理实现的解决方案。其中许多应用集中在流数据分析上，分析的数据流来自各种源，例如数据库事务、点击、传感器测量或IoT 设备。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-1.png?raw=true)\n\nApache Flink 非常适用于流分析应用程序，因为它支持事件时间语义，确保只处理一次，以及同时实现了高吞吐量和低延迟。因为这些特性，Flink 能够近实时对大量的输入数据计算出一个确定和精确的结果，并且在发生故障的时候提供一次性语义。\n\nFlink 的核心流处理API，DataStream API，非常具有表现力，并且为许多常见操作提供了原语。在其他特性中，它提供了高度可定制的窗口逻辑，不同表现特征下的不同状态原语，注册和响应定时器的钩子，以及高效的异步请求外部系统的工具。另一方面，许多流分析应用遵循相似的模式，并不需要DataStream API 提供的表现力级别。他们可以使用领域特定的语言来使用更自然和简洁的方式表达。总所周知，SQL 是数据分析的事实标准。对于流分析，SQL 可以让更多的人在数据流的特定应用中花费更少的时间。然而，目前还没有开源的流处理器提供令人满意的SQL 支持。\n\n### 1. 为什么流中的 SQL 很重要\n\nSQL 是数据分析使用最广泛的语言，有很多原因：\n- SQL 是声明式的：你指定你想要的东西，而不是如何去计算；\n- SQL 可以进行有效的优化：优化器计估算有效的计划来计算结果；\n- SQL 可以进行有效的评估：处理引擎准确的知道计算内容，以及如何有效的执行；\n- 最后，所有人都知道的，许多工具都理解SQL。\n\n因此，使用SQL 处理和分析数据流，可以为更多人提供流处理技术。此外，因为SQL 的声明性质和潜在的自动优化，它可以大大减少定义高效流分析应用的时间和精力。\n\n但是，SQL（以及关系数据模型和代数）并不是为流数据设计的。关系是（多）集合而不是无限序列的元组。当执行SQL 查询时，传统数据库系统和查询引擎读取和处理完整的可用数据集，并产生固定大小的结果。相比之下，数据流持续提供新的记录，使数据随着时间到达。因此，流查询需要不断的处理到达的数据，从来都不是“完整的”。\n\n话虽如此，使用SQL 处理流并不是不可能的。一些关系型数据库系统维护了物化视图，类似于在流数据中评估SQL 查询。物化视图被定义为一个SQL 查询，就像常规（虚拟）视图一样。但是，查询的结果实际上被保存（或者是物化）在内存或硬盘中，这样视图在查询时不需要实时计算。为了防止物化视图的数据过时，数据库系统需要在其基础关系（定义的SQL 查询引用的表）被修改时更新更新视图。如果我们将视图的基础关系修改视作修改流（或者是更改日志流），物化视图的维护和流中的SQL 的关系就变得很明确了。\n\n### 2. Flink 的关系API：Table API 和SQL\n\n从1.1.0版本（2016年8月发布）以来，Flink 提供了两个语义相当的关系API，语言内嵌的Table API（用于Java 和Scala）以及标准SQL。这两种API 被设计用于在线流和遗留的批处理数据API 的统一，这意味着无论输入是静态批处理数据还是流数据，查询产生完全相同的结果。\n\n统一流和批处理的API 非常重要。首先，用户只需要学习一个API 来处理静态和流数据。此外，可以使用同样的查询来分析批处理和流数据，这样可以在同一个查询里面同时分析历史和在线数据。在目前的状况下，我们尚未完全实现批处理和流式语义的统一，但社区在这个目标上取得了很大的进展。\n\n下面的代码片段展示了两个等效的Table API 和SQL 查询，用来在温度传感器测量数据流中计算一个简单的窗口聚合。SQL 查询的语法基于Apache Calcite 的分组窗口函数样式，并将在Flink 1.3.0版本中得到支持。\n\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval tEnv = TableEnvironment.getTableEnvironment(env)\n\n// define a table source to read sensor data (sensorId, time, room, temp)\nval sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...\n// register the table source\ntEnv.registerTableSource(\"sensors\", sensorTable)\n\n// Table API\nval tapiResult: Table = tEnv.scan(\"sensors\")   // scan sensors table\n .window(Tumble over 1.hour on 'rowtime as 'w) // define 1-hour window\n .groupBy('w, 'room)                           // group by window and room\n .select('room, 'w.end, 'temp.avg as 'avgTemp) // compute average temperature\n\n// SQL\nval sqlResult: Table = tEnv.sql(\"\"\"\n |SELECT room, TUMBLE_END(rowtime, INTERVAL '1' HOUR), AVG(temp) AS avgTemp\n |FROM sensors\n |GROUP BY TUMBLE(rowtime, INTERVAL '1' HOUR), room\n |\"\"\".stripMargin)\n```\n\n就像你看到的，两种API 以及Flink 主要的的DataStream 和DataSet API 是紧密结合的。Table 可以和DataSet 或DataStream 相互转换。因此，可以很简单的去扫描一个外部的表，例如数据库或者是Parquet 文件，使用Table API 查询做一些预处理，将结果转换为DataSet，并对其运行Gelly 图形算法。上述示例中定义的查询也可以通过更改执行环境来处理批量数据。\n\n在内部，两种API 都被转换成相同的逻辑表示，由Apache Calcite 进行优化，并被编译成DataStream 或是DataSet 程序。实际上，优化和转换程序并不知道查询是通过Table API 还是SQL 来定义的。如果你对优化过程的细节感兴趣，可以看看我们去年发布的一篇博客文章。由于Table API 和SQL 在语义方面等同，只是在样式上有些区别，在这篇文章中当我们谈论SQL 时我们通常引用这两种API。\n\n在当前的1.2.0版本中，Flink 的关系API 在数据流中，支持有限的关系操作，包括投影、过滤和窗口聚合。所有支持的操作有一个共同点，就是它们永远不会更新已经产生的结果记录。这对于时间记录操作，例如投影和过滤显然不是问题。但是，它会影响收集和处理多条记录的操作，例如窗口聚合。由于产生的结果不能被更新，在Flink 1.2.0中，输入的记录在产生结果之后不得不被丢弃。\n\n当前版本的限制对于将产生的数据发往Kafka 主题、消息队列或者是文件这些存储系统的应用是可以被接受的，因为它们只支持追加操作，没有更新和删除。遵循这种模式的常见用例是持续的ETL 和流存档应用，将流进行持久化存档，或者是准备数据用于进一步的在线（流）或者是离线分析。由于不可能更新之前产生的结果，这一类应用必须确保产生的结果是正确的，并且将来不需要更正。下图说明了这样的应用。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-2.png?raw=true)\n\n虽然只支持追加查询对有些类型的应用和存储系统有用，但是还是有一些流分析的用例需要更新结果。这些流应用包括不能丢弃延迟到达的记录，需要早期的结果用于（长期运行）窗口聚合，或者是需要非窗口的聚合。在每种情况下，之前产生的结果记录都需要被更新。结果更新查询通常将其结果保存在外部数据库或者是键值存储，使其可以让外部应用访问或者是查询。实现这种模式的应用有仪表板、报告应用或者是其他的应用，它们需要及时的访问持续更新的结果。下图说明了这一类应用\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-3.png?raw=true)\n\n### 3. 动态表的持续查询\n\n支持查询更新之前产生的结果是Flink 的关系API 的下一个重要步骤。这个功能非常重要，因为它大大增加了API 支持的用例的范围和种类。此外，一些新的用例可以采用DataStream API 来实现。\n\n因此，当添加对结果更新查询的支持时，我们必须保留之前的流和批处理输入的语义。我们通过动态表的概念来实现。动态表是持续更新，并且能够像常规的静态表一样查询的表。但是，与批处理表查询终止后返回一个静态表作为结果不同的是，动态表中的查询会持续运行，并根据输入表的修改产生一个持续更新的表。因此，结果表也是动态的。这个概念非常类似我们之前讨论的物化视图的维护。\n\n假设我们可以在动态表中运行查询并产生一个新的动态表，那会带来一个问题，流和动态表如何相互关联？答案是流和动态表可以相互转换。下图展示了在流中处理关系查询的概念模型。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-4.png?raw=true)\n\n首先，流被转换为动态表，动态表使用一个持续查询进行查询，产生一个新的动态表。最后，结果表被转换成流。要注意，这个只是逻辑模型，并不意味着查询是如何实际执行的。实际上，持续查询在内部被转换成传统的DataStream 程序。\n\n随后，我们描述了这个模型的不同步骤：\n- 在流中定义动态表\n- 查询动态表\n- 生成动态表\n\n### 3.1 在流中定义动态表\n\n评估动态表上的SQL 查询的第一步是在流中定义一个动态表。这意味着我们必须指定流中的记录如何修改动态表。流携带的记录必须具有映射到表的关系模式的模式。在流中定义动态表有两种模式：附加模式和更新模式。\n\n在附加模式中，流中的每条记录是对动态表的插入修改。因此，流中的所有记录都附加到动态表中，使得它的大小不断增长并且无限大。下图说明了附加模式。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-5.png?raw=true)\n\n在更新模式中，流中的记录可以作为动态表的插入、更新或者删除修改（附加模式实际上是一种特殊的更新模式）。当在流中通过更新模式定义一个动态表时，我们可以在表中指定一个唯一的键属性。在这种情况下，更新和删除操作会带着键属性一起执行。更新模式如下图所示。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-6.png?raw=true)\n\n#### 3.2 查询动态表\n\n一旦我们定义了动态表，我们可以在上面运行查询。由于动态表随着时间进行改变，我们必须定义查询动态表的意义。假定我们有一个特定时间的动态表的快照，这个快照可以作为一个标准的静态批处理表。我们将动态表A 在点t 的快照表示为A[t]，可以使用人意的SQL 查询来查询快照，该查询产生了一个标准的静态表作为结果，我们把在时间t 对动态表A 做的查询q 的结果表示为q(A[t])。如果我们反复在动态表的快照上计算查询结果，以获取进度时间点，我们将获得许多静态结果表，它们随着时间的推移而改变，并且有效的构成一个动态表。我们在动态表的查询中定义如下语义。\n\n查询q 在动态表A 上产生了一个动态表R，它在每个时间点t 等价于在A[t]上执行q 的结果，即R[t]=q(A[t])。该定义意味着在批处理表和流表上执行相同的查询q 会产生相同的结果。在下面的例子中，我们给出了两个例子来说明动态表查询的语义。\n\n在下图中，我们看到左侧的动态输入表A，定义成追加模式。在时间t=8时，A 由6行（标记成蓝色）组成。在时间t=9 和t=12 时，有一行追加到A（分别用绿色和橙色标记）。我们在表A 上运行一个如图中间所示的简单查询，这个查询根据属性k 分组，并统计每组的记录数。在右侧我们看到了t=8（蓝色），t\n=9（绿色）和t=12（橙色）时查询q 的结果。在每个时间点t，结果表等价于在时间t 时再动态表A 上执行批查询。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-7.png?raw=true)\n\n这个例子中的查询是一个简单的分组（但是没有窗口）聚合查询。因此，结果表的大小依赖于输入表的分组键的数量。此外，值得注意的是，这个查询会持续更新之前产生的结果行，而不只是添加新行。\n\n第二个例子展示了一个类似的查询，但是有一个很重要的差异。除了对属性k 分组以外，查询还将记录每5秒钟分组为一个滚动窗口，这意味着它每5秒钟计算一次k 的总数。再一次的，我们使用Calcite 的分组窗口函数来指定这个查询。在图的左侧，我们看到输入表A ，以及它在附加模式下随着时间而改变。在右侧，我们看到结果表，以及它随着时间演变。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-8.png?raw=true)\n\n与第一个例子的结果不同的是，这个结果表随着时间增长，例如每5秒钟计算出新的结果行（考虑到输入表在过去5秒收到更多的记录）。虽然非窗口查询（主要是）更新结果表的行，但是窗口聚合查询只追加新行到结果表中。\n\n虽然这篇博客专注于动态表的SQL 查询的语义，而不是如何有效的处理这样的查询，但是我们要指出的是，无论输入表什么时候更新，都不可能计算查询的完整结果。相反，查询编译成流应用，根据输入的变化持续更新它的结果。这意味着不是所有的有效SQL 都支持，只有那些持续性的、递增的和高效计算的被支持。我们计划在后续的博客文章中讨论关于评估动态表的SQL 查询的详细内容。\n\n#### 3.3 生成动态表\n\n查询动态表生成的动态表，其相当于查询结果。根据查询和它的输入表，结果表会通过插入、更新和删除持续更改，就像普通的数据表一样。它可能是一个不断被更新的单行表，一个只插入不更新的表，或者介于两者之间。\n\n传统的数据库系统在故障和复制的时候，通过日志重建表。有一些不同的日志技术，比如UNDO、REDO和UNDO/REDO日志。简而言之，UNDO 日志记录被修改元素之前的值来回滚不完整的事务，REDO 日志记录元素修改的新值来重做已完成事务丢失的改变，UNDO/REDO 日志同时记录了被修改元素的旧值和新值来撤销未完成的事务，并重做已完成事务丢失的改变。基于这些日志技术的原理，动态表可以转换成两类更改日志流：REDO 流和REDO+UNDO 流。\n\n通过将表中的修改转换为流消息，动态表被转换为redo+undo 流。插入修改生成一条新行的插入消息，删除修改生成一条旧行的删除消息，更新修改生成一条旧行的删除消息以及一条新行的插入消息。行为如下图所示。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-9.png?raw=true)\n\n左侧显示了一个维护在附加模式下的动态表，作为中间查询的输入。查询的结果转换为显示在底部的redo+undo 流。输入表的第一条记录(1,A)作为结果表的一条新纪录，因此插入了一条消息+(A,1)到流中。第二条输入记录k=‘A’(4,A)导致了结果表中 (A,1)记录的更新，从而产生了一条删除消息-(A,1)和一条插入消息+(A,2)。所有的下游操作或数据汇总都需要能够正确处理这两种类型的消息。\n\n在两种情况下，动态表会转换成redo 流：要么它只是一个附加表（即只有插入修改），要么它有一个唯一的键属性。动态表上的每一个插入修改会产生一条新行的插入消息到redo 流。由于redo 流的限制，只有带有唯一键的表能够进行更新和删除修改。如果一个键从动态表中删除，要么是因为行被删除，要么是因为行的键属性值被修改了，所以一条带有被移除键的删除消息发送到redo 流。更新修改生成带有更新的更新消息，比如新行。由于删除和更新修改根据唯一键来定义，下游操作需要能够根据键来访问之前的值。下图展示了如何将上述相同查询的结果表转换为redo 流。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-10.png?raw=true)\n\n插入到动态表的(1,A)产生了+(A,1)插入消息。产生更新的(4,A)生成了*(A,2)的更新消息。\n\nRedo 流的通常做法是将查询结果写到仅附加的存储系统，比如滚动文件或者Kafka 主题，或者是基于键访问的数据存储，比如Cassandra、关系型DBMS以及压缩的Kafka 主题。还可以实现将动态表作为流应用的关键的内嵌部分，来评价持续查询和对外部系统的查询能力，例如一个仪表盘应用。\n\n#### 3.4 切换到动态表发生的改变\n\n在1.2版本中，Flink 关系API 的所有流操作，例如过滤和分组窗口聚合，只会产生新行，并且不能更新先前发布的结果。 相比之下，动态表能够处理更新和删除修改。 现在你可能会问自己，当前版本的处理模式如何与新的动态表模型相关？ API 的语义会完全改变，我们需要从头开始重新实现API，以达到所需的语义？\n\n所有这些问题的答案很简单。当前的处理模型是动态表模型的一个子集。 使用我们在这篇文章中介绍的术语，当前的模型通过附加模式将流转换为动态表，即一个无限增长的表。 由于所有操作仅接受插入更改并在其结果表上生成插入更改（即，产生新行），因此所有在动态附加表上已经支持的查询，将使用重做模型转换回DataStreams，仅用于附加表。 因此，当前模型的语义被新的动态表模型完全覆盖和保留。\n\n### 4. 结论与展望\n\nFlink 的关系API 在任何时候都非常适合用于流分析应用，并在不同的生产环境中使用。在这篇博文中，我们讨论了Table API 和SQL 的未来。 这一努力将使Flink 和流处理更易于访问。 此外，用于查询历史和实时数据的统一语义以及查询和维护动态表的概念，将能够显着简化许多令人兴奋的用例和应用程序的实现。 由于这篇文章专注于流和动态表的关系查询的语义，我们没有讨论查询执行的细节，包括内部执行撤销，处理后期事件，支持结果预览，以及边界空间要求。 我们计划在稍后的时间点发布有关此主题的后续博客文章。\n\n近几个月来，Flink 社区的许多成员一直在讨论和贡献关系API。 到目前为止，我们取得了很大的进步。 虽然大多数工作都专注于以附加模式处理流，但是日程上的下一步是处理动态表以支持更新其结果的查询。 如果您对使用SQL处理流程的想法感到兴奋，并希望为此做出贡献，请提供反馈，加入邮件列表中的讨论或获取JIRA 问题。\n\n\n\n译文: http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table\n\n原文: http://flink.apache.org/news/2017/04/04/dynamic-tables.html\n","slug":"flink-sql-persistent-query-of-dynamic-table","published":1,"updated":"2018-02-02T08:47:50.237Z","comments":1,"photos":[],"link":"","_id":"cje58tipz000lordbcpq61bqe","content":"<p>越来越多的公司采用流处理，并将现有的批处理应用迁移到流处理，或者对新的用例采用流处理实现的解决方案。其中许多应用集中在流数据分析上，分析的数据流来自各种源，例如数据库事务、点击、传感器测量或IoT 设备。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-1.png?raw=true\" alt=\"\"></p>\n<p>Apache Flink 非常适用于流分析应用程序，因为它支持事件时间语义，确保只处理一次，以及同时实现了高吞吐量和低延迟。因为这些特性，Flink 能够近实时对大量的输入数据计算出一个确定和精确的结果，并且在发生故障的时候提供一次性语义。</p>\n<p>Flink 的核心流处理API，DataStream API，非常具有表现力，并且为许多常见操作提供了原语。在其他特性中，它提供了高度可定制的窗口逻辑，不同表现特征下的不同状态原语，注册和响应定时器的钩子，以及高效的异步请求外部系统的工具。另一方面，许多流分析应用遵循相似的模式，并不需要DataStream API 提供的表现力级别。他们可以使用领域特定的语言来使用更自然和简洁的方式表达。总所周知，SQL 是数据分析的事实标准。对于流分析，SQL 可以让更多的人在数据流的特定应用中花费更少的时间。然而，目前还没有开源的流处理器提供令人满意的SQL 支持。</p>\n<h3 id=\"1-为什么流中的-SQL-很重要\"><a href=\"#1-为什么流中的-SQL-很重要\" class=\"headerlink\" title=\"1. 为什么流中的 SQL 很重要\"></a>1. 为什么流中的 SQL 很重要</h3><p>SQL 是数据分析使用最广泛的语言，有很多原因：</p>\n<ul>\n<li>SQL 是声明式的：你指定你想要的东西，而不是如何去计算；</li>\n<li>SQL 可以进行有效的优化：优化器计估算有效的计划来计算结果；</li>\n<li>SQL 可以进行有效的评估：处理引擎准确的知道计算内容，以及如何有效的执行；</li>\n<li>最后，所有人都知道的，许多工具都理解SQL。</li>\n</ul>\n<p>因此，使用SQL 处理和分析数据流，可以为更多人提供流处理技术。此外，因为SQL 的声明性质和潜在的自动优化，它可以大大减少定义高效流分析应用的时间和精力。</p>\n<p>但是，SQL（以及关系数据模型和代数）并不是为流数据设计的。关系是（多）集合而不是无限序列的元组。当执行SQL 查询时，传统数据库系统和查询引擎读取和处理完整的可用数据集，并产生固定大小的结果。相比之下，数据流持续提供新的记录，使数据随着时间到达。因此，流查询需要不断的处理到达的数据，从来都不是“完整的”。</p>\n<p>话虽如此，使用SQL 处理流并不是不可能的。一些关系型数据库系统维护了物化视图，类似于在流数据中评估SQL 查询。物化视图被定义为一个SQL 查询，就像常规（虚拟）视图一样。但是，查询的结果实际上被保存（或者是物化）在内存或硬盘中，这样视图在查询时不需要实时计算。为了防止物化视图的数据过时，数据库系统需要在其基础关系（定义的SQL 查询引用的表）被修改时更新更新视图。如果我们将视图的基础关系修改视作修改流（或者是更改日志流），物化视图的维护和流中的SQL 的关系就变得很明确了。</p>\n<h3 id=\"2-Flink-的关系API：Table-API-和SQL\"><a href=\"#2-Flink-的关系API：Table-API-和SQL\" class=\"headerlink\" title=\"2. Flink 的关系API：Table API 和SQL\"></a>2. Flink 的关系API：Table API 和SQL</h3><p>从1.1.0版本（2016年8月发布）以来，Flink 提供了两个语义相当的关系API，语言内嵌的Table API（用于Java 和Scala）以及标准SQL。这两种API 被设计用于在线流和遗留的批处理数据API 的统一，这意味着无论输入是静态批处理数据还是流数据，查询产生完全相同的结果。</p>\n<p>统一流和批处理的API 非常重要。首先，用户只需要学习一个API 来处理静态和流数据。此外，可以使用同样的查询来分析批处理和流数据，这样可以在同一个查询里面同时分析历史和在线数据。在目前的状况下，我们尚未完全实现批处理和流式语义的统一，但社区在这个目标上取得了很大的进展。</p>\n<p>下面的代码片段展示了两个等效的Table API 和SQL 查询，用来在温度传感器测量数据流中计算一个简单的窗口聚合。SQL 查询的语法基于Apache Calcite 的分组窗口函数样式，并将在Flink 1.3.0版本中得到支持。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">// define a table source to read sensor data (sensorId, time, room, temp)</span><br><span class=\"line\">val sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...</span><br><span class=\"line\">// register the table source</span><br><span class=\"line\">tEnv.registerTableSource(&quot;sensors&quot;, sensorTable)</span><br><span class=\"line\"></span><br><span class=\"line\">// Table API</span><br><span class=\"line\">val tapiResult: Table = tEnv.scan(&quot;sensors&quot;)   // scan sensors table</span><br><span class=\"line\"> .window(Tumble over 1.hour on &apos;rowtime as &apos;w) // define 1-hour window</span><br><span class=\"line\"> .groupBy(&apos;w, &apos;room)                           // group by window and room</span><br><span class=\"line\"> .select(&apos;room, &apos;w.end, &apos;temp.avg as &apos;avgTemp) // compute average temperature</span><br><span class=\"line\"></span><br><span class=\"line\">// SQL</span><br><span class=\"line\">val sqlResult: Table = tEnv.sql(&quot;&quot;&quot;</span><br><span class=\"line\"> |SELECT room, TUMBLE_END(rowtime, INTERVAL &apos;1&apos; HOUR), AVG(temp) AS avgTemp</span><br><span class=\"line\"> |FROM sensors</span><br><span class=\"line\"> |GROUP BY TUMBLE(rowtime, INTERVAL &apos;1&apos; HOUR), room</span><br><span class=\"line\"> |&quot;&quot;&quot;.stripMargin)</span><br></pre></td></tr></table></figure>\n<p>就像你看到的，两种API 以及Flink 主要的的DataStream 和DataSet API 是紧密结合的。Table 可以和DataSet 或DataStream 相互转换。因此，可以很简单的去扫描一个外部的表，例如数据库或者是Parquet 文件，使用Table API 查询做一些预处理，将结果转换为DataSet，并对其运行Gelly 图形算法。上述示例中定义的查询也可以通过更改执行环境来处理批量数据。</p>\n<p>在内部，两种API 都被转换成相同的逻辑表示，由Apache Calcite 进行优化，并被编译成DataStream 或是DataSet 程序。实际上，优化和转换程序并不知道查询是通过Table API 还是SQL 来定义的。如果你对优化过程的细节感兴趣，可以看看我们去年发布的一篇博客文章。由于Table API 和SQL 在语义方面等同，只是在样式上有些区别，在这篇文章中当我们谈论SQL 时我们通常引用这两种API。</p>\n<p>在当前的1.2.0版本中，Flink 的关系API 在数据流中，支持有限的关系操作，包括投影、过滤和窗口聚合。所有支持的操作有一个共同点，就是它们永远不会更新已经产生的结果记录。这对于时间记录操作，例如投影和过滤显然不是问题。但是，它会影响收集和处理多条记录的操作，例如窗口聚合。由于产生的结果不能被更新，在Flink 1.2.0中，输入的记录在产生结果之后不得不被丢弃。</p>\n<p>当前版本的限制对于将产生的数据发往Kafka 主题、消息队列或者是文件这些存储系统的应用是可以被接受的，因为它们只支持追加操作，没有更新和删除。遵循这种模式的常见用例是持续的ETL 和流存档应用，将流进行持久化存档，或者是准备数据用于进一步的在线（流）或者是离线分析。由于不可能更新之前产生的结果，这一类应用必须确保产生的结果是正确的，并且将来不需要更正。下图说明了这样的应用。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-2.png?raw=true\" alt=\"\"></p>\n<p>虽然只支持追加查询对有些类型的应用和存储系统有用，但是还是有一些流分析的用例需要更新结果。这些流应用包括不能丢弃延迟到达的记录，需要早期的结果用于（长期运行）窗口聚合，或者是需要非窗口的聚合。在每种情况下，之前产生的结果记录都需要被更新。结果更新查询通常将其结果保存在外部数据库或者是键值存储，使其可以让外部应用访问或者是查询。实现这种模式的应用有仪表板、报告应用或者是其他的应用，它们需要及时的访问持续更新的结果。下图说明了这一类应用</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-动态表的持续查询\"><a href=\"#3-动态表的持续查询\" class=\"headerlink\" title=\"3. 动态表的持续查询\"></a>3. 动态表的持续查询</h3><p>支持查询更新之前产生的结果是Flink 的关系API 的下一个重要步骤。这个功能非常重要，因为它大大增加了API 支持的用例的范围和种类。此外，一些新的用例可以采用DataStream API 来实现。</p>\n<p>因此，当添加对结果更新查询的支持时，我们必须保留之前的流和批处理输入的语义。我们通过动态表的概念来实现。动态表是持续更新，并且能够像常规的静态表一样查询的表。但是，与批处理表查询终止后返回一个静态表作为结果不同的是，动态表中的查询会持续运行，并根据输入表的修改产生一个持续更新的表。因此，结果表也是动态的。这个概念非常类似我们之前讨论的物化视图的维护。</p>\n<p>假设我们可以在动态表中运行查询并产生一个新的动态表，那会带来一个问题，流和动态表如何相互关联？答案是流和动态表可以相互转换。下图展示了在流中处理关系查询的概念模型。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-4.png?raw=true\" alt=\"\"></p>\n<p>首先，流被转换为动态表，动态表使用一个持续查询进行查询，产生一个新的动态表。最后，结果表被转换成流。要注意，这个只是逻辑模型，并不意味着查询是如何实际执行的。实际上，持续查询在内部被转换成传统的DataStream 程序。</p>\n<p>随后，我们描述了这个模型的不同步骤：</p>\n<ul>\n<li>在流中定义动态表</li>\n<li>查询动态表</li>\n<li>生成动态表</li>\n</ul>\n<h3 id=\"3-1-在流中定义动态表\"><a href=\"#3-1-在流中定义动态表\" class=\"headerlink\" title=\"3.1 在流中定义动态表\"></a>3.1 在流中定义动态表</h3><p>评估动态表上的SQL 查询的第一步是在流中定义一个动态表。这意味着我们必须指定流中的记录如何修改动态表。流携带的记录必须具有映射到表的关系模式的模式。在流中定义动态表有两种模式：附加模式和更新模式。</p>\n<p>在附加模式中，流中的每条记录是对动态表的插入修改。因此，流中的所有记录都附加到动态表中，使得它的大小不断增长并且无限大。下图说明了附加模式。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-5.png?raw=true\" alt=\"\"></p>\n<p>在更新模式中，流中的记录可以作为动态表的插入、更新或者删除修改（附加模式实际上是一种特殊的更新模式）。当在流中通过更新模式定义一个动态表时，我们可以在表中指定一个唯一的键属性。在这种情况下，更新和删除操作会带着键属性一起执行。更新模式如下图所示。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-6.png?raw=true\" alt=\"\"></p>\n<h4 id=\"3-2-查询动态表\"><a href=\"#3-2-查询动态表\" class=\"headerlink\" title=\"3.2 查询动态表\"></a>3.2 查询动态表</h4><p>一旦我们定义了动态表，我们可以在上面运行查询。由于动态表随着时间进行改变，我们必须定义查询动态表的意义。假定我们有一个特定时间的动态表的快照，这个快照可以作为一个标准的静态批处理表。我们将动态表A 在点t 的快照表示为A[t]，可以使用人意的SQL 查询来查询快照，该查询产生了一个标准的静态表作为结果，我们把在时间t 对动态表A 做的查询q 的结果表示为q(A[t])。如果我们反复在动态表的快照上计算查询结果，以获取进度时间点，我们将获得许多静态结果表，它们随着时间的推移而改变，并且有效的构成一个动态表。我们在动态表的查询中定义如下语义。</p>\n<p>查询q 在动态表A 上产生了一个动态表R，它在每个时间点t 等价于在A[t]上执行q 的结果，即R[t]=q(A[t])。该定义意味着在批处理表和流表上执行相同的查询q 会产生相同的结果。在下面的例子中，我们给出了两个例子来说明动态表查询的语义。</p>\n<p>在下图中，我们看到左侧的动态输入表A，定义成追加模式。在时间t=8时，A 由6行（标记成蓝色）组成。在时间t=9 和t=12 时，有一行追加到A（分别用绿色和橙色标记）。我们在表A 上运行一个如图中间所示的简单查询，这个查询根据属性k 分组，并统计每组的记录数。在右侧我们看到了t=8（蓝色），t<br>=9（绿色）和t=12（橙色）时查询q 的结果。在每个时间点t，结果表等价于在时间t 时再动态表A 上执行批查询。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-7.png?raw=true\" alt=\"\"></p>\n<p>这个例子中的查询是一个简单的分组（但是没有窗口）聚合查询。因此，结果表的大小依赖于输入表的分组键的数量。此外，值得注意的是，这个查询会持续更新之前产生的结果行，而不只是添加新行。</p>\n<p>第二个例子展示了一个类似的查询，但是有一个很重要的差异。除了对属性k 分组以外，查询还将记录每5秒钟分组为一个滚动窗口，这意味着它每5秒钟计算一次k 的总数。再一次的，我们使用Calcite 的分组窗口函数来指定这个查询。在图的左侧，我们看到输入表A ，以及它在附加模式下随着时间而改变。在右侧，我们看到结果表，以及它随着时间演变。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-8.png?raw=true\" alt=\"\"></p>\n<p>与第一个例子的结果不同的是，这个结果表随着时间增长，例如每5秒钟计算出新的结果行（考虑到输入表在过去5秒收到更多的记录）。虽然非窗口查询（主要是）更新结果表的行，但是窗口聚合查询只追加新行到结果表中。</p>\n<p>虽然这篇博客专注于动态表的SQL 查询的语义，而不是如何有效的处理这样的查询，但是我们要指出的是，无论输入表什么时候更新，都不可能计算查询的完整结果。相反，查询编译成流应用，根据输入的变化持续更新它的结果。这意味着不是所有的有效SQL 都支持，只有那些持续性的、递增的和高效计算的被支持。我们计划在后续的博客文章中讨论关于评估动态表的SQL 查询的详细内容。</p>\n<h4 id=\"3-3-生成动态表\"><a href=\"#3-3-生成动态表\" class=\"headerlink\" title=\"3.3 生成动态表\"></a>3.3 生成动态表</h4><p>查询动态表生成的动态表，其相当于查询结果。根据查询和它的输入表，结果表会通过插入、更新和删除持续更改，就像普通的数据表一样。它可能是一个不断被更新的单行表，一个只插入不更新的表，或者介于两者之间。</p>\n<p>传统的数据库系统在故障和复制的时候，通过日志重建表。有一些不同的日志技术，比如UNDO、REDO和UNDO/REDO日志。简而言之，UNDO 日志记录被修改元素之前的值来回滚不完整的事务，REDO 日志记录元素修改的新值来重做已完成事务丢失的改变，UNDO/REDO 日志同时记录了被修改元素的旧值和新值来撤销未完成的事务，并重做已完成事务丢失的改变。基于这些日志技术的原理，动态表可以转换成两类更改日志流：REDO 流和REDO+UNDO 流。</p>\n<p>通过将表中的修改转换为流消息，动态表被转换为redo+undo 流。插入修改生成一条新行的插入消息，删除修改生成一条旧行的删除消息，更新修改生成一条旧行的删除消息以及一条新行的插入消息。行为如下图所示。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-9.png?raw=true\" alt=\"\"></p>\n<p>左侧显示了一个维护在附加模式下的动态表，作为中间查询的输入。查询的结果转换为显示在底部的redo+undo 流。输入表的第一条记录(1,A)作为结果表的一条新纪录，因此插入了一条消息+(A,1)到流中。第二条输入记录k=‘A’(4,A)导致了结果表中 (A,1)记录的更新，从而产生了一条删除消息-(A,1)和一条插入消息+(A,2)。所有的下游操作或数据汇总都需要能够正确处理这两种类型的消息。</p>\n<p>在两种情况下，动态表会转换成redo 流：要么它只是一个附加表（即只有插入修改），要么它有一个唯一的键属性。动态表上的每一个插入修改会产生一条新行的插入消息到redo 流。由于redo 流的限制，只有带有唯一键的表能够进行更新和删除修改。如果一个键从动态表中删除，要么是因为行被删除，要么是因为行的键属性值被修改了，所以一条带有被移除键的删除消息发送到redo 流。更新修改生成带有更新的更新消息，比如新行。由于删除和更新修改根据唯一键来定义，下游操作需要能够根据键来访问之前的值。下图展示了如何将上述相同查询的结果表转换为redo 流。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-10.png?raw=true\" alt=\"\"></p>\n<p>插入到动态表的(1,A)产生了+(A,1)插入消息。产生更新的(4,A)生成了*(A,2)的更新消息。</p>\n<p>Redo 流的通常做法是将查询结果写到仅附加的存储系统，比如滚动文件或者Kafka 主题，或者是基于键访问的数据存储，比如Cassandra、关系型DBMS以及压缩的Kafka 主题。还可以实现将动态表作为流应用的关键的内嵌部分，来评价持续查询和对外部系统的查询能力，例如一个仪表盘应用。</p>\n<h4 id=\"3-4-切换到动态表发生的改变\"><a href=\"#3-4-切换到动态表发生的改变\" class=\"headerlink\" title=\"3.4 切换到动态表发生的改变\"></a>3.4 切换到动态表发生的改变</h4><p>在1.2版本中，Flink 关系API 的所有流操作，例如过滤和分组窗口聚合，只会产生新行，并且不能更新先前发布的结果。 相比之下，动态表能够处理更新和删除修改。 现在你可能会问自己，当前版本的处理模式如何与新的动态表模型相关？ API 的语义会完全改变，我们需要从头开始重新实现API，以达到所需的语义？</p>\n<p>所有这些问题的答案很简单。当前的处理模型是动态表模型的一个子集。 使用我们在这篇文章中介绍的术语，当前的模型通过附加模式将流转换为动态表，即一个无限增长的表。 由于所有操作仅接受插入更改并在其结果表上生成插入更改（即，产生新行），因此所有在动态附加表上已经支持的查询，将使用重做模型转换回DataStreams，仅用于附加表。 因此，当前模型的语义被新的动态表模型完全覆盖和保留。</p>\n<h3 id=\"4-结论与展望\"><a href=\"#4-结论与展望\" class=\"headerlink\" title=\"4. 结论与展望\"></a>4. 结论与展望</h3><p>Flink 的关系API 在任何时候都非常适合用于流分析应用，并在不同的生产环境中使用。在这篇博文中，我们讨论了Table API 和SQL 的未来。 这一努力将使Flink 和流处理更易于访问。 此外，用于查询历史和实时数据的统一语义以及查询和维护动态表的概念，将能够显着简化许多令人兴奋的用例和应用程序的实现。 由于这篇文章专注于流和动态表的关系查询的语义，我们没有讨论查询执行的细节，包括内部执行撤销，处理后期事件，支持结果预览，以及边界空间要求。 我们计划在稍后的时间点发布有关此主题的后续博客文章。</p>\n<p>近几个月来，Flink 社区的许多成员一直在讨论和贡献关系API。 到目前为止，我们取得了很大的进步。 虽然大多数工作都专注于以附加模式处理流，但是日程上的下一步是处理动态表以支持更新其结果的查询。 如果您对使用SQL处理流程的想法感到兴奋，并希望为此做出贡献，请提供反馈，加入邮件列表中的讨论或获取JIRA 问题。</p>\n<p>译文: <a href=\"http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table</a></p>\n<p>原文: <a href=\"http://flink.apache.org/news/2017/04/04/dynamic-tables.html\" target=\"_blank\" rel=\"noopener\">http://flink.apache.org/news/2017/04/04/dynamic-tables.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>越来越多的公司采用流处理，并将现有的批处理应用迁移到流处理，或者对新的用例采用流处理实现的解决方案。其中许多应用集中在流数据分析上，分析的数据流来自各种源，例如数据库事务、点击、传感器测量或IoT 设备。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-1.png?raw=true\" alt=\"\"></p>\n<p>Apache Flink 非常适用于流分析应用程序，因为它支持事件时间语义，确保只处理一次，以及同时实现了高吞吐量和低延迟。因为这些特性，Flink 能够近实时对大量的输入数据计算出一个确定和精确的结果，并且在发生故障的时候提供一次性语义。</p>\n<p>Flink 的核心流处理API，DataStream API，非常具有表现力，并且为许多常见操作提供了原语。在其他特性中，它提供了高度可定制的窗口逻辑，不同表现特征下的不同状态原语，注册和响应定时器的钩子，以及高效的异步请求外部系统的工具。另一方面，许多流分析应用遵循相似的模式，并不需要DataStream API 提供的表现力级别。他们可以使用领域特定的语言来使用更自然和简洁的方式表达。总所周知，SQL 是数据分析的事实标准。对于流分析，SQL 可以让更多的人在数据流的特定应用中花费更少的时间。然而，目前还没有开源的流处理器提供令人满意的SQL 支持。</p>\n<h3 id=\"1-为什么流中的-SQL-很重要\"><a href=\"#1-为什么流中的-SQL-很重要\" class=\"headerlink\" title=\"1. 为什么流中的 SQL 很重要\"></a>1. 为什么流中的 SQL 很重要</h3><p>SQL 是数据分析使用最广泛的语言，有很多原因：</p>\n<ul>\n<li>SQL 是声明式的：你指定你想要的东西，而不是如何去计算；</li>\n<li>SQL 可以进行有效的优化：优化器计估算有效的计划来计算结果；</li>\n<li>SQL 可以进行有效的评估：处理引擎准确的知道计算内容，以及如何有效的执行；</li>\n<li>最后，所有人都知道的，许多工具都理解SQL。</li>\n</ul>\n<p>因此，使用SQL 处理和分析数据流，可以为更多人提供流处理技术。此外，因为SQL 的声明性质和潜在的自动优化，它可以大大减少定义高效流分析应用的时间和精力。</p>\n<p>但是，SQL（以及关系数据模型和代数）并不是为流数据设计的。关系是（多）集合而不是无限序列的元组。当执行SQL 查询时，传统数据库系统和查询引擎读取和处理完整的可用数据集，并产生固定大小的结果。相比之下，数据流持续提供新的记录，使数据随着时间到达。因此，流查询需要不断的处理到达的数据，从来都不是“完整的”。</p>\n<p>话虽如此，使用SQL 处理流并不是不可能的。一些关系型数据库系统维护了物化视图，类似于在流数据中评估SQL 查询。物化视图被定义为一个SQL 查询，就像常规（虚拟）视图一样。但是，查询的结果实际上被保存（或者是物化）在内存或硬盘中，这样视图在查询时不需要实时计算。为了防止物化视图的数据过时，数据库系统需要在其基础关系（定义的SQL 查询引用的表）被修改时更新更新视图。如果我们将视图的基础关系修改视作修改流（或者是更改日志流），物化视图的维护和流中的SQL 的关系就变得很明确了。</p>\n<h3 id=\"2-Flink-的关系API：Table-API-和SQL\"><a href=\"#2-Flink-的关系API：Table-API-和SQL\" class=\"headerlink\" title=\"2. Flink 的关系API：Table API 和SQL\"></a>2. Flink 的关系API：Table API 和SQL</h3><p>从1.1.0版本（2016年8月发布）以来，Flink 提供了两个语义相当的关系API，语言内嵌的Table API（用于Java 和Scala）以及标准SQL。这两种API 被设计用于在线流和遗留的批处理数据API 的统一，这意味着无论输入是静态批处理数据还是流数据，查询产生完全相同的结果。</p>\n<p>统一流和批处理的API 非常重要。首先，用户只需要学习一个API 来处理静态和流数据。此外，可以使用同样的查询来分析批处理和流数据，这样可以在同一个查询里面同时分析历史和在线数据。在目前的状况下，我们尚未完全实现批处理和流式语义的统一，但社区在这个目标上取得了很大的进展。</p>\n<p>下面的代码片段展示了两个等效的Table API 和SQL 查询，用来在温度传感器测量数据流中计算一个简单的窗口聚合。SQL 查询的语法基于Apache Calcite 的分组窗口函数样式，并将在Flink 1.3.0版本中得到支持。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">val tEnv = TableEnvironment.getTableEnvironment(env)</span><br><span class=\"line\"></span><br><span class=\"line\">// define a table source to read sensor data (sensorId, time, room, temp)</span><br><span class=\"line\">val sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...</span><br><span class=\"line\">// register the table source</span><br><span class=\"line\">tEnv.registerTableSource(&quot;sensors&quot;, sensorTable)</span><br><span class=\"line\"></span><br><span class=\"line\">// Table API</span><br><span class=\"line\">val tapiResult: Table = tEnv.scan(&quot;sensors&quot;)   // scan sensors table</span><br><span class=\"line\"> .window(Tumble over 1.hour on &apos;rowtime as &apos;w) // define 1-hour window</span><br><span class=\"line\"> .groupBy(&apos;w, &apos;room)                           // group by window and room</span><br><span class=\"line\"> .select(&apos;room, &apos;w.end, &apos;temp.avg as &apos;avgTemp) // compute average temperature</span><br><span class=\"line\"></span><br><span class=\"line\">// SQL</span><br><span class=\"line\">val sqlResult: Table = tEnv.sql(&quot;&quot;&quot;</span><br><span class=\"line\"> |SELECT room, TUMBLE_END(rowtime, INTERVAL &apos;1&apos; HOUR), AVG(temp) AS avgTemp</span><br><span class=\"line\"> |FROM sensors</span><br><span class=\"line\"> |GROUP BY TUMBLE(rowtime, INTERVAL &apos;1&apos; HOUR), room</span><br><span class=\"line\"> |&quot;&quot;&quot;.stripMargin)</span><br></pre></td></tr></table></figure>\n<p>就像你看到的，两种API 以及Flink 主要的的DataStream 和DataSet API 是紧密结合的。Table 可以和DataSet 或DataStream 相互转换。因此，可以很简单的去扫描一个外部的表，例如数据库或者是Parquet 文件，使用Table API 查询做一些预处理，将结果转换为DataSet，并对其运行Gelly 图形算法。上述示例中定义的查询也可以通过更改执行环境来处理批量数据。</p>\n<p>在内部，两种API 都被转换成相同的逻辑表示，由Apache Calcite 进行优化，并被编译成DataStream 或是DataSet 程序。实际上，优化和转换程序并不知道查询是通过Table API 还是SQL 来定义的。如果你对优化过程的细节感兴趣，可以看看我们去年发布的一篇博客文章。由于Table API 和SQL 在语义方面等同，只是在样式上有些区别，在这篇文章中当我们谈论SQL 时我们通常引用这两种API。</p>\n<p>在当前的1.2.0版本中，Flink 的关系API 在数据流中，支持有限的关系操作，包括投影、过滤和窗口聚合。所有支持的操作有一个共同点，就是它们永远不会更新已经产生的结果记录。这对于时间记录操作，例如投影和过滤显然不是问题。但是，它会影响收集和处理多条记录的操作，例如窗口聚合。由于产生的结果不能被更新，在Flink 1.2.0中，输入的记录在产生结果之后不得不被丢弃。</p>\n<p>当前版本的限制对于将产生的数据发往Kafka 主题、消息队列或者是文件这些存储系统的应用是可以被接受的，因为它们只支持追加操作，没有更新和删除。遵循这种模式的常见用例是持续的ETL 和流存档应用，将流进行持久化存档，或者是准备数据用于进一步的在线（流）或者是离线分析。由于不可能更新之前产生的结果，这一类应用必须确保产生的结果是正确的，并且将来不需要更正。下图说明了这样的应用。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-2.png?raw=true\" alt=\"\"></p>\n<p>虽然只支持追加查询对有些类型的应用和存储系统有用，但是还是有一些流分析的用例需要更新结果。这些流应用包括不能丢弃延迟到达的记录，需要早期的结果用于（长期运行）窗口聚合，或者是需要非窗口的聚合。在每种情况下，之前产生的结果记录都需要被更新。结果更新查询通常将其结果保存在外部数据库或者是键值存储，使其可以让外部应用访问或者是查询。实现这种模式的应用有仪表板、报告应用或者是其他的应用，它们需要及时的访问持续更新的结果。下图说明了这一类应用</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-动态表的持续查询\"><a href=\"#3-动态表的持续查询\" class=\"headerlink\" title=\"3. 动态表的持续查询\"></a>3. 动态表的持续查询</h3><p>支持查询更新之前产生的结果是Flink 的关系API 的下一个重要步骤。这个功能非常重要，因为它大大增加了API 支持的用例的范围和种类。此外，一些新的用例可以采用DataStream API 来实现。</p>\n<p>因此，当添加对结果更新查询的支持时，我们必须保留之前的流和批处理输入的语义。我们通过动态表的概念来实现。动态表是持续更新，并且能够像常规的静态表一样查询的表。但是，与批处理表查询终止后返回一个静态表作为结果不同的是，动态表中的查询会持续运行，并根据输入表的修改产生一个持续更新的表。因此，结果表也是动态的。这个概念非常类似我们之前讨论的物化视图的维护。</p>\n<p>假设我们可以在动态表中运行查询并产生一个新的动态表，那会带来一个问题，流和动态表如何相互关联？答案是流和动态表可以相互转换。下图展示了在流中处理关系查询的概念模型。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-4.png?raw=true\" alt=\"\"></p>\n<p>首先，流被转换为动态表，动态表使用一个持续查询进行查询，产生一个新的动态表。最后，结果表被转换成流。要注意，这个只是逻辑模型，并不意味着查询是如何实际执行的。实际上，持续查询在内部被转换成传统的DataStream 程序。</p>\n<p>随后，我们描述了这个模型的不同步骤：</p>\n<ul>\n<li>在流中定义动态表</li>\n<li>查询动态表</li>\n<li>生成动态表</li>\n</ul>\n<h3 id=\"3-1-在流中定义动态表\"><a href=\"#3-1-在流中定义动态表\" class=\"headerlink\" title=\"3.1 在流中定义动态表\"></a>3.1 在流中定义动态表</h3><p>评估动态表上的SQL 查询的第一步是在流中定义一个动态表。这意味着我们必须指定流中的记录如何修改动态表。流携带的记录必须具有映射到表的关系模式的模式。在流中定义动态表有两种模式：附加模式和更新模式。</p>\n<p>在附加模式中，流中的每条记录是对动态表的插入修改。因此，流中的所有记录都附加到动态表中，使得它的大小不断增长并且无限大。下图说明了附加模式。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-5.png?raw=true\" alt=\"\"></p>\n<p>在更新模式中，流中的记录可以作为动态表的插入、更新或者删除修改（附加模式实际上是一种特殊的更新模式）。当在流中通过更新模式定义一个动态表时，我们可以在表中指定一个唯一的键属性。在这种情况下，更新和删除操作会带着键属性一起执行。更新模式如下图所示。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-6.png?raw=true\" alt=\"\"></p>\n<h4 id=\"3-2-查询动态表\"><a href=\"#3-2-查询动态表\" class=\"headerlink\" title=\"3.2 查询动态表\"></a>3.2 查询动态表</h4><p>一旦我们定义了动态表，我们可以在上面运行查询。由于动态表随着时间进行改变，我们必须定义查询动态表的意义。假定我们有一个特定时间的动态表的快照，这个快照可以作为一个标准的静态批处理表。我们将动态表A 在点t 的快照表示为A[t]，可以使用人意的SQL 查询来查询快照，该查询产生了一个标准的静态表作为结果，我们把在时间t 对动态表A 做的查询q 的结果表示为q(A[t])。如果我们反复在动态表的快照上计算查询结果，以获取进度时间点，我们将获得许多静态结果表，它们随着时间的推移而改变，并且有效的构成一个动态表。我们在动态表的查询中定义如下语义。</p>\n<p>查询q 在动态表A 上产生了一个动态表R，它在每个时间点t 等价于在A[t]上执行q 的结果，即R[t]=q(A[t])。该定义意味着在批处理表和流表上执行相同的查询q 会产生相同的结果。在下面的例子中，我们给出了两个例子来说明动态表查询的语义。</p>\n<p>在下图中，我们看到左侧的动态输入表A，定义成追加模式。在时间t=8时，A 由6行（标记成蓝色）组成。在时间t=9 和t=12 时，有一行追加到A（分别用绿色和橙色标记）。我们在表A 上运行一个如图中间所示的简单查询，这个查询根据属性k 分组，并统计每组的记录数。在右侧我们看到了t=8（蓝色），t<br>=9（绿色）和t=12（橙色）时查询q 的结果。在每个时间点t，结果表等价于在时间t 时再动态表A 上执行批查询。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-7.png?raw=true\" alt=\"\"></p>\n<p>这个例子中的查询是一个简单的分组（但是没有窗口）聚合查询。因此，结果表的大小依赖于输入表的分组键的数量。此外，值得注意的是，这个查询会持续更新之前产生的结果行，而不只是添加新行。</p>\n<p>第二个例子展示了一个类似的查询，但是有一个很重要的差异。除了对属性k 分组以外，查询还将记录每5秒钟分组为一个滚动窗口，这意味着它每5秒钟计算一次k 的总数。再一次的，我们使用Calcite 的分组窗口函数来指定这个查询。在图的左侧，我们看到输入表A ，以及它在附加模式下随着时间而改变。在右侧，我们看到结果表，以及它随着时间演变。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-8.png?raw=true\" alt=\"\"></p>\n<p>与第一个例子的结果不同的是，这个结果表随着时间增长，例如每5秒钟计算出新的结果行（考虑到输入表在过去5秒收到更多的记录）。虽然非窗口查询（主要是）更新结果表的行，但是窗口聚合查询只追加新行到结果表中。</p>\n<p>虽然这篇博客专注于动态表的SQL 查询的语义，而不是如何有效的处理这样的查询，但是我们要指出的是，无论输入表什么时候更新，都不可能计算查询的完整结果。相反，查询编译成流应用，根据输入的变化持续更新它的结果。这意味着不是所有的有效SQL 都支持，只有那些持续性的、递增的和高效计算的被支持。我们计划在后续的博客文章中讨论关于评估动态表的SQL 查询的详细内容。</p>\n<h4 id=\"3-3-生成动态表\"><a href=\"#3-3-生成动态表\" class=\"headerlink\" title=\"3.3 生成动态表\"></a>3.3 生成动态表</h4><p>查询动态表生成的动态表，其相当于查询结果。根据查询和它的输入表，结果表会通过插入、更新和删除持续更改，就像普通的数据表一样。它可能是一个不断被更新的单行表，一个只插入不更新的表，或者介于两者之间。</p>\n<p>传统的数据库系统在故障和复制的时候，通过日志重建表。有一些不同的日志技术，比如UNDO、REDO和UNDO/REDO日志。简而言之，UNDO 日志记录被修改元素之前的值来回滚不完整的事务，REDO 日志记录元素修改的新值来重做已完成事务丢失的改变，UNDO/REDO 日志同时记录了被修改元素的旧值和新值来撤销未完成的事务，并重做已完成事务丢失的改变。基于这些日志技术的原理，动态表可以转换成两类更改日志流：REDO 流和REDO+UNDO 流。</p>\n<p>通过将表中的修改转换为流消息，动态表被转换为redo+undo 流。插入修改生成一条新行的插入消息，删除修改生成一条旧行的删除消息，更新修改生成一条旧行的删除消息以及一条新行的插入消息。行为如下图所示。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-9.png?raw=true\" alt=\"\"></p>\n<p>左侧显示了一个维护在附加模式下的动态表，作为中间查询的输入。查询的结果转换为显示在底部的redo+undo 流。输入表的第一条记录(1,A)作为结果表的一条新纪录，因此插入了一条消息+(A,1)到流中。第二条输入记录k=‘A’(4,A)导致了结果表中 (A,1)记录的更新，从而产生了一条删除消息-(A,1)和一条插入消息+(A,2)。所有的下游操作或数据汇总都需要能够正确处理这两种类型的消息。</p>\n<p>在两种情况下，动态表会转换成redo 流：要么它只是一个附加表（即只有插入修改），要么它有一个唯一的键属性。动态表上的每一个插入修改会产生一条新行的插入消息到redo 流。由于redo 流的限制，只有带有唯一键的表能够进行更新和删除修改。如果一个键从动态表中删除，要么是因为行被删除，要么是因为行的键属性值被修改了，所以一条带有被移除键的删除消息发送到redo 流。更新修改生成带有更新的更新消息，比如新行。由于删除和更新修改根据唯一键来定义，下游操作需要能够根据键来访问之前的值。下图展示了如何将上述相同查询的结果表转换为redo 流。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink-sql-persistent-query-of-dynamic-table-10.png?raw=true\" alt=\"\"></p>\n<p>插入到动态表的(1,A)产生了+(A,1)插入消息。产生更新的(4,A)生成了*(A,2)的更新消息。</p>\n<p>Redo 流的通常做法是将查询结果写到仅附加的存储系统，比如滚动文件或者Kafka 主题，或者是基于键访问的数据存储，比如Cassandra、关系型DBMS以及压缩的Kafka 主题。还可以实现将动态表作为流应用的关键的内嵌部分，来评价持续查询和对外部系统的查询能力，例如一个仪表盘应用。</p>\n<h4 id=\"3-4-切换到动态表发生的改变\"><a href=\"#3-4-切换到动态表发生的改变\" class=\"headerlink\" title=\"3.4 切换到动态表发生的改变\"></a>3.4 切换到动态表发生的改变</h4><p>在1.2版本中，Flink 关系API 的所有流操作，例如过滤和分组窗口聚合，只会产生新行，并且不能更新先前发布的结果。 相比之下，动态表能够处理更新和删除修改。 现在你可能会问自己，当前版本的处理模式如何与新的动态表模型相关？ API 的语义会完全改变，我们需要从头开始重新实现API，以达到所需的语义？</p>\n<p>所有这些问题的答案很简单。当前的处理模型是动态表模型的一个子集。 使用我们在这篇文章中介绍的术语，当前的模型通过附加模式将流转换为动态表，即一个无限增长的表。 由于所有操作仅接受插入更改并在其结果表上生成插入更改（即，产生新行），因此所有在动态附加表上已经支持的查询，将使用重做模型转换回DataStreams，仅用于附加表。 因此，当前模型的语义被新的动态表模型完全覆盖和保留。</p>\n<h3 id=\"4-结论与展望\"><a href=\"#4-结论与展望\" class=\"headerlink\" title=\"4. 结论与展望\"></a>4. 结论与展望</h3><p>Flink 的关系API 在任何时候都非常适合用于流分析应用，并在不同的生产环境中使用。在这篇博文中，我们讨论了Table API 和SQL 的未来。 这一努力将使Flink 和流处理更易于访问。 此外，用于查询历史和实时数据的统一语义以及查询和维护动态表的概念，将能够显着简化许多令人兴奋的用例和应用程序的实现。 由于这篇文章专注于流和动态表的关系查询的语义，我们没有讨论查询执行的细节，包括内部执行撤销，处理后期事件，支持结果预览，以及边界空间要求。 我们计划在稍后的时间点发布有关此主题的后续博客文章。</p>\n<p>近几个月来，Flink 社区的许多成员一直在讨论和贡献关系API。 到目前为止，我们取得了很大的进步。 虽然大多数工作都专注于以附加模式处理流，但是日程上的下一步是处理动态表以支持更新其结果的查询。 如果您对使用SQL处理流程的想法感到兴奋，并希望为此做出贡献，请提供反馈，加入邮件列表中的讨论或获取JIRA 问题。</p>\n<p>译文: <a href=\"http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/persistent-query-of-dynamic-table</a></p>\n<p>原文: <a href=\"http://flink.apache.org/news/2017/04/04/dynamic-tables.html\" target=\"_blank\" rel=\"noopener\">http://flink.apache.org/news/2017/04/04/dynamic-tables.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 事件时间与处理时间","date":"2018-01-04T08:47:01.000Z","_content":"\n\n`Flink`在数据流中支持几种不同概念的时间。\n\n### 1. 处理时间\n\n`Processing Time`(处理时间)是指执行相应操作机器的系统时间(Processing time refers to the system time of the machine that is executing the respective operation.)。\n\n当一个流程序以处理时间来运行时，所有基于时间的操作(如时间窗口)将使用运行算子(`operator`)所在机器的系统时间。例如:一个基于处理时间按每小时进行处理的时间窗口将包括以系统时间为标准在一个小时内到达指定算子的所有的记录(an hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour.)。\n\n处理时间是最简单的一个时间概念，不需要在数据流和机器之间进行协调。它有最好的性能和最低的延迟。然而，在分布式或者异步环境中，处理时间具有不确定性，因为容易受到记录到达系统速度的影响(例如从消息队列到达的记录)，还会受到系统内记录流在不同算子之间的流动速度的影响(speed at which records arrive in the system, and to the speed at which the records flow between operators inside the system)。\n\n### 2. 事件时间\n\n`Event Time`(事件时间)是每个独立事件在它生产设备上产生的时间。在进入`Flink`之前，事件时间通常要嵌入到记录中，并且事件时间也可以从记录中提取出来。一个基于事件时间按每小时进行处理的时间窗口将包含所有的记录，其事件时间都在这一小时之内，不管它们何时到达，以及它们以什么顺序到达。\n\n事件时间即使在乱序事件，延迟事件以及从备份或持久化日志中的重复数据也能获得正确的结果。对于事件时间，时间的进度取决于数据，而不是任何时钟。事件时间程序必须指定如何生成事件时间的`Watermarks`，这是表示事件时间进度的机制。\n\n按事件时间处理往往会导致一定的延迟，因为它要等待延迟事件和无序事件一段时间。因此，事件时间程序通常与处理时间操作相结合使用。\n\n### 3. 摄入时间\n\n`Ingestion Time`(摄入时间)是事件进入`Flink`的时间。在`source operator`中，每个记录将源的当前时间记为时间戳，基于时间的操作(如时间窗口)会使用该时间戳。\n\n摄入时间在概念上处于事件时间和处理时间之间。与处理时间相比，摄入时间的成本稍微更高一些，但是可以提供更可预测的结果。因为摄入时间的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，然而对于处理时间，每个窗口算子可能将记录分配给不同的窗口(基于本地系统时钟以及传输延迟)。\n\n与事件时间相比，摄入时间程序无法处理任何无序事件或延迟事件，但程序不必指定如何生成`watermarks`。\n\n在内部，摄入时间与事件时间非常相似，但事件时间会自动分配时间戳以及自动生成`watermark`(with automatic timestamp assignment and automatic watermark generation)。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4.png?raw=true)\n\n### 4. 选择时间特性\n\n`Flink DataStream`程序的第一部分通常设置基本的时间特性(base time characteristic)。该设置定义数据流源的行为方式(例如，它们是否产生时间戳)，以及窗口操作如`KeyedStream.timeWindow(Time.seconds(30))`应使用哪一类型时间，是事件时间还是处理时间等。\n\n以下示例展示了一个聚合每小时时间窗口内的事件的`Flink`程序。窗口的行为会与时间特性相匹配。\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);\n\n// alternatively:\n// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\nDataStream<MyEvent> stream = env.addSource(new FlinkKafkaConsumer09<MyEvent>(topic, schema, props));\n\nstream\n    .keyBy( (event) -> event.getUser() )\n    .timeWindow(Time.hours(1))\n    .reduce( (a, b) -> a.add(b) )\n    .addSink(...);\n```\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\n\nenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)\n\n// alternatively:\n// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)\n// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))\n\nstream\n    .keyBy( _.getUser )\n    .timeWindow(Time.hours(1))\n    .reduce( (a, b) => a.add(b) )\n    .addSink(...)\n```\n\n备注:\n```\n为了以事件时间运行此示例，程序需要使用定义了事件时间并自动产生watermarks的源，或者程序必须在源之后设置时间戳分配器和watermarks生成器。上述函数描述了如何获取事件时间戳，以及展现事件流的无序程度。\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time\n","source":"_posts/Flink/[Flink]Flink1.4 事件时间与处理时间.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 事件时间与处理时间\ndate: 2018-01-04 16:47:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n\n`Flink`在数据流中支持几种不同概念的时间。\n\n### 1. 处理时间\n\n`Processing Time`(处理时间)是指执行相应操作机器的系统时间(Processing time refers to the system time of the machine that is executing the respective operation.)。\n\n当一个流程序以处理时间来运行时，所有基于时间的操作(如时间窗口)将使用运行算子(`operator`)所在机器的系统时间。例如:一个基于处理时间按每小时进行处理的时间窗口将包括以系统时间为标准在一个小时内到达指定算子的所有的记录(an hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour.)。\n\n处理时间是最简单的一个时间概念，不需要在数据流和机器之间进行协调。它有最好的性能和最低的延迟。然而，在分布式或者异步环境中，处理时间具有不确定性，因为容易受到记录到达系统速度的影响(例如从消息队列到达的记录)，还会受到系统内记录流在不同算子之间的流动速度的影响(speed at which records arrive in the system, and to the speed at which the records flow between operators inside the system)。\n\n### 2. 事件时间\n\n`Event Time`(事件时间)是每个独立事件在它生产设备上产生的时间。在进入`Flink`之前，事件时间通常要嵌入到记录中，并且事件时间也可以从记录中提取出来。一个基于事件时间按每小时进行处理的时间窗口将包含所有的记录，其事件时间都在这一小时之内，不管它们何时到达，以及它们以什么顺序到达。\n\n事件时间即使在乱序事件，延迟事件以及从备份或持久化日志中的重复数据也能获得正确的结果。对于事件时间，时间的进度取决于数据，而不是任何时钟。事件时间程序必须指定如何生成事件时间的`Watermarks`，这是表示事件时间进度的机制。\n\n按事件时间处理往往会导致一定的延迟，因为它要等待延迟事件和无序事件一段时间。因此，事件时间程序通常与处理时间操作相结合使用。\n\n### 3. 摄入时间\n\n`Ingestion Time`(摄入时间)是事件进入`Flink`的时间。在`source operator`中，每个记录将源的当前时间记为时间戳，基于时间的操作(如时间窗口)会使用该时间戳。\n\n摄入时间在概念上处于事件时间和处理时间之间。与处理时间相比，摄入时间的成本稍微更高一些，但是可以提供更可预测的结果。因为摄入时间的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，然而对于处理时间，每个窗口算子可能将记录分配给不同的窗口(基于本地系统时钟以及传输延迟)。\n\n与事件时间相比，摄入时间程序无法处理任何无序事件或延迟事件，但程序不必指定如何生成`watermarks`。\n\n在内部，摄入时间与事件时间非常相似，但事件时间会自动分配时间戳以及自动生成`watermark`(with automatic timestamp assignment and automatic watermark generation)。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4.png?raw=true)\n\n### 4. 选择时间特性\n\n`Flink DataStream`程序的第一部分通常设置基本的时间特性(base time characteristic)。该设置定义数据流源的行为方式(例如，它们是否产生时间戳)，以及窗口操作如`KeyedStream.timeWindow(Time.seconds(30))`应使用哪一类型时间，是事件时间还是处理时间等。\n\n以下示例展示了一个聚合每小时时间窗口内的事件的`Flink`程序。窗口的行为会与时间特性相匹配。\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);\n\n// alternatively:\n// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\nDataStream<MyEvent> stream = env.addSource(new FlinkKafkaConsumer09<MyEvent>(topic, schema, props));\n\nstream\n    .keyBy( (event) -> event.getUser() )\n    .timeWindow(Time.hours(1))\n    .reduce( (a, b) -> a.add(b) )\n    .addSink(...);\n```\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\n\nenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)\n\n// alternatively:\n// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)\n// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))\n\nstream\n    .keyBy( _.getUser )\n    .timeWindow(Time.hours(1))\n    .reduce( (a, b) => a.add(b) )\n    .addSink(...)\n```\n\n备注:\n```\n为了以事件时间运行此示例，程序需要使用定义了事件时间并自动产生watermarks的源，或者程序必须在源之后设置时间戳分配器和watermarks生成器。上述函数描述了如何获取事件时间戳，以及展现事件流的无序程度。\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time\n","slug":"Flink/[Flink]Flink1.4 事件时间与处理时间","published":1,"updated":"2018-01-29T09:36:59.656Z","comments":1,"photos":[],"link":"","_id":"cje58tiq1000oordbpsdlu8xk","content":"<p><code>Flink</code>在数据流中支持几种不同概念的时间。</p>\n<h3 id=\"1-处理时间\"><a href=\"#1-处理时间\" class=\"headerlink\" title=\"1. 处理时间\"></a>1. 处理时间</h3><p><code>Processing Time</code>(处理时间)是指执行相应操作机器的系统时间(Processing time refers to the system time of the machine that is executing the respective operation.)。</p>\n<p>当一个流程序以处理时间来运行时，所有基于时间的操作(如时间窗口)将使用运行算子(<code>operator</code>)所在机器的系统时间。例如:一个基于处理时间按每小时进行处理的时间窗口将包括以系统时间为标准在一个小时内到达指定算子的所有的记录(an hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour.)。</p>\n<p>处理时间是最简单的一个时间概念，不需要在数据流和机器之间进行协调。它有最好的性能和最低的延迟。然而，在分布式或者异步环境中，处理时间具有不确定性，因为容易受到记录到达系统速度的影响(例如从消息队列到达的记录)，还会受到系统内记录流在不同算子之间的流动速度的影响(speed at which records arrive in the system, and to the speed at which the records flow between operators inside the system)。</p>\n<h3 id=\"2-事件时间\"><a href=\"#2-事件时间\" class=\"headerlink\" title=\"2. 事件时间\"></a>2. 事件时间</h3><p><code>Event Time</code>(事件时间)是每个独立事件在它生产设备上产生的时间。在进入<code>Flink</code>之前，事件时间通常要嵌入到记录中，并且事件时间也可以从记录中提取出来。一个基于事件时间按每小时进行处理的时间窗口将包含所有的记录，其事件时间都在这一小时之内，不管它们何时到达，以及它们以什么顺序到达。</p>\n<p>事件时间即使在乱序事件，延迟事件以及从备份或持久化日志中的重复数据也能获得正确的结果。对于事件时间，时间的进度取决于数据，而不是任何时钟。事件时间程序必须指定如何生成事件时间的<code>Watermarks</code>，这是表示事件时间进度的机制。</p>\n<p>按事件时间处理往往会导致一定的延迟，因为它要等待延迟事件和无序事件一段时间。因此，事件时间程序通常与处理时间操作相结合使用。</p>\n<h3 id=\"3-摄入时间\"><a href=\"#3-摄入时间\" class=\"headerlink\" title=\"3. 摄入时间\"></a>3. 摄入时间</h3><p><code>Ingestion Time</code>(摄入时间)是事件进入<code>Flink</code>的时间。在<code>source operator</code>中，每个记录将源的当前时间记为时间戳，基于时间的操作(如时间窗口)会使用该时间戳。</p>\n<p>摄入时间在概念上处于事件时间和处理时间之间。与处理时间相比，摄入时间的成本稍微更高一些，但是可以提供更可预测的结果。因为摄入时间的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，然而对于处理时间，每个窗口算子可能将记录分配给不同的窗口(基于本地系统时钟以及传输延迟)。</p>\n<p>与事件时间相比，摄入时间程序无法处理任何无序事件或延迟事件，但程序不必指定如何生成<code>watermarks</code>。</p>\n<p>在内部，摄入时间与事件时间非常相似，但事件时间会自动分配时间戳以及自动生成<code>watermark</code>(with automatic timestamp assignment and automatic watermark generation)。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-选择时间特性\"><a href=\"#4-选择时间特性\" class=\"headerlink\" title=\"4. 选择时间特性\"></a>4. 选择时间特性</h3><p><code>Flink DataStream</code>程序的第一部分通常设置基本的时间特性(base time characteristic)。该设置定义数据流源的行为方式(例如，它们是否产生时间戳)，以及窗口操作如<code>KeyedStream.timeWindow(Time.seconds(30))</code>应使用哪一类型时间，是事件时间还是处理时间等。</p>\n<p>以下示例展示了一个聚合每小时时间窗口内的事件的<code>Flink</code>程序。窗口的行为会与时间特性相匹配。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// alternatively:</span></span><br><span class=\"line\"><span class=\"comment\">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class=\"line\"><span class=\"comment\">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; stream = env.addSource(<span class=\"keyword\">new</span> FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));</span><br><span class=\"line\"></span><br><span class=\"line\">stream</span><br><span class=\"line\">    .keyBy( (event) -&gt; event.getUser() )</span><br><span class=\"line\">    .timeWindow(Time.hours(<span class=\"number\">1</span>))</span><br><span class=\"line\">    .reduce( (a, b) -&gt; a.add(b) )</span><br><span class=\"line\">    .addSink(...);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)</span><br><span class=\"line\"></span><br><span class=\"line\">// alternatively:</span><br><span class=\"line\">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)</span><br><span class=\"line\">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))</span><br><span class=\"line\"></span><br><span class=\"line\">stream</span><br><span class=\"line\">    .keyBy( _.getUser )</span><br><span class=\"line\">    .timeWindow(Time.hours(1))</span><br><span class=\"line\">    .reduce( (a, b) =&gt; a.add(b) )</span><br><span class=\"line\">    .addSink(...)</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">为了以事件时间运行此示例，程序需要使用定义了事件时间并自动产生watermarks的源，或者程序必须在源之后设置时间戳分配器和watermarks生成器。上述函数描述了如何获取事件时间戳，以及展现事件流的无序程度。</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>Flink</code>在数据流中支持几种不同概念的时间。</p>\n<h3 id=\"1-处理时间\"><a href=\"#1-处理时间\" class=\"headerlink\" title=\"1. 处理时间\"></a>1. 处理时间</h3><p><code>Processing Time</code>(处理时间)是指执行相应操作机器的系统时间(Processing time refers to the system time of the machine that is executing the respective operation.)。</p>\n<p>当一个流程序以处理时间来运行时，所有基于时间的操作(如时间窗口)将使用运行算子(<code>operator</code>)所在机器的系统时间。例如:一个基于处理时间按每小时进行处理的时间窗口将包括以系统时间为标准在一个小时内到达指定算子的所有的记录(an hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour.)。</p>\n<p>处理时间是最简单的一个时间概念，不需要在数据流和机器之间进行协调。它有最好的性能和最低的延迟。然而，在分布式或者异步环境中，处理时间具有不确定性，因为容易受到记录到达系统速度的影响(例如从消息队列到达的记录)，还会受到系统内记录流在不同算子之间的流动速度的影响(speed at which records arrive in the system, and to the speed at which the records flow between operators inside the system)。</p>\n<h3 id=\"2-事件时间\"><a href=\"#2-事件时间\" class=\"headerlink\" title=\"2. 事件时间\"></a>2. 事件时间</h3><p><code>Event Time</code>(事件时间)是每个独立事件在它生产设备上产生的时间。在进入<code>Flink</code>之前，事件时间通常要嵌入到记录中，并且事件时间也可以从记录中提取出来。一个基于事件时间按每小时进行处理的时间窗口将包含所有的记录，其事件时间都在这一小时之内，不管它们何时到达，以及它们以什么顺序到达。</p>\n<p>事件时间即使在乱序事件，延迟事件以及从备份或持久化日志中的重复数据也能获得正确的结果。对于事件时间，时间的进度取决于数据，而不是任何时钟。事件时间程序必须指定如何生成事件时间的<code>Watermarks</code>，这是表示事件时间进度的机制。</p>\n<p>按事件时间处理往往会导致一定的延迟，因为它要等待延迟事件和无序事件一段时间。因此，事件时间程序通常与处理时间操作相结合使用。</p>\n<h3 id=\"3-摄入时间\"><a href=\"#3-摄入时间\" class=\"headerlink\" title=\"3. 摄入时间\"></a>3. 摄入时间</h3><p><code>Ingestion Time</code>(摄入时间)是事件进入<code>Flink</code>的时间。在<code>source operator</code>中，每个记录将源的当前时间记为时间戳，基于时间的操作(如时间窗口)会使用该时间戳。</p>\n<p>摄入时间在概念上处于事件时间和处理时间之间。与处理时间相比，摄入时间的成本稍微更高一些，但是可以提供更可预测的结果。因为摄入时间的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，然而对于处理时间，每个窗口算子可能将记录分配给不同的窗口(基于本地系统时钟以及传输延迟)。</p>\n<p>与事件时间相比，摄入时间程序无法处理任何无序事件或延迟事件，但程序不必指定如何生成<code>watermarks</code>。</p>\n<p>在内部，摄入时间与事件时间非常相似，但事件时间会自动分配时间戳以及自动生成<code>watermark</code>(with automatic timestamp assignment and automatic watermark generation)。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-选择时间特性\"><a href=\"#4-选择时间特性\" class=\"headerlink\" title=\"4. 选择时间特性\"></a>4. 选择时间特性</h3><p><code>Flink DataStream</code>程序的第一部分通常设置基本的时间特性(base time characteristic)。该设置定义数据流源的行为方式(例如，它们是否产生时间戳)，以及窗口操作如<code>KeyedStream.timeWindow(Time.seconds(30))</code>应使用哪一类型时间，是事件时间还是处理时间等。</p>\n<p>以下示例展示了一个聚合每小时时间窗口内的事件的<code>Flink</code>程序。窗口的行为会与时间特性相匹配。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// alternatively:</span></span><br><span class=\"line\"><span class=\"comment\">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class=\"line\"><span class=\"comment\">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; stream = env.addSource(<span class=\"keyword\">new</span> FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));</span><br><span class=\"line\"></span><br><span class=\"line\">stream</span><br><span class=\"line\">    .keyBy( (event) -&gt; event.getUser() )</span><br><span class=\"line\">    .timeWindow(Time.hours(<span class=\"number\">1</span>))</span><br><span class=\"line\">    .reduce( (a, b) -&gt; a.add(b) )</span><br><span class=\"line\">    .addSink(...);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)</span><br><span class=\"line\"></span><br><span class=\"line\">// alternatively:</span><br><span class=\"line\">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)</span><br><span class=\"line\">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">val stream: DataStream[MyEvent] = env.addSource(new FlinkKafkaConsumer09[MyEvent](topic, schema, props))</span><br><span class=\"line\"></span><br><span class=\"line\">stream</span><br><span class=\"line\">    .keyBy( _.getUser )</span><br><span class=\"line\">    .timeWindow(Time.hours(1))</span><br><span class=\"line\">    .reduce( (a, b) =&gt; a.add(b) )</span><br><span class=\"line\">    .addSink(...)</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">为了以事件时间运行此示例，程序需要使用定义了事件时间并自动产生watermarks的源，或者程序必须在源之后设置时间戳分配器和watermarks生成器。上述函数描述了如何获取事件时间戳，以及展现事件流的无序程度。</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html#event-time--processing-time--ingestion-time</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 内置的时间戳提取器和Watermark生成器","date":"2018-01-16T08:30:17.000Z","_content":"\n如[Flink1.4 生成时间戳与Watermarks](http://smartsi.club/2018/01/15/Flink/[Flink]Flink1.4%20%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks/)所介绍的，`Flink`提供了一个抽象类，允许程序员可以分配自己的时间戳并发送`Watermark`。更具体地说，可以通过`AssignerWithPeriodicWatermarks`或`AssignerWithPunctuatedWatermarks`接口来实现，具体实现取决于用户具体情况。第一个接口将周期性的发送`Watermark`，第二个则基于传入记录的某些属性发送`Watermark`，例如，当在流中遇到特殊元素时。\n\n为了进一步缓解这些任务的编程工作，`Flink`带有一些内置的时间戳分配器。除了开箱即用的功能外，它们的实现也可以作为自定义实现的一个例子。\n\n### 1. 递增时间戳分配器\n\n周期性生成`Watermark`最简单的例子是给定数据源任务中的时间戳会递增顺序出现。在这种情况下，由于没有时间戳比当前时间戳还早到达的，所以当前时间戳可以始终充当`Watermark`。\n\n请注意，每个并行数据源任务的时间戳必须是升序的。例如，如果在特定设置中，一个并行数据源实例读取一个`Kafka`分区，那么只需要确保在每个`Kafka`分区内时间戳是升序的即可。每当并行数据流被`shuffle`，`union`，连接或合并时，`Flink`的`Watermark`合并机制能够产生正确的`watermarks`。\n\nJava版本:\n```java\nDataStream<MyEvent> stream = ...\n\nDataStream<MyEvent> withTimestampsAndWatermarks =\n    stream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor<MyEvent>() {\n\n        @Override\n        public long extractAscendingTimestamp(MyEvent element) {\n            return element.getCreationTime();\n        }\n});\n```\n\nScala版本:\n```\nval stream: DataStream[MyEvent] = ...\n\nval withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )\n```\n\n### 2. 允许固定数量延迟的分配器\n\n周期性生成`Watermark`的另一个例子是当`Watermark`落后于数据流中看到的最大时间戳(事件时间)一固定数量时间(a fixed amount of time)。这种情况涵盖了事先知道流中可能遇到的最大延迟的场景，例如，当创建一个测试用的自定义数据源时，其上每个元素的时间戳分布在一个固定时间段内。对于这些情况，`Flink`提供了`BoundedOutOfOrdernessTimestampExtractor`，带有一个`maxOutOfOrderness`参数，即在计算给定窗口最终结果一个元素在被忽略之前允许延迟的最大时间。延迟对应于`t-t_w`的结果，其中`t`是元素的(事件时间)时间戳，`t_w`是前一个`Watermark`时间戳。如果延迟大于0，则该元素被认为是迟到的，并且在计算其相应窗口的作业结果时默认为忽略该元素。\n\nJava版本:\n```java\nDataStream<MyEvent> stream = ...\n\nDataStream<MyEvent> withTimestampsAndWatermarks =\n    stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<MyEvent>(Time.seconds(10)) {\n\n        @Override\n        public long extractTimestamp(MyEvent element) {\n            return element.getCreationTime();\n        }\n});\n```\n\nScala版本:\n```\nval stream: DataStream[MyEvent] = ...\n\nval withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))\n```\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html\n","source":"_posts/Flink/[Flink]Flink1.4 内置时间戳提取器和Watermark生成器.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 内置的时间戳提取器和Watermark生成器\ndate: 2018-01-16 16:30:17\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n如[Flink1.4 生成时间戳与Watermarks](http://smartsi.club/2018/01/15/Flink/[Flink]Flink1.4%20%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks/)所介绍的，`Flink`提供了一个抽象类，允许程序员可以分配自己的时间戳并发送`Watermark`。更具体地说，可以通过`AssignerWithPeriodicWatermarks`或`AssignerWithPunctuatedWatermarks`接口来实现，具体实现取决于用户具体情况。第一个接口将周期性的发送`Watermark`，第二个则基于传入记录的某些属性发送`Watermark`，例如，当在流中遇到特殊元素时。\n\n为了进一步缓解这些任务的编程工作，`Flink`带有一些内置的时间戳分配器。除了开箱即用的功能外，它们的实现也可以作为自定义实现的一个例子。\n\n### 1. 递增时间戳分配器\n\n周期性生成`Watermark`最简单的例子是给定数据源任务中的时间戳会递增顺序出现。在这种情况下，由于没有时间戳比当前时间戳还早到达的，所以当前时间戳可以始终充当`Watermark`。\n\n请注意，每个并行数据源任务的时间戳必须是升序的。例如，如果在特定设置中，一个并行数据源实例读取一个`Kafka`分区，那么只需要确保在每个`Kafka`分区内时间戳是升序的即可。每当并行数据流被`shuffle`，`union`，连接或合并时，`Flink`的`Watermark`合并机制能够产生正确的`watermarks`。\n\nJava版本:\n```java\nDataStream<MyEvent> stream = ...\n\nDataStream<MyEvent> withTimestampsAndWatermarks =\n    stream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor<MyEvent>() {\n\n        @Override\n        public long extractAscendingTimestamp(MyEvent element) {\n            return element.getCreationTime();\n        }\n});\n```\n\nScala版本:\n```\nval stream: DataStream[MyEvent] = ...\n\nval withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )\n```\n\n### 2. 允许固定数量延迟的分配器\n\n周期性生成`Watermark`的另一个例子是当`Watermark`落后于数据流中看到的最大时间戳(事件时间)一固定数量时间(a fixed amount of time)。这种情况涵盖了事先知道流中可能遇到的最大延迟的场景，例如，当创建一个测试用的自定义数据源时，其上每个元素的时间戳分布在一个固定时间段内。对于这些情况，`Flink`提供了`BoundedOutOfOrdernessTimestampExtractor`，带有一个`maxOutOfOrderness`参数，即在计算给定窗口最终结果一个元素在被忽略之前允许延迟的最大时间。延迟对应于`t-t_w`的结果，其中`t`是元素的(事件时间)时间戳，`t_w`是前一个`Watermark`时间戳。如果延迟大于0，则该元素被认为是迟到的，并且在计算其相应窗口的作业结果时默认为忽略该元素。\n\nJava版本:\n```java\nDataStream<MyEvent> stream = ...\n\nDataStream<MyEvent> withTimestampsAndWatermarks =\n    stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<MyEvent>(Time.seconds(10)) {\n\n        @Override\n        public long extractTimestamp(MyEvent element) {\n            return element.getCreationTime();\n        }\n});\n```\n\nScala版本:\n```\nval stream: DataStream[MyEvent] = ...\n\nval withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))\n```\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html\n","slug":"Flink/[Flink]Flink1.4 内置时间戳提取器和Watermark生成器","published":1,"updated":"2018-01-29T09:36:59.656Z","comments":1,"photos":[],"link":"","_id":"cje58tiq6000sordbptk4ztaj","content":"<p>如<a href=\"http://smartsi.club/2018/01/15/Flink/[Flink]Flink1.4%20%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks/\" target=\"_blank\" rel=\"noopener\">Flink1.4 生成时间戳与Watermarks</a>所介绍的，<code>Flink</code>提供了一个抽象类，允许程序员可以分配自己的时间戳并发送<code>Watermark</code>。更具体地说，可以通过<code>AssignerWithPeriodicWatermarks</code>或<code>AssignerWithPunctuatedWatermarks</code>接口来实现，具体实现取决于用户具体情况。第一个接口将周期性的发送<code>Watermark</code>，第二个则基于传入记录的某些属性发送<code>Watermark</code>，例如，当在流中遇到特殊元素时。</p>\n<p>为了进一步缓解这些任务的编程工作，<code>Flink</code>带有一些内置的时间戳分配器。除了开箱即用的功能外，它们的实现也可以作为自定义实现的一个例子。</p>\n<h3 id=\"1-递增时间戳分配器\"><a href=\"#1-递增时间戳分配器\" class=\"headerlink\" title=\"1. 递增时间戳分配器\"></a>1. 递增时间戳分配器</h3><p>周期性生成<code>Watermark</code>最简单的例子是给定数据源任务中的时间戳会递增顺序出现。在这种情况下，由于没有时间戳比当前时间戳还早到达的，所以当前时间戳可以始终充当<code>Watermark</code>。</p>\n<p>请注意，每个并行数据源任务的时间戳必须是升序的。例如，如果在特定设置中，一个并行数据源实例读取一个<code>Kafka</code>分区，那么只需要确保在每个<code>Kafka</code>分区内时间戳是升序的即可。每当并行数据流被<code>shuffle</code>，<code>union</code>，连接或合并时，<code>Flink</code>的<code>Watermark</code>合并机制能够产生正确的<code>watermarks</code>。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;MyEvent&gt; stream = ...</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks =</span><br><span class=\"line\">    stream.assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> AscendingTimestampExtractor&lt;MyEvent&gt;() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractAscendingTimestamp</span><span class=\"params\">(MyEvent element)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyEvent] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-允许固定数量延迟的分配器\"><a href=\"#2-允许固定数量延迟的分配器\" class=\"headerlink\" title=\"2. 允许固定数量延迟的分配器\"></a>2. 允许固定数量延迟的分配器</h3><p>周期性生成<code>Watermark</code>的另一个例子是当<code>Watermark</code>落后于数据流中看到的最大时间戳(事件时间)一固定数量时间(a fixed amount of time)。这种情况涵盖了事先知道流中可能遇到的最大延迟的场景，例如，当创建一个测试用的自定义数据源时，其上每个元素的时间戳分布在一个固定时间段内。对于这些情况，<code>Flink</code>提供了<code>BoundedOutOfOrdernessTimestampExtractor</code>，带有一个<code>maxOutOfOrderness</code>参数，即在计算给定窗口最终结果一个元素在被忽略之前允许延迟的最大时间。延迟对应于<code>t-t_w</code>的结果，其中<code>t</code>是元素的(事件时间)时间戳，<code>t_w</code>是前一个<code>Watermark</code>时间戳。如果延迟大于0，则该元素被认为是迟到的，并且在计算其相应窗口的作业结果时默认为忽略该元素。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;MyEvent&gt; stream = ...</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks =</span><br><span class=\"line\">    stream.assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;MyEvent&gt;(Time.seconds(<span class=\"number\">10</span>)) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyEvent] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>如<a href=\"http://smartsi.club/2018/01/15/Flink/[Flink]Flink1.4%20%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks/\" target=\"_blank\" rel=\"noopener\">Flink1.4 生成时间戳与Watermarks</a>所介绍的，<code>Flink</code>提供了一个抽象类，允许程序员可以分配自己的时间戳并发送<code>Watermark</code>。更具体地说，可以通过<code>AssignerWithPeriodicWatermarks</code>或<code>AssignerWithPunctuatedWatermarks</code>接口来实现，具体实现取决于用户具体情况。第一个接口将周期性的发送<code>Watermark</code>，第二个则基于传入记录的某些属性发送<code>Watermark</code>，例如，当在流中遇到特殊元素时。</p>\n<p>为了进一步缓解这些任务的编程工作，<code>Flink</code>带有一些内置的时间戳分配器。除了开箱即用的功能外，它们的实现也可以作为自定义实现的一个例子。</p>\n<h3 id=\"1-递增时间戳分配器\"><a href=\"#1-递增时间戳分配器\" class=\"headerlink\" title=\"1. 递增时间戳分配器\"></a>1. 递增时间戳分配器</h3><p>周期性生成<code>Watermark</code>最简单的例子是给定数据源任务中的时间戳会递增顺序出现。在这种情况下，由于没有时间戳比当前时间戳还早到达的，所以当前时间戳可以始终充当<code>Watermark</code>。</p>\n<p>请注意，每个并行数据源任务的时间戳必须是升序的。例如，如果在特定设置中，一个并行数据源实例读取一个<code>Kafka</code>分区，那么只需要确保在每个<code>Kafka</code>分区内时间戳是升序的即可。每当并行数据流被<code>shuffle</code>，<code>union</code>，连接或合并时，<code>Flink</code>的<code>Watermark</code>合并机制能够产生正确的<code>watermarks</code>。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;MyEvent&gt; stream = ...</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks =</span><br><span class=\"line\">    stream.assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> AscendingTimestampExtractor&lt;MyEvent&gt;() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractAscendingTimestamp</span><span class=\"params\">(MyEvent element)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyEvent] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val withTimestampsAndWatermarks = stream.assignAscendingTimestamps( _.getCreationTime )</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-允许固定数量延迟的分配器\"><a href=\"#2-允许固定数量延迟的分配器\" class=\"headerlink\" title=\"2. 允许固定数量延迟的分配器\"></a>2. 允许固定数量延迟的分配器</h3><p>周期性生成<code>Watermark</code>的另一个例子是当<code>Watermark</code>落后于数据流中看到的最大时间戳(事件时间)一固定数量时间(a fixed amount of time)。这种情况涵盖了事先知道流中可能遇到的最大延迟的场景，例如，当创建一个测试用的自定义数据源时，其上每个元素的时间戳分布在一个固定时间段内。对于这些情况，<code>Flink</code>提供了<code>BoundedOutOfOrdernessTimestampExtractor</code>，带有一个<code>maxOutOfOrderness</code>参数，即在计算给定窗口最终结果一个元素在被忽略之前允许延迟的最大时间。延迟对应于<code>t-t_w</code>的结果，其中<code>t</code>是元素的(事件时间)时间戳，<code>t_w</code>是前一个<code>Watermark</code>时间戳。如果延迟大于0，则该元素被认为是迟到的，并且在计算其相应窗口的作业结果时默认为忽略该元素。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;MyEvent&gt; stream = ...</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks =</span><br><span class=\"line\">    stream.assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;MyEvent&gt;(Time.seconds(<span class=\"number\">10</span>)) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyEvent] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[MyEvent](Time.seconds(10))( _.getCreationTime ))</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 使用状态","date":"2018-01-16T12:32:17.000Z","_content":"\n### 1. Keyed State 与 Operator State\n\n`Flink`有两种基本的状态：`Keyed State`和`Operator State`。\n\n#### 1.1 Keyed State\n\n`Keyed State`总是与`key`相关，只能在`KeyedStream`的函数和运算符中使用。\n\n备注:\n```\nKeyedStream继承DataStream，表示根据指定的key进行分组的数据流。使用DataStream提供的KeySelector根据key对其上的算子State进行分区。\nDataStream支持的典型操作也可以在KeyedStream上进行，除了诸如shuffle，forward和keyBy之类的分区方法之外。\n\n一个KeyedStream可以通过调用DataStream.keyBy()来获得。而在KeyedStream上进行任何transformation都将转变回DataStream。\n```\n\n你可以将 `Keyed State` 视为已经分区或分片的`Operator State`，每个 `key` 对应一个状态分区。每个`Keyed State`在逻辑上只对应一个 `<并行算子实例，key>`，并且由于每个 `key` \"只属于\" 一个`Keyed Operator`的一个并行实例，我们可以简单地认为成 `<operator，key>`。\n\n`Keyed State` 被进一步组织成所谓的 `Key Group`。`Key Group` 是 `Flink` 可以重新分配 `Keyed State` 的最小单位；`Key Group`的数量与最大并行度一样多。在执行期间，`Keyed Operator`的每个并行实例都与一个或多个`Key Group`的`key`一起工作。\n\n#### 1.2 Operator State\n\n使用`Operator State` (或非`Keyed State`)，每个算子状态都绑定到一个并行算子实例。`Kafka Connector` 是在`Flink`中使用算子状态的一个很好的例子。`Kafka`消费者的每个并行实例都要维护一个`topic`分区和偏移量的map作为其`Operator State`。\n\n在并行度发生变化时，`Operator State`接口支持在并行算子实例之间进行重新分配状态。可以有不同的方案来处理这个重新分配。\n\n### 2. Raw State 与 Managed State\n\n`Keyed State`和`Operator State`以两种形式存在：托管状态`Managed State`和原始状态`Raw State`。\n\n`Managed State`由`Flink`运行时控制的数据结构表示，如内部哈希表或`RocksDB`。例如`ValueState`，`ListState`等。`Flink`的运行时对状态进行编码并将它们写入检查点。\n\n`Raw State`是指算子保持在它们自己数据结构中的状态。当检查点时，他们只写入一个字节序列到检查点。`Flink`对状态的数据结构一无所知，只能看到原始字节。\n\n所有数据流函数都可以使用`Managed State`，但`Raw State`接口只能在实现算子时使用。建议使用`Managed State`（而不是`Raw State`），因为在`Managed State`下，`Flin`k可以在并行度发生变化时自动重新分配状态，并且还可以更好地进行内存管理。\n\n备注:\n```\n如果你的Managed State需要自定义序列化逻辑，请参阅相应的指南以确保将来的兼容性。Flink的默认序列化器不需要特殊处理。\n```\n\n### 3. Managed Keyed State\n\n`Managed Keyed State`接口提供了对不同类型状态的访问，这些状态的作用域为当前输入元素的`key`。这意味着这种类型的状态只能用于`KeyedStream`，可以通过`stream.keyBy（...）`创建。\n\n现在，我们先看看可用状态的不同类型，然后我们将看到如何在一个程序中使用它们。可用状态是：\n- `ValueState <T>`：保存了一个可以更新和检索的值（如上所述，作用域为输入元素的`key`，所以操作看到的每个`key`可能有一个值）。该值可以使用`update（T）`来设置，使用`T value（）`来检索。\n- `ListState <T>`：保存了一个元素列表。可以追加元素并检索当前存储的所有元素的`Iterable`。使用`add（T）`添加元素，可以使用`Iterable <T> get（）`来检索`Iterable`。\n- `ReducingState <T>`：保存一个单一的值，表示添加到状态所有值的聚合。接口与`ListState`相同，但使用`add（T）`添加的元素，使用指定的`ReduceFunction`转换为聚合。\n- `AggregatingState <IN，OUT>`：保存一个单一的值，表示添加到状态所有值的聚合。与`ReducingState`不同，聚合后的类型可能与添加到状态的元素类型不同。接口与`ListState`相同，但使用`add（IN）`添加到状态的元素使用指定的`AggregateFunction`进行聚合。\n- `FoldingState <T，ACC>`：保存一个单一的值，表示添加到状态所有值的聚合。与`ReducingState`不同，聚合后类型可能与添加到状态的元素类型不同。接口与`ListState`相同，但使用`add（T）`添加到状态的元素使用指定的`FoldFunction`折叠成聚合。\n- MapState <UK，UV>：保存了一个映射列表。可以将键值对放入状态，并检索当前存储的所有映射的`Iterable`。使用`put（UK，UV）`或`putAll（Map <UK，UV>）`添加映射。与用户`key`相关的值可以使用`get（UK）`来检索。映射，键和值的迭代视图可分别使用`entries（）`，`keys（）`和`values（）`来检索。\n\n所有类型的状态都有一个`clear（）`方法，它清除了当前活跃`key`的状态，即输入元素的`key`。\n\n备注:\n```\nFoldingState和FoldingStateDescriptor已经在Flink 1.4中被弃用，将来会被彻底删除。请改用AggregatingState和AggregatingStateDescriptor。\n```\n\n请记住，这些状态对象仅能用于状态接口，这一点很重要。状态没有必要存储在内存中，也可以驻留在磁盘或其他地方。第二件要记住的是，你从状态获得的值取决于输入元素的`key`。因此，如果所涉及的`key`不同，那你在用户函数调用中获得的值可能与另一个调用中的值不同。\n\n为了得到一个状态句柄，你必须创建一个`StateDescriptor`。它包含了状态的名字（我们将在后面看到，你可以创建多个状态，必须有唯一的名称，以便引用它们），状态值的类型，以及用户自定义函数，如`ReduceFunction`。根据要检索的状态类型，你可以创建一个`ValueStateDescriptor`，`ListStateDescriptor`，`ReducingStateDescriptor`，`FoldingStateDescriptor`或`MapStateDescriptor`。\n\n使用`RuntimeContext`来访问状态，所以只能在`Rich`函数中使用。请参阅[这里](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#rich-functions)了解有关信息，我们会很快看到一个例子。 在`RichFunction`中可用的`RuntimeContext`具有下面访问状态的方法：\n- ValueState<T> getState(ValueStateDescriptor<T>)\n- ReducingState<T> getReducingState(ReducingStateDescriptor<T>)\n- ListState<T> getListState(ListStateDescriptor<T>)\n- AggregatingState<IN, OUT> getAggregatingState(AggregatingState<IN, OUT>)\n- FoldingState<T, ACC> getFoldingState(FoldingStateDescriptor<T, ACC>)\n- MapState<UK, UV> getMapState(MapStateDescriptor<UK, UV>)\n\n下面是`FlatMapFunction`的一个例子：\n\nJava版本:\n```java\npublic class CountWindowAverage extends RichFlatMapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>> {\n\n    /**\n     * The ValueState handle. The first field is the count, the second field a running sum.\n     */\n    private transient ValueState<Tuple2<Long, Long>> sum;\n\n    @Override\n    public void flatMap(Tuple2<Long, Long> input, Collector<Tuple2<Long, Long>> out) throws Exception {\n\n        // access the state value\n        Tuple2<Long, Long> currentSum = sum.value();\n\n        // update the count 个数\n        currentSum.f0 += 1;\n\n        // add the second field of the input value 总和\n        currentSum.f1 += input.f1;\n\n        // update the state\n        sum.update(currentSum);\n\n        // if the count reaches 2, emit the average and clear the state\n        if (currentSum.f0 >= 2) {\n            out.collect(new Tuple2<>(input.f0, currentSum.f1 / currentSum.f0));\n            sum.clear();\n        }\n    }\n\n    @Override\n    public void open(Configuration config) {\n        ValueStateDescriptor<Tuple2<Long, Long>> descriptor =\n                new ValueStateDescriptor<>(\n                        \"average\", // the state name\n                        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}), // type information\n                        Tuple2.of(0L, 0L)); // default value of the state, if nothing was set\n        sum = getRuntimeContext().getState(descriptor);\n    }\n}\n\n// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)\nenv.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L))\n        .keyBy(0)\n        .flatMap(new CountWindowAverage())\n        .print();\n\n// the printed output will be (1,4) and (1,5)\n```\n\nScala版本:\n```\nclass CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] {\n\n  private var sum: ValueState[(Long, Long)] = _\n\n  override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = {\n\n    // access the state value\n    val tmpCurrentSum = sum.value\n\n    // If it hasn't been used before, it will be null\n    val currentSum = if (tmpCurrentSum != null) {\n      tmpCurrentSum\n    } else {\n      (0L, 0L)\n    }\n\n    // update the count\n    val newSum = (currentSum._1 + 1, currentSum._2 + input._2)\n\n    // update the state\n    sum.update(newSum)\n\n    // if the count reaches 2, emit the average and clear the state\n    if (newSum._1 >= 2) {\n      out.collect((input._1, newSum._2 / newSum._1))\n      sum.clear()\n    }\n  }\n\n  override def open(parameters: Configuration): Unit = {\n    sum = getRuntimeContext.getState(\n      new ValueStateDescriptor[(Long, Long)](\"average\", createTypeInformation[(Long, Long)])\n    )\n  }\n}\n\n\nobject ExampleCountWindowAverage extends App {\n  val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n  env.fromCollection(List(\n    (1L, 3L),\n    (1L, 5L),\n    (1L, 7L),\n    (1L, 4L),\n    (1L, 2L)\n  )).keyBy(_._1)\n    .flatMap(new CountWindowAverage())\n    .print()\n  // the printed output will be (1,4) and (1,5)\n\n  env.execute(\"ExampleManagedState\")\n}\n```\n\n这个例子实现了一个穷人的计数窗口。我们通过第一个字段键入元组（在这个例子中都有相同的`key`为`1`）。该函数将计数和总和存储在`ValueState`中。一旦计数达到2，就输出平均值并清除状态，以便我们从0开始。注意，如果我们元组第一个字段具有不同值，那将为每个不同的输入`key`保持不同的状态值。\n\n#### 3.1 Scala DataStream API中的状态\n\n除了上面介绍的接口之外，Scala API还具有在`KeyedStream`上使用单个`ValueState`的有状态`map（）`或`flatMap（）`函数的快捷方式。用户函数可以在`Option`获取`ValueState`的当前值，并且必须返回将用于更新状态的更新值。\n```\nval stream: DataStream[(String, Int)] = ...\n\nval counts: DataStream[(String, Int)] = stream\n  .keyBy(_._1)\n  .mapWithState((in: (String, Int), count: Option[Int]) =>\n    count match {\n      case Some(c) => ( (in._1, c), Some(c + in._2) )\n      case None => ( (in._1, 0), Some(in._2) )\n    })\n```\n\n### 4. Managed Operator State\n\n要使用`Managed Operator State`，有状态函数可以实现更通用的`CheckpointedFunction`接口或`ListCheckpointed <T extends Serializable>`接口。\n\n#### 4.1 CheckpointedFunction\n\n`CheckpointedFunction`接口提供了使用不同的重分配方案对非`Ked State`的访问。它需要实现一下两种方法：\n```\nvoid snapshotState(FunctionSnapshotContext context) throws Exception;\n\nvoid initializeState(FunctionInitializationContext context) throws Exception;\n```\n\n每当执行检查点时，将调用`snapshotState（）`。每当用户自定义函数被初始化时，对应的`initializeState（）`都被调用，或当函数被初始化时，或者当函数实际上从早期的检查点恢复时被调用(The counterpart, initializeState(), is called every time the user-defined function is initialized, be that when the function is first initialized or be that when the function is actually recovering from an earlier checkpoint. )。鉴于此，`initializeState（）`不仅是初始化不同类型的状态的地方，而且还包括状态恢复逻辑的位置。\n\n目前支持列表式的`Managed Operator State`。状态应该是一个可序列化的对象列表，相互间彼此独立，因此可以在扩展时重新分配。换句话说，这些对象可以在非`Keyed State`中重新分配比较细的粒度。根据状态访问方法，定义了以下重新分配方案：\n- 均分再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，列表被平分为与并行算子一样多的子列表。每个算子都可以得到一个可以为空或者包含一个或多个元素的子列表。例如，如果并行度为`1`，算子的检查点状态包含元素`element1`和`element2`，将并行度增加到`2`时，`element1`在算子实例0上运行，而`element2`将转至算子实例1。\n- 合并再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，每个算子都可以获得完整的状态元素列表。\n\n下面是一个有状态的`SinkFunction`的例子，它使用`CheckpointedFunction`在将元素输出到外部之前进行缓冲元素。它演示了基本的均分再分配列表状态：\n\nJava版本:\n```java\npublic class BufferingSink\n        implements SinkFunction<Tuple2<String, Integer>>,\n                   CheckpointedFunction,\n                   CheckpointedRestoring<ArrayList<Tuple2<String, Integer>>> {\n\n    private final int threshold;\n\n    private transient ListState<Tuple2<String, Integer>> checkpointedState;\n\n    private List<Tuple2<String, Integer>> bufferedElements;\n\n    public BufferingSink(int threshold) {\n        this.threshold = threshold;\n        this.bufferedElements = new ArrayList<>();\n    }\n\n    @Override\n    public void invoke(Tuple2<String, Integer> value) throws Exception {\n        bufferedElements.add(value);\n        if (bufferedElements.size() == threshold) {\n            for (Tuple2<String, Integer> element: bufferedElements) {\n                // send it to the sink\n            }\n            bufferedElements.clear();\n        }\n    }\n\n    @Override\n    public void snapshotState(FunctionSnapshotContext context) throws Exception {\n        checkpointedState.clear();\n        for (Tuple2<String, Integer> element : bufferedElements) {\n            checkpointedState.add(element);\n        }\n    }\n\n    @Override\n    public void initializeState(FunctionInitializationContext context) throws Exception {\n        ListStateDescriptor<Tuple2<String, Integer>> descriptor =\n            new ListStateDescriptor<>(\n                \"buffered-elements\",\n                TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}));\n\n        checkpointedState = context.getOperatorStateStore().getListState(descriptor);\n\n        if (context.isRestored()) {\n            for (Tuple2<String, Integer> element : checkpointedState.get()) {\n                bufferedElements.add(element);\n            }\n        }\n    }\n\n    @Override\n    public void restoreState(ArrayList<Tuple2<String, Integer>> state) throws Exception {\n        // this is from the CheckpointedRestoring interface.\n        this.bufferedElements.addAll(state);\n    }\n}\n```\n\nScala版本:\n```\nclass BufferingSink(threshold: Int = 0)\n  extends SinkFunction[(String, Int)]\n    with CheckpointedFunction\n    with CheckpointedRestoring[List[(String, Int)]] {\n\n  @transient\n  private var checkpointedState: ListState[(String, Int)] = _\n\n  private val bufferedElements = ListBuffer[(String, Int)]()\n\n  override def invoke(value: (String, Int)): Unit = {\n    bufferedElements += value\n    if (bufferedElements.size == threshold) {\n      for (element <- bufferedElements) {\n        // send it to the sink\n      }\n      bufferedElements.clear()\n    }\n  }\n\n  override def snapshotState(context: FunctionSnapshotContext): Unit = {\n    checkpointedState.clear()\n    for (element <- bufferedElements) {\n      checkpointedState.add(element)\n    }\n  }\n\n  override def initializeState(context: FunctionInitializationContext): Unit = {\n    val descriptor = new ListStateDescriptor[(String, Int)](\n      \"buffered-elements\",\n      TypeInformation.of(new TypeHint[(String, Int)]() {})\n    )\n\n    checkpointedState = context.getOperatorStateStore.getListState(descriptor)\n\n    if(context.isRestored) {\n      for(element <- checkpointedState.get()) {\n        bufferedElements += element\n      }\n    }\n  }\n\n  override def restoreState(state: List[(String, Int)]): Unit = {\n    bufferedElements ++= state\n  }\n}\n```\n\n`initializeState`方法以`FunctionInitializationContext`为参数。这用来初始化非`keyed state`\"容器\"。这是一个`ListState`类型的容器，非`keyed state`对象将在检查点时存储。\n\n注意一下状态是如何被初始化，类似于`keyed state`状态，使用包含状态名称和状态值类型相关信息的`StateDescriptor`：\n\nJava版本:\n```Java\nListStateDescriptor<Tuple2<String, Integer>> descriptor =\n    new ListStateDescriptor<>(\n        \"buffered-elements\",\n        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}));\n\ncheckpointedState = context.getOperatorStateStore().getListState(descriptor);\n```\n\nScala版本:\n```\nval descriptor = new ListStateDescriptor[(String, Long)](\n    \"buffered-elements\",\n    TypeInformation.of(new TypeHint[(String, Long)]() {})\n)\n\ncheckpointedState = context.getOperatorStateStore.getListState(descriptor)\n```\n\n状态访问方法的命名约定包含其重新分配模式及其状态结构。 例如，要使用带有联合重新分配方案的列表状态进行恢复，请使用`getUnionListState（descriptor）`访问状态。如果方法名称不包含重新分配模式，例如 `getListState（descriptor）`，这表示使用基本的均分重分配方案。\n\n在初始化容器之后，我们使用上下文的`isRestored（）`方法来检查失败后是否正在恢复。如果是，即我们正在恢复，将会应用恢复逻辑。\n\n如修改后的`BufferingSink`的代码所示，在状态初始化期间恢复的这个`ListState`被保存在类变量中，以备将来在`snapshotState（）`中使用。 在那里`ListState`清除了前一个检查点包含的所有对象，然后用我们想要进行检查点的新对象填充。\n\n`Keyed State`也可以在`initializeState（）`方法中初始化。这可以使用提供的`FunctionInitializationContext`完成。\n\n#### 4.2 ListCheckpointed\n\n`ListCheckpointed`接口是`CheckpointedFunction`进行限制的一种变体，它只支持在恢复时使用均分再分配方案的列表样式状态。还需要实现以下两种方法：\n\n```\nList<T> snapshotState(long checkpointId, long timestamp) throws Exception;\n\nvoid restoreState(List<T> state) throws Exception;\n```\n\n`snapshotState()`方法应该返回一个对象列表来进行checkpoint，而`restoreState()`方法在恢复时必须处理这样一个列表。如果状态是不可重分区的，则可以在`snapshotState()`中返回一个`Collections.singletonList(MY_STATE)`。\n\n#### 4.2.1 Stateful Source Functions\n\n与其他算子相比，有状态的数据源需要得到更多的关注。为了能更新状态以及输出集合的原子性（在失败/恢复时需要一次性语义），用户需要从数据源的上下文中获取锁。\n\nJava版本:\n```java\npublic static class CounterSource\n        extends RichParallelSourceFunction<Long>\n        implements ListCheckpointed<Long> {\n\n    /**  current offset for exactly once semantics */\n    private Long offset;\n\n    /** flag for job cancellation */\n    private volatile boolean isRunning = true;\n\n    @Override\n    public void run(SourceContext<Long> ctx) {\n        final Object lock = ctx.getCheckpointLock();\n\n        while (isRunning) {\n            // output and state update are atomic\n            synchronized (lock) {\n                ctx.collect(offset);\n                offset += 1;\n            }\n        }\n    }\n\n    @Override\n    public void cancel() {\n        isRunning = false;\n    }\n\n    @Override\n    public List<Long> snapshotState(long checkpointId, long checkpointTimestamp) {\n        return Collections.singletonList(offset);\n    }\n\n    @Override\n    public void restoreState(List<Long> state) {\n        for (Long s : state)\n            offset = s;\n    }\n}\n```\n\nScala版本:\n```\nclass CounterSource\n       extends RichParallelSourceFunction[Long]\n       with ListCheckpointed[Long] {\n\n  @volatile\n  private var isRunning = true\n\n  private var offset = 0L\n\n  override def run(ctx: SourceFunction.SourceContext[Long]): Unit = {\n    val lock = ctx.getCheckpointLock\n\n    while (isRunning) {\n      // output and state update are atomic\n      lock.synchronized({\n        ctx.collect(offset)\n\n        offset += 1\n      })\n    }\n  }\n\n  override def cancel(): Unit = isRunning = false\n\n  override def restoreState(state: util.List[Long]): Unit =\n    for (s <- state) {\n      offset = s\n    }\n\n  override def snapshotState(checkpointId: Long, timestamp: Long): util.List[Long] =\n    Collections.singletonList(offset)\n\n}\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html\n","source":"_posts/Flink/[Flink]Flink1.4 使用状态.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 使用状态\ndate: 2018-01-16 20:32:17\ntags:\n  - Flink\n  - Flink 容错\n\ncategories: Flink\npermalink: flink_stream_working_with_state\n---\n\n### 1. Keyed State 与 Operator State\n\n`Flink`有两种基本的状态：`Keyed State`和`Operator State`。\n\n#### 1.1 Keyed State\n\n`Keyed State`总是与`key`相关，只能在`KeyedStream`的函数和运算符中使用。\n\n备注:\n```\nKeyedStream继承DataStream，表示根据指定的key进行分组的数据流。使用DataStream提供的KeySelector根据key对其上的算子State进行分区。\nDataStream支持的典型操作也可以在KeyedStream上进行，除了诸如shuffle，forward和keyBy之类的分区方法之外。\n\n一个KeyedStream可以通过调用DataStream.keyBy()来获得。而在KeyedStream上进行任何transformation都将转变回DataStream。\n```\n\n你可以将 `Keyed State` 视为已经分区或分片的`Operator State`，每个 `key` 对应一个状态分区。每个`Keyed State`在逻辑上只对应一个 `<并行算子实例，key>`，并且由于每个 `key` \"只属于\" 一个`Keyed Operator`的一个并行实例，我们可以简单地认为成 `<operator，key>`。\n\n`Keyed State` 被进一步组织成所谓的 `Key Group`。`Key Group` 是 `Flink` 可以重新分配 `Keyed State` 的最小单位；`Key Group`的数量与最大并行度一样多。在执行期间，`Keyed Operator`的每个并行实例都与一个或多个`Key Group`的`key`一起工作。\n\n#### 1.2 Operator State\n\n使用`Operator State` (或非`Keyed State`)，每个算子状态都绑定到一个并行算子实例。`Kafka Connector` 是在`Flink`中使用算子状态的一个很好的例子。`Kafka`消费者的每个并行实例都要维护一个`topic`分区和偏移量的map作为其`Operator State`。\n\n在并行度发生变化时，`Operator State`接口支持在并行算子实例之间进行重新分配状态。可以有不同的方案来处理这个重新分配。\n\n### 2. Raw State 与 Managed State\n\n`Keyed State`和`Operator State`以两种形式存在：托管状态`Managed State`和原始状态`Raw State`。\n\n`Managed State`由`Flink`运行时控制的数据结构表示，如内部哈希表或`RocksDB`。例如`ValueState`，`ListState`等。`Flink`的运行时对状态进行编码并将它们写入检查点。\n\n`Raw State`是指算子保持在它们自己数据结构中的状态。当检查点时，他们只写入一个字节序列到检查点。`Flink`对状态的数据结构一无所知，只能看到原始字节。\n\n所有数据流函数都可以使用`Managed State`，但`Raw State`接口只能在实现算子时使用。建议使用`Managed State`（而不是`Raw State`），因为在`Managed State`下，`Flin`k可以在并行度发生变化时自动重新分配状态，并且还可以更好地进行内存管理。\n\n备注:\n```\n如果你的Managed State需要自定义序列化逻辑，请参阅相应的指南以确保将来的兼容性。Flink的默认序列化器不需要特殊处理。\n```\n\n### 3. Managed Keyed State\n\n`Managed Keyed State`接口提供了对不同类型状态的访问，这些状态的作用域为当前输入元素的`key`。这意味着这种类型的状态只能用于`KeyedStream`，可以通过`stream.keyBy（...）`创建。\n\n现在，我们先看看可用状态的不同类型，然后我们将看到如何在一个程序中使用它们。可用状态是：\n- `ValueState <T>`：保存了一个可以更新和检索的值（如上所述，作用域为输入元素的`key`，所以操作看到的每个`key`可能有一个值）。该值可以使用`update（T）`来设置，使用`T value（）`来检索。\n- `ListState <T>`：保存了一个元素列表。可以追加元素并检索当前存储的所有元素的`Iterable`。使用`add（T）`添加元素，可以使用`Iterable <T> get（）`来检索`Iterable`。\n- `ReducingState <T>`：保存一个单一的值，表示添加到状态所有值的聚合。接口与`ListState`相同，但使用`add（T）`添加的元素，使用指定的`ReduceFunction`转换为聚合。\n- `AggregatingState <IN，OUT>`：保存一个单一的值，表示添加到状态所有值的聚合。与`ReducingState`不同，聚合后的类型可能与添加到状态的元素类型不同。接口与`ListState`相同，但使用`add（IN）`添加到状态的元素使用指定的`AggregateFunction`进行聚合。\n- `FoldingState <T，ACC>`：保存一个单一的值，表示添加到状态所有值的聚合。与`ReducingState`不同，聚合后类型可能与添加到状态的元素类型不同。接口与`ListState`相同，但使用`add（T）`添加到状态的元素使用指定的`FoldFunction`折叠成聚合。\n- MapState <UK，UV>：保存了一个映射列表。可以将键值对放入状态，并检索当前存储的所有映射的`Iterable`。使用`put（UK，UV）`或`putAll（Map <UK，UV>）`添加映射。与用户`key`相关的值可以使用`get（UK）`来检索。映射，键和值的迭代视图可分别使用`entries（）`，`keys（）`和`values（）`来检索。\n\n所有类型的状态都有一个`clear（）`方法，它清除了当前活跃`key`的状态，即输入元素的`key`。\n\n备注:\n```\nFoldingState和FoldingStateDescriptor已经在Flink 1.4中被弃用，将来会被彻底删除。请改用AggregatingState和AggregatingStateDescriptor。\n```\n\n请记住，这些状态对象仅能用于状态接口，这一点很重要。状态没有必要存储在内存中，也可以驻留在磁盘或其他地方。第二件要记住的是，你从状态获得的值取决于输入元素的`key`。因此，如果所涉及的`key`不同，那你在用户函数调用中获得的值可能与另一个调用中的值不同。\n\n为了得到一个状态句柄，你必须创建一个`StateDescriptor`。它包含了状态的名字（我们将在后面看到，你可以创建多个状态，必须有唯一的名称，以便引用它们），状态值的类型，以及用户自定义函数，如`ReduceFunction`。根据要检索的状态类型，你可以创建一个`ValueStateDescriptor`，`ListStateDescriptor`，`ReducingStateDescriptor`，`FoldingStateDescriptor`或`MapStateDescriptor`。\n\n使用`RuntimeContext`来访问状态，所以只能在`Rich`函数中使用。请参阅[这里](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#rich-functions)了解有关信息，我们会很快看到一个例子。 在`RichFunction`中可用的`RuntimeContext`具有下面访问状态的方法：\n- ValueState<T> getState(ValueStateDescriptor<T>)\n- ReducingState<T> getReducingState(ReducingStateDescriptor<T>)\n- ListState<T> getListState(ListStateDescriptor<T>)\n- AggregatingState<IN, OUT> getAggregatingState(AggregatingState<IN, OUT>)\n- FoldingState<T, ACC> getFoldingState(FoldingStateDescriptor<T, ACC>)\n- MapState<UK, UV> getMapState(MapStateDescriptor<UK, UV>)\n\n下面是`FlatMapFunction`的一个例子：\n\nJava版本:\n```java\npublic class CountWindowAverage extends RichFlatMapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>> {\n\n    /**\n     * The ValueState handle. The first field is the count, the second field a running sum.\n     */\n    private transient ValueState<Tuple2<Long, Long>> sum;\n\n    @Override\n    public void flatMap(Tuple2<Long, Long> input, Collector<Tuple2<Long, Long>> out) throws Exception {\n\n        // access the state value\n        Tuple2<Long, Long> currentSum = sum.value();\n\n        // update the count 个数\n        currentSum.f0 += 1;\n\n        // add the second field of the input value 总和\n        currentSum.f1 += input.f1;\n\n        // update the state\n        sum.update(currentSum);\n\n        // if the count reaches 2, emit the average and clear the state\n        if (currentSum.f0 >= 2) {\n            out.collect(new Tuple2<>(input.f0, currentSum.f1 / currentSum.f0));\n            sum.clear();\n        }\n    }\n\n    @Override\n    public void open(Configuration config) {\n        ValueStateDescriptor<Tuple2<Long, Long>> descriptor =\n                new ValueStateDescriptor<>(\n                        \"average\", // the state name\n                        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}), // type information\n                        Tuple2.of(0L, 0L)); // default value of the state, if nothing was set\n        sum = getRuntimeContext().getState(descriptor);\n    }\n}\n\n// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)\nenv.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L))\n        .keyBy(0)\n        .flatMap(new CountWindowAverage())\n        .print();\n\n// the printed output will be (1,4) and (1,5)\n```\n\nScala版本:\n```\nclass CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] {\n\n  private var sum: ValueState[(Long, Long)] = _\n\n  override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = {\n\n    // access the state value\n    val tmpCurrentSum = sum.value\n\n    // If it hasn't been used before, it will be null\n    val currentSum = if (tmpCurrentSum != null) {\n      tmpCurrentSum\n    } else {\n      (0L, 0L)\n    }\n\n    // update the count\n    val newSum = (currentSum._1 + 1, currentSum._2 + input._2)\n\n    // update the state\n    sum.update(newSum)\n\n    // if the count reaches 2, emit the average and clear the state\n    if (newSum._1 >= 2) {\n      out.collect((input._1, newSum._2 / newSum._1))\n      sum.clear()\n    }\n  }\n\n  override def open(parameters: Configuration): Unit = {\n    sum = getRuntimeContext.getState(\n      new ValueStateDescriptor[(Long, Long)](\"average\", createTypeInformation[(Long, Long)])\n    )\n  }\n}\n\n\nobject ExampleCountWindowAverage extends App {\n  val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n  env.fromCollection(List(\n    (1L, 3L),\n    (1L, 5L),\n    (1L, 7L),\n    (1L, 4L),\n    (1L, 2L)\n  )).keyBy(_._1)\n    .flatMap(new CountWindowAverage())\n    .print()\n  // the printed output will be (1,4) and (1,5)\n\n  env.execute(\"ExampleManagedState\")\n}\n```\n\n这个例子实现了一个穷人的计数窗口。我们通过第一个字段键入元组（在这个例子中都有相同的`key`为`1`）。该函数将计数和总和存储在`ValueState`中。一旦计数达到2，就输出平均值并清除状态，以便我们从0开始。注意，如果我们元组第一个字段具有不同值，那将为每个不同的输入`key`保持不同的状态值。\n\n#### 3.1 Scala DataStream API中的状态\n\n除了上面介绍的接口之外，Scala API还具有在`KeyedStream`上使用单个`ValueState`的有状态`map（）`或`flatMap（）`函数的快捷方式。用户函数可以在`Option`获取`ValueState`的当前值，并且必须返回将用于更新状态的更新值。\n```\nval stream: DataStream[(String, Int)] = ...\n\nval counts: DataStream[(String, Int)] = stream\n  .keyBy(_._1)\n  .mapWithState((in: (String, Int), count: Option[Int]) =>\n    count match {\n      case Some(c) => ( (in._1, c), Some(c + in._2) )\n      case None => ( (in._1, 0), Some(in._2) )\n    })\n```\n\n### 4. Managed Operator State\n\n要使用`Managed Operator State`，有状态函数可以实现更通用的`CheckpointedFunction`接口或`ListCheckpointed <T extends Serializable>`接口。\n\n#### 4.1 CheckpointedFunction\n\n`CheckpointedFunction`接口提供了使用不同的重分配方案对非`Ked State`的访问。它需要实现一下两种方法：\n```\nvoid snapshotState(FunctionSnapshotContext context) throws Exception;\n\nvoid initializeState(FunctionInitializationContext context) throws Exception;\n```\n\n每当执行检查点时，将调用`snapshotState（）`。每当用户自定义函数被初始化时，对应的`initializeState（）`都被调用，或当函数被初始化时，或者当函数实际上从早期的检查点恢复时被调用(The counterpart, initializeState(), is called every time the user-defined function is initialized, be that when the function is first initialized or be that when the function is actually recovering from an earlier checkpoint. )。鉴于此，`initializeState（）`不仅是初始化不同类型的状态的地方，而且还包括状态恢复逻辑的位置。\n\n目前支持列表式的`Managed Operator State`。状态应该是一个可序列化的对象列表，相互间彼此独立，因此可以在扩展时重新分配。换句话说，这些对象可以在非`Keyed State`中重新分配比较细的粒度。根据状态访问方法，定义了以下重新分配方案：\n- 均分再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，列表被平分为与并行算子一样多的子列表。每个算子都可以得到一个可以为空或者包含一个或多个元素的子列表。例如，如果并行度为`1`，算子的检查点状态包含元素`element1`和`element2`，将并行度增加到`2`时，`element1`在算子实例0上运行，而`element2`将转至算子实例1。\n- 合并再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，每个算子都可以获得完整的状态元素列表。\n\n下面是一个有状态的`SinkFunction`的例子，它使用`CheckpointedFunction`在将元素输出到外部之前进行缓冲元素。它演示了基本的均分再分配列表状态：\n\nJava版本:\n```java\npublic class BufferingSink\n        implements SinkFunction<Tuple2<String, Integer>>,\n                   CheckpointedFunction,\n                   CheckpointedRestoring<ArrayList<Tuple2<String, Integer>>> {\n\n    private final int threshold;\n\n    private transient ListState<Tuple2<String, Integer>> checkpointedState;\n\n    private List<Tuple2<String, Integer>> bufferedElements;\n\n    public BufferingSink(int threshold) {\n        this.threshold = threshold;\n        this.bufferedElements = new ArrayList<>();\n    }\n\n    @Override\n    public void invoke(Tuple2<String, Integer> value) throws Exception {\n        bufferedElements.add(value);\n        if (bufferedElements.size() == threshold) {\n            for (Tuple2<String, Integer> element: bufferedElements) {\n                // send it to the sink\n            }\n            bufferedElements.clear();\n        }\n    }\n\n    @Override\n    public void snapshotState(FunctionSnapshotContext context) throws Exception {\n        checkpointedState.clear();\n        for (Tuple2<String, Integer> element : bufferedElements) {\n            checkpointedState.add(element);\n        }\n    }\n\n    @Override\n    public void initializeState(FunctionInitializationContext context) throws Exception {\n        ListStateDescriptor<Tuple2<String, Integer>> descriptor =\n            new ListStateDescriptor<>(\n                \"buffered-elements\",\n                TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}));\n\n        checkpointedState = context.getOperatorStateStore().getListState(descriptor);\n\n        if (context.isRestored()) {\n            for (Tuple2<String, Integer> element : checkpointedState.get()) {\n                bufferedElements.add(element);\n            }\n        }\n    }\n\n    @Override\n    public void restoreState(ArrayList<Tuple2<String, Integer>> state) throws Exception {\n        // this is from the CheckpointedRestoring interface.\n        this.bufferedElements.addAll(state);\n    }\n}\n```\n\nScala版本:\n```\nclass BufferingSink(threshold: Int = 0)\n  extends SinkFunction[(String, Int)]\n    with CheckpointedFunction\n    with CheckpointedRestoring[List[(String, Int)]] {\n\n  @transient\n  private var checkpointedState: ListState[(String, Int)] = _\n\n  private val bufferedElements = ListBuffer[(String, Int)]()\n\n  override def invoke(value: (String, Int)): Unit = {\n    bufferedElements += value\n    if (bufferedElements.size == threshold) {\n      for (element <- bufferedElements) {\n        // send it to the sink\n      }\n      bufferedElements.clear()\n    }\n  }\n\n  override def snapshotState(context: FunctionSnapshotContext): Unit = {\n    checkpointedState.clear()\n    for (element <- bufferedElements) {\n      checkpointedState.add(element)\n    }\n  }\n\n  override def initializeState(context: FunctionInitializationContext): Unit = {\n    val descriptor = new ListStateDescriptor[(String, Int)](\n      \"buffered-elements\",\n      TypeInformation.of(new TypeHint[(String, Int)]() {})\n    )\n\n    checkpointedState = context.getOperatorStateStore.getListState(descriptor)\n\n    if(context.isRestored) {\n      for(element <- checkpointedState.get()) {\n        bufferedElements += element\n      }\n    }\n  }\n\n  override def restoreState(state: List[(String, Int)]): Unit = {\n    bufferedElements ++= state\n  }\n}\n```\n\n`initializeState`方法以`FunctionInitializationContext`为参数。这用来初始化非`keyed state`\"容器\"。这是一个`ListState`类型的容器，非`keyed state`对象将在检查点时存储。\n\n注意一下状态是如何被初始化，类似于`keyed state`状态，使用包含状态名称和状态值类型相关信息的`StateDescriptor`：\n\nJava版本:\n```Java\nListStateDescriptor<Tuple2<String, Integer>> descriptor =\n    new ListStateDescriptor<>(\n        \"buffered-elements\",\n        TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {}));\n\ncheckpointedState = context.getOperatorStateStore().getListState(descriptor);\n```\n\nScala版本:\n```\nval descriptor = new ListStateDescriptor[(String, Long)](\n    \"buffered-elements\",\n    TypeInformation.of(new TypeHint[(String, Long)]() {})\n)\n\ncheckpointedState = context.getOperatorStateStore.getListState(descriptor)\n```\n\n状态访问方法的命名约定包含其重新分配模式及其状态结构。 例如，要使用带有联合重新分配方案的列表状态进行恢复，请使用`getUnionListState（descriptor）`访问状态。如果方法名称不包含重新分配模式，例如 `getListState（descriptor）`，这表示使用基本的均分重分配方案。\n\n在初始化容器之后，我们使用上下文的`isRestored（）`方法来检查失败后是否正在恢复。如果是，即我们正在恢复，将会应用恢复逻辑。\n\n如修改后的`BufferingSink`的代码所示，在状态初始化期间恢复的这个`ListState`被保存在类变量中，以备将来在`snapshotState（）`中使用。 在那里`ListState`清除了前一个检查点包含的所有对象，然后用我们想要进行检查点的新对象填充。\n\n`Keyed State`也可以在`initializeState（）`方法中初始化。这可以使用提供的`FunctionInitializationContext`完成。\n\n#### 4.2 ListCheckpointed\n\n`ListCheckpointed`接口是`CheckpointedFunction`进行限制的一种变体，它只支持在恢复时使用均分再分配方案的列表样式状态。还需要实现以下两种方法：\n\n```\nList<T> snapshotState(long checkpointId, long timestamp) throws Exception;\n\nvoid restoreState(List<T> state) throws Exception;\n```\n\n`snapshotState()`方法应该返回一个对象列表来进行checkpoint，而`restoreState()`方法在恢复时必须处理这样一个列表。如果状态是不可重分区的，则可以在`snapshotState()`中返回一个`Collections.singletonList(MY_STATE)`。\n\n#### 4.2.1 Stateful Source Functions\n\n与其他算子相比，有状态的数据源需要得到更多的关注。为了能更新状态以及输出集合的原子性（在失败/恢复时需要一次性语义），用户需要从数据源的上下文中获取锁。\n\nJava版本:\n```java\npublic static class CounterSource\n        extends RichParallelSourceFunction<Long>\n        implements ListCheckpointed<Long> {\n\n    /**  current offset for exactly once semantics */\n    private Long offset;\n\n    /** flag for job cancellation */\n    private volatile boolean isRunning = true;\n\n    @Override\n    public void run(SourceContext<Long> ctx) {\n        final Object lock = ctx.getCheckpointLock();\n\n        while (isRunning) {\n            // output and state update are atomic\n            synchronized (lock) {\n                ctx.collect(offset);\n                offset += 1;\n            }\n        }\n    }\n\n    @Override\n    public void cancel() {\n        isRunning = false;\n    }\n\n    @Override\n    public List<Long> snapshotState(long checkpointId, long checkpointTimestamp) {\n        return Collections.singletonList(offset);\n    }\n\n    @Override\n    public void restoreState(List<Long> state) {\n        for (Long s : state)\n            offset = s;\n    }\n}\n```\n\nScala版本:\n```\nclass CounterSource\n       extends RichParallelSourceFunction[Long]\n       with ListCheckpointed[Long] {\n\n  @volatile\n  private var isRunning = true\n\n  private var offset = 0L\n\n  override def run(ctx: SourceFunction.SourceContext[Long]): Unit = {\n    val lock = ctx.getCheckpointLock\n\n    while (isRunning) {\n      // output and state update are atomic\n      lock.synchronized({\n        ctx.collect(offset)\n\n        offset += 1\n      })\n    }\n  }\n\n  override def cancel(): Unit = isRunning = false\n\n  override def restoreState(state: util.List[Long]): Unit =\n    for (s <- state) {\n      offset = s\n    }\n\n  override def snapshotState(checkpointId: Long, timestamp: Long): util.List[Long] =\n    Collections.singletonList(offset)\n\n}\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html\n","slug":"flink_stream_working_with_state","published":1,"updated":"2018-02-01T06:22:28.468Z","comments":1,"photos":[],"link":"","_id":"cje58tiq9000vordbrxspx5fh","content":"<h3 id=\"1-Keyed-State-与-Operator-State\"><a href=\"#1-Keyed-State-与-Operator-State\" class=\"headerlink\" title=\"1. Keyed State 与 Operator State\"></a>1. Keyed State 与 Operator State</h3><p><code>Flink</code>有两种基本的状态：<code>Keyed State</code>和<code>Operator State</code>。</p>\n<h4 id=\"1-1-Keyed-State\"><a href=\"#1-1-Keyed-State\" class=\"headerlink\" title=\"1.1 Keyed State\"></a>1.1 Keyed State</h4><p><code>Keyed State</code>总是与<code>key</code>相关，只能在<code>KeyedStream</code>的函数和运算符中使用。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">KeyedStream继承DataStream，表示根据指定的key进行分组的数据流。使用DataStream提供的KeySelector根据key对其上的算子State进行分区。</span><br><span class=\"line\">DataStream支持的典型操作也可以在KeyedStream上进行，除了诸如shuffle，forward和keyBy之类的分区方法之外。</span><br><span class=\"line\"></span><br><span class=\"line\">一个KeyedStream可以通过调用DataStream.keyBy()来获得。而在KeyedStream上进行任何transformation都将转变回DataStream。</span><br></pre></td></tr></table></figure></p>\n<p>你可以将 <code>Keyed State</code> 视为已经分区或分片的<code>Operator State</code>，每个 <code>key</code> 对应一个状态分区。每个<code>Keyed State</code>在逻辑上只对应一个 <code>&lt;并行算子实例，key&gt;</code>，并且由于每个 <code>key</code> “只属于” 一个<code>Keyed Operator</code>的一个并行实例，我们可以简单地认为成 <code>&lt;operator，key&gt;</code>。</p>\n<p><code>Keyed State</code> 被进一步组织成所谓的 <code>Key Group</code>。<code>Key Group</code> 是 <code>Flink</code> 可以重新分配 <code>Keyed State</code> 的最小单位；<code>Key Group</code>的数量与最大并行度一样多。在执行期间，<code>Keyed Operator</code>的每个并行实例都与一个或多个<code>Key Group</code>的<code>key</code>一起工作。</p>\n<h4 id=\"1-2-Operator-State\"><a href=\"#1-2-Operator-State\" class=\"headerlink\" title=\"1.2 Operator State\"></a>1.2 Operator State</h4><p>使用<code>Operator State</code> (或非<code>Keyed State</code>)，每个算子状态都绑定到一个并行算子实例。<code>Kafka Connector</code> 是在<code>Flink</code>中使用算子状态的一个很好的例子。<code>Kafka</code>消费者的每个并行实例都要维护一个<code>topic</code>分区和偏移量的map作为其<code>Operator State</code>。</p>\n<p>在并行度发生变化时，<code>Operator State</code>接口支持在并行算子实例之间进行重新分配状态。可以有不同的方案来处理这个重新分配。</p>\n<h3 id=\"2-Raw-State-与-Managed-State\"><a href=\"#2-Raw-State-与-Managed-State\" class=\"headerlink\" title=\"2. Raw State 与 Managed State\"></a>2. Raw State 与 Managed State</h3><p><code>Keyed State</code>和<code>Operator State</code>以两种形式存在：托管状态<code>Managed State</code>和原始状态<code>Raw State</code>。</p>\n<p><code>Managed State</code>由<code>Flink</code>运行时控制的数据结构表示，如内部哈希表或<code>RocksDB</code>。例如<code>ValueState</code>，<code>ListState</code>等。<code>Flink</code>的运行时对状态进行编码并将它们写入检查点。</p>\n<p><code>Raw State</code>是指算子保持在它们自己数据结构中的状态。当检查点时，他们只写入一个字节序列到检查点。<code>Flink</code>对状态的数据结构一无所知，只能看到原始字节。</p>\n<p>所有数据流函数都可以使用<code>Managed State</code>，但<code>Raw State</code>接口只能在实现算子时使用。建议使用<code>Managed State</code>（而不是<code>Raw State</code>），因为在<code>Managed State</code>下，<code>Flin</code>k可以在并行度发生变化时自动重新分配状态，并且还可以更好地进行内存管理。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果你的Managed State需要自定义序列化逻辑，请参阅相应的指南以确保将来的兼容性。Flink的默认序列化器不需要特殊处理。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-Managed-Keyed-State\"><a href=\"#3-Managed-Keyed-State\" class=\"headerlink\" title=\"3. Managed Keyed State\"></a>3. Managed Keyed State</h3><p><code>Managed Keyed State</code>接口提供了对不同类型状态的访问，这些状态的作用域为当前输入元素的<code>key</code>。这意味着这种类型的状态只能用于<code>KeyedStream</code>，可以通过<code>stream.keyBy（...）</code>创建。</p>\n<p>现在，我们先看看可用状态的不同类型，然后我们将看到如何在一个程序中使用它们。可用状态是：</p>\n<ul>\n<li><code>ValueState &lt;T&gt;</code>：保存了一个可以更新和检索的值（如上所述，作用域为输入元素的<code>key</code>，所以操作看到的每个<code>key</code>可能有一个值）。该值可以使用<code>update（T）</code>来设置，使用<code>T value（）</code>来检索。</li>\n<li><code>ListState &lt;T&gt;</code>：保存了一个元素列表。可以追加元素并检索当前存储的所有元素的<code>Iterable</code>。使用<code>add（T）</code>添加元素，可以使用<code>Iterable &lt;T&gt; get（）</code>来检索<code>Iterable</code>。</li>\n<li><code>ReducingState &lt;T&gt;</code>：保存一个单一的值，表示添加到状态所有值的聚合。接口与<code>ListState</code>相同，但使用<code>add（T）</code>添加的元素，使用指定的<code>ReduceFunction</code>转换为聚合。</li>\n<li><code>AggregatingState &lt;IN，OUT&gt;</code>：保存一个单一的值，表示添加到状态所有值的聚合。与<code>ReducingState</code>不同，聚合后的类型可能与添加到状态的元素类型不同。接口与<code>ListState</code>相同，但使用<code>add（IN）</code>添加到状态的元素使用指定的<code>AggregateFunction</code>进行聚合。</li>\n<li><code>FoldingState &lt;T，ACC&gt;</code>：保存一个单一的值，表示添加到状态所有值的聚合。与<code>ReducingState</code>不同，聚合后类型可能与添加到状态的元素类型不同。接口与<code>ListState</code>相同，但使用<code>add（T）</code>添加到状态的元素使用指定的<code>FoldFunction</code>折叠成聚合。</li>\n<li>MapState <uk，uv>：保存了一个映射列表。可以将键值对放入状态，并检索当前存储的所有映射的<code>Iterable</code>。使用<code>put（UK，UV）</code>或<code>putAll（Map &lt;UK，UV&gt;）</code>添加映射。与用户<code>key</code>相关的值可以使用<code>get（UK）</code>来检索。映射，键和值的迭代视图可分别使用<code>entries（）</code>，<code>keys（）</code>和<code>values（）</code>来检索。</uk，uv></li>\n</ul>\n<p>所有类型的状态都有一个<code>clear（）</code>方法，它清除了当前活跃<code>key</code>的状态，即输入元素的<code>key</code>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">FoldingState和FoldingStateDescriptor已经在Flink 1.4中被弃用，将来会被彻底删除。请改用AggregatingState和AggregatingStateDescriptor。</span><br></pre></td></tr></table></figure></p>\n<p>请记住，这些状态对象仅能用于状态接口，这一点很重要。状态没有必要存储在内存中，也可以驻留在磁盘或其他地方。第二件要记住的是，你从状态获得的值取决于输入元素的<code>key</code>。因此，如果所涉及的<code>key</code>不同，那你在用户函数调用中获得的值可能与另一个调用中的值不同。</p>\n<p>为了得到一个状态句柄，你必须创建一个<code>StateDescriptor</code>。它包含了状态的名字（我们将在后面看到，你可以创建多个状态，必须有唯一的名称，以便引用它们），状态值的类型，以及用户自定义函数，如<code>ReduceFunction</code>。根据要检索的状态类型，你可以创建一个<code>ValueStateDescriptor</code>，<code>ListStateDescriptor</code>，<code>ReducingStateDescriptor</code>，<code>FoldingStateDescriptor</code>或<code>MapStateDescriptor</code>。</p>\n<p>使用<code>RuntimeContext</code>来访问状态，所以只能在<code>Rich</code>函数中使用。请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#rich-functions\" target=\"_blank\" rel=\"noopener\">这里</a>了解有关信息，我们会很快看到一个例子。 在<code>RichFunction</code>中可用的<code>RuntimeContext</code>具有下面访问状态的方法：</p>\n<ul>\n<li>ValueState<t> getState(ValueStateDescriptor<t>)</t></t></li>\n<li>ReducingState<t> getReducingState(ReducingStateDescriptor<t>)</t></t></li>\n<li>ListState<t> getListState(ListStateDescriptor<t>)</t></t></li>\n<li>AggregatingState<in, out=\"\"> getAggregatingState(AggregatingState<in, out=\"\">)</in,></in,></li>\n<li>FoldingState<t, acc=\"\"> getFoldingState(FoldingStateDescriptor<t, acc=\"\">)</t,></t,></li>\n<li>MapState<uk, uv=\"\"> getMapState(MapStateDescriptor<uk, uv=\"\">)</uk,></uk,></li>\n</ul>\n<p>下面是<code>FlatMapFunction</code>的一个例子：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CountWindowAverage</span> <span class=\"keyword\">extends</span> <span class=\"title\">RichFlatMapFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Long</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Long</span>&gt;&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * The ValueState handle. The first field is the count, the second field a running sum.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">flatMap</span><span class=\"params\">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// access the state value</span></span><br><span class=\"line\">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// update the count 个数</span></span><br><span class=\"line\">        currentSum.f0 += <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// add the second field of the input value 总和</span></span><br><span class=\"line\">        currentSum.f1 += input.f1;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// update the state</span></span><br><span class=\"line\">        sum.update(currentSum);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// if the count reaches 2, emit the average and clear the state</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (currentSum.f0 &gt;= <span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">            out.collect(<span class=\"keyword\">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class=\"line\">            sum.clear();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">open</span><span class=\"params\">(Configuration config)</span> </span>&#123;</span><br><span class=\"line\">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class=\"line\">                <span class=\"keyword\">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class=\"line\">                        <span class=\"string\">\"average\"</span>, <span class=\"comment\">// the state name</span></span><br><span class=\"line\">                        TypeInformation.of(<span class=\"keyword\">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class=\"comment\">// type information</span></span><br><span class=\"line\">                        Tuple2.of(<span class=\"number\">0L</span>, <span class=\"number\">0L</span>)); <span class=\"comment\">// default value of the state, if nothing was set</span></span><br><span class=\"line\">        sum = getRuntimeContext().getState(descriptor);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)</span></span><br><span class=\"line\">env.fromElements(Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">3L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">5L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">7L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">4L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">2L</span>))</span><br><span class=\"line\">        .keyBy(<span class=\"number\">0</span>)</span><br><span class=\"line\">        .flatMap(<span class=\"keyword\">new</span> CountWindowAverage())</span><br><span class=\"line\">        .print();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// the printed output will be (1,4) and (1,5)</span></span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  private var sum: ValueState[(Long, Long)] = _</span><br><span class=\"line\"></span><br><span class=\"line\">  override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    // access the state value</span><br><span class=\"line\">    val tmpCurrentSum = sum.value</span><br><span class=\"line\"></span><br><span class=\"line\">    // If it hasn&apos;t been used before, it will be null</span><br><span class=\"line\">    val currentSum = if (tmpCurrentSum != null) &#123;</span><br><span class=\"line\">      tmpCurrentSum</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      (0L, 0L)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // update the count</span><br><span class=\"line\">    val newSum = (currentSum._1 + 1, currentSum._2 + input._2)</span><br><span class=\"line\"></span><br><span class=\"line\">    // update the state</span><br><span class=\"line\">    sum.update(newSum)</span><br><span class=\"line\"></span><br><span class=\"line\">    // if the count reaches 2, emit the average and clear the state</span><br><span class=\"line\">    if (newSum._1 &gt;= 2) &#123;</span><br><span class=\"line\">      out.collect((input._1, newSum._2 / newSum._1))</span><br><span class=\"line\">      sum.clear()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class=\"line\">    sum = getRuntimeContext.getState(</span><br><span class=\"line\">      new ValueStateDescriptor[(Long, Long)](&quot;average&quot;, createTypeInformation[(Long, Long)])</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">object ExampleCountWindowAverage extends App &#123;</span><br><span class=\"line\">  val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">  env.fromCollection(List(</span><br><span class=\"line\">    (1L, 3L),</span><br><span class=\"line\">    (1L, 5L),</span><br><span class=\"line\">    (1L, 7L),</span><br><span class=\"line\">    (1L, 4L),</span><br><span class=\"line\">    (1L, 2L)</span><br><span class=\"line\">  )).keyBy(_._1)</span><br><span class=\"line\">    .flatMap(new CountWindowAverage())</span><br><span class=\"line\">    .print()</span><br><span class=\"line\">  // the printed output will be (1,4) and (1,5)</span><br><span class=\"line\"></span><br><span class=\"line\">  env.execute(&quot;ExampleManagedState&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个例子实现了一个穷人的计数窗口。我们通过第一个字段键入元组（在这个例子中都有相同的<code>key</code>为<code>1</code>）。该函数将计数和总和存储在<code>ValueState</code>中。一旦计数达到2，就输出平均值并清除状态，以便我们从0开始。注意，如果我们元组第一个字段具有不同值，那将为每个不同的输入<code>key</code>保持不同的状态值。</p>\n<h4 id=\"3-1-Scala-DataStream-API中的状态\"><a href=\"#3-1-Scala-DataStream-API中的状态\" class=\"headerlink\" title=\"3.1 Scala DataStream API中的状态\"></a>3.1 Scala DataStream API中的状态</h4><p>除了上面介绍的接口之外，Scala API还具有在<code>KeyedStream</code>上使用单个<code>ValueState</code>的有状态<code>map（）</code>或<code>flatMap（）</code>函数的快捷方式。用户函数可以在<code>Option</code>获取<code>ValueState</code>的当前值，并且必须返回将用于更新状态的更新值。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[(String, Int)] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val counts: DataStream[(String, Int)] = stream</span><br><span class=\"line\">  .keyBy(_._1)</span><br><span class=\"line\">  .mapWithState((in: (String, Int), count: Option[Int]) =&gt;</span><br><span class=\"line\">    count match &#123;</span><br><span class=\"line\">      case Some(c) =&gt; ( (in._1, c), Some(c + in._2) )</span><br><span class=\"line\">      case None =&gt; ( (in._1, 0), Some(in._2) )</span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-Managed-Operator-State\"><a href=\"#4-Managed-Operator-State\" class=\"headerlink\" title=\"4. Managed Operator State\"></a>4. Managed Operator State</h3><p>要使用<code>Managed Operator State</code>，有状态函数可以实现更通用的<code>CheckpointedFunction</code>接口或<code>ListCheckpointed &lt;T extends Serializable&gt;</code>接口。</p>\n<h4 id=\"4-1-CheckpointedFunction\"><a href=\"#4-1-CheckpointedFunction\" class=\"headerlink\" title=\"4.1 CheckpointedFunction\"></a>4.1 CheckpointedFunction</h4><p><code>CheckpointedFunction</code>接口提供了使用不同的重分配方案对非<code>Ked State</code>的访问。它需要实现一下两种方法：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br><span class=\"line\"></span><br><span class=\"line\">void initializeState(FunctionInitializationContext context) throws Exception;</span><br></pre></td></tr></table></figure></p>\n<p>每当执行检查点时，将调用<code>snapshotState（）</code>。每当用户自定义函数被初始化时，对应的<code>initializeState（）</code>都被调用，或当函数被初始化时，或者当函数实际上从早期的检查点恢复时被调用(The counterpart, initializeState(), is called every time the user-defined function is initialized, be that when the function is first initialized or be that when the function is actually recovering from an earlier checkpoint. )。鉴于此，<code>initializeState（）</code>不仅是初始化不同类型的状态的地方，而且还包括状态恢复逻辑的位置。</p>\n<p>目前支持列表式的<code>Managed Operator State</code>。状态应该是一个可序列化的对象列表，相互间彼此独立，因此可以在扩展时重新分配。换句话说，这些对象可以在非<code>Keyed State</code>中重新分配比较细的粒度。根据状态访问方法，定义了以下重新分配方案：</p>\n<ul>\n<li>均分再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，列表被平分为与并行算子一样多的子列表。每个算子都可以得到一个可以为空或者包含一个或多个元素的子列表。例如，如果并行度为<code>1</code>，算子的检查点状态包含元素<code>element1</code>和<code>element2</code>，将并行度增加到<code>2</code>时，<code>element1</code>在算子实例0上运行，而<code>element2</code>将转至算子实例1。</li>\n<li>合并再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，每个算子都可以获得完整的状态元素列表。</li>\n</ul>\n<p>下面是一个有状态的<code>SinkFunction</code>的例子，它使用<code>CheckpointedFunction</code>在将元素输出到外部之前进行缓冲元素。它演示了基本的均分再分配列表状态：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BufferingSink</span></span></span><br><span class=\"line\"><span class=\"class\">        <span class=\"keyword\">implements</span> <span class=\"title\">SinkFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">Integer</span>&gt;&gt;,</span></span><br><span class=\"line\"><span class=\"class\">                   <span class=\"title\">CheckpointedFunction</span>,</span></span><br><span class=\"line\"><span class=\"class\">                   <span class=\"title\">CheckpointedRestoring</span>&lt;<span class=\"title\">ArrayList</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">Integer</span>&gt;&gt;&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> threshold;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">BufferingSink</span><span class=\"params\">(<span class=\"keyword\">int</span> threshold)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.threshold = threshold;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.bufferedElements = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">invoke</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; value)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        bufferedElements.add(value);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// send it to the sink</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            bufferedElements.clear();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">snapshotState</span><span class=\"params\">(FunctionSnapshotContext context)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        checkpointedState.clear();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class=\"line\">            checkpointedState.add(element);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">initializeState</span><span class=\"params\">(FunctionInitializationContext context)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class=\"line\">                <span class=\"string\">\"buffered-elements\"</span>,</span><br><span class=\"line\">                TypeInformation.of(<span class=\"keyword\">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class=\"line\"></span><br><span class=\"line\">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (context.isRestored()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class=\"line\">                bufferedElements.add(element);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">restoreState</span><span class=\"params\">(ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; state)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// this is from the CheckpointedRestoring interface.</span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>.bufferedElements.addAll(state);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class BufferingSink(threshold: Int = 0)</span><br><span class=\"line\">  extends SinkFunction[(String, Int)]</span><br><span class=\"line\">    with CheckpointedFunction</span><br><span class=\"line\">    with CheckpointedRestoring[List[(String, Int)]] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  @transient</span><br><span class=\"line\">  private var checkpointedState: ListState[(String, Int)] = _</span><br><span class=\"line\"></span><br><span class=\"line\">  private val bufferedElements = ListBuffer[(String, Int)]()</span><br><span class=\"line\"></span><br><span class=\"line\">  override def invoke(value: (String, Int)): Unit = &#123;</span><br><span class=\"line\">    bufferedElements += value</span><br><span class=\"line\">    if (bufferedElements.size == threshold) &#123;</span><br><span class=\"line\">      for (element &lt;- bufferedElements) &#123;</span><br><span class=\"line\">        // send it to the sink</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      bufferedElements.clear()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def snapshotState(context: FunctionSnapshotContext): Unit = &#123;</span><br><span class=\"line\">    checkpointedState.clear()</span><br><span class=\"line\">    for (element &lt;- bufferedElements) &#123;</span><br><span class=\"line\">      checkpointedState.add(element)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def initializeState(context: FunctionInitializationContext): Unit = &#123;</span><br><span class=\"line\">    val descriptor = new ListStateDescriptor[(String, Int)](</span><br><span class=\"line\">      &quot;buffered-elements&quot;,</span><br><span class=\"line\">      TypeInformation.of(new TypeHint[(String, Int)]() &#123;&#125;)</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    checkpointedState = context.getOperatorStateStore.getListState(descriptor)</span><br><span class=\"line\"></span><br><span class=\"line\">    if(context.isRestored) &#123;</span><br><span class=\"line\">      for(element &lt;- checkpointedState.get()) &#123;</span><br><span class=\"line\">        bufferedElements += element</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def restoreState(state: List[(String, Int)]): Unit = &#123;</span><br><span class=\"line\">    bufferedElements ++= state</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>initializeState</code>方法以<code>FunctionInitializationContext</code>为参数。这用来初始化非<code>keyed state</code>“容器”。这是一个<code>ListState</code>类型的容器，非<code>keyed state</code>对象将在检查点时存储。</p>\n<p>注意一下状态是如何被初始化，类似于<code>keyed state</code>状态，使用包含状态名称和状态值类型相关信息的<code>StateDescriptor</code>：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class=\"line\">    <span class=\"keyword\">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class=\"line\">        <span class=\"string\">\"buffered-elements\"</span>,</span><br><span class=\"line\">        TypeInformation.of(<span class=\"keyword\">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class=\"line\"></span><br><span class=\"line\">checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val descriptor = new ListStateDescriptor[(String, Long)](</span><br><span class=\"line\">    &quot;buffered-elements&quot;,</span><br><span class=\"line\">    TypeInformation.of(new TypeHint[(String, Long)]() &#123;&#125;)</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">checkpointedState = context.getOperatorStateStore.getListState(descriptor)</span><br></pre></td></tr></table></figure></p>\n<p>状态访问方法的命名约定包含其重新分配模式及其状态结构。 例如，要使用带有联合重新分配方案的列表状态进行恢复，请使用<code>getUnionListState（descriptor）</code>访问状态。如果方法名称不包含重新分配模式，例如 <code>getListState（descriptor）</code>，这表示使用基本的均分重分配方案。</p>\n<p>在初始化容器之后，我们使用上下文的<code>isRestored（）</code>方法来检查失败后是否正在恢复。如果是，即我们正在恢复，将会应用恢复逻辑。</p>\n<p>如修改后的<code>BufferingSink</code>的代码所示，在状态初始化期间恢复的这个<code>ListState</code>被保存在类变量中，以备将来在<code>snapshotState（）</code>中使用。 在那里<code>ListState</code>清除了前一个检查点包含的所有对象，然后用我们想要进行检查点的新对象填充。</p>\n<p><code>Keyed State</code>也可以在<code>initializeState（）</code>方法中初始化。这可以使用提供的<code>FunctionInitializationContext</code>完成。</p>\n<h4 id=\"4-2-ListCheckpointed\"><a href=\"#4-2-ListCheckpointed\" class=\"headerlink\" title=\"4.2 ListCheckpointed\"></a>4.2 ListCheckpointed</h4><p><code>ListCheckpointed</code>接口是<code>CheckpointedFunction</code>进行限制的一种变体，它只支持在恢复时使用均分再分配方案的列表样式状态。还需要实现以下两种方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;</span><br><span class=\"line\"></span><br><span class=\"line\">void restoreState(List&lt;T&gt; state) throws Exception;</span><br></pre></td></tr></table></figure>\n<p><code>snapshotState()</code>方法应该返回一个对象列表来进行checkpoint，而<code>restoreState()</code>方法在恢复时必须处理这样一个列表。如果状态是不可重分区的，则可以在<code>snapshotState()</code>中返回一个<code>Collections.singletonList(MY_STATE)</code>。</p>\n<h4 id=\"4-2-1-Stateful-Source-Functions\"><a href=\"#4-2-1-Stateful-Source-Functions\" class=\"headerlink\" title=\"4.2.1 Stateful Source Functions\"></a>4.2.1 Stateful Source Functions</h4><p>与其他算子相比，有状态的数据源需要得到更多的关注。为了能更新状态以及输出集合的原子性（在失败/恢复时需要一次性语义），用户需要从数据源的上下文中获取锁。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CounterSource</span></span></span><br><span class=\"line\"><span class=\"class\">        <span class=\"keyword\">extends</span> <span class=\"title\">RichParallelSourceFunction</span>&lt;<span class=\"title\">Long</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\">        <span class=\"keyword\">implements</span> <span class=\"title\">ListCheckpointed</span>&lt;<span class=\"title\">Long</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**  current offset for exactly once semantics */</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Long offset;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** flag for job cancellation */</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">volatile</span> <span class=\"keyword\">boolean</span> isRunning = <span class=\"keyword\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> Object lock = ctx.getCheckpointLock();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (isRunning) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// output and state update are atomic</span></span><br><span class=\"line\">            <span class=\"keyword\">synchronized</span> (lock) &#123;</span><br><span class=\"line\">                ctx.collect(offset);</span><br><span class=\"line\">                offset += <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">cancel</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        isRunning = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> List&lt;Long&gt; <span class=\"title\">snapshotState</span><span class=\"params\">(<span class=\"keyword\">long</span> checkpointId, <span class=\"keyword\">long</span> checkpointTimestamp)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Collections.singletonList(offset);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">restoreState</span><span class=\"params\">(List&lt;Long&gt; state)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Long s : state)</span><br><span class=\"line\">            offset = s;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class CounterSource</span><br><span class=\"line\">       extends RichParallelSourceFunction[Long]</span><br><span class=\"line\">       with ListCheckpointed[Long] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  @volatile</span><br><span class=\"line\">  private var isRunning = true</span><br><span class=\"line\"></span><br><span class=\"line\">  private var offset = 0L</span><br><span class=\"line\"></span><br><span class=\"line\">  override def run(ctx: SourceFunction.SourceContext[Long]): Unit = &#123;</span><br><span class=\"line\">    val lock = ctx.getCheckpointLock</span><br><span class=\"line\"></span><br><span class=\"line\">    while (isRunning) &#123;</span><br><span class=\"line\">      // output and state update are atomic</span><br><span class=\"line\">      lock.synchronized(&#123;</span><br><span class=\"line\">        ctx.collect(offset)</span><br><span class=\"line\"></span><br><span class=\"line\">        offset += 1</span><br><span class=\"line\">      &#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def cancel(): Unit = isRunning = false</span><br><span class=\"line\"></span><br><span class=\"line\">  override def restoreState(state: util.List[Long]): Unit =</span><br><span class=\"line\">    for (s &lt;- state) &#123;</span><br><span class=\"line\">      offset = s</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def snapshotState(checkpointId: Long, timestamp: Long): util.List[Long] =</span><br><span class=\"line\">    Collections.singletonList(offset)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-Keyed-State-与-Operator-State\"><a href=\"#1-Keyed-State-与-Operator-State\" class=\"headerlink\" title=\"1. Keyed State 与 Operator State\"></a>1. Keyed State 与 Operator State</h3><p><code>Flink</code>有两种基本的状态：<code>Keyed State</code>和<code>Operator State</code>。</p>\n<h4 id=\"1-1-Keyed-State\"><a href=\"#1-1-Keyed-State\" class=\"headerlink\" title=\"1.1 Keyed State\"></a>1.1 Keyed State</h4><p><code>Keyed State</code>总是与<code>key</code>相关，只能在<code>KeyedStream</code>的函数和运算符中使用。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">KeyedStream继承DataStream，表示根据指定的key进行分组的数据流。使用DataStream提供的KeySelector根据key对其上的算子State进行分区。</span><br><span class=\"line\">DataStream支持的典型操作也可以在KeyedStream上进行，除了诸如shuffle，forward和keyBy之类的分区方法之外。</span><br><span class=\"line\"></span><br><span class=\"line\">一个KeyedStream可以通过调用DataStream.keyBy()来获得。而在KeyedStream上进行任何transformation都将转变回DataStream。</span><br></pre></td></tr></table></figure></p>\n<p>你可以将 <code>Keyed State</code> 视为已经分区或分片的<code>Operator State</code>，每个 <code>key</code> 对应一个状态分区。每个<code>Keyed State</code>在逻辑上只对应一个 <code>&lt;并行算子实例，key&gt;</code>，并且由于每个 <code>key</code> “只属于” 一个<code>Keyed Operator</code>的一个并行实例，我们可以简单地认为成 <code>&lt;operator，key&gt;</code>。</p>\n<p><code>Keyed State</code> 被进一步组织成所谓的 <code>Key Group</code>。<code>Key Group</code> 是 <code>Flink</code> 可以重新分配 <code>Keyed State</code> 的最小单位；<code>Key Group</code>的数量与最大并行度一样多。在执行期间，<code>Keyed Operator</code>的每个并行实例都与一个或多个<code>Key Group</code>的<code>key</code>一起工作。</p>\n<h4 id=\"1-2-Operator-State\"><a href=\"#1-2-Operator-State\" class=\"headerlink\" title=\"1.2 Operator State\"></a>1.2 Operator State</h4><p>使用<code>Operator State</code> (或非<code>Keyed State</code>)，每个算子状态都绑定到一个并行算子实例。<code>Kafka Connector</code> 是在<code>Flink</code>中使用算子状态的一个很好的例子。<code>Kafka</code>消费者的每个并行实例都要维护一个<code>topic</code>分区和偏移量的map作为其<code>Operator State</code>。</p>\n<p>在并行度发生变化时，<code>Operator State</code>接口支持在并行算子实例之间进行重新分配状态。可以有不同的方案来处理这个重新分配。</p>\n<h3 id=\"2-Raw-State-与-Managed-State\"><a href=\"#2-Raw-State-与-Managed-State\" class=\"headerlink\" title=\"2. Raw State 与 Managed State\"></a>2. Raw State 与 Managed State</h3><p><code>Keyed State</code>和<code>Operator State</code>以两种形式存在：托管状态<code>Managed State</code>和原始状态<code>Raw State</code>。</p>\n<p><code>Managed State</code>由<code>Flink</code>运行时控制的数据结构表示，如内部哈希表或<code>RocksDB</code>。例如<code>ValueState</code>，<code>ListState</code>等。<code>Flink</code>的运行时对状态进行编码并将它们写入检查点。</p>\n<p><code>Raw State</code>是指算子保持在它们自己数据结构中的状态。当检查点时，他们只写入一个字节序列到检查点。<code>Flink</code>对状态的数据结构一无所知，只能看到原始字节。</p>\n<p>所有数据流函数都可以使用<code>Managed State</code>，但<code>Raw State</code>接口只能在实现算子时使用。建议使用<code>Managed State</code>（而不是<code>Raw State</code>），因为在<code>Managed State</code>下，<code>Flin</code>k可以在并行度发生变化时自动重新分配状态，并且还可以更好地进行内存管理。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果你的Managed State需要自定义序列化逻辑，请参阅相应的指南以确保将来的兼容性。Flink的默认序列化器不需要特殊处理。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-Managed-Keyed-State\"><a href=\"#3-Managed-Keyed-State\" class=\"headerlink\" title=\"3. Managed Keyed State\"></a>3. Managed Keyed State</h3><p><code>Managed Keyed State</code>接口提供了对不同类型状态的访问，这些状态的作用域为当前输入元素的<code>key</code>。这意味着这种类型的状态只能用于<code>KeyedStream</code>，可以通过<code>stream.keyBy（...）</code>创建。</p>\n<p>现在，我们先看看可用状态的不同类型，然后我们将看到如何在一个程序中使用它们。可用状态是：</p>\n<ul>\n<li><code>ValueState &lt;T&gt;</code>：保存了一个可以更新和检索的值（如上所述，作用域为输入元素的<code>key</code>，所以操作看到的每个<code>key</code>可能有一个值）。该值可以使用<code>update（T）</code>来设置，使用<code>T value（）</code>来检索。</li>\n<li><code>ListState &lt;T&gt;</code>：保存了一个元素列表。可以追加元素并检索当前存储的所有元素的<code>Iterable</code>。使用<code>add（T）</code>添加元素，可以使用<code>Iterable &lt;T&gt; get（）</code>来检索<code>Iterable</code>。</li>\n<li><code>ReducingState &lt;T&gt;</code>：保存一个单一的值，表示添加到状态所有值的聚合。接口与<code>ListState</code>相同，但使用<code>add（T）</code>添加的元素，使用指定的<code>ReduceFunction</code>转换为聚合。</li>\n<li><code>AggregatingState &lt;IN，OUT&gt;</code>：保存一个单一的值，表示添加到状态所有值的聚合。与<code>ReducingState</code>不同，聚合后的类型可能与添加到状态的元素类型不同。接口与<code>ListState</code>相同，但使用<code>add（IN）</code>添加到状态的元素使用指定的<code>AggregateFunction</code>进行聚合。</li>\n<li><code>FoldingState &lt;T，ACC&gt;</code>：保存一个单一的值，表示添加到状态所有值的聚合。与<code>ReducingState</code>不同，聚合后类型可能与添加到状态的元素类型不同。接口与<code>ListState</code>相同，但使用<code>add（T）</code>添加到状态的元素使用指定的<code>FoldFunction</code>折叠成聚合。</li>\n<li>MapState <uk，uv>：保存了一个映射列表。可以将键值对放入状态，并检索当前存储的所有映射的<code>Iterable</code>。使用<code>put（UK，UV）</code>或<code>putAll（Map &lt;UK，UV&gt;）</code>添加映射。与用户<code>key</code>相关的值可以使用<code>get（UK）</code>来检索。映射，键和值的迭代视图可分别使用<code>entries（）</code>，<code>keys（）</code>和<code>values（）</code>来检索。</uk，uv></li>\n</ul>\n<p>所有类型的状态都有一个<code>clear（）</code>方法，它清除了当前活跃<code>key</code>的状态，即输入元素的<code>key</code>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">FoldingState和FoldingStateDescriptor已经在Flink 1.4中被弃用，将来会被彻底删除。请改用AggregatingState和AggregatingStateDescriptor。</span><br></pre></td></tr></table></figure></p>\n<p>请记住，这些状态对象仅能用于状态接口，这一点很重要。状态没有必要存储在内存中，也可以驻留在磁盘或其他地方。第二件要记住的是，你从状态获得的值取决于输入元素的<code>key</code>。因此，如果所涉及的<code>key</code>不同，那你在用户函数调用中获得的值可能与另一个调用中的值不同。</p>\n<p>为了得到一个状态句柄，你必须创建一个<code>StateDescriptor</code>。它包含了状态的名字（我们将在后面看到，你可以创建多个状态，必须有唯一的名称，以便引用它们），状态值的类型，以及用户自定义函数，如<code>ReduceFunction</code>。根据要检索的状态类型，你可以创建一个<code>ValueStateDescriptor</code>，<code>ListStateDescriptor</code>，<code>ReducingStateDescriptor</code>，<code>FoldingStateDescriptor</code>或<code>MapStateDescriptor</code>。</p>\n<p>使用<code>RuntimeContext</code>来访问状态，所以只能在<code>Rich</code>函数中使用。请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#rich-functions\" target=\"_blank\" rel=\"noopener\">这里</a>了解有关信息，我们会很快看到一个例子。 在<code>RichFunction</code>中可用的<code>RuntimeContext</code>具有下面访问状态的方法：</p>\n<ul>\n<li>ValueState<t> getState(ValueStateDescriptor<t>)</t></t></li>\n<li>ReducingState<t> getReducingState(ReducingStateDescriptor<t>)</t></t></li>\n<li>ListState<t> getListState(ListStateDescriptor<t>)</t></t></li>\n<li>AggregatingState<in, out=\"\"> getAggregatingState(AggregatingState<in, out=\"\">)</in,></in,></li>\n<li>FoldingState<t, acc=\"\"> getFoldingState(FoldingStateDescriptor<t, acc=\"\">)</t,></t,></li>\n<li>MapState<uk, uv=\"\"> getMapState(MapStateDescriptor<uk, uv=\"\">)</uk,></uk,></li>\n</ul>\n<p>下面是<code>FlatMapFunction</code>的一个例子：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CountWindowAverage</span> <span class=\"keyword\">extends</span> <span class=\"title\">RichFlatMapFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Long</span>&gt;, <span class=\"title\">Tuple2</span>&lt;<span class=\"title\">Long</span>, <span class=\"title\">Long</span>&gt;&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * The ValueState handle. The first field is the count, the second field a running sum.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">flatMap</span><span class=\"params\">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// access the state value</span></span><br><span class=\"line\">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// update the count 个数</span></span><br><span class=\"line\">        currentSum.f0 += <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// add the second field of the input value 总和</span></span><br><span class=\"line\">        currentSum.f1 += input.f1;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// update the state</span></span><br><span class=\"line\">        sum.update(currentSum);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// if the count reaches 2, emit the average and clear the state</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (currentSum.f0 &gt;= <span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">            out.collect(<span class=\"keyword\">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class=\"line\">            sum.clear();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">open</span><span class=\"params\">(Configuration config)</span> </span>&#123;</span><br><span class=\"line\">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class=\"line\">                <span class=\"keyword\">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class=\"line\">                        <span class=\"string\">\"average\"</span>, <span class=\"comment\">// the state name</span></span><br><span class=\"line\">                        TypeInformation.of(<span class=\"keyword\">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class=\"comment\">// type information</span></span><br><span class=\"line\">                        Tuple2.of(<span class=\"number\">0L</span>, <span class=\"number\">0L</span>)); <span class=\"comment\">// default value of the state, if nothing was set</span></span><br><span class=\"line\">        sum = getRuntimeContext().getState(descriptor);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env)</span></span><br><span class=\"line\">env.fromElements(Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">3L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">5L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">7L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">4L</span>), Tuple2.of(<span class=\"number\">1L</span>, <span class=\"number\">2L</span>))</span><br><span class=\"line\">        .keyBy(<span class=\"number\">0</span>)</span><br><span class=\"line\">        .flatMap(<span class=\"keyword\">new</span> CountWindowAverage())</span><br><span class=\"line\">        .print();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// the printed output will be (1,4) and (1,5)</span></span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  private var sum: ValueState[(Long, Long)] = _</span><br><span class=\"line\"></span><br><span class=\"line\">  override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    // access the state value</span><br><span class=\"line\">    val tmpCurrentSum = sum.value</span><br><span class=\"line\"></span><br><span class=\"line\">    // If it hasn&apos;t been used before, it will be null</span><br><span class=\"line\">    val currentSum = if (tmpCurrentSum != null) &#123;</span><br><span class=\"line\">      tmpCurrentSum</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      (0L, 0L)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // update the count</span><br><span class=\"line\">    val newSum = (currentSum._1 + 1, currentSum._2 + input._2)</span><br><span class=\"line\"></span><br><span class=\"line\">    // update the state</span><br><span class=\"line\">    sum.update(newSum)</span><br><span class=\"line\"></span><br><span class=\"line\">    // if the count reaches 2, emit the average and clear the state</span><br><span class=\"line\">    if (newSum._1 &gt;= 2) &#123;</span><br><span class=\"line\">      out.collect((input._1, newSum._2 / newSum._1))</span><br><span class=\"line\">      sum.clear()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def open(parameters: Configuration): Unit = &#123;</span><br><span class=\"line\">    sum = getRuntimeContext.getState(</span><br><span class=\"line\">      new ValueStateDescriptor[(Long, Long)](&quot;average&quot;, createTypeInformation[(Long, Long)])</span><br><span class=\"line\">    )</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">object ExampleCountWindowAverage extends App &#123;</span><br><span class=\"line\">  val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">  env.fromCollection(List(</span><br><span class=\"line\">    (1L, 3L),</span><br><span class=\"line\">    (1L, 5L),</span><br><span class=\"line\">    (1L, 7L),</span><br><span class=\"line\">    (1L, 4L),</span><br><span class=\"line\">    (1L, 2L)</span><br><span class=\"line\">  )).keyBy(_._1)</span><br><span class=\"line\">    .flatMap(new CountWindowAverage())</span><br><span class=\"line\">    .print()</span><br><span class=\"line\">  // the printed output will be (1,4) and (1,5)</span><br><span class=\"line\"></span><br><span class=\"line\">  env.execute(&quot;ExampleManagedState&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个例子实现了一个穷人的计数窗口。我们通过第一个字段键入元组（在这个例子中都有相同的<code>key</code>为<code>1</code>）。该函数将计数和总和存储在<code>ValueState</code>中。一旦计数达到2，就输出平均值并清除状态，以便我们从0开始。注意，如果我们元组第一个字段具有不同值，那将为每个不同的输入<code>key</code>保持不同的状态值。</p>\n<h4 id=\"3-1-Scala-DataStream-API中的状态\"><a href=\"#3-1-Scala-DataStream-API中的状态\" class=\"headerlink\" title=\"3.1 Scala DataStream API中的状态\"></a>3.1 Scala DataStream API中的状态</h4><p>除了上面介绍的接口之外，Scala API还具有在<code>KeyedStream</code>上使用单个<code>ValueState</code>的有状态<code>map（）</code>或<code>flatMap（）</code>函数的快捷方式。用户函数可以在<code>Option</code>获取<code>ValueState</code>的当前值，并且必须返回将用于更新状态的更新值。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[(String, Int)] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val counts: DataStream[(String, Int)] = stream</span><br><span class=\"line\">  .keyBy(_._1)</span><br><span class=\"line\">  .mapWithState((in: (String, Int), count: Option[Int]) =&gt;</span><br><span class=\"line\">    count match &#123;</span><br><span class=\"line\">      case Some(c) =&gt; ( (in._1, c), Some(c + in._2) )</span><br><span class=\"line\">      case None =&gt; ( (in._1, 0), Some(in._2) )</span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-Managed-Operator-State\"><a href=\"#4-Managed-Operator-State\" class=\"headerlink\" title=\"4. Managed Operator State\"></a>4. Managed Operator State</h3><p>要使用<code>Managed Operator State</code>，有状态函数可以实现更通用的<code>CheckpointedFunction</code>接口或<code>ListCheckpointed &lt;T extends Serializable&gt;</code>接口。</p>\n<h4 id=\"4-1-CheckpointedFunction\"><a href=\"#4-1-CheckpointedFunction\" class=\"headerlink\" title=\"4.1 CheckpointedFunction\"></a>4.1 CheckpointedFunction</h4><p><code>CheckpointedFunction</code>接口提供了使用不同的重分配方案对非<code>Ked State</code>的访问。它需要实现一下两种方法：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br><span class=\"line\"></span><br><span class=\"line\">void initializeState(FunctionInitializationContext context) throws Exception;</span><br></pre></td></tr></table></figure></p>\n<p>每当执行检查点时，将调用<code>snapshotState（）</code>。每当用户自定义函数被初始化时，对应的<code>initializeState（）</code>都被调用，或当函数被初始化时，或者当函数实际上从早期的检查点恢复时被调用(The counterpart, initializeState(), is called every time the user-defined function is initialized, be that when the function is first initialized or be that when the function is actually recovering from an earlier checkpoint. )。鉴于此，<code>initializeState（）</code>不仅是初始化不同类型的状态的地方，而且还包括状态恢复逻辑的位置。</p>\n<p>目前支持列表式的<code>Managed Operator State</code>。状态应该是一个可序列化的对象列表，相互间彼此独立，因此可以在扩展时重新分配。换句话说，这些对象可以在非<code>Keyed State</code>中重新分配比较细的粒度。根据状态访问方法，定义了以下重新分配方案：</p>\n<ul>\n<li>均分再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，列表被平分为与并行算子一样多的子列表。每个算子都可以得到一个可以为空或者包含一个或多个元素的子列表。例如，如果并行度为<code>1</code>，算子的检查点状态包含元素<code>element1</code>和<code>element2</code>，将并行度增加到<code>2</code>时，<code>element1</code>在算子实例0上运行，而<code>element2</code>将转至算子实例1。</li>\n<li>合并再分配: 每个算子都返回一个状态元素列表。整个状态在逻辑上是所有列表的连接。在恢复/重新分配时，每个算子都可以获得完整的状态元素列表。</li>\n</ul>\n<p>下面是一个有状态的<code>SinkFunction</code>的例子，它使用<code>CheckpointedFunction</code>在将元素输出到外部之前进行缓冲元素。它演示了基本的均分再分配列表状态：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BufferingSink</span></span></span><br><span class=\"line\"><span class=\"class\">        <span class=\"keyword\">implements</span> <span class=\"title\">SinkFunction</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">Integer</span>&gt;&gt;,</span></span><br><span class=\"line\"><span class=\"class\">                   <span class=\"title\">CheckpointedFunction</span>,</span></span><br><span class=\"line\"><span class=\"class\">                   <span class=\"title\">CheckpointedRestoring</span>&lt;<span class=\"title\">ArrayList</span>&lt;<span class=\"title\">Tuple2</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">Integer</span>&gt;&gt;&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> threshold;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">BufferingSink</span><span class=\"params\">(<span class=\"keyword\">int</span> threshold)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.threshold = threshold;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.bufferedElements = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">invoke</span><span class=\"params\">(Tuple2&lt;String, Integer&gt; value)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        bufferedElements.add(value);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class=\"line\">                <span class=\"comment\">// send it to the sink</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            bufferedElements.clear();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">snapshotState</span><span class=\"params\">(FunctionSnapshotContext context)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        checkpointedState.clear();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class=\"line\">            checkpointedState.add(element);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">initializeState</span><span class=\"params\">(FunctionInitializationContext context)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class=\"line\">            <span class=\"keyword\">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class=\"line\">                <span class=\"string\">\"buffered-elements\"</span>,</span><br><span class=\"line\">                TypeInformation.of(<span class=\"keyword\">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class=\"line\"></span><br><span class=\"line\">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (context.isRestored()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class=\"line\">                bufferedElements.add(element);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">restoreState</span><span class=\"params\">(ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; state)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// this is from the CheckpointedRestoring interface.</span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>.bufferedElements.addAll(state);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class BufferingSink(threshold: Int = 0)</span><br><span class=\"line\">  extends SinkFunction[(String, Int)]</span><br><span class=\"line\">    with CheckpointedFunction</span><br><span class=\"line\">    with CheckpointedRestoring[List[(String, Int)]] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  @transient</span><br><span class=\"line\">  private var checkpointedState: ListState[(String, Int)] = _</span><br><span class=\"line\"></span><br><span class=\"line\">  private val bufferedElements = ListBuffer[(String, Int)]()</span><br><span class=\"line\"></span><br><span class=\"line\">  override def invoke(value: (String, Int)): Unit = &#123;</span><br><span class=\"line\">    bufferedElements += value</span><br><span class=\"line\">    if (bufferedElements.size == threshold) &#123;</span><br><span class=\"line\">      for (element &lt;- bufferedElements) &#123;</span><br><span class=\"line\">        // send it to the sink</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      bufferedElements.clear()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def snapshotState(context: FunctionSnapshotContext): Unit = &#123;</span><br><span class=\"line\">    checkpointedState.clear()</span><br><span class=\"line\">    for (element &lt;- bufferedElements) &#123;</span><br><span class=\"line\">      checkpointedState.add(element)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def initializeState(context: FunctionInitializationContext): Unit = &#123;</span><br><span class=\"line\">    val descriptor = new ListStateDescriptor[(String, Int)](</span><br><span class=\"line\">      &quot;buffered-elements&quot;,</span><br><span class=\"line\">      TypeInformation.of(new TypeHint[(String, Int)]() &#123;&#125;)</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    checkpointedState = context.getOperatorStateStore.getListState(descriptor)</span><br><span class=\"line\"></span><br><span class=\"line\">    if(context.isRestored) &#123;</span><br><span class=\"line\">      for(element &lt;- checkpointedState.get()) &#123;</span><br><span class=\"line\">        bufferedElements += element</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def restoreState(state: List[(String, Int)]): Unit = &#123;</span><br><span class=\"line\">    bufferedElements ++= state</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>initializeState</code>方法以<code>FunctionInitializationContext</code>为参数。这用来初始化非<code>keyed state</code>“容器”。这是一个<code>ListState</code>类型的容器，非<code>keyed state</code>对象将在检查点时存储。</p>\n<p>注意一下状态是如何被初始化，类似于<code>keyed state</code>状态，使用包含状态名称和状态值类型相关信息的<code>StateDescriptor</code>：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class=\"line\">    <span class=\"keyword\">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class=\"line\">        <span class=\"string\">\"buffered-elements\"</span>,</span><br><span class=\"line\">        TypeInformation.of(<span class=\"keyword\">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class=\"line\"></span><br><span class=\"line\">checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val descriptor = new ListStateDescriptor[(String, Long)](</span><br><span class=\"line\">    &quot;buffered-elements&quot;,</span><br><span class=\"line\">    TypeInformation.of(new TypeHint[(String, Long)]() &#123;&#125;)</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">checkpointedState = context.getOperatorStateStore.getListState(descriptor)</span><br></pre></td></tr></table></figure></p>\n<p>状态访问方法的命名约定包含其重新分配模式及其状态结构。 例如，要使用带有联合重新分配方案的列表状态进行恢复，请使用<code>getUnionListState（descriptor）</code>访问状态。如果方法名称不包含重新分配模式，例如 <code>getListState（descriptor）</code>，这表示使用基本的均分重分配方案。</p>\n<p>在初始化容器之后，我们使用上下文的<code>isRestored（）</code>方法来检查失败后是否正在恢复。如果是，即我们正在恢复，将会应用恢复逻辑。</p>\n<p>如修改后的<code>BufferingSink</code>的代码所示，在状态初始化期间恢复的这个<code>ListState</code>被保存在类变量中，以备将来在<code>snapshotState（）</code>中使用。 在那里<code>ListState</code>清除了前一个检查点包含的所有对象，然后用我们想要进行检查点的新对象填充。</p>\n<p><code>Keyed State</code>也可以在<code>initializeState（）</code>方法中初始化。这可以使用提供的<code>FunctionInitializationContext</code>完成。</p>\n<h4 id=\"4-2-ListCheckpointed\"><a href=\"#4-2-ListCheckpointed\" class=\"headerlink\" title=\"4.2 ListCheckpointed\"></a>4.2 ListCheckpointed</h4><p><code>ListCheckpointed</code>接口是<code>CheckpointedFunction</code>进行限制的一种变体，它只支持在恢复时使用均分再分配方案的列表样式状态。还需要实现以下两种方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">List&lt;T&gt; snapshotState(long checkpointId, long timestamp) throws Exception;</span><br><span class=\"line\"></span><br><span class=\"line\">void restoreState(List&lt;T&gt; state) throws Exception;</span><br></pre></td></tr></table></figure>\n<p><code>snapshotState()</code>方法应该返回一个对象列表来进行checkpoint，而<code>restoreState()</code>方法在恢复时必须处理这样一个列表。如果状态是不可重分区的，则可以在<code>snapshotState()</code>中返回一个<code>Collections.singletonList(MY_STATE)</code>。</p>\n<h4 id=\"4-2-1-Stateful-Source-Functions\"><a href=\"#4-2-1-Stateful-Source-Functions\" class=\"headerlink\" title=\"4.2.1 Stateful Source Functions\"></a>4.2.1 Stateful Source Functions</h4><p>与其他算子相比，有状态的数据源需要得到更多的关注。为了能更新状态以及输出集合的原子性（在失败/恢复时需要一次性语义），用户需要从数据源的上下文中获取锁。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CounterSource</span></span></span><br><span class=\"line\"><span class=\"class\">        <span class=\"keyword\">extends</span> <span class=\"title\">RichParallelSourceFunction</span>&lt;<span class=\"title\">Long</span>&gt;</span></span><br><span class=\"line\"><span class=\"class\">        <span class=\"keyword\">implements</span> <span class=\"title\">ListCheckpointed</span>&lt;<span class=\"title\">Long</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**  current offset for exactly once semantics */</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> Long offset;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** flag for job cancellation */</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">volatile</span> <span class=\"keyword\">boolean</span> isRunning = <span class=\"keyword\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> Object lock = ctx.getCheckpointLock();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (isRunning) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// output and state update are atomic</span></span><br><span class=\"line\">            <span class=\"keyword\">synchronized</span> (lock) &#123;</span><br><span class=\"line\">                ctx.collect(offset);</span><br><span class=\"line\">                offset += <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">cancel</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        isRunning = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> List&lt;Long&gt; <span class=\"title\">snapshotState</span><span class=\"params\">(<span class=\"keyword\">long</span> checkpointId, <span class=\"keyword\">long</span> checkpointTimestamp)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> Collections.singletonList(offset);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">restoreState</span><span class=\"params\">(List&lt;Long&gt; state)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (Long s : state)</span><br><span class=\"line\">            offset = s;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class CounterSource</span><br><span class=\"line\">       extends RichParallelSourceFunction[Long]</span><br><span class=\"line\">       with ListCheckpointed[Long] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  @volatile</span><br><span class=\"line\">  private var isRunning = true</span><br><span class=\"line\"></span><br><span class=\"line\">  private var offset = 0L</span><br><span class=\"line\"></span><br><span class=\"line\">  override def run(ctx: SourceFunction.SourceContext[Long]): Unit = &#123;</span><br><span class=\"line\">    val lock = ctx.getCheckpointLock</span><br><span class=\"line\"></span><br><span class=\"line\">    while (isRunning) &#123;</span><br><span class=\"line\">      // output and state update are atomic</span><br><span class=\"line\">      lock.synchronized(&#123;</span><br><span class=\"line\">        ctx.collect(offset)</span><br><span class=\"line\"></span><br><span class=\"line\">        offset += 1</span><br><span class=\"line\">      &#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def cancel(): Unit = isRunning = false</span><br><span class=\"line\"></span><br><span class=\"line\">  override def restoreState(state: util.List[Long]): Unit =</span><br><span class=\"line\">    for (s &lt;- state) &#123;</span><br><span class=\"line\">      offset = s</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def snapshotState(checkpointId: Long, timestamp: Long): util.List[Long] =</span><br><span class=\"line\">    Collections.singletonList(offset)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 内部原理之作业与调度","date":"2018-01-29T09:31:01.000Z","_content":"\n\n### 1. 调度\n\n`Flink`中的执行资源是通过任务槽定义。每个`TaskManager`都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 `MapFunction` 的第n个并行实例和 `ReduceFunction` 的第n个并行实例。请注意，`Flink`经常同时执行连续的任务：对于流式处理程序时刻发生，但是对于批处理程序来说却是经常发生。\n\n下图证明了这一点。考虑一个带有数据源，一个`MapFunction` 和 一个`ReduceFunction` 的程序。数据源和 `MapFunction` 以并行度`4`运行， `ReduceFunction`以并行度`3`运行。流水线由 `Source-Map-Reduce` 序列组成。在具有2个`TaskManager`（每个有3个插槽）的集群上，程序将按照下面的描述执行:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-1.png?raw=true)\n\n在内部，`Flink`通过[SlotSharingGroup](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroup.java)和 [CoLocationGroup]()定义哪些任务可以共享一个槽（允许），哪些任务必须严格放置在同一个槽中。\n\n### 2. JobManager 数据结构\n\n在作业执行期间，`JobManager` 追踪分布式任务，决定何时调度下一个任务（或任务集合），并对完成的任务或执行失败的任务进行相应的处理。\n\n`JobManager` 接收 [JobGraph](https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph)，`JobGraph`表示由算子（[JobVertex](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/JobVertex.java)）和中间结果（[IntermediateDataSet](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/IntermediateDataSet.java)）组成的数据流。每个算子都具有属性，如并行度和执行的代码等。另外，`JobGraph`还有一组附加的库，运行算子代码必需使用这些库。\n\n\n`JobManager` 将 `JobGraph` 转换成 [ExecutionGraph](https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph)。 `ExecutionGraph` 是 `JobGraph` 的并行版本：对于每个 `JobVertex`，对于每个并行子任务它都包含一个  [ExecutionVertex](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java)。例如并行度为100的算子会有一个 `JobVertex` 以及 100个 `ExecutionVertices`。 `ExecutionVertex`跟踪特定子任务的执行状态。`JobVertex` 中所有的 `ExecutionVertices` 都保存在一个 `ExecutionJobVertex` 中，该 [ExecutionJobVertex](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java) 跟踪整个算子的状态。除了顶点之外， `ExecutionGraph` 还包含 [IntermediateResult](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java) 和 [IntermediateResultPartition](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java)。前者跟踪 `IntermediateDataSet` 的状态，后者追踪每个分区的状态。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-2.png?raw=true)\n\n每个 `ExecutionGraph` 都有一个与之相关的作业状态。作业状态表示作业执行的当前状态。\n\n`Flink` 作业首先处于 `ctreated` 状态，然后切换到 `running` 状态，一旦所有工作完成后切换到 `finished` 状态。在出现故障的情况下，作业首先切换到 `failing` 状态，取消所有正在运行任务的地方。如果所有作业顶点已达到最终状态，并且作业不可重新启动，那么作业转换 `failed` 状态。如果作业可以重新启动，那么它将进入 `restarting` 状态。一旦作业重新启动完成后，将进入 `ctreated` 状态。\n\n在用户取消作业的情况下，将进入 `cancelling` 状态。这也需要取消所有正在运行的任务。一旦所有正在运行的任务都达到最终状态，作业将转换到 `cancelled` 状态。\n\n不同于表示全局终端状态以及触发清理工作的 `finished`， `canceled` 和 `failed` 状态，`suspended` 状态只是本地终端。本地终端的意思是作业的执行已在相应的 `JobManager` 上终止，但 `Flink` 集群的另一个 `JobManager` 可从持久性 `HA` 存储中检索作业并重新启动作业。因此，进入 `suspended` 状态的作业将不会完全清理。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-3.png?raw=true)\n\n在 `ExecutionGraph` 的执行过程中，每个并行任务都经历了从 `ctreated` 到 `finished` 或 `failed` 的多个阶段。下图说明了它们之间的状态和可能的转换。任务可以执行多次（例如在故障恢复过程中）。出于这个原因， `ExecutionVertex` 执行跟踪信息保存在 [Execution](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java) 中。 每个 `ExecutionVertex` 都有一个当前的`Execution`，以及之前的`Executions`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-4.png?raw=true)\n\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html\n","source":"_posts/Flink/[Flink]Flink1.4 内部原理之作业与调度.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 内部原理之作业与调度\ndate: 2018-01-29 17:31:01\ntags:\n  - Flink\n  - Flink内部原理\n\ncategories: Flink\npermalink: flink_internals_job_scheduling\n---\n\n\n### 1. 调度\n\n`Flink`中的执行资源是通过任务槽定义。每个`TaskManager`都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 `MapFunction` 的第n个并行实例和 `ReduceFunction` 的第n个并行实例。请注意，`Flink`经常同时执行连续的任务：对于流式处理程序时刻发生，但是对于批处理程序来说却是经常发生。\n\n下图证明了这一点。考虑一个带有数据源，一个`MapFunction` 和 一个`ReduceFunction` 的程序。数据源和 `MapFunction` 以并行度`4`运行， `ReduceFunction`以并行度`3`运行。流水线由 `Source-Map-Reduce` 序列组成。在具有2个`TaskManager`（每个有3个插槽）的集群上，程序将按照下面的描述执行:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-1.png?raw=true)\n\n在内部，`Flink`通过[SlotSharingGroup](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroup.java)和 [CoLocationGroup]()定义哪些任务可以共享一个槽（允许），哪些任务必须严格放置在同一个槽中。\n\n### 2. JobManager 数据结构\n\n在作业执行期间，`JobManager` 追踪分布式任务，决定何时调度下一个任务（或任务集合），并对完成的任务或执行失败的任务进行相应的处理。\n\n`JobManager` 接收 [JobGraph](https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph)，`JobGraph`表示由算子（[JobVertex](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/JobVertex.java)）和中间结果（[IntermediateDataSet](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/IntermediateDataSet.java)）组成的数据流。每个算子都具有属性，如并行度和执行的代码等。另外，`JobGraph`还有一组附加的库，运行算子代码必需使用这些库。\n\n\n`JobManager` 将 `JobGraph` 转换成 [ExecutionGraph](https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph)。 `ExecutionGraph` 是 `JobGraph` 的并行版本：对于每个 `JobVertex`，对于每个并行子任务它都包含一个  [ExecutionVertex](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java)。例如并行度为100的算子会有一个 `JobVertex` 以及 100个 `ExecutionVertices`。 `ExecutionVertex`跟踪特定子任务的执行状态。`JobVertex` 中所有的 `ExecutionVertices` 都保存在一个 `ExecutionJobVertex` 中，该 [ExecutionJobVertex](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java) 跟踪整个算子的状态。除了顶点之外， `ExecutionGraph` 还包含 [IntermediateResult](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java) 和 [IntermediateResultPartition](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java)。前者跟踪 `IntermediateDataSet` 的状态，后者追踪每个分区的状态。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-2.png?raw=true)\n\n每个 `ExecutionGraph` 都有一个与之相关的作业状态。作业状态表示作业执行的当前状态。\n\n`Flink` 作业首先处于 `ctreated` 状态，然后切换到 `running` 状态，一旦所有工作完成后切换到 `finished` 状态。在出现故障的情况下，作业首先切换到 `failing` 状态，取消所有正在运行任务的地方。如果所有作业顶点已达到最终状态，并且作业不可重新启动，那么作业转换 `failed` 状态。如果作业可以重新启动，那么它将进入 `restarting` 状态。一旦作业重新启动完成后，将进入 `ctreated` 状态。\n\n在用户取消作业的情况下，将进入 `cancelling` 状态。这也需要取消所有正在运行的任务。一旦所有正在运行的任务都达到最终状态，作业将转换到 `cancelled` 状态。\n\n不同于表示全局终端状态以及触发清理工作的 `finished`， `canceled` 和 `failed` 状态，`suspended` 状态只是本地终端。本地终端的意思是作业的执行已在相应的 `JobManager` 上终止，但 `Flink` 集群的另一个 `JobManager` 可从持久性 `HA` 存储中检索作业并重新启动作业。因此，进入 `suspended` 状态的作业将不会完全清理。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-3.png?raw=true)\n\n在 `ExecutionGraph` 的执行过程中，每个并行任务都经历了从 `ctreated` 到 `finished` 或 `failed` 的多个阶段。下图说明了它们之间的状态和可能的转换。任务可以执行多次（例如在故障恢复过程中）。出于这个原因， `ExecutionVertex` 执行跟踪信息保存在 [Execution](https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java) 中。 每个 `ExecutionVertex` 都有一个当前的`Execution`，以及之前的`Executions`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-4.png?raw=true)\n\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html\n","slug":"flink_internals_job_scheduling","published":1,"updated":"2018-01-30T04:03:39.799Z","comments":1,"photos":[],"link":"","_id":"cje58tiqd000zordbfo0knnrj","content":"<h3 id=\"1-调度\"><a href=\"#1-调度\" class=\"headerlink\" title=\"1. 调度\"></a>1. 调度</h3><p><code>Flink</code>中的执行资源是通过任务槽定义。每个<code>TaskManager</code>都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 <code>MapFunction</code> 的第n个并行实例和 <code>ReduceFunction</code> 的第n个并行实例。请注意，<code>Flink</code>经常同时执行连续的任务：对于流式处理程序时刻发生，但是对于批处理程序来说却是经常发生。</p>\n<p>下图证明了这一点。考虑一个带有数据源，一个<code>MapFunction</code> 和 一个<code>ReduceFunction</code> 的程序。数据源和 <code>MapFunction</code> 以并行度<code>4</code>运行， <code>ReduceFunction</code>以并行度<code>3</code>运行。流水线由 <code>Source-Map-Reduce</code> 序列组成。在具有2个<code>TaskManager</code>（每个有3个插槽）的集群上，程序将按照下面的描述执行:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-1.png?raw=true\" alt=\"\"></p>\n<p>在内部，<code>Flink</code>通过<a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroup.java\" target=\"_blank\" rel=\"noopener\">SlotSharingGroup</a>和 <a href=\"\">CoLocationGroup</a>定义哪些任务可以共享一个槽（允许），哪些任务必须严格放置在同一个槽中。</p>\n<h3 id=\"2-JobManager-数据结构\"><a href=\"#2-JobManager-数据结构\" class=\"headerlink\" title=\"2. JobManager 数据结构\"></a>2. JobManager 数据结构</h3><p>在作业执行期间，<code>JobManager</code> 追踪分布式任务，决定何时调度下一个任务（或任务集合），并对完成的任务或执行失败的任务进行相应的处理。</p>\n<p><code>JobManager</code> 接收 <a href=\"https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph\" target=\"_blank\" rel=\"noopener\">JobGraph</a>，<code>JobGraph</code>表示由算子（<a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/JobVertex.java\" target=\"_blank\" rel=\"noopener\">JobVertex</a>）和中间结果（<a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/IntermediateDataSet.java\" target=\"_blank\" rel=\"noopener\">IntermediateDataSet</a>）组成的数据流。每个算子都具有属性，如并行度和执行的代码等。另外，<code>JobGraph</code>还有一组附加的库，运行算子代码必需使用这些库。</p>\n<p><code>JobManager</code> 将 <code>JobGraph</code> 转换成 <a href=\"https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph\" target=\"_blank\" rel=\"noopener\">ExecutionGraph</a>。 <code>ExecutionGraph</code> 是 <code>JobGraph</code> 的并行版本：对于每个 <code>JobVertex</code>，对于每个并行子任务它都包含一个  <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java\" target=\"_blank\" rel=\"noopener\">ExecutionVertex</a>。例如并行度为100的算子会有一个 <code>JobVertex</code> 以及 100个 <code>ExecutionVertices</code>。 <code>ExecutionVertex</code>跟踪特定子任务的执行状态。<code>JobVertex</code> 中所有的 <code>ExecutionVertices</code> 都保存在一个 <code>ExecutionJobVertex</code> 中，该 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java\" target=\"_blank\" rel=\"noopener\">ExecutionJobVertex</a> 跟踪整个算子的状态。除了顶点之外， <code>ExecutionGraph</code> 还包含 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java\" target=\"_blank\" rel=\"noopener\">IntermediateResult</a> 和 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java\" target=\"_blank\" rel=\"noopener\">IntermediateResultPartition</a>。前者跟踪 <code>IntermediateDataSet</code> 的状态，后者追踪每个分区的状态。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-2.png?raw=true\" alt=\"\"></p>\n<p>每个 <code>ExecutionGraph</code> 都有一个与之相关的作业状态。作业状态表示作业执行的当前状态。</p>\n<p><code>Flink</code> 作业首先处于 <code>ctreated</code> 状态，然后切换到 <code>running</code> 状态，一旦所有工作完成后切换到 <code>finished</code> 状态。在出现故障的情况下，作业首先切换到 <code>failing</code> 状态，取消所有正在运行任务的地方。如果所有作业顶点已达到最终状态，并且作业不可重新启动，那么作业转换 <code>failed</code> 状态。如果作业可以重新启动，那么它将进入 <code>restarting</code> 状态。一旦作业重新启动完成后，将进入 <code>ctreated</code> 状态。</p>\n<p>在用户取消作业的情况下，将进入 <code>cancelling</code> 状态。这也需要取消所有正在运行的任务。一旦所有正在运行的任务都达到最终状态，作业将转换到 <code>cancelled</code> 状态。</p>\n<p>不同于表示全局终端状态以及触发清理工作的 <code>finished</code>， <code>canceled</code> 和 <code>failed</code> 状态，<code>suspended</code> 状态只是本地终端。本地终端的意思是作业的执行已在相应的 <code>JobManager</code> 上终止，但 <code>Flink</code> 集群的另一个 <code>JobManager</code> 可从持久性 <code>HA</code> 存储中检索作业并重新启动作业。因此，进入 <code>suspended</code> 状态的作业将不会完全清理。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-3.png?raw=true\" alt=\"\"></p>\n<p>在 <code>ExecutionGraph</code> 的执行过程中，每个并行任务都经历了从 <code>ctreated</code> 到 <code>finished</code> 或 <code>failed</code> 的多个阶段。下图说明了它们之间的状态和可能的转换。任务可以执行多次（例如在故障恢复过程中）。出于这个原因， <code>ExecutionVertex</code> 执行跟踪信息保存在 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java\" target=\"_blank\" rel=\"noopener\">Execution</a> 中。 每个 <code>ExecutionVertex</code> 都有一个当前的<code>Execution</code>，以及之前的<code>Executions</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-4.png?raw=true\" alt=\"\"></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-调度\"><a href=\"#1-调度\" class=\"headerlink\" title=\"1. 调度\"></a>1. 调度</h3><p><code>Flink</code>中的执行资源是通过任务槽定义。每个<code>TaskManager</code>都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 <code>MapFunction</code> 的第n个并行实例和 <code>ReduceFunction</code> 的第n个并行实例。请注意，<code>Flink</code>经常同时执行连续的任务：对于流式处理程序时刻发生，但是对于批处理程序来说却是经常发生。</p>\n<p>下图证明了这一点。考虑一个带有数据源，一个<code>MapFunction</code> 和 一个<code>ReduceFunction</code> 的程序。数据源和 <code>MapFunction</code> 以并行度<code>4</code>运行， <code>ReduceFunction</code>以并行度<code>3</code>运行。流水线由 <code>Source-Map-Reduce</code> 序列组成。在具有2个<code>TaskManager</code>（每个有3个插槽）的集群上，程序将按照下面的描述执行:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-1.png?raw=true\" alt=\"\"></p>\n<p>在内部，<code>Flink</code>通过<a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/SlotSharingGroup.java\" target=\"_blank\" rel=\"noopener\">SlotSharingGroup</a>和 <a href=\"\">CoLocationGroup</a>定义哪些任务可以共享一个槽（允许），哪些任务必须严格放置在同一个槽中。</p>\n<h3 id=\"2-JobManager-数据结构\"><a href=\"#2-JobManager-数据结构\" class=\"headerlink\" title=\"2. JobManager 数据结构\"></a>2. JobManager 数据结构</h3><p>在作业执行期间，<code>JobManager</code> 追踪分布式任务，决定何时调度下一个任务（或任务集合），并对完成的任务或执行失败的任务进行相应的处理。</p>\n<p><code>JobManager</code> 接收 <a href=\"https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph\" target=\"_blank\" rel=\"noopener\">JobGraph</a>，<code>JobGraph</code>表示由算子（<a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/JobVertex.java\" target=\"_blank\" rel=\"noopener\">JobVertex</a>）和中间结果（<a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/IntermediateDataSet.java\" target=\"_blank\" rel=\"noopener\">IntermediateDataSet</a>）组成的数据流。每个算子都具有属性，如并行度和执行的代码等。另外，<code>JobGraph</code>还有一组附加的库，运行算子代码必需使用这些库。</p>\n<p><code>JobManager</code> 将 <code>JobGraph</code> 转换成 <a href=\"https://github.com/apache/flink/tree/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph\" target=\"_blank\" rel=\"noopener\">ExecutionGraph</a>。 <code>ExecutionGraph</code> 是 <code>JobGraph</code> 的并行版本：对于每个 <code>JobVertex</code>，对于每个并行子任务它都包含一个  <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java\" target=\"_blank\" rel=\"noopener\">ExecutionVertex</a>。例如并行度为100的算子会有一个 <code>JobVertex</code> 以及 100个 <code>ExecutionVertices</code>。 <code>ExecutionVertex</code>跟踪特定子任务的执行状态。<code>JobVertex</code> 中所有的 <code>ExecutionVertices</code> 都保存在一个 <code>ExecutionJobVertex</code> 中，该 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java\" target=\"_blank\" rel=\"noopener\">ExecutionJobVertex</a> 跟踪整个算子的状态。除了顶点之外， <code>ExecutionGraph</code> 还包含 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResult.java\" target=\"_blank\" rel=\"noopener\">IntermediateResult</a> 和 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java\" target=\"_blank\" rel=\"noopener\">IntermediateResultPartition</a>。前者跟踪 <code>IntermediateDataSet</code> 的状态，后者追踪每个分区的状态。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-2.png?raw=true\" alt=\"\"></p>\n<p>每个 <code>ExecutionGraph</code> 都有一个与之相关的作业状态。作业状态表示作业执行的当前状态。</p>\n<p><code>Flink</code> 作业首先处于 <code>ctreated</code> 状态，然后切换到 <code>running</code> 状态，一旦所有工作完成后切换到 <code>finished</code> 状态。在出现故障的情况下，作业首先切换到 <code>failing</code> 状态，取消所有正在运行任务的地方。如果所有作业顶点已达到最终状态，并且作业不可重新启动，那么作业转换 <code>failed</code> 状态。如果作业可以重新启动，那么它将进入 <code>restarting</code> 状态。一旦作业重新启动完成后，将进入 <code>ctreated</code> 状态。</p>\n<p>在用户取消作业的情况下，将进入 <code>cancelling</code> 状态。这也需要取消所有正在运行的任务。一旦所有正在运行的任务都达到最终状态，作业将转换到 <code>cancelled</code> 状态。</p>\n<p>不同于表示全局终端状态以及触发清理工作的 <code>finished</code>， <code>canceled</code> 和 <code>failed</code> 状态，<code>suspended</code> 状态只是本地终端。本地终端的意思是作业的执行已在相应的 <code>JobManager</code> 上终止，但 <code>Flink</code> 集群的另一个 <code>JobManager</code> 可从持久性 <code>HA</code> 存储中检索作业并重新启动作业。因此，进入 <code>suspended</code> 状态的作业将不会完全清理。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-3.png?raw=true\" alt=\"\"></p>\n<p>在 <code>ExecutionGraph</code> 的执行过程中，每个并行任务都经历了从 <code>ctreated</code> 到 <code>finished</code> 或 <code>failed</code> 的多个阶段。下图说明了它们之间的状态和可能的转换。任务可以执行多次（例如在故障恢复过程中）。出于这个原因， <code>ExecutionVertex</code> 执行跟踪信息保存在 <a href=\"https://github.com/apache/flink/blob/master//flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java\" target=\"_blank\" rel=\"noopener\">Execution</a> 中。 每个 <code>ExecutionVertex</code> 都有一个当前的<code>Execution</code>，以及之前的<code>Executions</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_internals_job_scheduling-4.png?raw=true\" alt=\"\"></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/job_scheduling.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 内部原理之数据流容错","date":"2018-01-24T06:39:01.000Z","_content":"\n### 1. 概述\n\n`Apache Flink`提供了一个容错机制来持续恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态也将最终反映每条来自数据流的记录严格一次`exactly once`。 请注意，有一个开关可以降级为保证至少一次(`least once`)（如下所述）。\n\n容错机制连续生成分布式流数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁生成，而不会对性能造成太大影响。流应用程序的状态存储在可配置的位置（例如主节点或`HDFS`）。\n\n如果应用程序发生故障（由于机器，网络或软件故障），`Flink`会停止分布式流式数据流。然后系统重新启动算子并将其重置为最新的成功检查点。输入流被重置为状态快照的时间点。作为重新启动的并行数据流处理的任何记录都保证不属于先前检查点状态的一部分。\n\n注意:默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参[阅检查点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html)。\n\n为了实现这个机制的保证，数据流源（如消息队列或代理）需要能够将流重放到定义的最近时间点。`Apache Kafka`有这个能力，而`Flink`的Kafka连接器就是利用这个能力。有关`Flink`连接器提供的保证的更多信息，请参阅[数据源和接收器的容错保证](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/guarantees.html)。\n\n因为`Flink`的检查点是通过分布式快照实现的，所以我们交替使用`快照`和`检查点`两个概念。\n\n### 2. Checkpointing\n\n`Flink`的容错机制的核心部分是生成分布式数据流和算子状态的一致性快照。这些快照作为一个一致性检查点，在系统发生故障时可以回溯。`Flink`的生成这些快照的机制在[分布式数据流的轻量级异步快照](https://arxiv.org/abs/1506.08603)中进行详细的描述。它受分布式快照`Chandy-Lamport`算法的启发，并且专门针对`Flink`的执行模型量身定制。\n\n#### 2.1 Barriers\n\n`Flink`分布式快照的一个核心元素是数据流`Barriers`。这些`Barriers`被放入数据流中，并作为数据流的一部分与记录一起流动。`Barriers`永远不会超越记录，严格按照相对顺序流动。`Barriers`将数据流中的记录分成进入当前快照的记录集合和进入下一个快照的记录集合。每个`Barriers`都携带前面快照的ID。`Barriers`不会中断流的流动，因此非常轻。来自不同快照的多个`Barriers`可以同时在流中，这意味着不同快照可以同时发生。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-1.png?raw=true)\n\n`Barriers`在数据流源处被放入的并行数据流。快照`n`放入`Barriers`的位置（我们称之为`Sn`）是快照覆盖数据的源流中的位置。例如，在`Apache Kafka`中，这个位置是分区中最后一个记录的偏移量。该位置`Sn`会报告给检查点协调员（`Flink`的`JobManager`）。\n\n`Barriers`向下游流动。当中间算子从其所有输入流中接收到快照`n`的`Barriers`时，它会将快照`n`的`Barriers`发送到其所有输出流中。一旦`Sink`算子（流式`DAG`的末尾）从其所有输入流中接收到`Barriers n`，就向检查点协调器确认快照`n`。在所有`Sink`确认了快照之后，才被确认已经完成。\n\n一旦快照`n`完成，作业将不会再向数据源询问`Sn`之前的记录，因为那时这些记录（以及它们的后代记录）已经通过了整个数据流拓扑。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-2.png?raw=true)\n\n接收多个输入流的算子需要根据快照`Barriers`对其输入流。上图说明了这一点：\n- 只要算子从一个输入流接收到快照`Barriers n`时，就不能处理来自该数据流的任何记录(译者注:进行缓存)，当从其他输入流中接收到最后一个`Barriers n`时，才开始处理缓存的数据(即对齐的意思)。否则，就会混合属于快照`n`和快照`n + 1`的记录。\n- 报告`Barriers n`的数据流暂时搁置。从这些数据流接收到的记录不会被处理，而是放入输入缓冲区中(例如上图中的`aligning`部分)。\n- 一旦接收到最后一个流的`Barriers n`时，算子才发送所有待发送的记录，然后才发送快照`Barriers n`自己(例如上图中的`checkpoint`部分)。\n- 之后，恢复处理所有输入流中的记录，在处理来自数据流的记录之前优先处理来自输入缓冲区中的记录(例如上图中的`continue`部分)。\n\n#### 2.2 State\n\n当算子包含任何形式的状态时，这个状态也必须是快照的一部分。算子状态有不同的形式：\n- 用户自定义状态：这是由转换函数（如`map（）`或`filter（）`）直接创建和修改的状态。有关详细信息，请参阅[状态概述](http://smartsi.club/2018/01/16/Flink/[Flink]Flink1.4%20%E7%8A%B6%E6%80%81%E6%A6%82%E8%BF%B0/)\n- 系统状态：这种状态指的是作为算子计算一部分的数据缓冲区。这种状态的一个典型例子是窗口缓冲区，在窗口缓冲区中，系统为窗口收集（以及聚合）记录，直到窗口被计算和删除。\n\n在算子收到所有输入流中的所有快照`barriers`以及在`barriers`发送到输出流之前，算子对自己的状态进行快照。这时，At that point, all updates to the state from records before the barriers will have been made, and no updates that depend on records from after the barriers have been applied。由于快照的状态可能较大，因此需要其存储在可配置状态终端`state backend`中。默认情况下，会存储在`JobManager`的内存中，但是在生产环境下，应该配置为分布式可靠存储系统（如`HDFS`）。在状态被存储之后，算子确认检查点，将快照`barriers`发送到输出流，然后继续前行。\n\n生成的快照包含：\n- 对于每个并行流数据源，快照启动时在数据流中的偏移量/位置\n- 对于每个算子，指向作为快照中一部分的状态的指针\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-3.png?raw=true)\n\n#### 2.3 Exactly Once vs. At Least Once\n\n对齐步骤可能会给流式传输程序造成延迟。这个额外的延迟通常大约在几毫秒的数量级，但是我们已经看到一些因为异常值造成的延迟明显增加的情况。对于所有记录需要持续较低延迟（几毫秒）的应用程序而言，`Flink`有一个开关可以在检查点期间跳过流对齐。一旦算子看到每个输入的检查点`barriers`，就会生成检查点快照。\n\n当跳过对齐步骤时，即使在检查点`n`的一些检查点`barriers`到达之后，算子也会继续处理所有输入。这样，在为检查点`n`生成状态快照之前也会处理到属于检查点`n+1`的元素。在恢复时，这些记录将会重复出现，因为它们既包含在检查点`n`的状态快照中，也会在检查点`n`之后作为数据的一部分进行重放。\n\n备注:\n\n对齐仅发生在当算子具有多个输入（例如`join`）或者具有多个输出（在流`repartitioning`/`shuffle`之后）的情况。正因为如此，只有密集并行流操作(only embarrassingly parallel streaming operations)（`map（）`，`flatMap（）`，`filter（）`...）的数据流即使在至少一次模式下也只能提供严格一次。\n\n#### 2.4 异步状态快照\n\n请注意，上述机制意味着算子在状态终端存储状态快照时停止处理输入记录。这种同步状态快照在每次生成快照时都会造成延迟。\n\n可以让算子在存储其状态快照的同时继续处理输入记录，有效地让状态快照在后台异步发生。要做到这一点，算子必须能够产生一个以某种方式存储的状态对象，以至于对算子状态的进一步的修改不会影响状态对象。例如，`copy-on-write`数据结构（如`RocksDB`中使用的数据结构）具有这种功能。\n\n在接收到输入端的检查点`barriers`后，算子启动其状态的异步快照复制。`barriers`立即发送到输出流中，并继续进行正常的流处理。一旦后台复制过程完成，它就会向检查点协调器（JobManager）确认检查点。检查点现在只有在所有`sink`接收到`barriers`并且所有有状态的算子已经确认完成备份（可能在`barriers`到达`sink`之后）。\n\n有关状态快照的详细信息，请参阅[状态终端](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/state_backends.html)。\n\n### 3. 恢复\n\n在这种机制下恢复很简单：一旦失败，`Flink`选择最近完成的检查点`k`。然后系统重新部署整个分布式数据流，并为每个算子提供作为检查点`k`一部分的快照状态。数据源被设置为开始从位置`Sk`读取数据流。例如在`Apache Kafka`中，这意味着告诉消费者从偏移量`Sk`处开始提取数据。\n\n如果增量对状态进行快照，算子将从最新且完整的快照状态开始，然后对该状态应用一系列增量快照更新。\n\n请参阅[重启策略](http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/)了解更多信息。\n\n### 4. 实现算子快照\n\n对算子进行快照，有两部分：同步部分和异步部分。\n\n算子和状态终端将其快照作为`Java FutureTask`。该任务包含的状态同步部分已经完成异步部分挂起。然后异步部分由该检查点的后台线程执行。\n\n算子检查点只是同步返回一个已经完成的`FutureTask`。如果需要执行异步操作，则在`FutureTask的run（）`方法中执行。\n\n任务是可取消的，所以消耗句柄的数据流和其他资源是可以被释放。\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html\n","source":"_posts/Flink/[Flink]Flink1.4 内部原理之数据流容错.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 内部原理之数据流容错\ndate: 2018-01-24 14:39:01\ntags:\n  - Flink\n\ncategories: Flink\npermalink: flink_data_streaming_fault_tolerance\n---\n\n### 1. 概述\n\n`Apache Flink`提供了一个容错机制来持续恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态也将最终反映每条来自数据流的记录严格一次`exactly once`。 请注意，有一个开关可以降级为保证至少一次(`least once`)（如下所述）。\n\n容错机制连续生成分布式流数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁生成，而不会对性能造成太大影响。流应用程序的状态存储在可配置的位置（例如主节点或`HDFS`）。\n\n如果应用程序发生故障（由于机器，网络或软件故障），`Flink`会停止分布式流式数据流。然后系统重新启动算子并将其重置为最新的成功检查点。输入流被重置为状态快照的时间点。作为重新启动的并行数据流处理的任何记录都保证不属于先前检查点状态的一部分。\n\n注意:默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参[阅检查点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html)。\n\n为了实现这个机制的保证，数据流源（如消息队列或代理）需要能够将流重放到定义的最近时间点。`Apache Kafka`有这个能力，而`Flink`的Kafka连接器就是利用这个能力。有关`Flink`连接器提供的保证的更多信息，请参阅[数据源和接收器的容错保证](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/guarantees.html)。\n\n因为`Flink`的检查点是通过分布式快照实现的，所以我们交替使用`快照`和`检查点`两个概念。\n\n### 2. Checkpointing\n\n`Flink`的容错机制的核心部分是生成分布式数据流和算子状态的一致性快照。这些快照作为一个一致性检查点，在系统发生故障时可以回溯。`Flink`的生成这些快照的机制在[分布式数据流的轻量级异步快照](https://arxiv.org/abs/1506.08603)中进行详细的描述。它受分布式快照`Chandy-Lamport`算法的启发，并且专门针对`Flink`的执行模型量身定制。\n\n#### 2.1 Barriers\n\n`Flink`分布式快照的一个核心元素是数据流`Barriers`。这些`Barriers`被放入数据流中，并作为数据流的一部分与记录一起流动。`Barriers`永远不会超越记录，严格按照相对顺序流动。`Barriers`将数据流中的记录分成进入当前快照的记录集合和进入下一个快照的记录集合。每个`Barriers`都携带前面快照的ID。`Barriers`不会中断流的流动，因此非常轻。来自不同快照的多个`Barriers`可以同时在流中，这意味着不同快照可以同时发生。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-1.png?raw=true)\n\n`Barriers`在数据流源处被放入的并行数据流。快照`n`放入`Barriers`的位置（我们称之为`Sn`）是快照覆盖数据的源流中的位置。例如，在`Apache Kafka`中，这个位置是分区中最后一个记录的偏移量。该位置`Sn`会报告给检查点协调员（`Flink`的`JobManager`）。\n\n`Barriers`向下游流动。当中间算子从其所有输入流中接收到快照`n`的`Barriers`时，它会将快照`n`的`Barriers`发送到其所有输出流中。一旦`Sink`算子（流式`DAG`的末尾）从其所有输入流中接收到`Barriers n`，就向检查点协调器确认快照`n`。在所有`Sink`确认了快照之后，才被确认已经完成。\n\n一旦快照`n`完成，作业将不会再向数据源询问`Sn`之前的记录，因为那时这些记录（以及它们的后代记录）已经通过了整个数据流拓扑。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-2.png?raw=true)\n\n接收多个输入流的算子需要根据快照`Barriers`对其输入流。上图说明了这一点：\n- 只要算子从一个输入流接收到快照`Barriers n`时，就不能处理来自该数据流的任何记录(译者注:进行缓存)，当从其他输入流中接收到最后一个`Barriers n`时，才开始处理缓存的数据(即对齐的意思)。否则，就会混合属于快照`n`和快照`n + 1`的记录。\n- 报告`Barriers n`的数据流暂时搁置。从这些数据流接收到的记录不会被处理，而是放入输入缓冲区中(例如上图中的`aligning`部分)。\n- 一旦接收到最后一个流的`Barriers n`时，算子才发送所有待发送的记录，然后才发送快照`Barriers n`自己(例如上图中的`checkpoint`部分)。\n- 之后，恢复处理所有输入流中的记录，在处理来自数据流的记录之前优先处理来自输入缓冲区中的记录(例如上图中的`continue`部分)。\n\n#### 2.2 State\n\n当算子包含任何形式的状态时，这个状态也必须是快照的一部分。算子状态有不同的形式：\n- 用户自定义状态：这是由转换函数（如`map（）`或`filter（）`）直接创建和修改的状态。有关详细信息，请参阅[状态概述](http://smartsi.club/2018/01/16/Flink/[Flink]Flink1.4%20%E7%8A%B6%E6%80%81%E6%A6%82%E8%BF%B0/)\n- 系统状态：这种状态指的是作为算子计算一部分的数据缓冲区。这种状态的一个典型例子是窗口缓冲区，在窗口缓冲区中，系统为窗口收集（以及聚合）记录，直到窗口被计算和删除。\n\n在算子收到所有输入流中的所有快照`barriers`以及在`barriers`发送到输出流之前，算子对自己的状态进行快照。这时，At that point, all updates to the state from records before the barriers will have been made, and no updates that depend on records from after the barriers have been applied。由于快照的状态可能较大，因此需要其存储在可配置状态终端`state backend`中。默认情况下，会存储在`JobManager`的内存中，但是在生产环境下，应该配置为分布式可靠存储系统（如`HDFS`）。在状态被存储之后，算子确认检查点，将快照`barriers`发送到输出流，然后继续前行。\n\n生成的快照包含：\n- 对于每个并行流数据源，快照启动时在数据流中的偏移量/位置\n- 对于每个算子，指向作为快照中一部分的状态的指针\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-3.png?raw=true)\n\n#### 2.3 Exactly Once vs. At Least Once\n\n对齐步骤可能会给流式传输程序造成延迟。这个额外的延迟通常大约在几毫秒的数量级，但是我们已经看到一些因为异常值造成的延迟明显增加的情况。对于所有记录需要持续较低延迟（几毫秒）的应用程序而言，`Flink`有一个开关可以在检查点期间跳过流对齐。一旦算子看到每个输入的检查点`barriers`，就会生成检查点快照。\n\n当跳过对齐步骤时，即使在检查点`n`的一些检查点`barriers`到达之后，算子也会继续处理所有输入。这样，在为检查点`n`生成状态快照之前也会处理到属于检查点`n+1`的元素。在恢复时，这些记录将会重复出现，因为它们既包含在检查点`n`的状态快照中，也会在检查点`n`之后作为数据的一部分进行重放。\n\n备注:\n\n对齐仅发生在当算子具有多个输入（例如`join`）或者具有多个输出（在流`repartitioning`/`shuffle`之后）的情况。正因为如此，只有密集并行流操作(only embarrassingly parallel streaming operations)（`map（）`，`flatMap（）`，`filter（）`...）的数据流即使在至少一次模式下也只能提供严格一次。\n\n#### 2.4 异步状态快照\n\n请注意，上述机制意味着算子在状态终端存储状态快照时停止处理输入记录。这种同步状态快照在每次生成快照时都会造成延迟。\n\n可以让算子在存储其状态快照的同时继续处理输入记录，有效地让状态快照在后台异步发生。要做到这一点，算子必须能够产生一个以某种方式存储的状态对象，以至于对算子状态的进一步的修改不会影响状态对象。例如，`copy-on-write`数据结构（如`RocksDB`中使用的数据结构）具有这种功能。\n\n在接收到输入端的检查点`barriers`后，算子启动其状态的异步快照复制。`barriers`立即发送到输出流中，并继续进行正常的流处理。一旦后台复制过程完成，它就会向检查点协调器（JobManager）确认检查点。检查点现在只有在所有`sink`接收到`barriers`并且所有有状态的算子已经确认完成备份（可能在`barriers`到达`sink`之后）。\n\n有关状态快照的详细信息，请参阅[状态终端](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/state_backends.html)。\n\n### 3. 恢复\n\n在这种机制下恢复很简单：一旦失败，`Flink`选择最近完成的检查点`k`。然后系统重新部署整个分布式数据流，并为每个算子提供作为检查点`k`一部分的快照状态。数据源被设置为开始从位置`Sk`读取数据流。例如在`Apache Kafka`中，这意味着告诉消费者从偏移量`Sk`处开始提取数据。\n\n如果增量对状态进行快照，算子将从最新且完整的快照状态开始，然后对该状态应用一系列增量快照更新。\n\n请参阅[重启策略](http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/)了解更多信息。\n\n### 4. 实现算子快照\n\n对算子进行快照，有两部分：同步部分和异步部分。\n\n算子和状态终端将其快照作为`Java FutureTask`。该任务包含的状态同步部分已经完成异步部分挂起。然后异步部分由该检查点的后台线程执行。\n\n算子检查点只是同步返回一个已经完成的`FutureTask`。如果需要执行异步操作，则在`FutureTask的run（）`方法中执行。\n\n任务是可取消的，所以消耗句柄的数据流和其他资源是可以被释放。\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html\n","slug":"flink_data_streaming_fault_tolerance","published":1,"updated":"2018-01-29T09:28:41.682Z","comments":1,"photos":[],"link":"","_id":"cje58tiqf0012ordb5t2eqiy5","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p><code>Apache Flink</code>提供了一个容错机制来持续恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态也将最终反映每条来自数据流的记录严格一次<code>exactly once</code>。 请注意，有一个开关可以降级为保证至少一次(<code>least once</code>)（如下所述）。</p>\n<p>容错机制连续生成分布式流数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁生成，而不会对性能造成太大影响。流应用程序的状态存储在可配置的位置（例如主节点或<code>HDFS</code>）。</p>\n<p>如果应用程序发生故障（由于机器，网络或软件故障），<code>Flink</code>会停止分布式流式数据流。然后系统重新启动算子并将其重置为最新的成功检查点。输入流被重置为状态快照的时间点。作为重新启动的并行数据流处理的任何记录都保证不属于先前检查点状态的一部分。</p>\n<p>注意:默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html\" target=\"_blank\" rel=\"noopener\">阅检查点</a>。</p>\n<p>为了实现这个机制的保证，数据流源（如消息队列或代理）需要能够将流重放到定义的最近时间点。<code>Apache Kafka</code>有这个能力，而<code>Flink</code>的Kafka连接器就是利用这个能力。有关<code>Flink</code>连接器提供的保证的更多信息，请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/guarantees.html\" target=\"_blank\" rel=\"noopener\">数据源和接收器的容错保证</a>。</p>\n<p>因为<code>Flink</code>的检查点是通过分布式快照实现的，所以我们交替使用<code>快照</code>和<code>检查点</code>两个概念。</p>\n<h3 id=\"2-Checkpointing\"><a href=\"#2-Checkpointing\" class=\"headerlink\" title=\"2. Checkpointing\"></a>2. Checkpointing</h3><p><code>Flink</code>的容错机制的核心部分是生成分布式数据流和算子状态的一致性快照。这些快照作为一个一致性检查点，在系统发生故障时可以回溯。<code>Flink</code>的生成这些快照的机制在<a href=\"https://arxiv.org/abs/1506.08603\" target=\"_blank\" rel=\"noopener\">分布式数据流的轻量级异步快照</a>中进行详细的描述。它受分布式快照<code>Chandy-Lamport</code>算法的启发，并且专门针对<code>Flink</code>的执行模型量身定制。</p>\n<h4 id=\"2-1-Barriers\"><a href=\"#2-1-Barriers\" class=\"headerlink\" title=\"2.1 Barriers\"></a>2.1 Barriers</h4><p><code>Flink</code>分布式快照的一个核心元素是数据流<code>Barriers</code>。这些<code>Barriers</code>被放入数据流中，并作为数据流的一部分与记录一起流动。<code>Barriers</code>永远不会超越记录，严格按照相对顺序流动。<code>Barriers</code>将数据流中的记录分成进入当前快照的记录集合和进入下一个快照的记录集合。每个<code>Barriers</code>都携带前面快照的ID。<code>Barriers</code>不会中断流的流动，因此非常轻。来自不同快照的多个<code>Barriers</code>可以同时在流中，这意味着不同快照可以同时发生。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-1.png?raw=true\" alt=\"\"></p>\n<p><code>Barriers</code>在数据流源处被放入的并行数据流。快照<code>n</code>放入<code>Barriers</code>的位置（我们称之为<code>Sn</code>）是快照覆盖数据的源流中的位置。例如，在<code>Apache Kafka</code>中，这个位置是分区中最后一个记录的偏移量。该位置<code>Sn</code>会报告给检查点协调员（<code>Flink</code>的<code>JobManager</code>）。</p>\n<p><code>Barriers</code>向下游流动。当中间算子从其所有输入流中接收到快照<code>n</code>的<code>Barriers</code>时，它会将快照<code>n</code>的<code>Barriers</code>发送到其所有输出流中。一旦<code>Sink</code>算子（流式<code>DAG</code>的末尾）从其所有输入流中接收到<code>Barriers n</code>，就向检查点协调器确认快照<code>n</code>。在所有<code>Sink</code>确认了快照之后，才被确认已经完成。</p>\n<p>一旦快照<code>n</code>完成，作业将不会再向数据源询问<code>Sn</code>之前的记录，因为那时这些记录（以及它们的后代记录）已经通过了整个数据流拓扑。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-2.png?raw=true\" alt=\"\"></p>\n<p>接收多个输入流的算子需要根据快照<code>Barriers</code>对其输入流。上图说明了这一点：</p>\n<ul>\n<li>只要算子从一个输入流接收到快照<code>Barriers n</code>时，就不能处理来自该数据流的任何记录(译者注:进行缓存)，当从其他输入流中接收到最后一个<code>Barriers n</code>时，才开始处理缓存的数据(即对齐的意思)。否则，就会混合属于快照<code>n</code>和快照<code>n + 1</code>的记录。</li>\n<li>报告<code>Barriers n</code>的数据流暂时搁置。从这些数据流接收到的记录不会被处理，而是放入输入缓冲区中(例如上图中的<code>aligning</code>部分)。</li>\n<li>一旦接收到最后一个流的<code>Barriers n</code>时，算子才发送所有待发送的记录，然后才发送快照<code>Barriers n</code>自己(例如上图中的<code>checkpoint</code>部分)。</li>\n<li>之后，恢复处理所有输入流中的记录，在处理来自数据流的记录之前优先处理来自输入缓冲区中的记录(例如上图中的<code>continue</code>部分)。</li>\n</ul>\n<h4 id=\"2-2-State\"><a href=\"#2-2-State\" class=\"headerlink\" title=\"2.2 State\"></a>2.2 State</h4><p>当算子包含任何形式的状态时，这个状态也必须是快照的一部分。算子状态有不同的形式：</p>\n<ul>\n<li>用户自定义状态：这是由转换函数（如<code>map（）</code>或<code>filter（）</code>）直接创建和修改的状态。有关详细信息，请参阅<a href=\"http://smartsi.club/2018/01/16/Flink/[Flink]Flink1.4%20%E7%8A%B6%E6%80%81%E6%A6%82%E8%BF%B0/\" target=\"_blank\" rel=\"noopener\">状态概述</a></li>\n<li>系统状态：这种状态指的是作为算子计算一部分的数据缓冲区。这种状态的一个典型例子是窗口缓冲区，在窗口缓冲区中，系统为窗口收集（以及聚合）记录，直到窗口被计算和删除。</li>\n</ul>\n<p>在算子收到所有输入流中的所有快照<code>barriers</code>以及在<code>barriers</code>发送到输出流之前，算子对自己的状态进行快照。这时，At that point, all updates to the state from records before the barriers will have been made, and no updates that depend on records from after the barriers have been applied。由于快照的状态可能较大，因此需要其存储在可配置状态终端<code>state backend</code>中。默认情况下，会存储在<code>JobManager</code>的内存中，但是在生产环境下，应该配置为分布式可靠存储系统（如<code>HDFS</code>）。在状态被存储之后，算子确认检查点，将快照<code>barriers</code>发送到输出流，然后继续前行。</p>\n<p>生成的快照包含：</p>\n<ul>\n<li>对于每个并行流数据源，快照启动时在数据流中的偏移量/位置</li>\n<li>对于每个算子，指向作为快照中一部分的状态的指针</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-Exactly-Once-vs-At-Least-Once\"><a href=\"#2-3-Exactly-Once-vs-At-Least-Once\" class=\"headerlink\" title=\"2.3 Exactly Once vs. At Least Once\"></a>2.3 Exactly Once vs. At Least Once</h4><p>对齐步骤可能会给流式传输程序造成延迟。这个额外的延迟通常大约在几毫秒的数量级，但是我们已经看到一些因为异常值造成的延迟明显增加的情况。对于所有记录需要持续较低延迟（几毫秒）的应用程序而言，<code>Flink</code>有一个开关可以在检查点期间跳过流对齐。一旦算子看到每个输入的检查点<code>barriers</code>，就会生成检查点快照。</p>\n<p>当跳过对齐步骤时，即使在检查点<code>n</code>的一些检查点<code>barriers</code>到达之后，算子也会继续处理所有输入。这样，在为检查点<code>n</code>生成状态快照之前也会处理到属于检查点<code>n+1</code>的元素。在恢复时，这些记录将会重复出现，因为它们既包含在检查点<code>n</code>的状态快照中，也会在检查点<code>n</code>之后作为数据的一部分进行重放。</p>\n<p>备注:</p>\n<p>对齐仅发生在当算子具有多个输入（例如<code>join</code>）或者具有多个输出（在流<code>repartitioning</code>/<code>shuffle</code>之后）的情况。正因为如此，只有密集并行流操作(only embarrassingly parallel streaming operations)（<code>map（）</code>，<code>flatMap（）</code>，<code>filter（）</code>…）的数据流即使在至少一次模式下也只能提供严格一次。</p>\n<h4 id=\"2-4-异步状态快照\"><a href=\"#2-4-异步状态快照\" class=\"headerlink\" title=\"2.4 异步状态快照\"></a>2.4 异步状态快照</h4><p>请注意，上述机制意味着算子在状态终端存储状态快照时停止处理输入记录。这种同步状态快照在每次生成快照时都会造成延迟。</p>\n<p>可以让算子在存储其状态快照的同时继续处理输入记录，有效地让状态快照在后台异步发生。要做到这一点，算子必须能够产生一个以某种方式存储的状态对象，以至于对算子状态的进一步的修改不会影响状态对象。例如，<code>copy-on-write</code>数据结构（如<code>RocksDB</code>中使用的数据结构）具有这种功能。</p>\n<p>在接收到输入端的检查点<code>barriers</code>后，算子启动其状态的异步快照复制。<code>barriers</code>立即发送到输出流中，并继续进行正常的流处理。一旦后台复制过程完成，它就会向检查点协调器（JobManager）确认检查点。检查点现在只有在所有<code>sink</code>接收到<code>barriers</code>并且所有有状态的算子已经确认完成备份（可能在<code>barriers</code>到达<code>sink</code>之后）。</p>\n<p>有关状态快照的详细信息，请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/state_backends.html\" target=\"_blank\" rel=\"noopener\">状态终端</a>。</p>\n<h3 id=\"3-恢复\"><a href=\"#3-恢复\" class=\"headerlink\" title=\"3. 恢复\"></a>3. 恢复</h3><p>在这种机制下恢复很简单：一旦失败，<code>Flink</code>选择最近完成的检查点<code>k</code>。然后系统重新部署整个分布式数据流，并为每个算子提供作为检查点<code>k</code>一部分的快照状态。数据源被设置为开始从位置<code>Sk</code>读取数据流。例如在<code>Apache Kafka</code>中，这意味着告诉消费者从偏移量<code>Sk</code>处开始提取数据。</p>\n<p>如果增量对状态进行快照，算子将从最新且完整的快照状态开始，然后对该状态应用一系列增量快照更新。</p>\n<p>请参阅<a href=\"http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/\" target=\"_blank\" rel=\"noopener\">重启策略</a>了解更多信息。</p>\n<h3 id=\"4-实现算子快照\"><a href=\"#4-实现算子快照\" class=\"headerlink\" title=\"4. 实现算子快照\"></a>4. 实现算子快照</h3><p>对算子进行快照，有两部分：同步部分和异步部分。</p>\n<p>算子和状态终端将其快照作为<code>Java FutureTask</code>。该任务包含的状态同步部分已经完成异步部分挂起。然后异步部分由该检查点的后台线程执行。</p>\n<p>算子检查点只是同步返回一个已经完成的<code>FutureTask</code>。如果需要执行异步操作，则在<code>FutureTask的run（）</code>方法中执行。</p>\n<p>任务是可取消的，所以消耗句柄的数据流和其他资源是可以被释放。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p><code>Apache Flink</code>提供了一个容错机制来持续恢复数据流应用程序的状态。该机制确保即使在出现故障的情况下，程序的状态也将最终反映每条来自数据流的记录严格一次<code>exactly once</code>。 请注意，有一个开关可以降级为保证至少一次(<code>least once</code>)（如下所述）。</p>\n<p>容错机制连续生成分布式流数据流的快照。对于状态较小的流式应用程序，这些快照非常轻量级，可以频繁生成，而不会对性能造成太大影响。流应用程序的状态存储在可配置的位置（例如主节点或<code>HDFS</code>）。</p>\n<p>如果应用程序发生故障（由于机器，网络或软件故障），<code>Flink</code>会停止分布式流式数据流。然后系统重新启动算子并将其重置为最新的成功检查点。输入流被重置为状态快照的时间点。作为重新启动的并行数据流处理的任何记录都保证不属于先前检查点状态的一部分。</p>\n<p>注意:默认情况下，检查点被禁用。有关如何启用和配置检查点的详细信息，请参<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html\" target=\"_blank\" rel=\"noopener\">阅检查点</a>。</p>\n<p>为了实现这个机制的保证，数据流源（如消息队列或代理）需要能够将流重放到定义的最近时间点。<code>Apache Kafka</code>有这个能力，而<code>Flink</code>的Kafka连接器就是利用这个能力。有关<code>Flink</code>连接器提供的保证的更多信息，请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/guarantees.html\" target=\"_blank\" rel=\"noopener\">数据源和接收器的容错保证</a>。</p>\n<p>因为<code>Flink</code>的检查点是通过分布式快照实现的，所以我们交替使用<code>快照</code>和<code>检查点</code>两个概念。</p>\n<h3 id=\"2-Checkpointing\"><a href=\"#2-Checkpointing\" class=\"headerlink\" title=\"2. Checkpointing\"></a>2. Checkpointing</h3><p><code>Flink</code>的容错机制的核心部分是生成分布式数据流和算子状态的一致性快照。这些快照作为一个一致性检查点，在系统发生故障时可以回溯。<code>Flink</code>的生成这些快照的机制在<a href=\"https://arxiv.org/abs/1506.08603\" target=\"_blank\" rel=\"noopener\">分布式数据流的轻量级异步快照</a>中进行详细的描述。它受分布式快照<code>Chandy-Lamport</code>算法的启发，并且专门针对<code>Flink</code>的执行模型量身定制。</p>\n<h4 id=\"2-1-Barriers\"><a href=\"#2-1-Barriers\" class=\"headerlink\" title=\"2.1 Barriers\"></a>2.1 Barriers</h4><p><code>Flink</code>分布式快照的一个核心元素是数据流<code>Barriers</code>。这些<code>Barriers</code>被放入数据流中，并作为数据流的一部分与记录一起流动。<code>Barriers</code>永远不会超越记录，严格按照相对顺序流动。<code>Barriers</code>将数据流中的记录分成进入当前快照的记录集合和进入下一个快照的记录集合。每个<code>Barriers</code>都携带前面快照的ID。<code>Barriers</code>不会中断流的流动，因此非常轻。来自不同快照的多个<code>Barriers</code>可以同时在流中，这意味着不同快照可以同时发生。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-1.png?raw=true\" alt=\"\"></p>\n<p><code>Barriers</code>在数据流源处被放入的并行数据流。快照<code>n</code>放入<code>Barriers</code>的位置（我们称之为<code>Sn</code>）是快照覆盖数据的源流中的位置。例如，在<code>Apache Kafka</code>中，这个位置是分区中最后一个记录的偏移量。该位置<code>Sn</code>会报告给检查点协调员（<code>Flink</code>的<code>JobManager</code>）。</p>\n<p><code>Barriers</code>向下游流动。当中间算子从其所有输入流中接收到快照<code>n</code>的<code>Barriers</code>时，它会将快照<code>n</code>的<code>Barriers</code>发送到其所有输出流中。一旦<code>Sink</code>算子（流式<code>DAG</code>的末尾）从其所有输入流中接收到<code>Barriers n</code>，就向检查点协调器确认快照<code>n</code>。在所有<code>Sink</code>确认了快照之后，才被确认已经完成。</p>\n<p>一旦快照<code>n</code>完成，作业将不会再向数据源询问<code>Sn</code>之前的记录，因为那时这些记录（以及它们的后代记录）已经通过了整个数据流拓扑。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-2.png?raw=true\" alt=\"\"></p>\n<p>接收多个输入流的算子需要根据快照<code>Barriers</code>对其输入流。上图说明了这一点：</p>\n<ul>\n<li>只要算子从一个输入流接收到快照<code>Barriers n</code>时，就不能处理来自该数据流的任何记录(译者注:进行缓存)，当从其他输入流中接收到最后一个<code>Barriers n</code>时，才开始处理缓存的数据(即对齐的意思)。否则，就会混合属于快照<code>n</code>和快照<code>n + 1</code>的记录。</li>\n<li>报告<code>Barriers n</code>的数据流暂时搁置。从这些数据流接收到的记录不会被处理，而是放入输入缓冲区中(例如上图中的<code>aligning</code>部分)。</li>\n<li>一旦接收到最后一个流的<code>Barriers n</code>时，算子才发送所有待发送的记录，然后才发送快照<code>Barriers n</code>自己(例如上图中的<code>checkpoint</code>部分)。</li>\n<li>之后，恢复处理所有输入流中的记录，在处理来自数据流的记录之前优先处理来自输入缓冲区中的记录(例如上图中的<code>continue</code>部分)。</li>\n</ul>\n<h4 id=\"2-2-State\"><a href=\"#2-2-State\" class=\"headerlink\" title=\"2.2 State\"></a>2.2 State</h4><p>当算子包含任何形式的状态时，这个状态也必须是快照的一部分。算子状态有不同的形式：</p>\n<ul>\n<li>用户自定义状态：这是由转换函数（如<code>map（）</code>或<code>filter（）</code>）直接创建和修改的状态。有关详细信息，请参阅<a href=\"http://smartsi.club/2018/01/16/Flink/[Flink]Flink1.4%20%E7%8A%B6%E6%80%81%E6%A6%82%E8%BF%B0/\" target=\"_blank\" rel=\"noopener\">状态概述</a></li>\n<li>系统状态：这种状态指的是作为算子计算一部分的数据缓冲区。这种状态的一个典型例子是窗口缓冲区，在窗口缓冲区中，系统为窗口收集（以及聚合）记录，直到窗口被计算和删除。</li>\n</ul>\n<p>在算子收到所有输入流中的所有快照<code>barriers</code>以及在<code>barriers</code>发送到输出流之前，算子对自己的状态进行快照。这时，At that point, all updates to the state from records before the barriers will have been made, and no updates that depend on records from after the barriers have been applied。由于快照的状态可能较大，因此需要其存储在可配置状态终端<code>state backend</code>中。默认情况下，会存储在<code>JobManager</code>的内存中，但是在生产环境下，应该配置为分布式可靠存储系统（如<code>HDFS</code>）。在状态被存储之后，算子确认检查点，将快照<code>barriers</code>发送到输出流，然后继续前行。</p>\n<p>生成的快照包含：</p>\n<ul>\n<li>对于每个并行流数据源，快照启动时在数据流中的偏移量/位置</li>\n<li>对于每个算子，指向作为快照中一部分的状态的指针</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E5%AE%B9%E9%94%99-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-Exactly-Once-vs-At-Least-Once\"><a href=\"#2-3-Exactly-Once-vs-At-Least-Once\" class=\"headerlink\" title=\"2.3 Exactly Once vs. At Least Once\"></a>2.3 Exactly Once vs. At Least Once</h4><p>对齐步骤可能会给流式传输程序造成延迟。这个额外的延迟通常大约在几毫秒的数量级，但是我们已经看到一些因为异常值造成的延迟明显增加的情况。对于所有记录需要持续较低延迟（几毫秒）的应用程序而言，<code>Flink</code>有一个开关可以在检查点期间跳过流对齐。一旦算子看到每个输入的检查点<code>barriers</code>，就会生成检查点快照。</p>\n<p>当跳过对齐步骤时，即使在检查点<code>n</code>的一些检查点<code>barriers</code>到达之后，算子也会继续处理所有输入。这样，在为检查点<code>n</code>生成状态快照之前也会处理到属于检查点<code>n+1</code>的元素。在恢复时，这些记录将会重复出现，因为它们既包含在检查点<code>n</code>的状态快照中，也会在检查点<code>n</code>之后作为数据的一部分进行重放。</p>\n<p>备注:</p>\n<p>对齐仅发生在当算子具有多个输入（例如<code>join</code>）或者具有多个输出（在流<code>repartitioning</code>/<code>shuffle</code>之后）的情况。正因为如此，只有密集并行流操作(only embarrassingly parallel streaming operations)（<code>map（）</code>，<code>flatMap（）</code>，<code>filter（）</code>…）的数据流即使在至少一次模式下也只能提供严格一次。</p>\n<h4 id=\"2-4-异步状态快照\"><a href=\"#2-4-异步状态快照\" class=\"headerlink\" title=\"2.4 异步状态快照\"></a>2.4 异步状态快照</h4><p>请注意，上述机制意味着算子在状态终端存储状态快照时停止处理输入记录。这种同步状态快照在每次生成快照时都会造成延迟。</p>\n<p>可以让算子在存储其状态快照的同时继续处理输入记录，有效地让状态快照在后台异步发生。要做到这一点，算子必须能够产生一个以某种方式存储的状态对象，以至于对算子状态的进一步的修改不会影响状态对象。例如，<code>copy-on-write</code>数据结构（如<code>RocksDB</code>中使用的数据结构）具有这种功能。</p>\n<p>在接收到输入端的检查点<code>barriers</code>后，算子启动其状态的异步快照复制。<code>barriers</code>立即发送到输出流中，并继续进行正常的流处理。一旦后台复制过程完成，它就会向检查点协调器（JobManager）确认检查点。检查点现在只有在所有<code>sink</code>接收到<code>barriers</code>并且所有有状态的算子已经确认完成备份（可能在<code>barriers</code>到达<code>sink</code>之后）。</p>\n<p>有关状态快照的详细信息，请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/state_backends.html\" target=\"_blank\" rel=\"noopener\">状态终端</a>。</p>\n<h3 id=\"3-恢复\"><a href=\"#3-恢复\" class=\"headerlink\" title=\"3. 恢复\"></a>3. 恢复</h3><p>在这种机制下恢复很简单：一旦失败，<code>Flink</code>选择最近完成的检查点<code>k</code>。然后系统重新部署整个分布式数据流，并为每个算子提供作为检查点<code>k</code>一部分的快照状态。数据源被设置为开始从位置<code>Sk</code>读取数据流。例如在<code>Apache Kafka</code>中，这意味着告诉消费者从偏移量<code>Sk</code>处开始提取数据。</p>\n<p>如果增量对状态进行快照，算子将从最新且完整的快照状态开始，然后对该状态应用一系列增量快照更新。</p>\n<p>请参阅<a href=\"http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/\" target=\"_blank\" rel=\"noopener\">重启策略</a>了解更多信息。</p>\n<h3 id=\"4-实现算子快照\"><a href=\"#4-实现算子快照\" class=\"headerlink\" title=\"4. 实现算子快照\"></a>4. 实现算子快照</h3><p>对算子进行快照，有两部分：同步部分和异步部分。</p>\n<p>算子和状态终端将其快照作为<code>Java FutureTask</code>。该任务包含的状态同步部分已经完成异步部分挂起。然后异步部分由该检查点的后台线程执行。</p>\n<p>算子检查点只是同步返回一个已经完成的<code>FutureTask</code>。如果需要执行异步操作，则在<code>FutureTask的run（）</code>方法中执行。</p>\n<p>任务是可取消的，所以消耗句柄的数据流和其他资源是可以被释放。</p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 命令行界面","date":"2018-01-30T07:30:17.000Z","_content":"\n### 1. 概述\n\n`Flink` 提供了一个命令行接口（CLI）用来运行打成JAR包的程序，并且可以控制程序的运行。命令行接口在 `Flink` 安装完之后即可拥有，本地单节点或是分布式部署安装都会有命令行接口。命令行接口启动脚本是 `$FLINK_HOME/bin` 目录下的 `flink` 脚本， 默认情况下会连接运行中的 `Flink master(JobManager)`， `JobManager` 的启动脚本与 `CLI` 在同一安装目录下。\n\n使用命令行接口的前提条件是 `JobManager` 已经被启动(通过`$FLINK_HOME/bin/start-local.sh` 或是 `$FLINK_HOME/bin/start-cluster.sh`)或是 `Flink YARN` 环境可用。 `JobManager` 可以通过如下命令启动:\n```\n$FLINK_HOME/bin/start-local.sh\n或\n$FLINK_HOME/bin/start-cluster.sh\n```\n### 2. Example\n\n(1) 运行示例程序，不传参数：\n```\n./bin/flink run ./examples/batch/WordCount.jar\n```\n(2) 运行示例程序，带输入和输出文件参数：\n```\n./bin/flink run ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(3) 运行示例程序，带输入和输出文件参数,并设置16个并发度：\n```\n./bin/flink run -p 16 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(4) 运行示例程序，并禁止 `Flink` 输出日志\n```\n./bin/flink run -q ./examples/batch/WordCount.jar\n```\n(5) 以独立(detached)模式运行示例程序\n```\n./bin/flink run -d ./examples/batch/WordCount.jar\n```\n(6) 在指定 `JobManager` 上运行示例程序\n```\n./bin/flink run -m myJMHost:6123 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(7) 运行示例程序，指定程序入口类(Main方法所在类)：\n```\n./bin/flink run -c org.apache.flink.examples.java.wordcount.WordCount ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(8) 运行示例程序，使用带有2个 `TaskManager` 的[per-job YARN 集群](https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/yarn_setup.html#run-a-single-flink-job-on-hadoop-yarn)\n```\n./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar --input hdfs:///xiaosi/a.txt --output hdfs:///xiaosi/result.txt\n```\n(9) 以JSON格式输出 `WordCount` 示例程序优化执行计划：\n```\n./bin/flink info ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(10) 列出已经调度的和正在运行的Job(包含Job ID信息)\n```\n./bin/flink list\n```\n(11) 列出已经调度的Job(包含Job ID信息)\n```\n./bin/flink list -s\n```\n(13) 列出正在运行的Job(包含Job ID信息)\n```\n./bin/flink list -r\n```\n(14) 列出在Flink YARN中运行Job\n```\n./bin/flink list -m yarn-cluster -yid <yarnApplicationID> -r\n```\n(15) 取消一个Job\n```\n./bin/flink cancel <jobID>\n```\n(16) 取消一个带有保存点(savepoint)的Job\n```\n./bin/flink cancel -s [targetDirectory] <jobID>\n```\n(17) 停止一个Job(只适用于流计算Job)\n```\n./bin/flink stop <jobID>\n```\n\n取消和停止一个作业的区别如下：\n- 调用取消作业时，作业中的算子立即收到一个调用`cancel()`方法的指令以尽快取消它们。如果算子在调用取消操作后没有停止，`Flink` 将定期开启中断线程来取消作业直到作业停止。\n- 停止作业是一种停止正在运行的流作业的更加优雅的方法。停止仅适用于使用实现`StoppableFunction`接口的数据源的那些作业。当用户请求停止作业时，所有数据源将收到调用`stop()`方法指令。但是作业还是会继续运行，直到所有数据源正确关闭。这允许作业处理完所有正在传输的数据(inflight data)。\n\n### 3. 保存点\n\n保存点通过命令行客户端进行控制：\n\n#### 3.1 触发保存点\n\n```\n./bin/flink savepoint <jobID> [savepointDirectory]\n```\n这会触发作业ID为`jobId`的保存点，并返回创建的保存点的路径。你需要此路径来还原和处理保存点。\n\n此外，你可以选择指定一个目标文件系统目录来存储保存点。目录可以被 `JobManager` 访问。\n\n如果你不指定目标目录，则需要配置默认目录（请参阅[保存点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration)）。 否则，触发保存点将失败。\n\n#### 3.2 使用YARN触发保存点\n\n```\n./bin/flink savepoint <jobId> [savepointDirectory] -yid <yarnAppId>\n```\n\n这将触发作业ID为 `jobId` 以及 `YARN` 应用程序ID为 `yarnAppId` 的保存点，并返回创建的保存点的路径。\n\n其他一切与上面的触发保存点中描述的相同。\n\n#### 3.3 根据保存点取消Job\n\n你可以自动触发一个保存点并取消作业:\n```\n./bin/flink cancel -s  [savepointDirectory] <jobID>\n```\n如果没有配置保存点目录，则需要为 `Flink` 安装配置默认的保存点目录(请参阅[保存点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration)）。\n\n只有保存点触发成功，作业才被取消\n\n#### 3.4 恢复保存点\n\n```\n./bin/flink run -s <savepointPath> ...\n```\n这个`run`命令提交作业时带有一个保存点标记，这使得程序可以从保存点中恢复状态。保存点路径是通过保存点触发命令得到的。\n\n默认情况下，我们尝试将所有的保存点状态与正在提交的作业进行匹配。如果你想允许跳过无法使用新作业恢复的保存点状态，则可以设置`allowNonRestoredState`标志。当保存点触发时，如果想从程序中删除一个算子（作为程序的一部分），并且仍然想要使用这个保存点，则需要允许这一点。\n\n```\n./bin/flink run -s <savepointPath> -n ...\n```\n如果想从程序中删除算子(作为保存点一部分的)，这时会非常有用。\n\n#### 3.5 销毁保存点\n\n```\n./bin/flink savepoint -d <savepointPath>\n```\n销毁一个保存点同样需要一个路径。这个保存点路径是通过保存点触发命令得到的。\n\n如果使用自定义状态实例（例如自定义 `reducing` 状态或 `RocksDB` 状态），则必须指定程序JAR的路径以及被触发的保存点，以便使用用户代码类加载器来销毁保存点：\n```\n./bin/flink savepoint -d <savepointPath> -j <jarFile>\n```\n否则，你将遇到 `ClassNotFoundException`。\n\n### 4. 用法\n\n下面是Flink命令行接口的用法:\n```\nxiaosi@yoona:~/qunar/company/opt/flink-1.3.2$ ./bin/flink\n./flink <ACTION> [OPTIONS] [ARGUMENTS]\n\nThe following actions are available:\n\nAction \"run\" compiles and runs a program.\n\n  Syntax: run [OPTIONS] <jar-file> <arguments>\n  \"run\" action options:\n     -c,--class <classname>                         Class with the program entry\n                                                    point (\"main\" method or\n                                                    \"getPlan()\" method. Only\n                                                    needed if the JAR file does\n                                                    not specify the class in its\n                                                    manifest.\n     -C,--classpath <url>                           Adds a URL to each user code\n                                                    classloader  on all nodes in\n                                                    the cluster. The paths must\n                                                    specify a protocol (e.g.\n                                                    file://) and be accessible\n                                                    on all nodes (e.g. by means\n                                                    of a NFS share). You can use\n                                                    this option multiple times\n                                                    for specifying more than one\n                                                    URL. The protocol must be\n                                                    supported by the {@link\n                                                    java.net.URLClassLoader}.\n     -d,--detached                                  If present, runs the job in\n                                                    detached mode\n     -m,--jobmanager <host:port>                    Address of the JobManager\n                                                    (master) to which to\n                                                    connect. Use this flag to\n                                                    connect to a different\n                                                    JobManager than the one\n                                                    specified in the\n                                                    configuration.\n     -n,--allowNonRestoredState                     Allow to skip savepoint\n                                                    state that cannot be\n                                                    restored. You need to allow\n                                                    this if you removed an\n                                                    operator from your program\n                                                    that was part of the program\n                                                    when the savepoint was\n                                                    triggered.\n     -p,--parallelism <parallelism>                 The parallelism with which\n                                                    to run the program. Optional\n                                                    flag to override the default\n                                                    value specified in the\n                                                    configuration.\n     -q,--sysoutLogging                             If present, suppress logging\n                                                    output to standard out.\n     -s,--fromSavepoint <savepointPath>             Path to a savepoint to\n                                                    restore the job from (for\n                                                    example\n                                                    hdfs:///flink/savepoint-1537\n                                                    ).\n     -z,--zookeeperNamespace <zookeeperNamespace>   Namespace to create the\n                                                    Zookeeper sub-paths for high\n                                                    availability mode\n  Options for yarn-cluster mode:\n     -yD <arg>                            Dynamic properties\n     -yd,--yarndetached                   Start detached\n     -yid,--yarnapplicationId <arg>       Attach to running YARN session\n     -yj,--yarnjar <arg>                  Path to Flink jar file\n     -yjm,--yarnjobManagerMemory <arg>    Memory for JobManager Container [in\n                                          MB]\n     -yn,--yarncontainer <arg>            Number of YARN container to allocate\n                                          (=Number of Task Managers)\n     -ynm,--yarnname <arg>                Set a custom name for the application\n                                          on YARN\n     -yq,--yarnquery                      Display available YARN resources\n                                          (memory, cores)\n     -yqu,--yarnqueue <arg>               Specify YARN queue.\n     -ys,--yarnslots <arg>                Number of slots per TaskManager\n     -yst,--yarnstreaming                 Start Flink in streaming mode\n     -yt,--yarnship <arg>                 Ship files in the specified directory\n                                          (t for transfer)\n     -ytm,--yarntaskManagerMemory <arg>   Memory per TaskManager Container [in\n                                          MB]\n     -yz,--yarnzookeeperNamespace <arg>   Namespace to create the Zookeeper\n                                          sub-paths for high availability mode\n\n  Options for yarn mode:\n     -ya,--yarnattached                   Start attached\n     -yD <arg>                            Dynamic properties\n     -yj,--yarnjar <arg>                  Path to Flink jar file\n     -yjm,--yarnjobManagerMemory <arg>    Memory for JobManager Container [in\n                                          MB]\n     -yqu,--yarnqueue <arg>               Specify YARN queue.\n     -yt,--yarnship <arg>                 Ship files in the specified directory\n                                          (t for transfer)\n     -yz,--yarnzookeeperNamespace <arg>   Namespace to create the Zookeeper\n                                          sub-paths for high availability mode\n\n\n\nAction \"info\" shows the optimized execution plan of the program (JSON).\n\n  Syntax: info [OPTIONS] <jar-file> <arguments>\n  \"info\" action options:\n     -c,--class <classname>           Class with the program entry point (\"main\"\n                                      method or \"getPlan()\" method. Only needed\n                                      if the JAR file does not specify the class\n                                      in its manifest.\n     -p,--parallelism <parallelism>   The parallelism with which to run the\n                                      program. Optional flag to override the\n                                      default value specified in the\n                                      configuration.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"list\" lists running and scheduled programs.\n\n  Syntax: list [OPTIONS]\n  \"list\" action options:\n     -m,--jobmanager <host:port>   Address of the JobManager (master) to which\n                                   to connect. Use this flag to connect to a\n                                   different JobManager than the one specified\n                                   in the configuration.\n     -r,--running                  Show only running programs and their JobIDs\n     -s,--scheduled                Show only scheduled programs and their JobIDs\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"stop\" stops a running program (streaming jobs only).\n\n  Syntax: stop [OPTIONS] <Job ID>\n  \"stop\" action options:\n     -m,--jobmanager <host:port>   Address of the JobManager (master) to which\n                                   to connect. Use this flag to connect to a\n                                   different JobManager than the one specified\n                                   in the configuration.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"cancel\" cancels a running program.\n\n  Syntax: cancel [OPTIONS] <Job ID>\n  \"cancel\" action options:\n     -m,--jobmanager <host:port>            Address of the JobManager (master)\n                                            to which to connect. Use this flag\n                                            to connect to a different JobManager\n                                            than the one specified in the\n                                            configuration.\n     -s,--withSavepoint <targetDirectory>   Trigger savepoint and cancel job.\n                                            The target directory is optional. If\n                                            no directory is specified, the\n                                            configured default directory\n                                            (state.savepoints.dir) is used.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"savepoint\" triggers savepoints for a running job or disposes existing ones.\n\n  Syntax: savepoint [OPTIONS] <Job ID> [<target directory>]\n  \"savepoint\" action options:\n     -d,--dispose <arg>            Path of savepoint to dispose.\n     -j,--jarfile <jarfile>        Flink program JAR file.\n     -m,--jobmanager <host:port>   Address of the JobManager (master) to which\n                                   to connect. Use this flag to connect to a\n                                   different JobManager than the one specified\n                                   in the configuration.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n  Please specify an action.\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface\n","source":"_posts/Flink/[Flink]Flink1.4 命令行接口.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 命令行界面\ndate: 2018-01-30 15:30:17\ntags:\n  - Flink\n  - Flink 基础\n\ncategories: Flink\npermalink: flink_basic_command_line_interface\n---\n\n### 1. 概述\n\n`Flink` 提供了一个命令行接口（CLI）用来运行打成JAR包的程序，并且可以控制程序的运行。命令行接口在 `Flink` 安装完之后即可拥有，本地单节点或是分布式部署安装都会有命令行接口。命令行接口启动脚本是 `$FLINK_HOME/bin` 目录下的 `flink` 脚本， 默认情况下会连接运行中的 `Flink master(JobManager)`， `JobManager` 的启动脚本与 `CLI` 在同一安装目录下。\n\n使用命令行接口的前提条件是 `JobManager` 已经被启动(通过`$FLINK_HOME/bin/start-local.sh` 或是 `$FLINK_HOME/bin/start-cluster.sh`)或是 `Flink YARN` 环境可用。 `JobManager` 可以通过如下命令启动:\n```\n$FLINK_HOME/bin/start-local.sh\n或\n$FLINK_HOME/bin/start-cluster.sh\n```\n### 2. Example\n\n(1) 运行示例程序，不传参数：\n```\n./bin/flink run ./examples/batch/WordCount.jar\n```\n(2) 运行示例程序，带输入和输出文件参数：\n```\n./bin/flink run ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(3) 运行示例程序，带输入和输出文件参数,并设置16个并发度：\n```\n./bin/flink run -p 16 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(4) 运行示例程序，并禁止 `Flink` 输出日志\n```\n./bin/flink run -q ./examples/batch/WordCount.jar\n```\n(5) 以独立(detached)模式运行示例程序\n```\n./bin/flink run -d ./examples/batch/WordCount.jar\n```\n(6) 在指定 `JobManager` 上运行示例程序\n```\n./bin/flink run -m myJMHost:6123 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(7) 运行示例程序，指定程序入口类(Main方法所在类)：\n```\n./bin/flink run -c org.apache.flink.examples.java.wordcount.WordCount ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(8) 运行示例程序，使用带有2个 `TaskManager` 的[per-job YARN 集群](https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/yarn_setup.html#run-a-single-flink-job-on-hadoop-yarn)\n```\n./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar --input hdfs:///xiaosi/a.txt --output hdfs:///xiaosi/result.txt\n```\n(9) 以JSON格式输出 `WordCount` 示例程序优化执行计划：\n```\n./bin/flink info ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt\n```\n(10) 列出已经调度的和正在运行的Job(包含Job ID信息)\n```\n./bin/flink list\n```\n(11) 列出已经调度的Job(包含Job ID信息)\n```\n./bin/flink list -s\n```\n(13) 列出正在运行的Job(包含Job ID信息)\n```\n./bin/flink list -r\n```\n(14) 列出在Flink YARN中运行Job\n```\n./bin/flink list -m yarn-cluster -yid <yarnApplicationID> -r\n```\n(15) 取消一个Job\n```\n./bin/flink cancel <jobID>\n```\n(16) 取消一个带有保存点(savepoint)的Job\n```\n./bin/flink cancel -s [targetDirectory] <jobID>\n```\n(17) 停止一个Job(只适用于流计算Job)\n```\n./bin/flink stop <jobID>\n```\n\n取消和停止一个作业的区别如下：\n- 调用取消作业时，作业中的算子立即收到一个调用`cancel()`方法的指令以尽快取消它们。如果算子在调用取消操作后没有停止，`Flink` 将定期开启中断线程来取消作业直到作业停止。\n- 停止作业是一种停止正在运行的流作业的更加优雅的方法。停止仅适用于使用实现`StoppableFunction`接口的数据源的那些作业。当用户请求停止作业时，所有数据源将收到调用`stop()`方法指令。但是作业还是会继续运行，直到所有数据源正确关闭。这允许作业处理完所有正在传输的数据(inflight data)。\n\n### 3. 保存点\n\n保存点通过命令行客户端进行控制：\n\n#### 3.1 触发保存点\n\n```\n./bin/flink savepoint <jobID> [savepointDirectory]\n```\n这会触发作业ID为`jobId`的保存点，并返回创建的保存点的路径。你需要此路径来还原和处理保存点。\n\n此外，你可以选择指定一个目标文件系统目录来存储保存点。目录可以被 `JobManager` 访问。\n\n如果你不指定目标目录，则需要配置默认目录（请参阅[保存点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration)）。 否则，触发保存点将失败。\n\n#### 3.2 使用YARN触发保存点\n\n```\n./bin/flink savepoint <jobId> [savepointDirectory] -yid <yarnAppId>\n```\n\n这将触发作业ID为 `jobId` 以及 `YARN` 应用程序ID为 `yarnAppId` 的保存点，并返回创建的保存点的路径。\n\n其他一切与上面的触发保存点中描述的相同。\n\n#### 3.3 根据保存点取消Job\n\n你可以自动触发一个保存点并取消作业:\n```\n./bin/flink cancel -s  [savepointDirectory] <jobID>\n```\n如果没有配置保存点目录，则需要为 `Flink` 安装配置默认的保存点目录(请参阅[保存点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration)）。\n\n只有保存点触发成功，作业才被取消\n\n#### 3.4 恢复保存点\n\n```\n./bin/flink run -s <savepointPath> ...\n```\n这个`run`命令提交作业时带有一个保存点标记，这使得程序可以从保存点中恢复状态。保存点路径是通过保存点触发命令得到的。\n\n默认情况下，我们尝试将所有的保存点状态与正在提交的作业进行匹配。如果你想允许跳过无法使用新作业恢复的保存点状态，则可以设置`allowNonRestoredState`标志。当保存点触发时，如果想从程序中删除一个算子（作为程序的一部分），并且仍然想要使用这个保存点，则需要允许这一点。\n\n```\n./bin/flink run -s <savepointPath> -n ...\n```\n如果想从程序中删除算子(作为保存点一部分的)，这时会非常有用。\n\n#### 3.5 销毁保存点\n\n```\n./bin/flink savepoint -d <savepointPath>\n```\n销毁一个保存点同样需要一个路径。这个保存点路径是通过保存点触发命令得到的。\n\n如果使用自定义状态实例（例如自定义 `reducing` 状态或 `RocksDB` 状态），则必须指定程序JAR的路径以及被触发的保存点，以便使用用户代码类加载器来销毁保存点：\n```\n./bin/flink savepoint -d <savepointPath> -j <jarFile>\n```\n否则，你将遇到 `ClassNotFoundException`。\n\n### 4. 用法\n\n下面是Flink命令行接口的用法:\n```\nxiaosi@yoona:~/qunar/company/opt/flink-1.3.2$ ./bin/flink\n./flink <ACTION> [OPTIONS] [ARGUMENTS]\n\nThe following actions are available:\n\nAction \"run\" compiles and runs a program.\n\n  Syntax: run [OPTIONS] <jar-file> <arguments>\n  \"run\" action options:\n     -c,--class <classname>                         Class with the program entry\n                                                    point (\"main\" method or\n                                                    \"getPlan()\" method. Only\n                                                    needed if the JAR file does\n                                                    not specify the class in its\n                                                    manifest.\n     -C,--classpath <url>                           Adds a URL to each user code\n                                                    classloader  on all nodes in\n                                                    the cluster. The paths must\n                                                    specify a protocol (e.g.\n                                                    file://) and be accessible\n                                                    on all nodes (e.g. by means\n                                                    of a NFS share). You can use\n                                                    this option multiple times\n                                                    for specifying more than one\n                                                    URL. The protocol must be\n                                                    supported by the {@link\n                                                    java.net.URLClassLoader}.\n     -d,--detached                                  If present, runs the job in\n                                                    detached mode\n     -m,--jobmanager <host:port>                    Address of the JobManager\n                                                    (master) to which to\n                                                    connect. Use this flag to\n                                                    connect to a different\n                                                    JobManager than the one\n                                                    specified in the\n                                                    configuration.\n     -n,--allowNonRestoredState                     Allow to skip savepoint\n                                                    state that cannot be\n                                                    restored. You need to allow\n                                                    this if you removed an\n                                                    operator from your program\n                                                    that was part of the program\n                                                    when the savepoint was\n                                                    triggered.\n     -p,--parallelism <parallelism>                 The parallelism with which\n                                                    to run the program. Optional\n                                                    flag to override the default\n                                                    value specified in the\n                                                    configuration.\n     -q,--sysoutLogging                             If present, suppress logging\n                                                    output to standard out.\n     -s,--fromSavepoint <savepointPath>             Path to a savepoint to\n                                                    restore the job from (for\n                                                    example\n                                                    hdfs:///flink/savepoint-1537\n                                                    ).\n     -z,--zookeeperNamespace <zookeeperNamespace>   Namespace to create the\n                                                    Zookeeper sub-paths for high\n                                                    availability mode\n  Options for yarn-cluster mode:\n     -yD <arg>                            Dynamic properties\n     -yd,--yarndetached                   Start detached\n     -yid,--yarnapplicationId <arg>       Attach to running YARN session\n     -yj,--yarnjar <arg>                  Path to Flink jar file\n     -yjm,--yarnjobManagerMemory <arg>    Memory for JobManager Container [in\n                                          MB]\n     -yn,--yarncontainer <arg>            Number of YARN container to allocate\n                                          (=Number of Task Managers)\n     -ynm,--yarnname <arg>                Set a custom name for the application\n                                          on YARN\n     -yq,--yarnquery                      Display available YARN resources\n                                          (memory, cores)\n     -yqu,--yarnqueue <arg>               Specify YARN queue.\n     -ys,--yarnslots <arg>                Number of slots per TaskManager\n     -yst,--yarnstreaming                 Start Flink in streaming mode\n     -yt,--yarnship <arg>                 Ship files in the specified directory\n                                          (t for transfer)\n     -ytm,--yarntaskManagerMemory <arg>   Memory per TaskManager Container [in\n                                          MB]\n     -yz,--yarnzookeeperNamespace <arg>   Namespace to create the Zookeeper\n                                          sub-paths for high availability mode\n\n  Options for yarn mode:\n     -ya,--yarnattached                   Start attached\n     -yD <arg>                            Dynamic properties\n     -yj,--yarnjar <arg>                  Path to Flink jar file\n     -yjm,--yarnjobManagerMemory <arg>    Memory for JobManager Container [in\n                                          MB]\n     -yqu,--yarnqueue <arg>               Specify YARN queue.\n     -yt,--yarnship <arg>                 Ship files in the specified directory\n                                          (t for transfer)\n     -yz,--yarnzookeeperNamespace <arg>   Namespace to create the Zookeeper\n                                          sub-paths for high availability mode\n\n\n\nAction \"info\" shows the optimized execution plan of the program (JSON).\n\n  Syntax: info [OPTIONS] <jar-file> <arguments>\n  \"info\" action options:\n     -c,--class <classname>           Class with the program entry point (\"main\"\n                                      method or \"getPlan()\" method. Only needed\n                                      if the JAR file does not specify the class\n                                      in its manifest.\n     -p,--parallelism <parallelism>   The parallelism with which to run the\n                                      program. Optional flag to override the\n                                      default value specified in the\n                                      configuration.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"list\" lists running and scheduled programs.\n\n  Syntax: list [OPTIONS]\n  \"list\" action options:\n     -m,--jobmanager <host:port>   Address of the JobManager (master) to which\n                                   to connect. Use this flag to connect to a\n                                   different JobManager than the one specified\n                                   in the configuration.\n     -r,--running                  Show only running programs and their JobIDs\n     -s,--scheduled                Show only scheduled programs and their JobIDs\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"stop\" stops a running program (streaming jobs only).\n\n  Syntax: stop [OPTIONS] <Job ID>\n  \"stop\" action options:\n     -m,--jobmanager <host:port>   Address of the JobManager (master) to which\n                                   to connect. Use this flag to connect to a\n                                   different JobManager than the one specified\n                                   in the configuration.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"cancel\" cancels a running program.\n\n  Syntax: cancel [OPTIONS] <Job ID>\n  \"cancel\" action options:\n     -m,--jobmanager <host:port>            Address of the JobManager (master)\n                                            to which to connect. Use this flag\n                                            to connect to a different JobManager\n                                            than the one specified in the\n                                            configuration.\n     -s,--withSavepoint <targetDirectory>   Trigger savepoint and cancel job.\n                                            The target directory is optional. If\n                                            no directory is specified, the\n                                            configured default directory\n                                            (state.savepoints.dir) is used.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n\n\n\nAction \"savepoint\" triggers savepoints for a running job or disposes existing ones.\n\n  Syntax: savepoint [OPTIONS] <Job ID> [<target directory>]\n  \"savepoint\" action options:\n     -d,--dispose <arg>            Path of savepoint to dispose.\n     -j,--jarfile <jarfile>        Flink program JAR file.\n     -m,--jobmanager <host:port>   Address of the JobManager (master) to which\n                                   to connect. Use this flag to connect to a\n                                   different JobManager than the one specified\n                                   in the configuration.\n  Options for yarn-cluster mode:\n     -yid,--yarnapplicationId <arg>   Attach to running YARN session\n\n  Options for yarn mode:\n\n  Please specify an action.\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface\n","slug":"flink_basic_command_line_interface","published":1,"updated":"2018-01-30T10:32:08.441Z","comments":1,"photos":[],"link":"","_id":"cje58tiqi0016ordbmf99oy04","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p><code>Flink</code> 提供了一个命令行接口（CLI）用来运行打成JAR包的程序，并且可以控制程序的运行。命令行接口在 <code>Flink</code> 安装完之后即可拥有，本地单节点或是分布式部署安装都会有命令行接口。命令行接口启动脚本是 <code>$FLINK_HOME/bin</code> 目录下的 <code>flink</code> 脚本， 默认情况下会连接运行中的 <code>Flink master(JobManager)</code>， <code>JobManager</code> 的启动脚本与 <code>CLI</code> 在同一安装目录下。</p>\n<p>使用命令行接口的前提条件是 <code>JobManager</code> 已经被启动(通过<code>$FLINK_HOME/bin/start-local.sh</code> 或是 <code>$FLINK_HOME/bin/start-cluster.sh</code>)或是 <code>Flink YARN</code> 环境可用。 <code>JobManager</code> 可以通过如下命令启动:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">$FLINK_HOME/bin/start-local.sh</span><br><span class=\"line\">或</span><br><span class=\"line\">$FLINK_HOME/bin/start-cluster.sh</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-Example\"><a href=\"#2-Example\" class=\"headerlink\" title=\"2. Example\"></a>2. Example</h3><p>(1) 运行示例程序，不传参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p>\n<p>(2) 运行示例程序，带输入和输出文件参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(3) 运行示例程序，带输入和输出文件参数,并设置16个并发度：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -p 16 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(4) 运行示例程序，并禁止 <code>Flink</code> 输出日志<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -q ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p>\n<p>(5) 以独立(detached)模式运行示例程序<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -d ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p>\n<p>(6) 在指定 <code>JobManager</code> 上运行示例程序<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -m myJMHost:6123 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(7) 运行示例程序，指定程序入口类(Main方法所在类)：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -c org.apache.flink.examples.java.wordcount.WordCount ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(8) 运行示例程序，使用带有2个 <code>TaskManager</code> 的<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/yarn_setup.html#run-a-single-flink-job-on-hadoop-yarn\" target=\"_blank\" rel=\"noopener\">per-job YARN 集群</a><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar --input hdfs:///xiaosi/a.txt --output hdfs:///xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(9) 以JSON格式输出 <code>WordCount</code> 示例程序优化执行计划：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink info ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(10) 列出已经调度的和正在运行的Job(包含Job ID信息)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list</span><br></pre></td></tr></table></figure></p>\n<p>(11) 列出已经调度的Job(包含Job ID信息)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list -s</span><br></pre></td></tr></table></figure></p>\n<p>(13) 列出正在运行的Job(包含Job ID信息)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list -r</span><br></pre></td></tr></table></figure></p>\n<p>(14) 列出在Flink YARN中运行Job<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list -m yarn-cluster -yid &lt;yarnApplicationID&gt; -r</span><br></pre></td></tr></table></figure></p>\n<p>(15) 取消一个Job<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink cancel &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>(16) 取消一个带有保存点(savepoint)的Job<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink cancel -s [targetDirectory] &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>(17) 停止一个Job(只适用于流计算Job)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink stop &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>取消和停止一个作业的区别如下：</p>\n<ul>\n<li>调用取消作业时，作业中的算子立即收到一个调用<code>cancel()</code>方法的指令以尽快取消它们。如果算子在调用取消操作后没有停止，<code>Flink</code> 将定期开启中断线程来取消作业直到作业停止。</li>\n<li>停止作业是一种停止正在运行的流作业的更加优雅的方法。停止仅适用于使用实现<code>StoppableFunction</code>接口的数据源的那些作业。当用户请求停止作业时，所有数据源将收到调用<code>stop()</code>方法指令。但是作业还是会继续运行，直到所有数据源正确关闭。这允许作业处理完所有正在传输的数据(inflight data)。</li>\n</ul>\n<h3 id=\"3-保存点\"><a href=\"#3-保存点\" class=\"headerlink\" title=\"3. 保存点\"></a>3. 保存点</h3><p>保存点通过命令行客户端进行控制：</p>\n<h4 id=\"3-1-触发保存点\"><a href=\"#3-1-触发保存点\" class=\"headerlink\" title=\"3.1 触发保存点\"></a>3.1 触发保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint &lt;jobID&gt; [savepointDirectory]</span><br></pre></td></tr></table></figure>\n<p>这会触发作业ID为<code>jobId</code>的保存点，并返回创建的保存点的路径。你需要此路径来还原和处理保存点。</p>\n<p>此外，你可以选择指定一个目标文件系统目录来存储保存点。目录可以被 <code>JobManager</code> 访问。</p>\n<p>如果你不指定目标目录，则需要配置默认目录（请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration\" target=\"_blank\" rel=\"noopener\">保存点</a>）。 否则，触发保存点将失败。</p>\n<h4 id=\"3-2-使用YARN触发保存点\"><a href=\"#3-2-使用YARN触发保存点\" class=\"headerlink\" title=\"3.2 使用YARN触发保存点\"></a>3.2 使用YARN触发保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint &lt;jobId&gt; [savepointDirectory] -yid &lt;yarnAppId&gt;</span><br></pre></td></tr></table></figure>\n<p>这将触发作业ID为 <code>jobId</code> 以及 <code>YARN</code> 应用程序ID为 <code>yarnAppId</code> 的保存点，并返回创建的保存点的路径。</p>\n<p>其他一切与上面的触发保存点中描述的相同。</p>\n<h4 id=\"3-3-根据保存点取消Job\"><a href=\"#3-3-根据保存点取消Job\" class=\"headerlink\" title=\"3.3 根据保存点取消Job\"></a>3.3 根据保存点取消Job</h4><p>你可以自动触发一个保存点并取消作业:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink cancel -s  [savepointDirectory] &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>如果没有配置保存点目录，则需要为 <code>Flink</code> 安装配置默认的保存点目录(请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration\" target=\"_blank\" rel=\"noopener\">保存点</a>）。</p>\n<p>只有保存点触发成功，作业才被取消</p>\n<h4 id=\"3-4-恢复保存点\"><a href=\"#3-4-恢复保存点\" class=\"headerlink\" title=\"3.4 恢复保存点\"></a>3.4 恢复保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -s &lt;savepointPath&gt; ...</span><br></pre></td></tr></table></figure>\n<p>这个<code>run</code>命令提交作业时带有一个保存点标记，这使得程序可以从保存点中恢复状态。保存点路径是通过保存点触发命令得到的。</p>\n<p>默认情况下，我们尝试将所有的保存点状态与正在提交的作业进行匹配。如果你想允许跳过无法使用新作业恢复的保存点状态，则可以设置<code>allowNonRestoredState</code>标志。当保存点触发时，如果想从程序中删除一个算子（作为程序的一部分），并且仍然想要使用这个保存点，则需要允许这一点。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -s &lt;savepointPath&gt; -n ...</span><br></pre></td></tr></table></figure>\n<p>如果想从程序中删除算子(作为保存点一部分的)，这时会非常有用。</p>\n<h4 id=\"3-5-销毁保存点\"><a href=\"#3-5-销毁保存点\" class=\"headerlink\" title=\"3.5 销毁保存点\"></a>3.5 销毁保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint -d &lt;savepointPath&gt;</span><br></pre></td></tr></table></figure>\n<p>销毁一个保存点同样需要一个路径。这个保存点路径是通过保存点触发命令得到的。</p>\n<p>如果使用自定义状态实例（例如自定义 <code>reducing</code> 状态或 <code>RocksDB</code> 状态），则必须指定程序JAR的路径以及被触发的保存点，以便使用用户代码类加载器来销毁保存点：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint -d &lt;savepointPath&gt; -j &lt;jarFile&gt;</span><br></pre></td></tr></table></figure></p>\n<p>否则，你将遇到 <code>ClassNotFoundException</code>。</p>\n<h3 id=\"4-用法\"><a href=\"#4-用法\" class=\"headerlink\" title=\"4. 用法\"></a>4. 用法</h3><p>下面是Flink命令行接口的用法:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/qunar/company/opt/flink-1.3.2$ ./bin/flink</span><br><span class=\"line\">./flink &lt;ACTION&gt; [OPTIONS] [ARGUMENTS]</span><br><span class=\"line\"></span><br><span class=\"line\">The following actions are available:</span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;run&quot; compiles and runs a program.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class=\"line\">  &quot;run&quot; action options:</span><br><span class=\"line\">     -c,--class &lt;classname&gt;                         Class with the program entry</span><br><span class=\"line\">                                                    point (&quot;main&quot; method or</span><br><span class=\"line\">                                                    &quot;getPlan()&quot; method. Only</span><br><span class=\"line\">                                                    needed if the JAR file does</span><br><span class=\"line\">                                                    not specify the class in its</span><br><span class=\"line\">                                                    manifest.</span><br><span class=\"line\">     -C,--classpath &lt;url&gt;                           Adds a URL to each user code</span><br><span class=\"line\">                                                    classloader  on all nodes in</span><br><span class=\"line\">                                                    the cluster. The paths must</span><br><span class=\"line\">                                                    specify a protocol (e.g.</span><br><span class=\"line\">                                                    file://) and be accessible</span><br><span class=\"line\">                                                    on all nodes (e.g. by means</span><br><span class=\"line\">                                                    of a NFS share). You can use</span><br><span class=\"line\">                                                    this option multiple times</span><br><span class=\"line\">                                                    for specifying more than one</span><br><span class=\"line\">                                                    URL. The protocol must be</span><br><span class=\"line\">                                                    supported by the &#123;@link</span><br><span class=\"line\">                                                    java.net.URLClassLoader&#125;.</span><br><span class=\"line\">     -d,--detached                                  If present, runs the job in</span><br><span class=\"line\">                                                    detached mode</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;                    Address of the JobManager</span><br><span class=\"line\">                                                    (master) to which to</span><br><span class=\"line\">                                                    connect. Use this flag to</span><br><span class=\"line\">                                                    connect to a different</span><br><span class=\"line\">                                                    JobManager than the one</span><br><span class=\"line\">                                                    specified in the</span><br><span class=\"line\">                                                    configuration.</span><br><span class=\"line\">     -n,--allowNonRestoredState                     Allow to skip savepoint</span><br><span class=\"line\">                                                    state that cannot be</span><br><span class=\"line\">                                                    restored. You need to allow</span><br><span class=\"line\">                                                    this if you removed an</span><br><span class=\"line\">                                                    operator from your program</span><br><span class=\"line\">                                                    that was part of the program</span><br><span class=\"line\">                                                    when the savepoint was</span><br><span class=\"line\">                                                    triggered.</span><br><span class=\"line\">     -p,--parallelism &lt;parallelism&gt;                 The parallelism with which</span><br><span class=\"line\">                                                    to run the program. Optional</span><br><span class=\"line\">                                                    flag to override the default</span><br><span class=\"line\">                                                    value specified in the</span><br><span class=\"line\">                                                    configuration.</span><br><span class=\"line\">     -q,--sysoutLogging                             If present, suppress logging</span><br><span class=\"line\">                                                    output to standard out.</span><br><span class=\"line\">     -s,--fromSavepoint &lt;savepointPath&gt;             Path to a savepoint to</span><br><span class=\"line\">                                                    restore the job from (for</span><br><span class=\"line\">                                                    example</span><br><span class=\"line\">                                                    hdfs:///flink/savepoint-1537</span><br><span class=\"line\">                                                    ).</span><br><span class=\"line\">     -z,--zookeeperNamespace &lt;zookeeperNamespace&gt;   Namespace to create the</span><br><span class=\"line\">                                                    Zookeeper sub-paths for high</span><br><span class=\"line\">                                                    availability mode</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yD &lt;arg&gt;                            Dynamic properties</span><br><span class=\"line\">     -yd,--yarndetached                   Start detached</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;       Attach to running YARN session</span><br><span class=\"line\">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class=\"line\">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container [in</span><br><span class=\"line\">                                          MB]</span><br><span class=\"line\">     -yn,--yarncontainer &lt;arg&gt;            Number of YARN container to allocate</span><br><span class=\"line\">                                          (=Number of Task Managers)</span><br><span class=\"line\">     -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class=\"line\">                                          on YARN</span><br><span class=\"line\">     -yq,--yarnquery                      Display available YARN resources</span><br><span class=\"line\">                                          (memory, cores)</span><br><span class=\"line\">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class=\"line\">     -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class=\"line\">     -yst,--yarnstreaming                 Start Flink in streaming mode</span><br><span class=\"line\">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class=\"line\">                                          (t for transfer)</span><br><span class=\"line\">     -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container [in</span><br><span class=\"line\">                                          MB]</span><br><span class=\"line\">     -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class=\"line\">                                          sub-paths for high availability mode</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\">     -ya,--yarnattached                   Start attached</span><br><span class=\"line\">     -yD &lt;arg&gt;                            Dynamic properties</span><br><span class=\"line\">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class=\"line\">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container [in</span><br><span class=\"line\">                                          MB]</span><br><span class=\"line\">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class=\"line\">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class=\"line\">                                          (t for transfer)</span><br><span class=\"line\">     -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class=\"line\">                                          sub-paths for high availability mode</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;info&quot; shows the optimized execution plan of the program (JSON).</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: info [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class=\"line\">  &quot;info&quot; action options:</span><br><span class=\"line\">     -c,--class &lt;classname&gt;           Class with the program entry point (&quot;main&quot;</span><br><span class=\"line\">                                      method or &quot;getPlan()&quot; method. Only needed</span><br><span class=\"line\">                                      if the JAR file does not specify the class</span><br><span class=\"line\">                                      in its manifest.</span><br><span class=\"line\">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class=\"line\">                                      program. Optional flag to override the</span><br><span class=\"line\">                                      default value specified in the</span><br><span class=\"line\">                                      configuration.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;list&quot; lists running and scheduled programs.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: list [OPTIONS]</span><br><span class=\"line\">  &quot;list&quot; action options:</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class=\"line\">                                   to connect. Use this flag to connect to a</span><br><span class=\"line\">                                   different JobManager than the one specified</span><br><span class=\"line\">                                   in the configuration.</span><br><span class=\"line\">     -r,--running                  Show only running programs and their JobIDs</span><br><span class=\"line\">     -s,--scheduled                Show only scheduled programs and their JobIDs</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;stop&quot; stops a running program (streaming jobs only).</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: stop [OPTIONS] &lt;Job ID&gt;</span><br><span class=\"line\">  &quot;stop&quot; action options:</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class=\"line\">                                   to connect. Use this flag to connect to a</span><br><span class=\"line\">                                   different JobManager than the one specified</span><br><span class=\"line\">                                   in the configuration.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;cancel&quot; cancels a running program.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: cancel [OPTIONS] &lt;Job ID&gt;</span><br><span class=\"line\">  &quot;cancel&quot; action options:</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;            Address of the JobManager (master)</span><br><span class=\"line\">                                            to which to connect. Use this flag</span><br><span class=\"line\">                                            to connect to a different JobManager</span><br><span class=\"line\">                                            than the one specified in the</span><br><span class=\"line\">                                            configuration.</span><br><span class=\"line\">     -s,--withSavepoint &lt;targetDirectory&gt;   Trigger savepoint and cancel job.</span><br><span class=\"line\">                                            The target directory is optional. If</span><br><span class=\"line\">                                            no directory is specified, the</span><br><span class=\"line\">                                            configured default directory</span><br><span class=\"line\">                                            (state.savepoints.dir) is used.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;savepoint&quot; triggers savepoints for a running job or disposes existing ones.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: savepoint [OPTIONS] &lt;Job ID&gt; [&lt;target directory&gt;]</span><br><span class=\"line\">  &quot;savepoint&quot; action options:</span><br><span class=\"line\">     -d,--dispose &lt;arg&gt;            Path of savepoint to dispose.</span><br><span class=\"line\">     -j,--jarfile &lt;jarfile&gt;        Flink program JAR file.</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class=\"line\">                                   to connect. Use this flag to connect to a</span><br><span class=\"line\">                                   different JobManager than the one specified</span><br><span class=\"line\">                                   in the configuration.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\">  Please specify an action.</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p><code>Flink</code> 提供了一个命令行接口（CLI）用来运行打成JAR包的程序，并且可以控制程序的运行。命令行接口在 <code>Flink</code> 安装完之后即可拥有，本地单节点或是分布式部署安装都会有命令行接口。命令行接口启动脚本是 <code>$FLINK_HOME/bin</code> 目录下的 <code>flink</code> 脚本， 默认情况下会连接运行中的 <code>Flink master(JobManager)</code>， <code>JobManager</code> 的启动脚本与 <code>CLI</code> 在同一安装目录下。</p>\n<p>使用命令行接口的前提条件是 <code>JobManager</code> 已经被启动(通过<code>$FLINK_HOME/bin/start-local.sh</code> 或是 <code>$FLINK_HOME/bin/start-cluster.sh</code>)或是 <code>Flink YARN</code> 环境可用。 <code>JobManager</code> 可以通过如下命令启动:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">$FLINK_HOME/bin/start-local.sh</span><br><span class=\"line\">或</span><br><span class=\"line\">$FLINK_HOME/bin/start-cluster.sh</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-Example\"><a href=\"#2-Example\" class=\"headerlink\" title=\"2. Example\"></a>2. Example</h3><p>(1) 运行示例程序，不传参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p>\n<p>(2) 运行示例程序，带输入和输出文件参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(3) 运行示例程序，带输入和输出文件参数,并设置16个并发度：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -p 16 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(4) 运行示例程序，并禁止 <code>Flink</code> 输出日志<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -q ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p>\n<p>(5) 以独立(detached)模式运行示例程序<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -d ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p>\n<p>(6) 在指定 <code>JobManager</code> 上运行示例程序<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -m myJMHost:6123 ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(7) 运行示例程序，指定程序入口类(Main方法所在类)：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -c org.apache.flink.examples.java.wordcount.WordCount ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(8) 运行示例程序，使用带有2个 <code>TaskManager</code> 的<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/yarn_setup.html#run-a-single-flink-job-on-hadoop-yarn\" target=\"_blank\" rel=\"noopener\">per-job YARN 集群</a><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar --input hdfs:///xiaosi/a.txt --output hdfs:///xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(9) 以JSON格式输出 <code>WordCount</code> 示例程序优化执行计划：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink info ./examples/batch/WordCount.jar --input file:///home/xiaosi/a.txt --output file:///home/xiaosi/result.txt</span><br></pre></td></tr></table></figure></p>\n<p>(10) 列出已经调度的和正在运行的Job(包含Job ID信息)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list</span><br></pre></td></tr></table></figure></p>\n<p>(11) 列出已经调度的Job(包含Job ID信息)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list -s</span><br></pre></td></tr></table></figure></p>\n<p>(13) 列出正在运行的Job(包含Job ID信息)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list -r</span><br></pre></td></tr></table></figure></p>\n<p>(14) 列出在Flink YARN中运行Job<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink list -m yarn-cluster -yid &lt;yarnApplicationID&gt; -r</span><br></pre></td></tr></table></figure></p>\n<p>(15) 取消一个Job<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink cancel &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>(16) 取消一个带有保存点(savepoint)的Job<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink cancel -s [targetDirectory] &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>(17) 停止一个Job(只适用于流计算Job)<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink stop &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>取消和停止一个作业的区别如下：</p>\n<ul>\n<li>调用取消作业时，作业中的算子立即收到一个调用<code>cancel()</code>方法的指令以尽快取消它们。如果算子在调用取消操作后没有停止，<code>Flink</code> 将定期开启中断线程来取消作业直到作业停止。</li>\n<li>停止作业是一种停止正在运行的流作业的更加优雅的方法。停止仅适用于使用实现<code>StoppableFunction</code>接口的数据源的那些作业。当用户请求停止作业时，所有数据源将收到调用<code>stop()</code>方法指令。但是作业还是会继续运行，直到所有数据源正确关闭。这允许作业处理完所有正在传输的数据(inflight data)。</li>\n</ul>\n<h3 id=\"3-保存点\"><a href=\"#3-保存点\" class=\"headerlink\" title=\"3. 保存点\"></a>3. 保存点</h3><p>保存点通过命令行客户端进行控制：</p>\n<h4 id=\"3-1-触发保存点\"><a href=\"#3-1-触发保存点\" class=\"headerlink\" title=\"3.1 触发保存点\"></a>3.1 触发保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint &lt;jobID&gt; [savepointDirectory]</span><br></pre></td></tr></table></figure>\n<p>这会触发作业ID为<code>jobId</code>的保存点，并返回创建的保存点的路径。你需要此路径来还原和处理保存点。</p>\n<p>此外，你可以选择指定一个目标文件系统目录来存储保存点。目录可以被 <code>JobManager</code> 访问。</p>\n<p>如果你不指定目标目录，则需要配置默认目录（请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration\" target=\"_blank\" rel=\"noopener\">保存点</a>）。 否则，触发保存点将失败。</p>\n<h4 id=\"3-2-使用YARN触发保存点\"><a href=\"#3-2-使用YARN触发保存点\" class=\"headerlink\" title=\"3.2 使用YARN触发保存点\"></a>3.2 使用YARN触发保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint &lt;jobId&gt; [savepointDirectory] -yid &lt;yarnAppId&gt;</span><br></pre></td></tr></table></figure>\n<p>这将触发作业ID为 <code>jobId</code> 以及 <code>YARN</code> 应用程序ID为 <code>yarnAppId</code> 的保存点，并返回创建的保存点的路径。</p>\n<p>其他一切与上面的触发保存点中描述的相同。</p>\n<h4 id=\"3-3-根据保存点取消Job\"><a href=\"#3-3-根据保存点取消Job\" class=\"headerlink\" title=\"3.3 根据保存点取消Job\"></a>3.3 根据保存点取消Job</h4><p>你可以自动触发一个保存点并取消作业:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink cancel -s  [savepointDirectory] &lt;jobID&gt;</span><br></pre></td></tr></table></figure></p>\n<p>如果没有配置保存点目录，则需要为 <code>Flink</code> 安装配置默认的保存点目录(请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/savepoints.html#configuration\" target=\"_blank\" rel=\"noopener\">保存点</a>）。</p>\n<p>只有保存点触发成功，作业才被取消</p>\n<h4 id=\"3-4-恢复保存点\"><a href=\"#3-4-恢复保存点\" class=\"headerlink\" title=\"3.4 恢复保存点\"></a>3.4 恢复保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -s &lt;savepointPath&gt; ...</span><br></pre></td></tr></table></figure>\n<p>这个<code>run</code>命令提交作业时带有一个保存点标记，这使得程序可以从保存点中恢复状态。保存点路径是通过保存点触发命令得到的。</p>\n<p>默认情况下，我们尝试将所有的保存点状态与正在提交的作业进行匹配。如果你想允许跳过无法使用新作业恢复的保存点状态，则可以设置<code>allowNonRestoredState</code>标志。当保存点触发时，如果想从程序中删除一个算子（作为程序的一部分），并且仍然想要使用这个保存点，则需要允许这一点。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -s &lt;savepointPath&gt; -n ...</span><br></pre></td></tr></table></figure>\n<p>如果想从程序中删除算子(作为保存点一部分的)，这时会非常有用。</p>\n<h4 id=\"3-5-销毁保存点\"><a href=\"#3-5-销毁保存点\" class=\"headerlink\" title=\"3.5 销毁保存点\"></a>3.5 销毁保存点</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint -d &lt;savepointPath&gt;</span><br></pre></td></tr></table></figure>\n<p>销毁一个保存点同样需要一个路径。这个保存点路径是通过保存点触发命令得到的。</p>\n<p>如果使用自定义状态实例（例如自定义 <code>reducing</code> 状态或 <code>RocksDB</code> 状态），则必须指定程序JAR的路径以及被触发的保存点，以便使用用户代码类加载器来销毁保存点：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink savepoint -d &lt;savepointPath&gt; -j &lt;jarFile&gt;</span><br></pre></td></tr></table></figure></p>\n<p>否则，你将遇到 <code>ClassNotFoundException</code>。</p>\n<h3 id=\"4-用法\"><a href=\"#4-用法\" class=\"headerlink\" title=\"4. 用法\"></a>4. 用法</h3><p>下面是Flink命令行接口的用法:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/qunar/company/opt/flink-1.3.2$ ./bin/flink</span><br><span class=\"line\">./flink &lt;ACTION&gt; [OPTIONS] [ARGUMENTS]</span><br><span class=\"line\"></span><br><span class=\"line\">The following actions are available:</span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;run&quot; compiles and runs a program.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class=\"line\">  &quot;run&quot; action options:</span><br><span class=\"line\">     -c,--class &lt;classname&gt;                         Class with the program entry</span><br><span class=\"line\">                                                    point (&quot;main&quot; method or</span><br><span class=\"line\">                                                    &quot;getPlan()&quot; method. Only</span><br><span class=\"line\">                                                    needed if the JAR file does</span><br><span class=\"line\">                                                    not specify the class in its</span><br><span class=\"line\">                                                    manifest.</span><br><span class=\"line\">     -C,--classpath &lt;url&gt;                           Adds a URL to each user code</span><br><span class=\"line\">                                                    classloader  on all nodes in</span><br><span class=\"line\">                                                    the cluster. The paths must</span><br><span class=\"line\">                                                    specify a protocol (e.g.</span><br><span class=\"line\">                                                    file://) and be accessible</span><br><span class=\"line\">                                                    on all nodes (e.g. by means</span><br><span class=\"line\">                                                    of a NFS share). You can use</span><br><span class=\"line\">                                                    this option multiple times</span><br><span class=\"line\">                                                    for specifying more than one</span><br><span class=\"line\">                                                    URL. The protocol must be</span><br><span class=\"line\">                                                    supported by the &#123;@link</span><br><span class=\"line\">                                                    java.net.URLClassLoader&#125;.</span><br><span class=\"line\">     -d,--detached                                  If present, runs the job in</span><br><span class=\"line\">                                                    detached mode</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;                    Address of the JobManager</span><br><span class=\"line\">                                                    (master) to which to</span><br><span class=\"line\">                                                    connect. Use this flag to</span><br><span class=\"line\">                                                    connect to a different</span><br><span class=\"line\">                                                    JobManager than the one</span><br><span class=\"line\">                                                    specified in the</span><br><span class=\"line\">                                                    configuration.</span><br><span class=\"line\">     -n,--allowNonRestoredState                     Allow to skip savepoint</span><br><span class=\"line\">                                                    state that cannot be</span><br><span class=\"line\">                                                    restored. You need to allow</span><br><span class=\"line\">                                                    this if you removed an</span><br><span class=\"line\">                                                    operator from your program</span><br><span class=\"line\">                                                    that was part of the program</span><br><span class=\"line\">                                                    when the savepoint was</span><br><span class=\"line\">                                                    triggered.</span><br><span class=\"line\">     -p,--parallelism &lt;parallelism&gt;                 The parallelism with which</span><br><span class=\"line\">                                                    to run the program. Optional</span><br><span class=\"line\">                                                    flag to override the default</span><br><span class=\"line\">                                                    value specified in the</span><br><span class=\"line\">                                                    configuration.</span><br><span class=\"line\">     -q,--sysoutLogging                             If present, suppress logging</span><br><span class=\"line\">                                                    output to standard out.</span><br><span class=\"line\">     -s,--fromSavepoint &lt;savepointPath&gt;             Path to a savepoint to</span><br><span class=\"line\">                                                    restore the job from (for</span><br><span class=\"line\">                                                    example</span><br><span class=\"line\">                                                    hdfs:///flink/savepoint-1537</span><br><span class=\"line\">                                                    ).</span><br><span class=\"line\">     -z,--zookeeperNamespace &lt;zookeeperNamespace&gt;   Namespace to create the</span><br><span class=\"line\">                                                    Zookeeper sub-paths for high</span><br><span class=\"line\">                                                    availability mode</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yD &lt;arg&gt;                            Dynamic properties</span><br><span class=\"line\">     -yd,--yarndetached                   Start detached</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;       Attach to running YARN session</span><br><span class=\"line\">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class=\"line\">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container [in</span><br><span class=\"line\">                                          MB]</span><br><span class=\"line\">     -yn,--yarncontainer &lt;arg&gt;            Number of YARN container to allocate</span><br><span class=\"line\">                                          (=Number of Task Managers)</span><br><span class=\"line\">     -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class=\"line\">                                          on YARN</span><br><span class=\"line\">     -yq,--yarnquery                      Display available YARN resources</span><br><span class=\"line\">                                          (memory, cores)</span><br><span class=\"line\">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class=\"line\">     -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class=\"line\">     -yst,--yarnstreaming                 Start Flink in streaming mode</span><br><span class=\"line\">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class=\"line\">                                          (t for transfer)</span><br><span class=\"line\">     -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container [in</span><br><span class=\"line\">                                          MB]</span><br><span class=\"line\">     -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class=\"line\">                                          sub-paths for high availability mode</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\">     -ya,--yarnattached                   Start attached</span><br><span class=\"line\">     -yD &lt;arg&gt;                            Dynamic properties</span><br><span class=\"line\">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class=\"line\">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container [in</span><br><span class=\"line\">                                          MB]</span><br><span class=\"line\">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class=\"line\">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class=\"line\">                                          (t for transfer)</span><br><span class=\"line\">     -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class=\"line\">                                          sub-paths for high availability mode</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;info&quot; shows the optimized execution plan of the program (JSON).</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: info [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class=\"line\">  &quot;info&quot; action options:</span><br><span class=\"line\">     -c,--class &lt;classname&gt;           Class with the program entry point (&quot;main&quot;</span><br><span class=\"line\">                                      method or &quot;getPlan()&quot; method. Only needed</span><br><span class=\"line\">                                      if the JAR file does not specify the class</span><br><span class=\"line\">                                      in its manifest.</span><br><span class=\"line\">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class=\"line\">                                      program. Optional flag to override the</span><br><span class=\"line\">                                      default value specified in the</span><br><span class=\"line\">                                      configuration.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;list&quot; lists running and scheduled programs.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: list [OPTIONS]</span><br><span class=\"line\">  &quot;list&quot; action options:</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class=\"line\">                                   to connect. Use this flag to connect to a</span><br><span class=\"line\">                                   different JobManager than the one specified</span><br><span class=\"line\">                                   in the configuration.</span><br><span class=\"line\">     -r,--running                  Show only running programs and their JobIDs</span><br><span class=\"line\">     -s,--scheduled                Show only scheduled programs and their JobIDs</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;stop&quot; stops a running program (streaming jobs only).</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: stop [OPTIONS] &lt;Job ID&gt;</span><br><span class=\"line\">  &quot;stop&quot; action options:</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class=\"line\">                                   to connect. Use this flag to connect to a</span><br><span class=\"line\">                                   different JobManager than the one specified</span><br><span class=\"line\">                                   in the configuration.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;cancel&quot; cancels a running program.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: cancel [OPTIONS] &lt;Job ID&gt;</span><br><span class=\"line\">  &quot;cancel&quot; action options:</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;            Address of the JobManager (master)</span><br><span class=\"line\">                                            to which to connect. Use this flag</span><br><span class=\"line\">                                            to connect to a different JobManager</span><br><span class=\"line\">                                            than the one specified in the</span><br><span class=\"line\">                                            configuration.</span><br><span class=\"line\">     -s,--withSavepoint &lt;targetDirectory&gt;   Trigger savepoint and cancel job.</span><br><span class=\"line\">                                            The target directory is optional. If</span><br><span class=\"line\">                                            no directory is specified, the</span><br><span class=\"line\">                                            configured default directory</span><br><span class=\"line\">                                            (state.savepoints.dir) is used.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Action &quot;savepoint&quot; triggers savepoints for a running job or disposes existing ones.</span><br><span class=\"line\"></span><br><span class=\"line\">  Syntax: savepoint [OPTIONS] &lt;Job ID&gt; [&lt;target directory&gt;]</span><br><span class=\"line\">  &quot;savepoint&quot; action options:</span><br><span class=\"line\">     -d,--dispose &lt;arg&gt;            Path of savepoint to dispose.</span><br><span class=\"line\">     -j,--jarfile &lt;jarfile&gt;        Flink program JAR file.</span><br><span class=\"line\">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class=\"line\">                                   to connect. Use this flag to connect to a</span><br><span class=\"line\">                                   different JobManager than the one specified</span><br><span class=\"line\">                                   in the configuration.</span><br><span class=\"line\">  Options for yarn-cluster mode:</span><br><span class=\"line\">     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session</span><br><span class=\"line\"></span><br><span class=\"line\">  Options for yarn mode:</span><br><span class=\"line\"></span><br><span class=\"line\">  Please specify an action.</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#command-line-interface</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 保存点之回溯时间","date":"2018-01-31T07:30:17.000Z","_content":"\n这篇文章是系列文章的第一篇，数据工匠团队会在这里为大家展示一些Apache Flink的核心功能。\n\n流处理通常被大家与`动态数据`关联起来，相应的系统差不多会在数据被创造出来的那一刻就立刻对其进行处理或响应。像延迟、吞吐量、水印和处理迟到的数据等等都是大家讨论得最多的流处理话题，通常是关注现在，而不是过去。\n\n可在实际项目中，却有许多种场景需要你的流处理程序把以前处理过的数据再重新处理一遍。这里有些例子：\n- 为你的程序部署一个新版本，可能是有新功能、修复了问题、或者采用了更好的机器学习模型；\n- 使用相同的源数据流对应用程序的不同版本进行A/B测试，两边都从同一个点开始测试，这样就不会牺牲之前的状态；\n- 评估或开展将应用程序迁移到更新版本的处理框架上，或是一个不同的集群上；\n\n`Apache Flink` 的保存点（`Savepoint`）功能可以支持上面的所有场景，并且也是让 `Flink` 与其它分布式开源流处理器不同的一个显著区别点。\n\n在本文中，我们会讲述如何使用保存点功能来重新处理数据，并一定程度地深入底层，讲述这个功能在Flink中是怎么实现的。\n\n### 1. \"重新处理\"到底是什么意思？\n\n为了保证大家对重新处理数据的理解是一致的，我们先讨论一个你可能需要重新处理数据的业务例子。想像一个社交媒体公司，她除了基本的发贴功能之外，还发布了一种付费的、或者说是推广发贴的功能。\n\n公司的用户可以访问一个简单的、基于 `Flink` 实现的仪表板，显示他们的所有文章（不管是普通的还是付费的）被大家查看、点击等等的次数。几个星期之后，从用户的反馈中就可以清晰地看到，这个仪表板如果能把普通的发贴数据和付费的发贴数据区别开来，那就会更好用。\n\n要实现这个功能，就有必要返回到付费发贴功能最初发布的那个时刻，然后从那个时刻开始，把所有数据全都重新处理一遍。这一次要把付费贴和普通贴的展示和交互全都拆开来。如果要把从公司创立伊始产生的数据全都重新处理一遍，这就实在有点强人所难，所以能够从付费发贴的功能发布的时候开始重新处理，同时还保留之前的计算结果，这个功能就很有必要了。\n\n所以当我们用到 `重新处理` 这个词时，我们的意思就是回到一个系统以前的、一致的状态（按开发者的定义，不一定非要是流的最早状态），然后从那个状态开始再处理一遍，可能也要在更改了你的 `Flink` 程序之后。\n\n读者们可以看到的好消息就是： `Flink` 为大家免费提供了上述重新处理功能，相应的功能就叫保存点。我们说\"免费\"，意思是只要你的程序是容错的，并且可以从错误中恢复，那你就可以在 `Flink` 中创建一个保存点并重新处理数据，花费的额外准备工作量几乎为零。\n\n### 2. 简单说说保存点到底是什么\n\n简而言之，一个 `Flink` 程序的保存点就是关于以下两点的全局一致的镜像：\n- 所有数据源的位置；\n- 所有并行算子的状态；\n\n\"全局一致\" 意味着所有并行算子的状态都在所有输入的相同的明确定义的位置处被记录下来了。\n\n如果在过去的某个时刻，你为某个应用程序记下了保存点，那你就可以从那个保存点的位置开始启动一个新程序。新的程序将使用那个保存点位置保存下来的算子的状态进行初始化，并且会从记录的保存点里各个数据源的相应位置开始，重新处理全部数据。\n\n因为 `Flink` 的保存点之间是相互完全独立的，所以对每个程序你都可以有多个保存点，这样你就可以根据这些不同的保存点的信息，回到不同的位置，启动多次、甚至不同的程序（如下图所示）。这个功能对于派生你的流处理程序，或者为它们打不同的版本，是非常有用的。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_stream_turning_back_time_savepoints-1.png?raw=true)\n\n我们应该注意，在从某个保存点开始重新处理数据时，对事件的时间处理是非常重要的。重新处理基本上就意味着从过去到现在进行快速回放，也就是说，是全速地从某些存储系统中读出数据，直到赶上了当前的状态，然后再继续实时地处理新到达的数据。\n\n因为程序对于时间的处理或者插入时间都是要依赖当前的本地时间的，那么如果在根据保存点启动程序时不使用事件的时间，而使用别的时间，对程序的逻辑而言就很可能导致错误的结果。\n\n### 3. 听起来不错，那我该做什么？\n\n不用做很多！事实上，所有支持故障恢复的程序都是自动支持保存点的。因此，大多数进行有状态计算的程序已经满足了需要的条件。如果没有，可以对它们进行快速更新，让它们具备：\n- 启用检查点功能：在每种情况下，我们都推荐在构建 `Flink` 程序的同时，把检查点功能打开，事实上在你的 `Flink` 程序中加上检查点只是需要增加几行代码而已。\n- 可以重置的数据源（即`Apache Kafka`、`Amazon Kinesis`，或者文件系统等）：数据源必须能按照你想要重新处理的点开始，重放数据。\n- 所有的状态都通过 `Flink` 的管理状态接口保存：所有具体的算子的状态都必须保存在 `Flink` 的容错状态数据结构中，这让它可以按照某个之前的保存点位置被重置。\n- 配置一个合适的状态后台：`Flink` 提供了不同的状态后台来将检查点和保存点持久化。默认地，保存点都保存在 `JobManager` 中，但你要为你的程序配置一个适当的后台状态程序，比如 `RocksDB` 等。\n\n如果你已经在运行一个容错的程序了，那就创建一个保存点，然后从保存点的位置开始重新启动程序，这只需要在 `Flink` 命令行里敲几个命令就可以了。咱们接下来挨个看看。\n\n(1) 第一步：创建一个保存点\n\n首先，获得所有运行中的 `Flink` 任务的列表：\n```\nuser$ flink list\n------------Running/Restarting Jobs------------\n10.10.2016 16:20:33 : job_id : Sample Job (RUNNING)\n```\n（运行上面的命令时，你的真实任务ID会是一个包括字母和数字的字符串。）\n\n然后，用相应的任务ID创建一个保存点：\n```\nuser$ flink savepoint job_id\n```\n现在你的保存点就已经可用了。\n\n如果你准备马上根据你的保存点来重新启动任务，你通常会想要把现在正在运行的任务先停掉。你已经有了相应任务的ID，那把它停掉只要几秒钟就够了：\n```\nuser$ flink cancel job_id\n```\n\n(2) 第二步：从一个保存点开始启动任务\n\n当你更新完程序之后，就可以从你的保存点开始启动任务了。\n```\nuser$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar\n```\n如果你想在一个示例程序中自己重做这些步骤，我们推荐你看看一篇之前的博客文章，我们在那里讲了怎么做这件事。\n\n### 4. 如果我想升级我的程序，该怎样做？\n\n如果你想从一个保存点开始启动一个修改过的程序，有几件事是要考虑的。我们可以区别下面这两种情况：\n- 改变一个用户定义的函数的逻辑，比如MapFunction；\n- 改变一个程序的架构，也就是增加或减少算子等；\n\n第一种情况很简单，不需要什么特别的准备。你可以按你的需要去修改函数代码。不过，如果你用一个修改了的架构从保存点开始启动程序，那么为了能够恢复算子的状态，`Flink` 必须能够将保存点程序的算子与使用了新架构的新程序的算子对应起来。\n\n在这种情况下，你就要手动地将算子ID分配给最初的和更新了的程序。因为如果没有算子ID的话，是没办法修改程序的架构的。所以最佳实践经验就要求一定要分配算子ID。\n\n下面的代码段显示了如何为算子分配ID：\n```\nDataStream stream = env.\n // Stateful source (e.g. Kafka) with ID\n .addSource(new StatefulSource())\n .uid(“source-id”)\n .shuffle()\n // The stateful mapper with ID\n .map(new StatefulMapper())\n .uid(“mapper-id”)\n\n// Stateless sink (no specific ID required)\nstream.print()\n```\n请查阅文档，了解更多关于升级程序和保存点的细节。\n\n### 5. 关于保存点的最佳实践\n\n要更好的利用上文中描述的 `Flink` 的重新处理功能，你应该经常触发，生成新的保存点。我们建议要根据某些时刻表（比如每天一次，每周一次，等等）自动地生成保存点，而且每当你关闭某个任务或发布程序的新版本时，也最好先生成保存点。\n\n依据你想用 `Flink` 做的事件不同，生成保存点的最佳方法也会不同，但总的来说，在构建你的程序时你应该花些时间考虑如何使用这些保存点。\n\n### 6. 这些东西是怎么工作的呢？\n\n保存点事实上只是检查点的一个延伸，这就是 `Flink` 的容错机制。如果开启了检查点功能，`Flink` 就会周期性地为所有的算子状态生成一个一致的检查点。在[文档](https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html)中详细的描述了检查点的细节，如果你是个 `Flink` 新手，花些时间去读读是非常值得的。\n\n你可能会以为要生成一个一致的检查点，就得暂停数据处理，因为 `Flink` 必须要等着，直到所有没处理完的记录全被处理掉了，然后做个镜像，镜像生成之后再回去继续处理数据。事实并非如此！ `Flink` 是持续处理数据的，即使在生成检查点的时候也是这样。文档中的`Barriers`一节讲了实现这个功能的原理。\n\n两者之间的关键区别：检查点是基于某些规定的时间间隔自动生成的，而保存点是由用户显式地触发生成的，而且不会象检查点那样过了一定的时间之后就会被删掉。\n\n### 7. 总结\n\n我们讨论了 `Apache Flink` 的保存点和数据重处理功能，因为我们相信这就是 `Flink` 与开源世界中其它流处理器之间的重要区别之一。而且最重要的，在容错的 `Flink` 程序中获得重处理功能几乎是不需要任何代价的，只需要很少的改动。\n\n`Flink` 社区现在还在积极地工作着，要把保存点功能做得更好，包括在改变并发度的情况下保存状态的解决方案等。有些相应的功能（比如Flink-3755）已经发布到主分支上了，而且会被包含到下一个小版本Flink 1.2.0中。\n\n所以，当你需要把程序多部署一份，或者上个新版本，或者要做A/B测试，或者要让多个程序从同一个点开始处理数据时，你可以这么做了，而且不会丢失那些宝贵的状态数据。\n\n当有真实的需求时，流处理基于实时的特性不应该阻挡你把时间调回过去的动作。\n\n有兴趣了解关于 `Apache FLink` 的保存点的更多内容吗？数据工匠CTO Stephan Ewen做了一个关于这个话题的[七分钟白板演练](https://mapr.com/blog/savepoints-apache-flink-stream-processing-whiteboard-walkthrough/)，你可以在MapR博客上看到相关内容。\n\n\n原文: https://data-artisans.com/blog/turning-back-time-savepoints\n\n译文: http://www.infoq.com/cn/articles/turning-back-time-savepoints\n","source":"_posts/Flink/[Flink]Flink1.4 保存点之回溯时间.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 保存点之回溯时间\ndate: 2018-01-31 15:30:17\ntags:\n  - Flink\n  - Flink 容错\n\ncategories: Flink\npermalink: flink_stream_turning_back_time_savepoints\n---\n\n这篇文章是系列文章的第一篇，数据工匠团队会在这里为大家展示一些Apache Flink的核心功能。\n\n流处理通常被大家与`动态数据`关联起来，相应的系统差不多会在数据被创造出来的那一刻就立刻对其进行处理或响应。像延迟、吞吐量、水印和处理迟到的数据等等都是大家讨论得最多的流处理话题，通常是关注现在，而不是过去。\n\n可在实际项目中，却有许多种场景需要你的流处理程序把以前处理过的数据再重新处理一遍。这里有些例子：\n- 为你的程序部署一个新版本，可能是有新功能、修复了问题、或者采用了更好的机器学习模型；\n- 使用相同的源数据流对应用程序的不同版本进行A/B测试，两边都从同一个点开始测试，这样就不会牺牲之前的状态；\n- 评估或开展将应用程序迁移到更新版本的处理框架上，或是一个不同的集群上；\n\n`Apache Flink` 的保存点（`Savepoint`）功能可以支持上面的所有场景，并且也是让 `Flink` 与其它分布式开源流处理器不同的一个显著区别点。\n\n在本文中，我们会讲述如何使用保存点功能来重新处理数据，并一定程度地深入底层，讲述这个功能在Flink中是怎么实现的。\n\n### 1. \"重新处理\"到底是什么意思？\n\n为了保证大家对重新处理数据的理解是一致的，我们先讨论一个你可能需要重新处理数据的业务例子。想像一个社交媒体公司，她除了基本的发贴功能之外，还发布了一种付费的、或者说是推广发贴的功能。\n\n公司的用户可以访问一个简单的、基于 `Flink` 实现的仪表板，显示他们的所有文章（不管是普通的还是付费的）被大家查看、点击等等的次数。几个星期之后，从用户的反馈中就可以清晰地看到，这个仪表板如果能把普通的发贴数据和付费的发贴数据区别开来，那就会更好用。\n\n要实现这个功能，就有必要返回到付费发贴功能最初发布的那个时刻，然后从那个时刻开始，把所有数据全都重新处理一遍。这一次要把付费贴和普通贴的展示和交互全都拆开来。如果要把从公司创立伊始产生的数据全都重新处理一遍，这就实在有点强人所难，所以能够从付费发贴的功能发布的时候开始重新处理，同时还保留之前的计算结果，这个功能就很有必要了。\n\n所以当我们用到 `重新处理` 这个词时，我们的意思就是回到一个系统以前的、一致的状态（按开发者的定义，不一定非要是流的最早状态），然后从那个状态开始再处理一遍，可能也要在更改了你的 `Flink` 程序之后。\n\n读者们可以看到的好消息就是： `Flink` 为大家免费提供了上述重新处理功能，相应的功能就叫保存点。我们说\"免费\"，意思是只要你的程序是容错的，并且可以从错误中恢复，那你就可以在 `Flink` 中创建一个保存点并重新处理数据，花费的额外准备工作量几乎为零。\n\n### 2. 简单说说保存点到底是什么\n\n简而言之，一个 `Flink` 程序的保存点就是关于以下两点的全局一致的镜像：\n- 所有数据源的位置；\n- 所有并行算子的状态；\n\n\"全局一致\" 意味着所有并行算子的状态都在所有输入的相同的明确定义的位置处被记录下来了。\n\n如果在过去的某个时刻，你为某个应用程序记下了保存点，那你就可以从那个保存点的位置开始启动一个新程序。新的程序将使用那个保存点位置保存下来的算子的状态进行初始化，并且会从记录的保存点里各个数据源的相应位置开始，重新处理全部数据。\n\n因为 `Flink` 的保存点之间是相互完全独立的，所以对每个程序你都可以有多个保存点，这样你就可以根据这些不同的保存点的信息，回到不同的位置，启动多次、甚至不同的程序（如下图所示）。这个功能对于派生你的流处理程序，或者为它们打不同的版本，是非常有用的。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_stream_turning_back_time_savepoints-1.png?raw=true)\n\n我们应该注意，在从某个保存点开始重新处理数据时，对事件的时间处理是非常重要的。重新处理基本上就意味着从过去到现在进行快速回放，也就是说，是全速地从某些存储系统中读出数据，直到赶上了当前的状态，然后再继续实时地处理新到达的数据。\n\n因为程序对于时间的处理或者插入时间都是要依赖当前的本地时间的，那么如果在根据保存点启动程序时不使用事件的时间，而使用别的时间，对程序的逻辑而言就很可能导致错误的结果。\n\n### 3. 听起来不错，那我该做什么？\n\n不用做很多！事实上，所有支持故障恢复的程序都是自动支持保存点的。因此，大多数进行有状态计算的程序已经满足了需要的条件。如果没有，可以对它们进行快速更新，让它们具备：\n- 启用检查点功能：在每种情况下，我们都推荐在构建 `Flink` 程序的同时，把检查点功能打开，事实上在你的 `Flink` 程序中加上检查点只是需要增加几行代码而已。\n- 可以重置的数据源（即`Apache Kafka`、`Amazon Kinesis`，或者文件系统等）：数据源必须能按照你想要重新处理的点开始，重放数据。\n- 所有的状态都通过 `Flink` 的管理状态接口保存：所有具体的算子的状态都必须保存在 `Flink` 的容错状态数据结构中，这让它可以按照某个之前的保存点位置被重置。\n- 配置一个合适的状态后台：`Flink` 提供了不同的状态后台来将检查点和保存点持久化。默认地，保存点都保存在 `JobManager` 中，但你要为你的程序配置一个适当的后台状态程序，比如 `RocksDB` 等。\n\n如果你已经在运行一个容错的程序了，那就创建一个保存点，然后从保存点的位置开始重新启动程序，这只需要在 `Flink` 命令行里敲几个命令就可以了。咱们接下来挨个看看。\n\n(1) 第一步：创建一个保存点\n\n首先，获得所有运行中的 `Flink` 任务的列表：\n```\nuser$ flink list\n------------Running/Restarting Jobs------------\n10.10.2016 16:20:33 : job_id : Sample Job (RUNNING)\n```\n（运行上面的命令时，你的真实任务ID会是一个包括字母和数字的字符串。）\n\n然后，用相应的任务ID创建一个保存点：\n```\nuser$ flink savepoint job_id\n```\n现在你的保存点就已经可用了。\n\n如果你准备马上根据你的保存点来重新启动任务，你通常会想要把现在正在运行的任务先停掉。你已经有了相应任务的ID，那把它停掉只要几秒钟就够了：\n```\nuser$ flink cancel job_id\n```\n\n(2) 第二步：从一个保存点开始启动任务\n\n当你更新完程序之后，就可以从你的保存点开始启动任务了。\n```\nuser$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar\n```\n如果你想在一个示例程序中自己重做这些步骤，我们推荐你看看一篇之前的博客文章，我们在那里讲了怎么做这件事。\n\n### 4. 如果我想升级我的程序，该怎样做？\n\n如果你想从一个保存点开始启动一个修改过的程序，有几件事是要考虑的。我们可以区别下面这两种情况：\n- 改变一个用户定义的函数的逻辑，比如MapFunction；\n- 改变一个程序的架构，也就是增加或减少算子等；\n\n第一种情况很简单，不需要什么特别的准备。你可以按你的需要去修改函数代码。不过，如果你用一个修改了的架构从保存点开始启动程序，那么为了能够恢复算子的状态，`Flink` 必须能够将保存点程序的算子与使用了新架构的新程序的算子对应起来。\n\n在这种情况下，你就要手动地将算子ID分配给最初的和更新了的程序。因为如果没有算子ID的话，是没办法修改程序的架构的。所以最佳实践经验就要求一定要分配算子ID。\n\n下面的代码段显示了如何为算子分配ID：\n```\nDataStream stream = env.\n // Stateful source (e.g. Kafka) with ID\n .addSource(new StatefulSource())\n .uid(“source-id”)\n .shuffle()\n // The stateful mapper with ID\n .map(new StatefulMapper())\n .uid(“mapper-id”)\n\n// Stateless sink (no specific ID required)\nstream.print()\n```\n请查阅文档，了解更多关于升级程序和保存点的细节。\n\n### 5. 关于保存点的最佳实践\n\n要更好的利用上文中描述的 `Flink` 的重新处理功能，你应该经常触发，生成新的保存点。我们建议要根据某些时刻表（比如每天一次，每周一次，等等）自动地生成保存点，而且每当你关闭某个任务或发布程序的新版本时，也最好先生成保存点。\n\n依据你想用 `Flink` 做的事件不同，生成保存点的最佳方法也会不同，但总的来说，在构建你的程序时你应该花些时间考虑如何使用这些保存点。\n\n### 6. 这些东西是怎么工作的呢？\n\n保存点事实上只是检查点的一个延伸，这就是 `Flink` 的容错机制。如果开启了检查点功能，`Flink` 就会周期性地为所有的算子状态生成一个一致的检查点。在[文档](https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html)中详细的描述了检查点的细节，如果你是个 `Flink` 新手，花些时间去读读是非常值得的。\n\n你可能会以为要生成一个一致的检查点，就得暂停数据处理，因为 `Flink` 必须要等着，直到所有没处理完的记录全被处理掉了，然后做个镜像，镜像生成之后再回去继续处理数据。事实并非如此！ `Flink` 是持续处理数据的，即使在生成检查点的时候也是这样。文档中的`Barriers`一节讲了实现这个功能的原理。\n\n两者之间的关键区别：检查点是基于某些规定的时间间隔自动生成的，而保存点是由用户显式地触发生成的，而且不会象检查点那样过了一定的时间之后就会被删掉。\n\n### 7. 总结\n\n我们讨论了 `Apache Flink` 的保存点和数据重处理功能，因为我们相信这就是 `Flink` 与开源世界中其它流处理器之间的重要区别之一。而且最重要的，在容错的 `Flink` 程序中获得重处理功能几乎是不需要任何代价的，只需要很少的改动。\n\n`Flink` 社区现在还在积极地工作着，要把保存点功能做得更好，包括在改变并发度的情况下保存状态的解决方案等。有些相应的功能（比如Flink-3755）已经发布到主分支上了，而且会被包含到下一个小版本Flink 1.2.0中。\n\n所以，当你需要把程序多部署一份，或者上个新版本，或者要做A/B测试，或者要让多个程序从同一个点开始处理数据时，你可以这么做了，而且不会丢失那些宝贵的状态数据。\n\n当有真实的需求时，流处理基于实时的特性不应该阻挡你把时间调回过去的动作。\n\n有兴趣了解关于 `Apache FLink` 的保存点的更多内容吗？数据工匠CTO Stephan Ewen做了一个关于这个话题的[七分钟白板演练](https://mapr.com/blog/savepoints-apache-flink-stream-processing-whiteboard-walkthrough/)，你可以在MapR博客上看到相关内容。\n\n\n原文: https://data-artisans.com/blog/turning-back-time-savepoints\n\n译文: http://www.infoq.com/cn/articles/turning-back-time-savepoints\n","slug":"flink_stream_turning_back_time_savepoints","published":1,"updated":"2018-01-31T09:24:32.826Z","comments":1,"photos":[],"link":"","_id":"cje58tiqn0019ordbbvcmboih","content":"<p>这篇文章是系列文章的第一篇，数据工匠团队会在这里为大家展示一些Apache Flink的核心功能。</p>\n<p>流处理通常被大家与<code>动态数据</code>关联起来，相应的系统差不多会在数据被创造出来的那一刻就立刻对其进行处理或响应。像延迟、吞吐量、水印和处理迟到的数据等等都是大家讨论得最多的流处理话题，通常是关注现在，而不是过去。</p>\n<p>可在实际项目中，却有许多种场景需要你的流处理程序把以前处理过的数据再重新处理一遍。这里有些例子：</p>\n<ul>\n<li>为你的程序部署一个新版本，可能是有新功能、修复了问题、或者采用了更好的机器学习模型；</li>\n<li>使用相同的源数据流对应用程序的不同版本进行A/B测试，两边都从同一个点开始测试，这样就不会牺牲之前的状态；</li>\n<li>评估或开展将应用程序迁移到更新版本的处理框架上，或是一个不同的集群上；</li>\n</ul>\n<p><code>Apache Flink</code> 的保存点（<code>Savepoint</code>）功能可以支持上面的所有场景，并且也是让 <code>Flink</code> 与其它分布式开源流处理器不同的一个显著区别点。</p>\n<p>在本文中，我们会讲述如何使用保存点功能来重新处理数据，并一定程度地深入底层，讲述这个功能在Flink中是怎么实现的。</p>\n<h3 id=\"1-“重新处理”到底是什么意思？\"><a href=\"#1-“重新处理”到底是什么意思？\" class=\"headerlink\" title=\"1. “重新处理”到底是什么意思？\"></a>1. “重新处理”到底是什么意思？</h3><p>为了保证大家对重新处理数据的理解是一致的，我们先讨论一个你可能需要重新处理数据的业务例子。想像一个社交媒体公司，她除了基本的发贴功能之外，还发布了一种付费的、或者说是推广发贴的功能。</p>\n<p>公司的用户可以访问一个简单的、基于 <code>Flink</code> 实现的仪表板，显示他们的所有文章（不管是普通的还是付费的）被大家查看、点击等等的次数。几个星期之后，从用户的反馈中就可以清晰地看到，这个仪表板如果能把普通的发贴数据和付费的发贴数据区别开来，那就会更好用。</p>\n<p>要实现这个功能，就有必要返回到付费发贴功能最初发布的那个时刻，然后从那个时刻开始，把所有数据全都重新处理一遍。这一次要把付费贴和普通贴的展示和交互全都拆开来。如果要把从公司创立伊始产生的数据全都重新处理一遍，这就实在有点强人所难，所以能够从付费发贴的功能发布的时候开始重新处理，同时还保留之前的计算结果，这个功能就很有必要了。</p>\n<p>所以当我们用到 <code>重新处理</code> 这个词时，我们的意思就是回到一个系统以前的、一致的状态（按开发者的定义，不一定非要是流的最早状态），然后从那个状态开始再处理一遍，可能也要在更改了你的 <code>Flink</code> 程序之后。</p>\n<p>读者们可以看到的好消息就是： <code>Flink</code> 为大家免费提供了上述重新处理功能，相应的功能就叫保存点。我们说”免费”，意思是只要你的程序是容错的，并且可以从错误中恢复，那你就可以在 <code>Flink</code> 中创建一个保存点并重新处理数据，花费的额外准备工作量几乎为零。</p>\n<h3 id=\"2-简单说说保存点到底是什么\"><a href=\"#2-简单说说保存点到底是什么\" class=\"headerlink\" title=\"2. 简单说说保存点到底是什么\"></a>2. 简单说说保存点到底是什么</h3><p>简而言之，一个 <code>Flink</code> 程序的保存点就是关于以下两点的全局一致的镜像：</p>\n<ul>\n<li>所有数据源的位置；</li>\n<li>所有并行算子的状态；</li>\n</ul>\n<p>“全局一致” 意味着所有并行算子的状态都在所有输入的相同的明确定义的位置处被记录下来了。</p>\n<p>如果在过去的某个时刻，你为某个应用程序记下了保存点，那你就可以从那个保存点的位置开始启动一个新程序。新的程序将使用那个保存点位置保存下来的算子的状态进行初始化，并且会从记录的保存点里各个数据源的相应位置开始，重新处理全部数据。</p>\n<p>因为 <code>Flink</code> 的保存点之间是相互完全独立的，所以对每个程序你都可以有多个保存点，这样你就可以根据这些不同的保存点的信息，回到不同的位置，启动多次、甚至不同的程序（如下图所示）。这个功能对于派生你的流处理程序，或者为它们打不同的版本，是非常有用的。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_stream_turning_back_time_savepoints-1.png?raw=true\" alt=\"\"></p>\n<p>我们应该注意，在从某个保存点开始重新处理数据时，对事件的时间处理是非常重要的。重新处理基本上就意味着从过去到现在进行快速回放，也就是说，是全速地从某些存储系统中读出数据，直到赶上了当前的状态，然后再继续实时地处理新到达的数据。</p>\n<p>因为程序对于时间的处理或者插入时间都是要依赖当前的本地时间的，那么如果在根据保存点启动程序时不使用事件的时间，而使用别的时间，对程序的逻辑而言就很可能导致错误的结果。</p>\n<h3 id=\"3-听起来不错，那我该做什么？\"><a href=\"#3-听起来不错，那我该做什么？\" class=\"headerlink\" title=\"3. 听起来不错，那我该做什么？\"></a>3. 听起来不错，那我该做什么？</h3><p>不用做很多！事实上，所有支持故障恢复的程序都是自动支持保存点的。因此，大多数进行有状态计算的程序已经满足了需要的条件。如果没有，可以对它们进行快速更新，让它们具备：</p>\n<ul>\n<li>启用检查点功能：在每种情况下，我们都推荐在构建 <code>Flink</code> 程序的同时，把检查点功能打开，事实上在你的 <code>Flink</code> 程序中加上检查点只是需要增加几行代码而已。</li>\n<li>可以重置的数据源（即<code>Apache Kafka</code>、<code>Amazon Kinesis</code>，或者文件系统等）：数据源必须能按照你想要重新处理的点开始，重放数据。</li>\n<li>所有的状态都通过 <code>Flink</code> 的管理状态接口保存：所有具体的算子的状态都必须保存在 <code>Flink</code> 的容错状态数据结构中，这让它可以按照某个之前的保存点位置被重置。</li>\n<li>配置一个合适的状态后台：<code>Flink</code> 提供了不同的状态后台来将检查点和保存点持久化。默认地，保存点都保存在 <code>JobManager</code> 中，但你要为你的程序配置一个适当的后台状态程序，比如 <code>RocksDB</code> 等。</li>\n</ul>\n<p>如果你已经在运行一个容错的程序了，那就创建一个保存点，然后从保存点的位置开始重新启动程序，这只需要在 <code>Flink</code> 命令行里敲几个命令就可以了。咱们接下来挨个看看。</p>\n<p>(1) 第一步：创建一个保存点</p>\n<p>首先，获得所有运行中的 <code>Flink</code> 任务的列表：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink list</span><br><span class=\"line\">------------Running/Restarting Jobs------------</span><br><span class=\"line\">10.10.2016 16:20:33 : job_id : Sample Job (RUNNING)</span><br></pre></td></tr></table></figure></p>\n<p>（运行上面的命令时，你的真实任务ID会是一个包括字母和数字的字符串。）</p>\n<p>然后，用相应的任务ID创建一个保存点：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink savepoint job_id</span><br></pre></td></tr></table></figure></p>\n<p>现在你的保存点就已经可用了。</p>\n<p>如果你准备马上根据你的保存点来重新启动任务，你通常会想要把现在正在运行的任务先停掉。你已经有了相应任务的ID，那把它停掉只要几秒钟就够了：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink cancel job_id</span><br></pre></td></tr></table></figure></p>\n<p>(2) 第二步：从一个保存点开始启动任务</p>\n<p>当你更新完程序之后，就可以从你的保存点开始启动任务了。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar</span><br></pre></td></tr></table></figure></p>\n<p>如果你想在一个示例程序中自己重做这些步骤，我们推荐你看看一篇之前的博客文章，我们在那里讲了怎么做这件事。</p>\n<h3 id=\"4-如果我想升级我的程序，该怎样做？\"><a href=\"#4-如果我想升级我的程序，该怎样做？\" class=\"headerlink\" title=\"4. 如果我想升级我的程序，该怎样做？\"></a>4. 如果我想升级我的程序，该怎样做？</h3><p>如果你想从一个保存点开始启动一个修改过的程序，有几件事是要考虑的。我们可以区别下面这两种情况：</p>\n<ul>\n<li>改变一个用户定义的函数的逻辑，比如MapFunction；</li>\n<li>改变一个程序的架构，也就是增加或减少算子等；</li>\n</ul>\n<p>第一种情况很简单，不需要什么特别的准备。你可以按你的需要去修改函数代码。不过，如果你用一个修改了的架构从保存点开始启动程序，那么为了能够恢复算子的状态，<code>Flink</code> 必须能够将保存点程序的算子与使用了新架构的新程序的算子对应起来。</p>\n<p>在这种情况下，你就要手动地将算子ID分配给最初的和更新了的程序。因为如果没有算子ID的话，是没办法修改程序的架构的。所以最佳实践经验就要求一定要分配算子ID。</p>\n<p>下面的代码段显示了如何为算子分配ID：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream stream = env.</span><br><span class=\"line\"> // Stateful source (e.g. Kafka) with ID</span><br><span class=\"line\"> .addSource(new StatefulSource())</span><br><span class=\"line\"> .uid(“source-id”)</span><br><span class=\"line\"> .shuffle()</span><br><span class=\"line\"> // The stateful mapper with ID</span><br><span class=\"line\"> .map(new StatefulMapper())</span><br><span class=\"line\"> .uid(“mapper-id”)</span><br><span class=\"line\"></span><br><span class=\"line\">// Stateless sink (no specific ID required)</span><br><span class=\"line\">stream.print()</span><br></pre></td></tr></table></figure></p>\n<p>请查阅文档，了解更多关于升级程序和保存点的细节。</p>\n<h3 id=\"5-关于保存点的最佳实践\"><a href=\"#5-关于保存点的最佳实践\" class=\"headerlink\" title=\"5. 关于保存点的最佳实践\"></a>5. 关于保存点的最佳实践</h3><p>要更好的利用上文中描述的 <code>Flink</code> 的重新处理功能，你应该经常触发，生成新的保存点。我们建议要根据某些时刻表（比如每天一次，每周一次，等等）自动地生成保存点，而且每当你关闭某个任务或发布程序的新版本时，也最好先生成保存点。</p>\n<p>依据你想用 <code>Flink</code> 做的事件不同，生成保存点的最佳方法也会不同，但总的来说，在构建你的程序时你应该花些时间考虑如何使用这些保存点。</p>\n<h3 id=\"6-这些东西是怎么工作的呢？\"><a href=\"#6-这些东西是怎么工作的呢？\" class=\"headerlink\" title=\"6. 这些东西是怎么工作的呢？\"></a>6. 这些东西是怎么工作的呢？</h3><p>保存点事实上只是检查点的一个延伸，这就是 <code>Flink</code> 的容错机制。如果开启了检查点功能，<code>Flink</code> 就会周期性地为所有的算子状态生成一个一致的检查点。在<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html\" target=\"_blank\" rel=\"noopener\">文档</a>中详细的描述了检查点的细节，如果你是个 <code>Flink</code> 新手，花些时间去读读是非常值得的。</p>\n<p>你可能会以为要生成一个一致的检查点，就得暂停数据处理，因为 <code>Flink</code> 必须要等着，直到所有没处理完的记录全被处理掉了，然后做个镜像，镜像生成之后再回去继续处理数据。事实并非如此！ <code>Flink</code> 是持续处理数据的，即使在生成检查点的时候也是这样。文档中的<code>Barriers</code>一节讲了实现这个功能的原理。</p>\n<p>两者之间的关键区别：检查点是基于某些规定的时间间隔自动生成的，而保存点是由用户显式地触发生成的，而且不会象检查点那样过了一定的时间之后就会被删掉。</p>\n<h3 id=\"7-总结\"><a href=\"#7-总结\" class=\"headerlink\" title=\"7. 总结\"></a>7. 总结</h3><p>我们讨论了 <code>Apache Flink</code> 的保存点和数据重处理功能，因为我们相信这就是 <code>Flink</code> 与开源世界中其它流处理器之间的重要区别之一。而且最重要的，在容错的 <code>Flink</code> 程序中获得重处理功能几乎是不需要任何代价的，只需要很少的改动。</p>\n<p><code>Flink</code> 社区现在还在积极地工作着，要把保存点功能做得更好，包括在改变并发度的情况下保存状态的解决方案等。有些相应的功能（比如Flink-3755）已经发布到主分支上了，而且会被包含到下一个小版本Flink 1.2.0中。</p>\n<p>所以，当你需要把程序多部署一份，或者上个新版本，或者要做A/B测试，或者要让多个程序从同一个点开始处理数据时，你可以这么做了，而且不会丢失那些宝贵的状态数据。</p>\n<p>当有真实的需求时，流处理基于实时的特性不应该阻挡你把时间调回过去的动作。</p>\n<p>有兴趣了解关于 <code>Apache FLink</code> 的保存点的更多内容吗？数据工匠CTO Stephan Ewen做了一个关于这个话题的<a href=\"https://mapr.com/blog/savepoints-apache-flink-stream-processing-whiteboard-walkthrough/\" target=\"_blank\" rel=\"noopener\">七分钟白板演练</a>，你可以在MapR博客上看到相关内容。</p>\n<p>原文: <a href=\"https://data-artisans.com/blog/turning-back-time-savepoints\" target=\"_blank\" rel=\"noopener\">https://data-artisans.com/blog/turning-back-time-savepoints</a></p>\n<p>译文: <a href=\"http://www.infoq.com/cn/articles/turning-back-time-savepoints\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/turning-back-time-savepoints</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>这篇文章是系列文章的第一篇，数据工匠团队会在这里为大家展示一些Apache Flink的核心功能。</p>\n<p>流处理通常被大家与<code>动态数据</code>关联起来，相应的系统差不多会在数据被创造出来的那一刻就立刻对其进行处理或响应。像延迟、吞吐量、水印和处理迟到的数据等等都是大家讨论得最多的流处理话题，通常是关注现在，而不是过去。</p>\n<p>可在实际项目中，却有许多种场景需要你的流处理程序把以前处理过的数据再重新处理一遍。这里有些例子：</p>\n<ul>\n<li>为你的程序部署一个新版本，可能是有新功能、修复了问题、或者采用了更好的机器学习模型；</li>\n<li>使用相同的源数据流对应用程序的不同版本进行A/B测试，两边都从同一个点开始测试，这样就不会牺牲之前的状态；</li>\n<li>评估或开展将应用程序迁移到更新版本的处理框架上，或是一个不同的集群上；</li>\n</ul>\n<p><code>Apache Flink</code> 的保存点（<code>Savepoint</code>）功能可以支持上面的所有场景，并且也是让 <code>Flink</code> 与其它分布式开源流处理器不同的一个显著区别点。</p>\n<p>在本文中，我们会讲述如何使用保存点功能来重新处理数据，并一定程度地深入底层，讲述这个功能在Flink中是怎么实现的。</p>\n<h3 id=\"1-“重新处理”到底是什么意思？\"><a href=\"#1-“重新处理”到底是什么意思？\" class=\"headerlink\" title=\"1. “重新处理”到底是什么意思？\"></a>1. “重新处理”到底是什么意思？</h3><p>为了保证大家对重新处理数据的理解是一致的，我们先讨论一个你可能需要重新处理数据的业务例子。想像一个社交媒体公司，她除了基本的发贴功能之外，还发布了一种付费的、或者说是推广发贴的功能。</p>\n<p>公司的用户可以访问一个简单的、基于 <code>Flink</code> 实现的仪表板，显示他们的所有文章（不管是普通的还是付费的）被大家查看、点击等等的次数。几个星期之后，从用户的反馈中就可以清晰地看到，这个仪表板如果能把普通的发贴数据和付费的发贴数据区别开来，那就会更好用。</p>\n<p>要实现这个功能，就有必要返回到付费发贴功能最初发布的那个时刻，然后从那个时刻开始，把所有数据全都重新处理一遍。这一次要把付费贴和普通贴的展示和交互全都拆开来。如果要把从公司创立伊始产生的数据全都重新处理一遍，这就实在有点强人所难，所以能够从付费发贴的功能发布的时候开始重新处理，同时还保留之前的计算结果，这个功能就很有必要了。</p>\n<p>所以当我们用到 <code>重新处理</code> 这个词时，我们的意思就是回到一个系统以前的、一致的状态（按开发者的定义，不一定非要是流的最早状态），然后从那个状态开始再处理一遍，可能也要在更改了你的 <code>Flink</code> 程序之后。</p>\n<p>读者们可以看到的好消息就是： <code>Flink</code> 为大家免费提供了上述重新处理功能，相应的功能就叫保存点。我们说”免费”，意思是只要你的程序是容错的，并且可以从错误中恢复，那你就可以在 <code>Flink</code> 中创建一个保存点并重新处理数据，花费的额外准备工作量几乎为零。</p>\n<h3 id=\"2-简单说说保存点到底是什么\"><a href=\"#2-简单说说保存点到底是什么\" class=\"headerlink\" title=\"2. 简单说说保存点到底是什么\"></a>2. 简单说说保存点到底是什么</h3><p>简而言之，一个 <code>Flink</code> 程序的保存点就是关于以下两点的全局一致的镜像：</p>\n<ul>\n<li>所有数据源的位置；</li>\n<li>所有并行算子的状态；</li>\n</ul>\n<p>“全局一致” 意味着所有并行算子的状态都在所有输入的相同的明确定义的位置处被记录下来了。</p>\n<p>如果在过去的某个时刻，你为某个应用程序记下了保存点，那你就可以从那个保存点的位置开始启动一个新程序。新的程序将使用那个保存点位置保存下来的算子的状态进行初始化，并且会从记录的保存点里各个数据源的相应位置开始，重新处理全部数据。</p>\n<p>因为 <code>Flink</code> 的保存点之间是相互完全独立的，所以对每个程序你都可以有多个保存点，这样你就可以根据这些不同的保存点的信息，回到不同的位置，启动多次、甚至不同的程序（如下图所示）。这个功能对于派生你的流处理程序，或者为它们打不同的版本，是非常有用的。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/flink_stream_turning_back_time_savepoints-1.png?raw=true\" alt=\"\"></p>\n<p>我们应该注意，在从某个保存点开始重新处理数据时，对事件的时间处理是非常重要的。重新处理基本上就意味着从过去到现在进行快速回放，也就是说，是全速地从某些存储系统中读出数据，直到赶上了当前的状态，然后再继续实时地处理新到达的数据。</p>\n<p>因为程序对于时间的处理或者插入时间都是要依赖当前的本地时间的，那么如果在根据保存点启动程序时不使用事件的时间，而使用别的时间，对程序的逻辑而言就很可能导致错误的结果。</p>\n<h3 id=\"3-听起来不错，那我该做什么？\"><a href=\"#3-听起来不错，那我该做什么？\" class=\"headerlink\" title=\"3. 听起来不错，那我该做什么？\"></a>3. 听起来不错，那我该做什么？</h3><p>不用做很多！事实上，所有支持故障恢复的程序都是自动支持保存点的。因此，大多数进行有状态计算的程序已经满足了需要的条件。如果没有，可以对它们进行快速更新，让它们具备：</p>\n<ul>\n<li>启用检查点功能：在每种情况下，我们都推荐在构建 <code>Flink</code> 程序的同时，把检查点功能打开，事实上在你的 <code>Flink</code> 程序中加上检查点只是需要增加几行代码而已。</li>\n<li>可以重置的数据源（即<code>Apache Kafka</code>、<code>Amazon Kinesis</code>，或者文件系统等）：数据源必须能按照你想要重新处理的点开始，重放数据。</li>\n<li>所有的状态都通过 <code>Flink</code> 的管理状态接口保存：所有具体的算子的状态都必须保存在 <code>Flink</code> 的容错状态数据结构中，这让它可以按照某个之前的保存点位置被重置。</li>\n<li>配置一个合适的状态后台：<code>Flink</code> 提供了不同的状态后台来将检查点和保存点持久化。默认地，保存点都保存在 <code>JobManager</code> 中，但你要为你的程序配置一个适当的后台状态程序，比如 <code>RocksDB</code> 等。</li>\n</ul>\n<p>如果你已经在运行一个容错的程序了，那就创建一个保存点，然后从保存点的位置开始重新启动程序，这只需要在 <code>Flink</code> 命令行里敲几个命令就可以了。咱们接下来挨个看看。</p>\n<p>(1) 第一步：创建一个保存点</p>\n<p>首先，获得所有运行中的 <code>Flink</code> 任务的列表：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink list</span><br><span class=\"line\">------------Running/Restarting Jobs------------</span><br><span class=\"line\">10.10.2016 16:20:33 : job_id : Sample Job (RUNNING)</span><br></pre></td></tr></table></figure></p>\n<p>（运行上面的命令时，你的真实任务ID会是一个包括字母和数字的字符串。）</p>\n<p>然后，用相应的任务ID创建一个保存点：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink savepoint job_id</span><br></pre></td></tr></table></figure></p>\n<p>现在你的保存点就已经可用了。</p>\n<p>如果你准备马上根据你的保存点来重新启动任务，你通常会想要把现在正在运行的任务先停掉。你已经有了相应任务的ID，那把它停掉只要几秒钟就够了：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink cancel job_id</span><br></pre></td></tr></table></figure></p>\n<p>(2) 第二步：从一个保存点开始启动任务</p>\n<p>当你更新完程序之后，就可以从你的保存点开始启动任务了。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">user$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar</span><br></pre></td></tr></table></figure></p>\n<p>如果你想在一个示例程序中自己重做这些步骤，我们推荐你看看一篇之前的博客文章，我们在那里讲了怎么做这件事。</p>\n<h3 id=\"4-如果我想升级我的程序，该怎样做？\"><a href=\"#4-如果我想升级我的程序，该怎样做？\" class=\"headerlink\" title=\"4. 如果我想升级我的程序，该怎样做？\"></a>4. 如果我想升级我的程序，该怎样做？</h3><p>如果你想从一个保存点开始启动一个修改过的程序，有几件事是要考虑的。我们可以区别下面这两种情况：</p>\n<ul>\n<li>改变一个用户定义的函数的逻辑，比如MapFunction；</li>\n<li>改变一个程序的架构，也就是增加或减少算子等；</li>\n</ul>\n<p>第一种情况很简单，不需要什么特别的准备。你可以按你的需要去修改函数代码。不过，如果你用一个修改了的架构从保存点开始启动程序，那么为了能够恢复算子的状态，<code>Flink</code> 必须能够将保存点程序的算子与使用了新架构的新程序的算子对应起来。</p>\n<p>在这种情况下，你就要手动地将算子ID分配给最初的和更新了的程序。因为如果没有算子ID的话，是没办法修改程序的架构的。所以最佳实践经验就要求一定要分配算子ID。</p>\n<p>下面的代码段显示了如何为算子分配ID：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream stream = env.</span><br><span class=\"line\"> // Stateful source (e.g. Kafka) with ID</span><br><span class=\"line\"> .addSource(new StatefulSource())</span><br><span class=\"line\"> .uid(“source-id”)</span><br><span class=\"line\"> .shuffle()</span><br><span class=\"line\"> // The stateful mapper with ID</span><br><span class=\"line\"> .map(new StatefulMapper())</span><br><span class=\"line\"> .uid(“mapper-id”)</span><br><span class=\"line\"></span><br><span class=\"line\">// Stateless sink (no specific ID required)</span><br><span class=\"line\">stream.print()</span><br></pre></td></tr></table></figure></p>\n<p>请查阅文档，了解更多关于升级程序和保存点的细节。</p>\n<h3 id=\"5-关于保存点的最佳实践\"><a href=\"#5-关于保存点的最佳实践\" class=\"headerlink\" title=\"5. 关于保存点的最佳实践\"></a>5. 关于保存点的最佳实践</h3><p>要更好的利用上文中描述的 <code>Flink</code> 的重新处理功能，你应该经常触发，生成新的保存点。我们建议要根据某些时刻表（比如每天一次，每周一次，等等）自动地生成保存点，而且每当你关闭某个任务或发布程序的新版本时，也最好先生成保存点。</p>\n<p>依据你想用 <code>Flink</code> 做的事件不同，生成保存点的最佳方法也会不同，但总的来说，在构建你的程序时你应该花些时间考虑如何使用这些保存点。</p>\n<h3 id=\"6-这些东西是怎么工作的呢？\"><a href=\"#6-这些东西是怎么工作的呢？\" class=\"headerlink\" title=\"6. 这些东西是怎么工作的呢？\"></a>6. 这些东西是怎么工作的呢？</h3><p>保存点事实上只是检查点的一个延伸，这就是 <code>Flink</code> 的容错机制。如果开启了检查点功能，<code>Flink</code> 就会周期性地为所有的算子状态生成一个一致的检查点。在<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/internals/stream_checkpointing.html\" target=\"_blank\" rel=\"noopener\">文档</a>中详细的描述了检查点的细节，如果你是个 <code>Flink</code> 新手，花些时间去读读是非常值得的。</p>\n<p>你可能会以为要生成一个一致的检查点，就得暂停数据处理，因为 <code>Flink</code> 必须要等着，直到所有没处理完的记录全被处理掉了，然后做个镜像，镜像生成之后再回去继续处理数据。事实并非如此！ <code>Flink</code> 是持续处理数据的，即使在生成检查点的时候也是这样。文档中的<code>Barriers</code>一节讲了实现这个功能的原理。</p>\n<p>两者之间的关键区别：检查点是基于某些规定的时间间隔自动生成的，而保存点是由用户显式地触发生成的，而且不会象检查点那样过了一定的时间之后就会被删掉。</p>\n<h3 id=\"7-总结\"><a href=\"#7-总结\" class=\"headerlink\" title=\"7. 总结\"></a>7. 总结</h3><p>我们讨论了 <code>Apache Flink</code> 的保存点和数据重处理功能，因为我们相信这就是 <code>Flink</code> 与开源世界中其它流处理器之间的重要区别之一。而且最重要的，在容错的 <code>Flink</code> 程序中获得重处理功能几乎是不需要任何代价的，只需要很少的改动。</p>\n<p><code>Flink</code> 社区现在还在积极地工作着，要把保存点功能做得更好，包括在改变并发度的情况下保存状态的解决方案等。有些相应的功能（比如Flink-3755）已经发布到主分支上了，而且会被包含到下一个小版本Flink 1.2.0中。</p>\n<p>所以，当你需要把程序多部署一份，或者上个新版本，或者要做A/B测试，或者要让多个程序从同一个点开始处理数据时，你可以这么做了，而且不会丢失那些宝贵的状态数据。</p>\n<p>当有真实的需求时，流处理基于实时的特性不应该阻挡你把时间调回过去的动作。</p>\n<p>有兴趣了解关于 <code>Apache FLink</code> 的保存点的更多内容吗？数据工匠CTO Stephan Ewen做了一个关于这个话题的<a href=\"https://mapr.com/blog/savepoints-apache-flink-stream-processing-whiteboard-walkthrough/\" target=\"_blank\" rel=\"noopener\">七分钟白板演练</a>，你可以在MapR博客上看到相关内容。</p>\n<p>原文: <a href=\"https://data-artisans.com/blog/turning-back-time-savepoints\" target=\"_blank\" rel=\"noopener\">https://data-artisans.com/blog/turning-back-time-savepoints</a></p>\n<p>译文: <a href=\"http://www.infoq.com/cn/articles/turning-back-time-savepoints\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/turning-back-time-savepoints</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 图解Watermark","date":"2018-01-15T06:47:01.000Z","_content":"\n如果你正在构建实时流处理应用程序，那么事件时间处理是你迟早必须使用的功能之一。因为在现实世界的大多数用例中，消息到达都是无序的，应该有一些方法，通过你建立的系统知道消息可能延迟到达，并且有相应的处理方案。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中使用它。\n\n`EventTime`是事件在现实世界中发生的时间，`ProcessingTime`是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于处理时间的系统，看看它的缺点。\n\n我们创建一个大小为10秒的滑动窗口，每5秒滑动一次，在窗口结束时，系统将发送在此期间收到的消息数。 一旦了解了`EventTime`处理在滑动窗口如何工作，那么了解其在滚动窗口中如何工作也就不是难事。所以让我们开始吧。\n\n### 1. 基于处理时间的系统\n\n在这个例子中，我们期望消息具有一定格式的值，时间戳就是消息的那个值，同时时间戳是在源产生此消息的时间。由于我们正在构建基于处理时间的系统，因此以下代码忽略了时间戳部分。\n\n我们需要知道消息中应包含消息产生时间是很重要的。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己生成这个产生时间。稍后我们将看到，事件时间处理提取此时间戳信息来处理延迟消息。\n\n```\nval text = senv.socketTextStream(\"localhost\", 9999)\nval counts = text.map {(m: String) => (m.split(\",\")(0), 1) }\n    .keyBy(0)\n    .timeWindow(Time.seconds(10), Time.seconds(5))\n    .sum(1)\ncounts.print\nsenv.execute(\"ProcessingTime processing example\")\n```\n\n#### 1.1 消息无延迟到达\n\n假设源分别在第13秒产生两个类型a的消息以及在第16秒产生一个消息。(小时和分钟不重要，因为窗口大小只有10秒)。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-0.png?raw=true)\n\n这些消息将落入如下所示窗口中。前两个在第13秒产生的消息将落入窗口1`[5s-15s]`和窗口2`[10s-20s]`中，第三个在第16秒产生的消息将落入窗口2`[10s-20s]`和窗口3`[15s-25s]`中。每个窗口得到的最终计数分别为(a，2)，(a，3)和(a，1)。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true)\n\n该输出跟预期的输出是一样的。现在我们看看当一个消息延迟到达系统时会发生什么。\n\n#### 1.2 消息延迟到达\n\n现在假设其中一条消息(在第13秒产生)可能由于网络拥塞延迟6秒(第19秒到达)。你能猜测出这个消息会落入哪个窗口？\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true)\n\n延迟的消息落入窗口2和窗口3中，因为19在10-20和15-25之间。窗口2的计算没有任何问题(因为消息本应该落入这个窗口)，但是它影响了窗口1和窗口3的计算结果。现在我们将尝试使用基于`EventTime`处理来解决这个问题。\n\n### 2. 基于EventTime的系统\n\n要使用基于`EventTime`处理，我们需要一个时间戳提取器，从消息中提取事件时间信息。请记住，消息是有格式值，时间戳。 `extractTimestamp`方法获取时间戳并将其作为Long类型返回。现在忽略`getCurrentWatermark`方法，我们稍后会介绍：\n\n```\nclass TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable {\n  override def extractTimestamp(e: String, prevElementTimestamp: Long) = {\n    e.split(\",\")(1).toLong\n  }\n  override def getCurrentWatermark(): Watermark = {\n      new Watermark(System.currentTimeMillis)\n  }\n}\n```\n\n现在我们需要设置这个时间戳提取器，并将`TimeCharactersistic`设置为`EventTime`。其余的代码与`ProcessingTime`的情况保持一致：\n\n```\nsenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\nval text = senv.socketTextStream(\"localhost\", 9999)\n                .assignTimestampsAndWatermarks(new TimestampExtractor)\nval counts = text.map {(m: String) => (m.split(\",\")(0), 1) }\n      .keyBy(0)\n      .timeWindow(Time.seconds(10), Time.seconds(5))\n      .sum(1)\ncounts.print\nsenv.execute(\"EventTime processing example\")\n```\n运行上述代码的结果如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-4.png?raw=true)\n\n结果看起来更好一些，窗口2和3现在是正确的结果，但是窗口1仍然是有问题的。Flink没有将延迟的消息分配给窗口3，是因为在当前检查消息的事件时间，知道它不应该出现在窗口3中。但是为什么没有将消息分配给窗口1？原因是当延迟的信息到达系统时(第19秒)，窗口1的评估(\nevaluation)已经完成了(第15秒)。现在让我们尝试通过使用`Watermark`来解决这个问题。\n\n### 3. Watermark\n\n`Watermark`是一个非常重要概念，我将尽力给你一个简短的概述。如果你有兴趣了解更多信息，你可以从`Google`中观看这个[演讲](https://www.youtube.com/watch?v=3UfZN59Nsk8)，还可以从`dataArtisans`那里阅读此[博客](https://data-artisans.com/blog/how-apache-flink-enables-new-streaming-applications-part-1)。 `Watermark`本质上是一个时间戳。当`Flink`中的算子(operator)接收到`Watermark`时，它明白它不会再看到比该时间戳更早的消息。因此`Watermark`也可以被认为是告诉`Flink`在`EventTime`中多远的一种方式。\n\n在这个例子的目的，就是把`Watermark`看作是告诉`Flink`一个消息可能延迟多少的方式。在上一次尝试中，我们将`Watermark`设置为当前系统时间。因此，期望消息没有任何的延迟。现在我们将`Watermark`设置为当前时间减去5秒，这就告诉`Flink`我们期望消息最多延迟5秒钟，这是因为每个窗口仅在`Watermark`通过时被评估。由于我们的`Watermark`是当前时间减去5秒，所以第一个窗口`[5s-15s]`将会在第20秒被评估。类似地，窗口`[10s-20s]`将会在第25秒进行评估，依此类推(译者注:窗口延迟评估)。\n\n```\noverride def getCurrentWatermark(): Watermark = {\n      new Watermark(System.currentTimeMillis - 5000)\n}\n```\n这里我们假定事件时间比当前系统时间晚5秒，但事实并非总是如此(有可能6秒，7秒等等)。在许多情况下，最好保留迄今为止收到的最大时间戳(从消息中提取)。使用迄今为止收到的最大时间戳减去预期的延迟时间来代替用当前系统时间减去预期的延迟时间。\n\n进行上述更改后运行代码的结果是：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-5.png?raw=true)\n\n\n最后我们得到了正确的结果，所有窗口都按照预期输出计数，(a，2)，(a，3)和(a，1)。\n\n### 4. Allowed Lateness\n\n我们也可以使用`AllowedLateness`功能设置消息的最大允许延迟时间来解决这个问题。\n\n在我们之前使用`Watermark - delay`的方法中，只有当`Watermark`超过`window_length + delay`时，窗口才会被触发计算。如果你想要适应延迟事件，并希望窗口按时触发，则可以使用`Allowed Lateness`。 如果设置了允许延迟，`Flink`不会丢弃消息，除非它超过了`window_end_time + delay`的延迟时间。一旦收到一个延迟消息，`Flink`会提取它的时间戳并检查是否在允许的延迟时间内，然后检查是否触发窗口(按照触发器设置)。 因此，请注意，在这种方法中可能会多次触发窗口，如果你仅需要一次处理，你需要使你的`sink`具有幂等性。\n\n### 5. 结论\n\n实时流处理系统的重要性日益增长，延迟消息的处理是你构建任何此类系统的一部分。在这篇博文中，我们看到延迟到达的消息会影响系统的结果，以及如何使用ApacheFlink的事件时间功能来解决它们。\n\n\n原文:http://vishnuviswanath.com/flink_eventtime.html\n","source":"_posts/Flink/[Flink]Flink1.4 图解Watermark.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 图解Watermark\ndate: 2018-01-15 14:47:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n如果你正在构建实时流处理应用程序，那么事件时间处理是你迟早必须使用的功能之一。因为在现实世界的大多数用例中，消息到达都是无序的，应该有一些方法，通过你建立的系统知道消息可能延迟到达，并且有相应的处理方案。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中使用它。\n\n`EventTime`是事件在现实世界中发生的时间，`ProcessingTime`是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于处理时间的系统，看看它的缺点。\n\n我们创建一个大小为10秒的滑动窗口，每5秒滑动一次，在窗口结束时，系统将发送在此期间收到的消息数。 一旦了解了`EventTime`处理在滑动窗口如何工作，那么了解其在滚动窗口中如何工作也就不是难事。所以让我们开始吧。\n\n### 1. 基于处理时间的系统\n\n在这个例子中，我们期望消息具有一定格式的值，时间戳就是消息的那个值，同时时间戳是在源产生此消息的时间。由于我们正在构建基于处理时间的系统，因此以下代码忽略了时间戳部分。\n\n我们需要知道消息中应包含消息产生时间是很重要的。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己生成这个产生时间。稍后我们将看到，事件时间处理提取此时间戳信息来处理延迟消息。\n\n```\nval text = senv.socketTextStream(\"localhost\", 9999)\nval counts = text.map {(m: String) => (m.split(\",\")(0), 1) }\n    .keyBy(0)\n    .timeWindow(Time.seconds(10), Time.seconds(5))\n    .sum(1)\ncounts.print\nsenv.execute(\"ProcessingTime processing example\")\n```\n\n#### 1.1 消息无延迟到达\n\n假设源分别在第13秒产生两个类型a的消息以及在第16秒产生一个消息。(小时和分钟不重要，因为窗口大小只有10秒)。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-0.png?raw=true)\n\n这些消息将落入如下所示窗口中。前两个在第13秒产生的消息将落入窗口1`[5s-15s]`和窗口2`[10s-20s]`中，第三个在第16秒产生的消息将落入窗口2`[10s-20s]`和窗口3`[15s-25s]`中。每个窗口得到的最终计数分别为(a，2)，(a，3)和(a，1)。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true)\n\n该输出跟预期的输出是一样的。现在我们看看当一个消息延迟到达系统时会发生什么。\n\n#### 1.2 消息延迟到达\n\n现在假设其中一条消息(在第13秒产生)可能由于网络拥塞延迟6秒(第19秒到达)。你能猜测出这个消息会落入哪个窗口？\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true)\n\n延迟的消息落入窗口2和窗口3中，因为19在10-20和15-25之间。窗口2的计算没有任何问题(因为消息本应该落入这个窗口)，但是它影响了窗口1和窗口3的计算结果。现在我们将尝试使用基于`EventTime`处理来解决这个问题。\n\n### 2. 基于EventTime的系统\n\n要使用基于`EventTime`处理，我们需要一个时间戳提取器，从消息中提取事件时间信息。请记住，消息是有格式值，时间戳。 `extractTimestamp`方法获取时间戳并将其作为Long类型返回。现在忽略`getCurrentWatermark`方法，我们稍后会介绍：\n\n```\nclass TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable {\n  override def extractTimestamp(e: String, prevElementTimestamp: Long) = {\n    e.split(\",\")(1).toLong\n  }\n  override def getCurrentWatermark(): Watermark = {\n      new Watermark(System.currentTimeMillis)\n  }\n}\n```\n\n现在我们需要设置这个时间戳提取器，并将`TimeCharactersistic`设置为`EventTime`。其余的代码与`ProcessingTime`的情况保持一致：\n\n```\nsenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\nval text = senv.socketTextStream(\"localhost\", 9999)\n                .assignTimestampsAndWatermarks(new TimestampExtractor)\nval counts = text.map {(m: String) => (m.split(\",\")(0), 1) }\n      .keyBy(0)\n      .timeWindow(Time.seconds(10), Time.seconds(5))\n      .sum(1)\ncounts.print\nsenv.execute(\"EventTime processing example\")\n```\n运行上述代码的结果如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-4.png?raw=true)\n\n结果看起来更好一些，窗口2和3现在是正确的结果，但是窗口1仍然是有问题的。Flink没有将延迟的消息分配给窗口3，是因为在当前检查消息的事件时间，知道它不应该出现在窗口3中。但是为什么没有将消息分配给窗口1？原因是当延迟的信息到达系统时(第19秒)，窗口1的评估(\nevaluation)已经完成了(第15秒)。现在让我们尝试通过使用`Watermark`来解决这个问题。\n\n### 3. Watermark\n\n`Watermark`是一个非常重要概念，我将尽力给你一个简短的概述。如果你有兴趣了解更多信息，你可以从`Google`中观看这个[演讲](https://www.youtube.com/watch?v=3UfZN59Nsk8)，还可以从`dataArtisans`那里阅读此[博客](https://data-artisans.com/blog/how-apache-flink-enables-new-streaming-applications-part-1)。 `Watermark`本质上是一个时间戳。当`Flink`中的算子(operator)接收到`Watermark`时，它明白它不会再看到比该时间戳更早的消息。因此`Watermark`也可以被认为是告诉`Flink`在`EventTime`中多远的一种方式。\n\n在这个例子的目的，就是把`Watermark`看作是告诉`Flink`一个消息可能延迟多少的方式。在上一次尝试中，我们将`Watermark`设置为当前系统时间。因此，期望消息没有任何的延迟。现在我们将`Watermark`设置为当前时间减去5秒，这就告诉`Flink`我们期望消息最多延迟5秒钟，这是因为每个窗口仅在`Watermark`通过时被评估。由于我们的`Watermark`是当前时间减去5秒，所以第一个窗口`[5s-15s]`将会在第20秒被评估。类似地，窗口`[10s-20s]`将会在第25秒进行评估，依此类推(译者注:窗口延迟评估)。\n\n```\noverride def getCurrentWatermark(): Watermark = {\n      new Watermark(System.currentTimeMillis - 5000)\n}\n```\n这里我们假定事件时间比当前系统时间晚5秒，但事实并非总是如此(有可能6秒，7秒等等)。在许多情况下，最好保留迄今为止收到的最大时间戳(从消息中提取)。使用迄今为止收到的最大时间戳减去预期的延迟时间来代替用当前系统时间减去预期的延迟时间。\n\n进行上述更改后运行代码的结果是：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-5.png?raw=true)\n\n\n最后我们得到了正确的结果，所有窗口都按照预期输出计数，(a，2)，(a，3)和(a，1)。\n\n### 4. Allowed Lateness\n\n我们也可以使用`AllowedLateness`功能设置消息的最大允许延迟时间来解决这个问题。\n\n在我们之前使用`Watermark - delay`的方法中，只有当`Watermark`超过`window_length + delay`时，窗口才会被触发计算。如果你想要适应延迟事件，并希望窗口按时触发，则可以使用`Allowed Lateness`。 如果设置了允许延迟，`Flink`不会丢弃消息，除非它超过了`window_end_time + delay`的延迟时间。一旦收到一个延迟消息，`Flink`会提取它的时间戳并检查是否在允许的延迟时间内，然后检查是否触发窗口(按照触发器设置)。 因此，请注意，在这种方法中可能会多次触发窗口，如果你仅需要一次处理，你需要使你的`sink`具有幂等性。\n\n### 5. 结论\n\n实时流处理系统的重要性日益增长，延迟消息的处理是你构建任何此类系统的一部分。在这篇博文中，我们看到延迟到达的消息会影响系统的结果，以及如何使用ApacheFlink的事件时间功能来解决它们。\n\n\n原文:http://vishnuviswanath.com/flink_eventtime.html\n","slug":"Flink/[Flink]Flink1.4 图解Watermark","published":1,"updated":"2018-01-29T09:36:59.655Z","comments":1,"photos":[],"link":"","_id":"cje58tiqq001cordb5r3i1xt1","content":"<p>如果你正在构建实时流处理应用程序，那么事件时间处理是你迟早必须使用的功能之一。因为在现实世界的大多数用例中，消息到达都是无序的，应该有一些方法，通过你建立的系统知道消息可能延迟到达，并且有相应的处理方案。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中使用它。</p>\n<p><code>EventTime</code>是事件在现实世界中发生的时间，<code>ProcessingTime</code>是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于处理时间的系统，看看它的缺点。</p>\n<p>我们创建一个大小为10秒的滑动窗口，每5秒滑动一次，在窗口结束时，系统将发送在此期间收到的消息数。 一旦了解了<code>EventTime</code>处理在滑动窗口如何工作，那么了解其在滚动窗口中如何工作也就不是难事。所以让我们开始吧。</p>\n<h3 id=\"1-基于处理时间的系统\"><a href=\"#1-基于处理时间的系统\" class=\"headerlink\" title=\"1. 基于处理时间的系统\"></a>1. 基于处理时间的系统</h3><p>在这个例子中，我们期望消息具有一定格式的值，时间戳就是消息的那个值，同时时间戳是在源产生此消息的时间。由于我们正在构建基于处理时间的系统，因此以下代码忽略了时间戳部分。</p>\n<p>我们需要知道消息中应包含消息产生时间是很重要的。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己生成这个产生时间。稍后我们将看到，事件时间处理提取此时间戳信息来处理延迟消息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val text = senv.socketTextStream(&quot;localhost&quot;, 9999)</span><br><span class=\"line\">val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125;</span><br><span class=\"line\">    .keyBy(0)</span><br><span class=\"line\">    .timeWindow(Time.seconds(10), Time.seconds(5))</span><br><span class=\"line\">    .sum(1)</span><br><span class=\"line\">counts.print</span><br><span class=\"line\">senv.execute(&quot;ProcessingTime processing example&quot;)</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-1-消息无延迟到达\"><a href=\"#1-1-消息无延迟到达\" class=\"headerlink\" title=\"1.1 消息无延迟到达\"></a>1.1 消息无延迟到达</h4><p>假设源分别在第13秒产生两个类型a的消息以及在第16秒产生一个消息。(小时和分钟不重要，因为窗口大小只有10秒)。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-0.png?raw=true\" alt=\"\"></p>\n<p>这些消息将落入如下所示窗口中。前两个在第13秒产生的消息将落入窗口1<code>[5s-15s]</code>和窗口2<code>[10s-20s]</code>中，第三个在第16秒产生的消息将落入窗口2<code>[10s-20s]</code>和窗口3<code>[15s-25s]</code>中。每个窗口得到的最终计数分别为(a，2)，(a，3)和(a，1)。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true\" alt=\"\"></p>\n<p>该输出跟预期的输出是一样的。现在我们看看当一个消息延迟到达系统时会发生什么。</p>\n<h4 id=\"1-2-消息延迟到达\"><a href=\"#1-2-消息延迟到达\" class=\"headerlink\" title=\"1.2 消息延迟到达\"></a>1.2 消息延迟到达</h4><p>现在假设其中一条消息(在第13秒产生)可能由于网络拥塞延迟6秒(第19秒到达)。你能猜测出这个消息会落入哪个窗口？</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true\" alt=\"\"></p>\n<p>延迟的消息落入窗口2和窗口3中，因为19在10-20和15-25之间。窗口2的计算没有任何问题(因为消息本应该落入这个窗口)，但是它影响了窗口1和窗口3的计算结果。现在我们将尝试使用基于<code>EventTime</code>处理来解决这个问题。</p>\n<h3 id=\"2-基于EventTime的系统\"><a href=\"#2-基于EventTime的系统\" class=\"headerlink\" title=\"2. 基于EventTime的系统\"></a>2. 基于EventTime的系统</h3><p>要使用基于<code>EventTime</code>处理，我们需要一个时间戳提取器，从消息中提取事件时间信息。请记住，消息是有格式值，时间戳。 <code>extractTimestamp</code>方法获取时间戳并将其作为Long类型返回。现在忽略<code>getCurrentWatermark</code>方法，我们稍后会介绍：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable &#123;</span><br><span class=\"line\">  override def extractTimestamp(e: String, prevElementTimestamp: Long) = &#123;</span><br><span class=\"line\">    e.split(&quot;,&quot;)(1).toLong</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">      new Watermark(System.currentTimeMillis)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>现在我们需要设置这个时间戳提取器，并将<code>TimeCharactersistic</code>设置为<code>EventTime</code>。其余的代码与<code>ProcessingTime</code>的情况保持一致：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">senv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">val text = senv.socketTextStream(&quot;localhost&quot;, 9999)</span><br><span class=\"line\">                .assignTimestampsAndWatermarks(new TimestampExtractor)</span><br><span class=\"line\">val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125;</span><br><span class=\"line\">      .keyBy(0)</span><br><span class=\"line\">      .timeWindow(Time.seconds(10), Time.seconds(5))</span><br><span class=\"line\">      .sum(1)</span><br><span class=\"line\">counts.print</span><br><span class=\"line\">senv.execute(&quot;EventTime processing example&quot;)</span><br></pre></td></tr></table></figure>\n<p>运行上述代码的结果如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-4.png?raw=true\" alt=\"\"></p>\n<p>结果看起来更好一些，窗口2和3现在是正确的结果，但是窗口1仍然是有问题的。Flink没有将延迟的消息分配给窗口3，是因为在当前检查消息的事件时间，知道它不应该出现在窗口3中。但是为什么没有将消息分配给窗口1？原因是当延迟的信息到达系统时(第19秒)，窗口1的评估(<br>evaluation)已经完成了(第15秒)。现在让我们尝试通过使用<code>Watermark</code>来解决这个问题。</p>\n<h3 id=\"3-Watermark\"><a href=\"#3-Watermark\" class=\"headerlink\" title=\"3. Watermark\"></a>3. Watermark</h3><p><code>Watermark</code>是一个非常重要概念，我将尽力给你一个简短的概述。如果你有兴趣了解更多信息，你可以从<code>Google</code>中观看这个<a href=\"https://www.youtube.com/watch?v=3UfZN59Nsk8\" target=\"_blank\" rel=\"noopener\">演讲</a>，还可以从<code>dataArtisans</code>那里阅读此<a href=\"https://data-artisans.com/blog/how-apache-flink-enables-new-streaming-applications-part-1\" target=\"_blank\" rel=\"noopener\">博客</a>。 <code>Watermark</code>本质上是一个时间戳。当<code>Flink</code>中的算子(operator)接收到<code>Watermark</code>时，它明白它不会再看到比该时间戳更早的消息。因此<code>Watermark</code>也可以被认为是告诉<code>Flink</code>在<code>EventTime</code>中多远的一种方式。</p>\n<p>在这个例子的目的，就是把<code>Watermark</code>看作是告诉<code>Flink</code>一个消息可能延迟多少的方式。在上一次尝试中，我们将<code>Watermark</code>设置为当前系统时间。因此，期望消息没有任何的延迟。现在我们将<code>Watermark</code>设置为当前时间减去5秒，这就告诉<code>Flink</code>我们期望消息最多延迟5秒钟，这是因为每个窗口仅在<code>Watermark</code>通过时被评估。由于我们的<code>Watermark</code>是当前时间减去5秒，所以第一个窗口<code>[5s-15s]</code>将会在第20秒被评估。类似地，窗口<code>[10s-20s]</code>将会在第25秒进行评估，依此类推(译者注:窗口延迟评估)。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">      new Watermark(System.currentTimeMillis - 5000)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里我们假定事件时间比当前系统时间晚5秒，但事实并非总是如此(有可能6秒，7秒等等)。在许多情况下，最好保留迄今为止收到的最大时间戳(从消息中提取)。使用迄今为止收到的最大时间戳减去预期的延迟时间来代替用当前系统时间减去预期的延迟时间。</p>\n<p>进行上述更改后运行代码的结果是：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-5.png?raw=true\" alt=\"\"></p>\n<p>最后我们得到了正确的结果，所有窗口都按照预期输出计数，(a，2)，(a，3)和(a，1)。</p>\n<h3 id=\"4-Allowed-Lateness\"><a href=\"#4-Allowed-Lateness\" class=\"headerlink\" title=\"4. Allowed Lateness\"></a>4. Allowed Lateness</h3><p>我们也可以使用<code>AllowedLateness</code>功能设置消息的最大允许延迟时间来解决这个问题。</p>\n<p>在我们之前使用<code>Watermark - delay</code>的方法中，只有当<code>Watermark</code>超过<code>window_length + delay</code>时，窗口才会被触发计算。如果你想要适应延迟事件，并希望窗口按时触发，则可以使用<code>Allowed Lateness</code>。 如果设置了允许延迟，<code>Flink</code>不会丢弃消息，除非它超过了<code>window_end_time + delay</code>的延迟时间。一旦收到一个延迟消息，<code>Flink</code>会提取它的时间戳并检查是否在允许的延迟时间内，然后检查是否触发窗口(按照触发器设置)。 因此，请注意，在这种方法中可能会多次触发窗口，如果你仅需要一次处理，你需要使你的<code>sink</code>具有幂等性。</p>\n<h3 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5. 结论\"></a>5. 结论</h3><p>实时流处理系统的重要性日益增长，延迟消息的处理是你构建任何此类系统的一部分。在这篇博文中，我们看到延迟到达的消息会影响系统的结果，以及如何使用ApacheFlink的事件时间功能来解决它们。</p>\n<p>原文:<a href=\"http://vishnuviswanath.com/flink_eventtime.html\" target=\"_blank\" rel=\"noopener\">http://vishnuviswanath.com/flink_eventtime.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>如果你正在构建实时流处理应用程序，那么事件时间处理是你迟早必须使用的功能之一。因为在现实世界的大多数用例中，消息到达都是无序的，应该有一些方法，通过你建立的系统知道消息可能延迟到达，并且有相应的处理方案。在这篇博文中，我们将看到为什么我们需要事件时间处理，以及我们如何在ApacheFlink中使用它。</p>\n<p><code>EventTime</code>是事件在现实世界中发生的时间，<code>ProcessingTime</code>是Flink系统处理该事件的时间。要了解事件时间处理的重要性，我们首先要建立一个基于处理时间的系统，看看它的缺点。</p>\n<p>我们创建一个大小为10秒的滑动窗口，每5秒滑动一次，在窗口结束时，系统将发送在此期间收到的消息数。 一旦了解了<code>EventTime</code>处理在滑动窗口如何工作，那么了解其在滚动窗口中如何工作也就不是难事。所以让我们开始吧。</p>\n<h3 id=\"1-基于处理时间的系统\"><a href=\"#1-基于处理时间的系统\" class=\"headerlink\" title=\"1. 基于处理时间的系统\"></a>1. 基于处理时间的系统</h3><p>在这个例子中，我们期望消息具有一定格式的值，时间戳就是消息的那个值，同时时间戳是在源产生此消息的时间。由于我们正在构建基于处理时间的系统，因此以下代码忽略了时间戳部分。</p>\n<p>我们需要知道消息中应包含消息产生时间是很重要的。Flink或任何其他系统不是一个魔术盒，可以以某种方式自己生成这个产生时间。稍后我们将看到，事件时间处理提取此时间戳信息来处理延迟消息。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val text = senv.socketTextStream(&quot;localhost&quot;, 9999)</span><br><span class=\"line\">val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125;</span><br><span class=\"line\">    .keyBy(0)</span><br><span class=\"line\">    .timeWindow(Time.seconds(10), Time.seconds(5))</span><br><span class=\"line\">    .sum(1)</span><br><span class=\"line\">counts.print</span><br><span class=\"line\">senv.execute(&quot;ProcessingTime processing example&quot;)</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-1-消息无延迟到达\"><a href=\"#1-1-消息无延迟到达\" class=\"headerlink\" title=\"1.1 消息无延迟到达\"></a>1.1 消息无延迟到达</h4><p>假设源分别在第13秒产生两个类型a的消息以及在第16秒产生一个消息。(小时和分钟不重要，因为窗口大小只有10秒)。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-0.png?raw=true\" alt=\"\"></p>\n<p>这些消息将落入如下所示窗口中。前两个在第13秒产生的消息将落入窗口1<code>[5s-15s]</code>和窗口2<code>[10s-20s]</code>中，第三个在第16秒产生的消息将落入窗口2<code>[10s-20s]</code>和窗口3<code>[15s-25s]</code>中。每个窗口得到的最终计数分别为(a，2)，(a，3)和(a，1)。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-2.png?raw=true\" alt=\"\"></p>\n<p>该输出跟预期的输出是一样的。现在我们看看当一个消息延迟到达系统时会发生什么。</p>\n<h4 id=\"1-2-消息延迟到达\"><a href=\"#1-2-消息延迟到达\" class=\"headerlink\" title=\"1.2 消息延迟到达\"></a>1.2 消息延迟到达</h4><p>现在假设其中一条消息(在第13秒产生)可能由于网络拥塞延迟6秒(第19秒到达)。你能猜测出这个消息会落入哪个窗口？</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-3.png?raw=true\" alt=\"\"></p>\n<p>延迟的消息落入窗口2和窗口3中，因为19在10-20和15-25之间。窗口2的计算没有任何问题(因为消息本应该落入这个窗口)，但是它影响了窗口1和窗口3的计算结果。现在我们将尝试使用基于<code>EventTime</code>处理来解决这个问题。</p>\n<h3 id=\"2-基于EventTime的系统\"><a href=\"#2-基于EventTime的系统\" class=\"headerlink\" title=\"2. 基于EventTime的系统\"></a>2. 基于EventTime的系统</h3><p>要使用基于<code>EventTime</code>处理，我们需要一个时间戳提取器，从消息中提取事件时间信息。请记住，消息是有格式值，时间戳。 <code>extractTimestamp</code>方法获取时间戳并将其作为Long类型返回。现在忽略<code>getCurrentWatermark</code>方法，我们稍后会介绍：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class TimestampExtractor extends AssignerWithPeriodicWatermarks[String] with Serializable &#123;</span><br><span class=\"line\">  override def extractTimestamp(e: String, prevElementTimestamp: Long) = &#123;</span><br><span class=\"line\">    e.split(&quot;,&quot;)(1).toLong</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">      new Watermark(System.currentTimeMillis)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>现在我们需要设置这个时间戳提取器，并将<code>TimeCharactersistic</code>设置为<code>EventTime</code>。其余的代码与<code>ProcessingTime</code>的情况保持一致：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">senv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\">val text = senv.socketTextStream(&quot;localhost&quot;, 9999)</span><br><span class=\"line\">                .assignTimestampsAndWatermarks(new TimestampExtractor)</span><br><span class=\"line\">val counts = text.map &#123;(m: String) =&gt; (m.split(&quot;,&quot;)(0), 1) &#125;</span><br><span class=\"line\">      .keyBy(0)</span><br><span class=\"line\">      .timeWindow(Time.seconds(10), Time.seconds(5))</span><br><span class=\"line\">      .sum(1)</span><br><span class=\"line\">counts.print</span><br><span class=\"line\">senv.execute(&quot;EventTime processing example&quot;)</span><br></pre></td></tr></table></figure>\n<p>运行上述代码的结果如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-4.png?raw=true\" alt=\"\"></p>\n<p>结果看起来更好一些，窗口2和3现在是正确的结果，但是窗口1仍然是有问题的。Flink没有将延迟的消息分配给窗口3，是因为在当前检查消息的事件时间，知道它不应该出现在窗口3中。但是为什么没有将消息分配给窗口1？原因是当延迟的信息到达系统时(第19秒)，窗口1的评估(<br>evaluation)已经完成了(第15秒)。现在让我们尝试通过使用<code>Watermark</code>来解决这个问题。</p>\n<h3 id=\"3-Watermark\"><a href=\"#3-Watermark\" class=\"headerlink\" title=\"3. Watermark\"></a>3. Watermark</h3><p><code>Watermark</code>是一个非常重要概念，我将尽力给你一个简短的概述。如果你有兴趣了解更多信息，你可以从<code>Google</code>中观看这个<a href=\"https://www.youtube.com/watch?v=3UfZN59Nsk8\" target=\"_blank\" rel=\"noopener\">演讲</a>，还可以从<code>dataArtisans</code>那里阅读此<a href=\"https://data-artisans.com/blog/how-apache-flink-enables-new-streaming-applications-part-1\" target=\"_blank\" rel=\"noopener\">博客</a>。 <code>Watermark</code>本质上是一个时间戳。当<code>Flink</code>中的算子(operator)接收到<code>Watermark</code>时，它明白它不会再看到比该时间戳更早的消息。因此<code>Watermark</code>也可以被认为是告诉<code>Flink</code>在<code>EventTime</code>中多远的一种方式。</p>\n<p>在这个例子的目的，就是把<code>Watermark</code>看作是告诉<code>Flink</code>一个消息可能延迟多少的方式。在上一次尝试中，我们将<code>Watermark</code>设置为当前系统时间。因此，期望消息没有任何的延迟。现在我们将<code>Watermark</code>设置为当前时间减去5秒，这就告诉<code>Flink</code>我们期望消息最多延迟5秒钟，这是因为每个窗口仅在<code>Watermark</code>通过时被评估。由于我们的<code>Watermark</code>是当前时间减去5秒，所以第一个窗口<code>[5s-15s]</code>将会在第20秒被评估。类似地，窗口<code>[10s-20s]</code>将会在第25秒进行评估，依此类推(译者注:窗口延迟评估)。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">      new Watermark(System.currentTimeMillis - 5000)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里我们假定事件时间比当前系统时间晚5秒，但事实并非总是如此(有可能6秒，7秒等等)。在许多情况下，最好保留迄今为止收到的最大时间戳(从消息中提取)。使用迄今为止收到的最大时间戳减去预期的延迟时间来代替用当前系统时间减去预期的延迟时间。</p>\n<p>进行上述更改后运行代码的结果是：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E5%9B%BE%E8%A7%A3%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8EWatermarks-5.png?raw=true\" alt=\"\"></p>\n<p>最后我们得到了正确的结果，所有窗口都按照预期输出计数，(a，2)，(a，3)和(a，1)。</p>\n<h3 id=\"4-Allowed-Lateness\"><a href=\"#4-Allowed-Lateness\" class=\"headerlink\" title=\"4. Allowed Lateness\"></a>4. Allowed Lateness</h3><p>我们也可以使用<code>AllowedLateness</code>功能设置消息的最大允许延迟时间来解决这个问题。</p>\n<p>在我们之前使用<code>Watermark - delay</code>的方法中，只有当<code>Watermark</code>超过<code>window_length + delay</code>时，窗口才会被触发计算。如果你想要适应延迟事件，并希望窗口按时触发，则可以使用<code>Allowed Lateness</code>。 如果设置了允许延迟，<code>Flink</code>不会丢弃消息，除非它超过了<code>window_end_time + delay</code>的延迟时间。一旦收到一个延迟消息，<code>Flink</code>会提取它的时间戳并检查是否在允许的延迟时间内，然后检查是否触发窗口(按照触发器设置)。 因此，请注意，在这种方法中可能会多次触发窗口，如果你仅需要一次处理，你需要使你的<code>sink</code>具有幂等性。</p>\n<h3 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5. 结论\"></a>5. 结论</h3><p>实时流处理系统的重要性日益增长，延迟消息的处理是你构建任何此类系统的一部分。在这篇博文中，我们看到延迟到达的消息会影响系统的结果，以及如何使用ApacheFlink的事件时间功能来解决它们。</p>\n<p>原文:<a href=\"http://vishnuviswanath.com/flink_eventtime.html\" target=\"_blank\" rel=\"noopener\">http://vishnuviswanath.com/flink_eventtime.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 定义keys的几种方法","date":"2018-01-04T03:07:01.000Z","_content":"\n一些转换(例如，`join`，`coGroup`，`keyBy`，`groupBy`)要求在一组元素上定义一个key。其他转换(`Reduce`，`GroupReduce`，`Aggregate`，`Windows`)允许在使用这些函数之前根据`key`对数据进行分组。\n\n一个`DataSet`进行分组如下:\n```java\nDataSet<...> input = // [...]\nDataSet<...> reduced = input\n.groupBy(/*define key here*/)\n.reduceGroup(/*do something*/);\n```\n`DataStream`也可以指定一个`key`:\n```java\nDataStream<...> input = // [...]\nDataStream<...> windowed = input\n.keyBy(/*define key here*/)\n.window(/*window specification*/);\n```\nFlink的数据模型不是基于键值对。因此，没有必要将数据集类型打包成`keys`和`values`。`keys`是\"虚拟\"：它们只是被定义在实际数据之上的函数，以指导分组算子使用。\n\n备注:\n```\n在下面的讨论中，我们将使用DataStream API和keyBy。对于DataSet API，你只需要替换为DataSet和groupBy即可。\n```\n\n下面介绍几种`Flink`定义`keys`方法。\n\n### 1. 为Tuples类型定义keys\n\n最简单的情况就是在元组的一个或多个字段上对元组进行分组。下面是在元组的第一个字段(整数类型)上进行分组：\n\nJava版本:\n```java\nDataStream<Tuple3<Integer,String,Long>> input = // [...]\nKeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0)\n```\nScala版本:\n```\nval input: DataStream[(Int, String, Long)] = // [...]\nval keyed = input.keyBy(0)\n```\n下面，我们将在复合key上对元组进行分组，复合key包含元组的第一个和第二个字段:\n\nJava版本:\n```java\nDataStream<Tuple3<Integer,String,Long>> input = // [...]\nKeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0,1)\n```\nScala版本:\n```\nval input: DataSet[(Int, String, Long)] = // [...]\nval grouped = input.groupBy(0,1)\n```\n\n如果你有一个包含嵌套元组的`DataStream`，例如：\n```\nDataStream<Tuple3<Tuple2<Integer, Float>,String,Long>> ds;\n```\n如果指定`keyBy(0)`，则使用整个`Tuple2`作为`key`(以`Integer`和`Float`为`key`)。如果要使用嵌套中`Tuple2`的某个字段，则必须使用下面介绍的字段表达式指定`keys`。\n\n### 2. 使用字段表达式定义keys\n\n你可以使用基于字符串的字段表达式来引用嵌套字段以及定义`keys`来进行分组，排序，连接或`coGrouping`。字段表达式可以非常容易地选择(嵌套)复合类型(如`Tuple`和`POJO`类型)中的字段。\n\n在下面的例子中，我们有一个`WC POJO`，它有两个字段`word`和`count`。如果想通过`word`字段分组，我们只需将`word`传递给`keyBy()`函数即可。\n\n```Java\n// some ordinary POJO (Plain old Java Object)\npublic class WC {\n  public String word;\n  public int count;\n}\nDataStream<WC> words = // [...]\nDataStream<WC> wordCounts = words.keyBy(\"word\").window(/*window specification*/);\n```\n\n字段表达式语法:\n\n(1) 按其字段名称选择`POJO`字段。例如，`user`是指向`POJO`类型的`user`字段。\n\n(2) 通过字段名称或0到`offset`的数值字段索引来选择元组字段(field name or 0-offset field index)。例如，`f0`和`5`分别指向`Java`元组类型的第一和第六字段。\n\n(3) 你可以在`POJO`和元组中选择嵌套字段。例如，`user.zip`是指`POJO`类型`user`字段中的`zip`字段。支持`POJO`和`Tuples`的任意嵌套和组合，如`f1.user.zip`或`user.f3.1.zip`。\n\n(4) 你可以使用`*`通配符表达式选择所有类型。这也适用于不是元组或`POJO`类型的类型。\n\nExample:\n```Java\npublic static class WC {\n  public ComplexNestedClass complex; //nested POJO\n  private int count;\n  // getter / setter for private field (count)\n  public int getCount() {\n    return count;\n  }\n  public void setCount(int c) {\n    this.count = c;\n  }\n}\npublic static class ComplexNestedClass {\n  public Integer someNumber;\n  public float someFloat;\n  public Tuple3<Long, Long, String> word;\n  public IntWritable hadoopCitizen;\n}\n```\n下面是上述示例代码的有效字段表达式：\n```\ncount：WC类中的count字段。\ncomplex：递归地选择复合字段POJO类型ComplexNestedClass的所有字段。\ncomplex.word.f2：选择嵌套字段Tuple3的最后一个字段。\ncomplex.hadoopCitizen：选择Hadoop IntWritable类型。\n```\n\n### 3. 使用key Selector 函数定义keys\n\n定义`key`的另一种方法是`key选择器`函数。key选择器函数将单个元素作为输入，并返回元素的key。`key`可以是任何类型的。\n\n以下示例显示了一个key选择器函数，它只返回一个对象的字段：\n\nJava版本:\n```java\npublic class WC {\n  public String word; public int count;\n}\n\nDataStream<WC> words = // [...]\nKeyedStream<WC> kyed = words.keyBy(new KeySelector<WC, String>() {\n     public String getKey(WC wc) { return wc.word; }\n});\n```\n\nScala版本:\n```\ncase class WC(word: String, count: Int)\nval words: DataStream[WC] = // [...]\nval keyed = words.keyBy( _.word )\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys\n","source":"_posts/Flink/[Flink]Flink1.4 定义keys的几种方法.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 定义keys的几种方法\ndate: 2018-01-04 11:07:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n一些转换(例如，`join`，`coGroup`，`keyBy`，`groupBy`)要求在一组元素上定义一个key。其他转换(`Reduce`，`GroupReduce`，`Aggregate`，`Windows`)允许在使用这些函数之前根据`key`对数据进行分组。\n\n一个`DataSet`进行分组如下:\n```java\nDataSet<...> input = // [...]\nDataSet<...> reduced = input\n.groupBy(/*define key here*/)\n.reduceGroup(/*do something*/);\n```\n`DataStream`也可以指定一个`key`:\n```java\nDataStream<...> input = // [...]\nDataStream<...> windowed = input\n.keyBy(/*define key here*/)\n.window(/*window specification*/);\n```\nFlink的数据模型不是基于键值对。因此，没有必要将数据集类型打包成`keys`和`values`。`keys`是\"虚拟\"：它们只是被定义在实际数据之上的函数，以指导分组算子使用。\n\n备注:\n```\n在下面的讨论中，我们将使用DataStream API和keyBy。对于DataSet API，你只需要替换为DataSet和groupBy即可。\n```\n\n下面介绍几种`Flink`定义`keys`方法。\n\n### 1. 为Tuples类型定义keys\n\n最简单的情况就是在元组的一个或多个字段上对元组进行分组。下面是在元组的第一个字段(整数类型)上进行分组：\n\nJava版本:\n```java\nDataStream<Tuple3<Integer,String,Long>> input = // [...]\nKeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0)\n```\nScala版本:\n```\nval input: DataStream[(Int, String, Long)] = // [...]\nval keyed = input.keyBy(0)\n```\n下面，我们将在复合key上对元组进行分组，复合key包含元组的第一个和第二个字段:\n\nJava版本:\n```java\nDataStream<Tuple3<Integer,String,Long>> input = // [...]\nKeyedStream<Tuple3<Integer,String,Long>,Tuple> keyed = input.keyBy(0,1)\n```\nScala版本:\n```\nval input: DataSet[(Int, String, Long)] = // [...]\nval grouped = input.groupBy(0,1)\n```\n\n如果你有一个包含嵌套元组的`DataStream`，例如：\n```\nDataStream<Tuple3<Tuple2<Integer, Float>,String,Long>> ds;\n```\n如果指定`keyBy(0)`，则使用整个`Tuple2`作为`key`(以`Integer`和`Float`为`key`)。如果要使用嵌套中`Tuple2`的某个字段，则必须使用下面介绍的字段表达式指定`keys`。\n\n### 2. 使用字段表达式定义keys\n\n你可以使用基于字符串的字段表达式来引用嵌套字段以及定义`keys`来进行分组，排序，连接或`coGrouping`。字段表达式可以非常容易地选择(嵌套)复合类型(如`Tuple`和`POJO`类型)中的字段。\n\n在下面的例子中，我们有一个`WC POJO`，它有两个字段`word`和`count`。如果想通过`word`字段分组，我们只需将`word`传递给`keyBy()`函数即可。\n\n```Java\n// some ordinary POJO (Plain old Java Object)\npublic class WC {\n  public String word;\n  public int count;\n}\nDataStream<WC> words = // [...]\nDataStream<WC> wordCounts = words.keyBy(\"word\").window(/*window specification*/);\n```\n\n字段表达式语法:\n\n(1) 按其字段名称选择`POJO`字段。例如，`user`是指向`POJO`类型的`user`字段。\n\n(2) 通过字段名称或0到`offset`的数值字段索引来选择元组字段(field name or 0-offset field index)。例如，`f0`和`5`分别指向`Java`元组类型的第一和第六字段。\n\n(3) 你可以在`POJO`和元组中选择嵌套字段。例如，`user.zip`是指`POJO`类型`user`字段中的`zip`字段。支持`POJO`和`Tuples`的任意嵌套和组合，如`f1.user.zip`或`user.f3.1.zip`。\n\n(4) 你可以使用`*`通配符表达式选择所有类型。这也适用于不是元组或`POJO`类型的类型。\n\nExample:\n```Java\npublic static class WC {\n  public ComplexNestedClass complex; //nested POJO\n  private int count;\n  // getter / setter for private field (count)\n  public int getCount() {\n    return count;\n  }\n  public void setCount(int c) {\n    this.count = c;\n  }\n}\npublic static class ComplexNestedClass {\n  public Integer someNumber;\n  public float someFloat;\n  public Tuple3<Long, Long, String> word;\n  public IntWritable hadoopCitizen;\n}\n```\n下面是上述示例代码的有效字段表达式：\n```\ncount：WC类中的count字段。\ncomplex：递归地选择复合字段POJO类型ComplexNestedClass的所有字段。\ncomplex.word.f2：选择嵌套字段Tuple3的最后一个字段。\ncomplex.hadoopCitizen：选择Hadoop IntWritable类型。\n```\n\n### 3. 使用key Selector 函数定义keys\n\n定义`key`的另一种方法是`key选择器`函数。key选择器函数将单个元素作为输入，并返回元素的key。`key`可以是任何类型的。\n\n以下示例显示了一个key选择器函数，它只返回一个对象的字段：\n\nJava版本:\n```java\npublic class WC {\n  public String word; public int count;\n}\n\nDataStream<WC> words = // [...]\nKeyedStream<WC> kyed = words.keyBy(new KeySelector<WC, String>() {\n     public String getKey(WC wc) { return wc.word; }\n});\n```\n\nScala版本:\n```\ncase class WC(word: String, count: Int)\nval words: DataStream[WC] = // [...]\nval keyed = words.keyBy( _.word )\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys\n","slug":"Flink/[Flink]Flink1.4 定义keys的几种方法","published":1,"updated":"2018-01-29T09:36:59.655Z","comments":1,"photos":[],"link":"","_id":"cje58tiqu001gordbcqwwdbo5","content":"<p>一些转换(例如，<code>join</code>，<code>coGroup</code>，<code>keyBy</code>，<code>groupBy</code>)要求在一组元素上定义一个key。其他转换(<code>Reduce</code>，<code>GroupReduce</code>，<code>Aggregate</code>，<code>Windows</code>)允许在使用这些函数之前根据<code>key</code>对数据进行分组。</p>\n<p>一个<code>DataSet</code>进行分组如下:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataSet&lt;...&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">DataSet&lt;...&gt; reduced = input</span><br><span class=\"line\">.groupBy(<span class=\"comment\">/*define key here*/</span>)</span><br><span class=\"line\">.reduceGroup(<span class=\"comment\">/*do something*/</span>);</span><br></pre></td></tr></table></figure></p>\n<p><code>DataStream</code>也可以指定一个<code>key</code>:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;...&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">DataStream&lt;...&gt; windowed = input</span><br><span class=\"line\">.keyBy(<span class=\"comment\">/*define key here*/</span>)</span><br><span class=\"line\">.window(<span class=\"comment\">/*window specification*/</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Flink的数据模型不是基于键值对。因此，没有必要将数据集类型打包成<code>keys</code>和<code>values</code>。<code>keys</code>是”虚拟”：它们只是被定义在实际数据之上的函数，以指导分组算子使用。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">在下面的讨论中，我们将使用DataStream API和keyBy。对于DataSet API，你只需要替换为DataSet和groupBy即可。</span><br></pre></td></tr></table></figure></p>\n<p>下面介绍几种<code>Flink</code>定义<code>keys</code>方法。</p>\n<h3 id=\"1-为Tuples类型定义keys\"><a href=\"#1-为Tuples类型定义keys\" class=\"headerlink\" title=\"1. 为Tuples类型定义keys\"></a>1. 为Tuples类型定义keys</h3><p>最简单的情况就是在元组的一个或多个字段上对元组进行分组。下面是在元组的第一个字段(整数类型)上进行分组：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataStream[(Int, String, Long)] = // [...]</span><br><span class=\"line\">val keyed = input.keyBy(0)</span><br></pre></td></tr></table></figure></p>\n<p>下面，我们将在复合key上对元组进行分组，复合key包含元组的第一个和第二个字段:</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(<span class=\"number\">0</span>,<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataSet[(Int, String, Long)] = // [...]</span><br><span class=\"line\">val grouped = input.groupBy(0,1)</span><br></pre></td></tr></table></figure></p>\n<p>如果你有一个包含嵌套元组的<code>DataStream</code>，例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;Tuple3&lt;Tuple2&lt;Integer, Float&gt;,String,Long&gt;&gt; ds;</span><br></pre></td></tr></table></figure></p>\n<p>如果指定<code>keyBy(0)</code>，则使用整个<code>Tuple2</code>作为<code>key</code>(以<code>Integer</code>和<code>Float</code>为<code>key</code>)。如果要使用嵌套中<code>Tuple2</code>的某个字段，则必须使用下面介绍的字段表达式指定<code>keys</code>。</p>\n<h3 id=\"2-使用字段表达式定义keys\"><a href=\"#2-使用字段表达式定义keys\" class=\"headerlink\" title=\"2. 使用字段表达式定义keys\"></a>2. 使用字段表达式定义keys</h3><p>你可以使用基于字符串的字段表达式来引用嵌套字段以及定义<code>keys</code>来进行分组，排序，连接或<code>coGrouping</code>。字段表达式可以非常容易地选择(嵌套)复合类型(如<code>Tuple</code>和<code>POJO</code>类型)中的字段。</p>\n<p>在下面的例子中，我们有一个<code>WC POJO</code>，它有两个字段<code>word</code>和<code>count</code>。如果想通过<code>word</code>字段分组，我们只需将<code>word</code>传递给<code>keyBy()</code>函数即可。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// some ordinary POJO (Plain old Java Object)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WC</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> String word;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> count;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">DataStream&lt;WC&gt; words = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">DataStream&lt;WC&gt; wordCounts = words.keyBy(<span class=\"string\">\"word\"</span>).window(<span class=\"comment\">/*window specification*/</span>);</span><br></pre></td></tr></table></figure>\n<p>字段表达式语法:</p>\n<p>(1) 按其字段名称选择<code>POJO</code>字段。例如，<code>user</code>是指向<code>POJO</code>类型的<code>user</code>字段。</p>\n<p>(2) 通过字段名称或0到<code>offset</code>的数值字段索引来选择元组字段(field name or 0-offset field index)。例如，<code>f0</code>和<code>5</code>分别指向<code>Java</code>元组类型的第一和第六字段。</p>\n<p>(3) 你可以在<code>POJO</code>和元组中选择嵌套字段。例如，<code>user.zip</code>是指<code>POJO</code>类型<code>user</code>字段中的<code>zip</code>字段。支持<code>POJO</code>和<code>Tuples</code>的任意嵌套和组合，如<code>f1.user.zip</code>或<code>user.f3.1.zip</code>。</p>\n<p>(4) 你可以使用<code>*</code>通配符表达式选择所有类型。这也适用于不是元组或<code>POJO</code>类型的类型。</p>\n<p>Example:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WC</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> ComplexNestedClass complex; <span class=\"comment\">//nested POJO</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> count;</span><br><span class=\"line\">  <span class=\"comment\">// getter / setter for private field (count)</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getCount</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> count;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setCount</span><span class=\"params\">(<span class=\"keyword\">int</span> c)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.count = c;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ComplexNestedClass</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> Integer someNumber;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">float</span> someFloat;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> Tuple3&lt;Long, Long, String&gt; word;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> IntWritable hadoopCitizen;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>下面是上述示例代码的有效字段表达式：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">count：WC类中的count字段。</span><br><span class=\"line\">complex：递归地选择复合字段POJO类型ComplexNestedClass的所有字段。</span><br><span class=\"line\">complex.word.f2：选择嵌套字段Tuple3的最后一个字段。</span><br><span class=\"line\">complex.hadoopCitizen：选择Hadoop IntWritable类型。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-使用key-Selector-函数定义keys\"><a href=\"#3-使用key-Selector-函数定义keys\" class=\"headerlink\" title=\"3. 使用key Selector 函数定义keys\"></a>3. 使用key Selector 函数定义keys</h3><p>定义<code>key</code>的另一种方法是<code>key选择器</code>函数。key选择器函数将单个元素作为输入，并返回元素的key。<code>key</code>可以是任何类型的。</p>\n<p>以下示例显示了一个key选择器函数，它只返回一个对象的字段：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WC</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> String word; <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> count;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;WC&gt; words = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">KeyedStream&lt;WC&gt; kyed = words.keyBy(<span class=\"keyword\">new</span> KeySelector&lt;WC, String&gt;() &#123;</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getKey</span><span class=\"params\">(WC wc)</span> </span>&#123; <span class=\"keyword\">return</span> wc.word; &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">case class WC(word: String, count: Int)</span><br><span class=\"line\">val words: DataStream[WC] = // [...]</span><br><span class=\"line\">val keyed = words.keyBy( _.word )</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>一些转换(例如，<code>join</code>，<code>coGroup</code>，<code>keyBy</code>，<code>groupBy</code>)要求在一组元素上定义一个key。其他转换(<code>Reduce</code>，<code>GroupReduce</code>，<code>Aggregate</code>，<code>Windows</code>)允许在使用这些函数之前根据<code>key</code>对数据进行分组。</p>\n<p>一个<code>DataSet</code>进行分组如下:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataSet&lt;...&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">DataSet&lt;...&gt; reduced = input</span><br><span class=\"line\">.groupBy(<span class=\"comment\">/*define key here*/</span>)</span><br><span class=\"line\">.reduceGroup(<span class=\"comment\">/*do something*/</span>);</span><br></pre></td></tr></table></figure></p>\n<p><code>DataStream</code>也可以指定一个<code>key</code>:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;...&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">DataStream&lt;...&gt; windowed = input</span><br><span class=\"line\">.keyBy(<span class=\"comment\">/*define key here*/</span>)</span><br><span class=\"line\">.window(<span class=\"comment\">/*window specification*/</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Flink的数据模型不是基于键值对。因此，没有必要将数据集类型打包成<code>keys</code>和<code>values</code>。<code>keys</code>是”虚拟”：它们只是被定义在实际数据之上的函数，以指导分组算子使用。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">在下面的讨论中，我们将使用DataStream API和keyBy。对于DataSet API，你只需要替换为DataSet和groupBy即可。</span><br></pre></td></tr></table></figure></p>\n<p>下面介绍几种<code>Flink</code>定义<code>keys</code>方法。</p>\n<h3 id=\"1-为Tuples类型定义keys\"><a href=\"#1-为Tuples类型定义keys\" class=\"headerlink\" title=\"1. 为Tuples类型定义keys\"></a>1. 为Tuples类型定义keys</h3><p>最简单的情况就是在元组的一个或多个字段上对元组进行分组。下面是在元组的第一个字段(整数类型)上进行分组：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataStream[(Int, String, Long)] = // [...]</span><br><span class=\"line\">val keyed = input.keyBy(0)</span><br></pre></td></tr></table></figure></p>\n<p>下面，我们将在复合key上对元组进行分组，复合key包含元组的第一个和第二个字段:</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;Tuple3&lt;Integer,String,Long&gt;&gt; input = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">KeyedStream&lt;Tuple3&lt;Integer,String,Long&gt;,Tuple&gt; keyed = input.keyBy(<span class=\"number\">0</span>,<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataSet[(Int, String, Long)] = // [...]</span><br><span class=\"line\">val grouped = input.groupBy(0,1)</span><br></pre></td></tr></table></figure></p>\n<p>如果你有一个包含嵌套元组的<code>DataStream</code>，例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;Tuple3&lt;Tuple2&lt;Integer, Float&gt;,String,Long&gt;&gt; ds;</span><br></pre></td></tr></table></figure></p>\n<p>如果指定<code>keyBy(0)</code>，则使用整个<code>Tuple2</code>作为<code>key</code>(以<code>Integer</code>和<code>Float</code>为<code>key</code>)。如果要使用嵌套中<code>Tuple2</code>的某个字段，则必须使用下面介绍的字段表达式指定<code>keys</code>。</p>\n<h3 id=\"2-使用字段表达式定义keys\"><a href=\"#2-使用字段表达式定义keys\" class=\"headerlink\" title=\"2. 使用字段表达式定义keys\"></a>2. 使用字段表达式定义keys</h3><p>你可以使用基于字符串的字段表达式来引用嵌套字段以及定义<code>keys</code>来进行分组，排序，连接或<code>coGrouping</code>。字段表达式可以非常容易地选择(嵌套)复合类型(如<code>Tuple</code>和<code>POJO</code>类型)中的字段。</p>\n<p>在下面的例子中，我们有一个<code>WC POJO</code>，它有两个字段<code>word</code>和<code>count</code>。如果想通过<code>word</code>字段分组，我们只需将<code>word</code>传递给<code>keyBy()</code>函数即可。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// some ordinary POJO (Plain old Java Object)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WC</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> String word;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> count;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">DataStream&lt;WC&gt; words = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">DataStream&lt;WC&gt; wordCounts = words.keyBy(<span class=\"string\">\"word\"</span>).window(<span class=\"comment\">/*window specification*/</span>);</span><br></pre></td></tr></table></figure>\n<p>字段表达式语法:</p>\n<p>(1) 按其字段名称选择<code>POJO</code>字段。例如，<code>user</code>是指向<code>POJO</code>类型的<code>user</code>字段。</p>\n<p>(2) 通过字段名称或0到<code>offset</code>的数值字段索引来选择元组字段(field name or 0-offset field index)。例如，<code>f0</code>和<code>5</code>分别指向<code>Java</code>元组类型的第一和第六字段。</p>\n<p>(3) 你可以在<code>POJO</code>和元组中选择嵌套字段。例如，<code>user.zip</code>是指<code>POJO</code>类型<code>user</code>字段中的<code>zip</code>字段。支持<code>POJO</code>和<code>Tuples</code>的任意嵌套和组合，如<code>f1.user.zip</code>或<code>user.f3.1.zip</code>。</p>\n<p>(4) 你可以使用<code>*</code>通配符表达式选择所有类型。这也适用于不是元组或<code>POJO</code>类型的类型。</p>\n<p>Example:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WC</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> ComplexNestedClass complex; <span class=\"comment\">//nested POJO</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> count;</span><br><span class=\"line\">  <span class=\"comment\">// getter / setter for private field (count)</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getCount</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> count;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setCount</span><span class=\"params\">(<span class=\"keyword\">int</span> c)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.count = c;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ComplexNestedClass</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> Integer someNumber;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">float</span> someFloat;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> Tuple3&lt;Long, Long, String&gt; word;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> IntWritable hadoopCitizen;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>下面是上述示例代码的有效字段表达式：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">count：WC类中的count字段。</span><br><span class=\"line\">complex：递归地选择复合字段POJO类型ComplexNestedClass的所有字段。</span><br><span class=\"line\">complex.word.f2：选择嵌套字段Tuple3的最后一个字段。</span><br><span class=\"line\">complex.hadoopCitizen：选择Hadoop IntWritable类型。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-使用key-Selector-函数定义keys\"><a href=\"#3-使用key-Selector-函数定义keys\" class=\"headerlink\" title=\"3. 使用key Selector 函数定义keys\"></a>3. 使用key Selector 函数定义keys</h3><p>定义<code>key</code>的另一种方法是<code>key选择器</code>函数。key选择器函数将单个元素作为输入，并返回元素的key。<code>key</code>可以是任何类型的。</p>\n<p>以下示例显示了一个key选择器函数，它只返回一个对象的字段：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WC</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> String word; <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> count;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;WC&gt; words = <span class=\"comment\">// [...]</span></span><br><span class=\"line\">KeyedStream&lt;WC&gt; kyed = words.keyBy(<span class=\"keyword\">new</span> KeySelector&lt;WC, String&gt;() &#123;</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">getKey</span><span class=\"params\">(WC wc)</span> </span>&#123; <span class=\"keyword\">return</span> wc.word; &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">case class WC(word: String, count: Int)</span><br><span class=\"line\">val words: DataStream[WC] = // [...]</span><br><span class=\"line\">val keyed = words.keyBy( _.word )</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#specifying-keys</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 外部检查点","date":"2018-01-30T06:22:17.000Z","_content":"\n### 1. 概述\n\n检查点通过恢复状态和对应流位置来实现 `Flink` 状态容错，从而为应用程序提供与无故障执行相同的语义。\n\n请参阅[检查点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html)以了解如何为你的应用程序启用和配置检查点。\n\n### 2. 外部检查点 Externalized Checkpoints\n\n默认情况下检查点不会持久化存储在外部系统中，只是用来从故障中恢复作业。当一个程序被取消时它们会被删除。但是，你可以配置检查点定期持久化存储在外部系统中，类似于保存点(`savepoints`)。这些外部持久化的检查点将其元数据写入持久性存储中，即使在作业失败时也不会自动清除。这样，如果你的作业失败时，你会有一个检查点用于恢复作业。\n\n```java\nCheckpointConfig config = env.getCheckpointConfig();\nconfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n```\n`ExternalizedCheckpointCleanup`模式配置当你取消作业时外部检查点如何操作：\n\n(1) `ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION`：作业取消时保留外部检查点。请注意，在这种情况下，你必须手动清除取消后的检查点状态。\n\n(2) `ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION`: 作业取消时删除外部检查点。检查点状态只有在作业失败时才可用。\n\n#### 2.1 目录结构\n\n与保存点类似，外部检查点由元数据文件组成，一些其他数据文件（取决于状态后端）。外部检查点元数据的目标目录是由配置属性`state.checkpoints.dir`确定的，目前它只能通过配置文件来设置。\n\n```\nstate.checkpoints.dir: hdfs:///checkpoints/\n```\n\n该目录包含恢复检查点所需的检查点元数据。对于`MemoryStateBackend`，这个元数据文件是独立的(`self-contained`)，不需要其他文件。\n\n`FsStateBackend` 和 `RocksDBStateBackend` 需要写到不同的数据文件中，只需将这些文件的路径写入元数据文件。这些数据文件存储在状态后端指定的路径上。\n\n```\nenv.setStateBackend(new RocksDBStateBackend(\"hdfs:///checkpoints-data/\");\n```\n\n#### 2.2 与保存点的区别\n\n外部检查点与保存点有一些差异。他们\n- 使用状态后端指定的（低层次）数据格式\n- 可能是增量存储的\n- 不支持 `Flink` 部分功能（如重新调整）。\n\n#### 2.3 从外部检查点恢复\n\n作业可以通过使用检查点的元数据文件从外部检查点中恢复，就像从保存点恢复一样（请参阅[保存点恢复](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#restore-a-savepoint)）。请注意，如果元数据文件不是独立的，`jobmanager` 需要访问它所引用的数据文件（参见上面的目录结构）。\n\n```\n$ bin/flink run -s :checkpointMetaDataPath [:runArgs]\n```\n\n\n备注:\n```\nFlink版本:1.4\n```\n\n术语翻译:\n\n术语|翻译\n---|---\nCheckpoints|检查点\nExternalized Checkpoints|外部检查点\nsavepoints|保存点\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html\n","source":"_posts/Flink/[Flink]Flink1.4 外部检查点.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 外部检查点\ndate: 2018-01-30 14:22:17\ntags:\n  - Flink\n  - Flink 容错\n\ncategories: Flink\npermalink: flink_stream_deployment_externalized_checkpoints\n---\n\n### 1. 概述\n\n检查点通过恢复状态和对应流位置来实现 `Flink` 状态容错，从而为应用程序提供与无故障执行相同的语义。\n\n请参阅[检查点](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html)以了解如何为你的应用程序启用和配置检查点。\n\n### 2. 外部检查点 Externalized Checkpoints\n\n默认情况下检查点不会持久化存储在外部系统中，只是用来从故障中恢复作业。当一个程序被取消时它们会被删除。但是，你可以配置检查点定期持久化存储在外部系统中，类似于保存点(`savepoints`)。这些外部持久化的检查点将其元数据写入持久性存储中，即使在作业失败时也不会自动清除。这样，如果你的作业失败时，你会有一个检查点用于恢复作业。\n\n```java\nCheckpointConfig config = env.getCheckpointConfig();\nconfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n```\n`ExternalizedCheckpointCleanup`模式配置当你取消作业时外部检查点如何操作：\n\n(1) `ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION`：作业取消时保留外部检查点。请注意，在这种情况下，你必须手动清除取消后的检查点状态。\n\n(2) `ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION`: 作业取消时删除外部检查点。检查点状态只有在作业失败时才可用。\n\n#### 2.1 目录结构\n\n与保存点类似，外部检查点由元数据文件组成，一些其他数据文件（取决于状态后端）。外部检查点元数据的目标目录是由配置属性`state.checkpoints.dir`确定的，目前它只能通过配置文件来设置。\n\n```\nstate.checkpoints.dir: hdfs:///checkpoints/\n```\n\n该目录包含恢复检查点所需的检查点元数据。对于`MemoryStateBackend`，这个元数据文件是独立的(`self-contained`)，不需要其他文件。\n\n`FsStateBackend` 和 `RocksDBStateBackend` 需要写到不同的数据文件中，只需将这些文件的路径写入元数据文件。这些数据文件存储在状态后端指定的路径上。\n\n```\nenv.setStateBackend(new RocksDBStateBackend(\"hdfs:///checkpoints-data/\");\n```\n\n#### 2.2 与保存点的区别\n\n外部检查点与保存点有一些差异。他们\n- 使用状态后端指定的（低层次）数据格式\n- 可能是增量存储的\n- 不支持 `Flink` 部分功能（如重新调整）。\n\n#### 2.3 从外部检查点恢复\n\n作业可以通过使用检查点的元数据文件从外部检查点中恢复，就像从保存点恢复一样（请参阅[保存点恢复](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#restore-a-savepoint)）。请注意，如果元数据文件不是独立的，`jobmanager` 需要访问它所引用的数据文件（参见上面的目录结构）。\n\n```\n$ bin/flink run -s :checkpointMetaDataPath [:runArgs]\n```\n\n\n备注:\n```\nFlink版本:1.4\n```\n\n术语翻译:\n\n术语|翻译\n---|---\nCheckpoints|检查点\nExternalized Checkpoints|外部检查点\nsavepoints|保存点\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html\n","slug":"flink_stream_deployment_externalized_checkpoints","published":1,"updated":"2018-01-30T07:26:25.425Z","comments":1,"photos":[],"link":"","_id":"cje58tiqy001jordb3takyx2y","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>检查点通过恢复状态和对应流位置来实现 <code>Flink</code> 状态容错，从而为应用程序提供与无故障执行相同的语义。</p>\n<p>请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html\" target=\"_blank\" rel=\"noopener\">检查点</a>以了解如何为你的应用程序启用和配置检查点。</p>\n<h3 id=\"2-外部检查点-Externalized-Checkpoints\"><a href=\"#2-外部检查点-Externalized-Checkpoints\" class=\"headerlink\" title=\"2. 外部检查点 Externalized Checkpoints\"></a>2. 外部检查点 Externalized Checkpoints</h3><p>默认情况下检查点不会持久化存储在外部系统中，只是用来从故障中恢复作业。当一个程序被取消时它们会被删除。但是，你可以配置检查点定期持久化存储在外部系统中，类似于保存点(<code>savepoints</code>)。这些外部持久化的检查点将其元数据写入持久性存储中，即使在作业失败时也不会自动清除。这样，如果你的作业失败时，你会有一个检查点用于恢复作业。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">CheckpointConfig config = env.getCheckpointConfig();</span><br><span class=\"line\">config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br></pre></td></tr></table></figure>\n<p><code>ExternalizedCheckpointCleanup</code>模式配置当你取消作业时外部检查点如何操作：</p>\n<p>(1) <code>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION</code>：作业取消时保留外部检查点。请注意，在这种情况下，你必须手动清除取消后的检查点状态。</p>\n<p>(2) <code>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION</code>: 作业取消时删除外部检查点。检查点状态只有在作业失败时才可用。</p>\n<h4 id=\"2-1-目录结构\"><a href=\"#2-1-目录结构\" class=\"headerlink\" title=\"2.1 目录结构\"></a>2.1 目录结构</h4><p>与保存点类似，外部检查点由元数据文件组成，一些其他数据文件（取决于状态后端）。外部检查点元数据的目标目录是由配置属性<code>state.checkpoints.dir</code>确定的，目前它只能通过配置文件来设置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">state.checkpoints.dir: hdfs:///checkpoints/</span><br></pre></td></tr></table></figure>\n<p>该目录包含恢复检查点所需的检查点元数据。对于<code>MemoryStateBackend</code>，这个元数据文件是独立的(<code>self-contained</code>)，不需要其他文件。</p>\n<p><code>FsStateBackend</code> 和 <code>RocksDBStateBackend</code> 需要写到不同的数据文件中，只需将这些文件的路径写入元数据文件。这些数据文件存储在状态后端指定的路径上。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">env.setStateBackend(new RocksDBStateBackend(&quot;hdfs:///checkpoints-data/&quot;);</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-2-与保存点的区别\"><a href=\"#2-2-与保存点的区别\" class=\"headerlink\" title=\"2.2 与保存点的区别\"></a>2.2 与保存点的区别</h4><p>外部检查点与保存点有一些差异。他们</p>\n<ul>\n<li>使用状态后端指定的（低层次）数据格式</li>\n<li>可能是增量存储的</li>\n<li>不支持 <code>Flink</code> 部分功能（如重新调整）。</li>\n</ul>\n<h4 id=\"2-3-从外部检查点恢复\"><a href=\"#2-3-从外部检查点恢复\" class=\"headerlink\" title=\"2.3 从外部检查点恢复\"></a>2.3 从外部检查点恢复</h4><p>作业可以通过使用检查点的元数据文件从外部检查点中恢复，就像从保存点恢复一样（请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#restore-a-savepoint\" target=\"_blank\" rel=\"noopener\">保存点恢复</a>）。请注意，如果元数据文件不是独立的，<code>jobmanager</code> 需要访问它所引用的数据文件（参见上面的目录结构）。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ bin/flink run -s :checkpointMetaDataPath [:runArgs]</span><br></pre></td></tr></table></figure>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>术语翻译:</p>\n<table>\n<thead>\n<tr>\n<th>术语</th>\n<th>翻译</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Checkpoints</td>\n<td>检查点</td>\n</tr>\n<tr>\n<td>Externalized Checkpoints</td>\n<td>外部检查点</td>\n</tr>\n<tr>\n<td>savepoints</td>\n<td>保存点</td>\n</tr>\n</tbody>\n</table>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>检查点通过恢复状态和对应流位置来实现 <code>Flink</code> 状态容错，从而为应用程序提供与无故障执行相同的语义。</p>\n<p>请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/checkpointing.html\" target=\"_blank\" rel=\"noopener\">检查点</a>以了解如何为你的应用程序启用和配置检查点。</p>\n<h3 id=\"2-外部检查点-Externalized-Checkpoints\"><a href=\"#2-外部检查点-Externalized-Checkpoints\" class=\"headerlink\" title=\"2. 外部检查点 Externalized Checkpoints\"></a>2. 外部检查点 Externalized Checkpoints</h3><p>默认情况下检查点不会持久化存储在外部系统中，只是用来从故障中恢复作业。当一个程序被取消时它们会被删除。但是，你可以配置检查点定期持久化存储在外部系统中，类似于保存点(<code>savepoints</code>)。这些外部持久化的检查点将其元数据写入持久性存储中，即使在作业失败时也不会自动清除。这样，如果你的作业失败时，你会有一个检查点用于恢复作业。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">CheckpointConfig config = env.getCheckpointConfig();</span><br><span class=\"line\">config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br></pre></td></tr></table></figure>\n<p><code>ExternalizedCheckpointCleanup</code>模式配置当你取消作业时外部检查点如何操作：</p>\n<p>(1) <code>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION</code>：作业取消时保留外部检查点。请注意，在这种情况下，你必须手动清除取消后的检查点状态。</p>\n<p>(2) <code>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION</code>: 作业取消时删除外部检查点。检查点状态只有在作业失败时才可用。</p>\n<h4 id=\"2-1-目录结构\"><a href=\"#2-1-目录结构\" class=\"headerlink\" title=\"2.1 目录结构\"></a>2.1 目录结构</h4><p>与保存点类似，外部检查点由元数据文件组成，一些其他数据文件（取决于状态后端）。外部检查点元数据的目标目录是由配置属性<code>state.checkpoints.dir</code>确定的，目前它只能通过配置文件来设置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">state.checkpoints.dir: hdfs:///checkpoints/</span><br></pre></td></tr></table></figure>\n<p>该目录包含恢复检查点所需的检查点元数据。对于<code>MemoryStateBackend</code>，这个元数据文件是独立的(<code>self-contained</code>)，不需要其他文件。</p>\n<p><code>FsStateBackend</code> 和 <code>RocksDBStateBackend</code> 需要写到不同的数据文件中，只需将这些文件的路径写入元数据文件。这些数据文件存储在状态后端指定的路径上。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">env.setStateBackend(new RocksDBStateBackend(&quot;hdfs:///checkpoints-data/&quot;);</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-2-与保存点的区别\"><a href=\"#2-2-与保存点的区别\" class=\"headerlink\" title=\"2.2 与保存点的区别\"></a>2.2 与保存点的区别</h4><p>外部检查点与保存点有一些差异。他们</p>\n<ul>\n<li>使用状态后端指定的（低层次）数据格式</li>\n<li>可能是增量存储的</li>\n<li>不支持 <code>Flink</code> 部分功能（如重新调整）。</li>\n</ul>\n<h4 id=\"2-3-从外部检查点恢复\"><a href=\"#2-3-从外部检查点恢复\" class=\"headerlink\" title=\"2.3 从外部检查点恢复\"></a>2.3 从外部检查点恢复</h4><p>作业可以通过使用检查点的元数据文件从外部检查点中恢复，就像从保存点恢复一样（请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/cli.html#restore-a-savepoint\" target=\"_blank\" rel=\"noopener\">保存点恢复</a>）。请注意，如果元数据文件不是独立的，<code>jobmanager</code> 需要访问它所引用的数据文件（参见上面的目录结构）。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ bin/flink run -s :checkpointMetaDataPath [:runArgs]</span><br></pre></td></tr></table></figure>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>术语翻译:</p>\n<table>\n<thead>\n<tr>\n<th>术语</th>\n<th>翻译</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Checkpoints</td>\n<td>检查点</td>\n</tr>\n<tr>\n<td>Externalized Checkpoints</td>\n<td>外部检查点</td>\n</tr>\n<tr>\n<td>savepoints</td>\n<td>保存点</td>\n</tr>\n</tbody>\n</table>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 处理背压","date":"2018-02-11T02:39:01.000Z","_content":"\n人们经常会问`Flink`是如何处理背压(backpressure)效应的。 答案很简单：`Flink`不使用任何复杂的机制，因为它不需要任何处理机制。它只凭借数据流引擎，就可以从容地应对背压。在这篇博文中，我们介绍一下背压。然后，我们深入了解 `Flink` 运行时如何在任务之间传送缓冲区中的数据，并展示流数传输自然双倍下降的背压机制(how streaming data shipping naturally doubles down as a backpressure mechanism)。 我们最终通过一个小实验展示了这一点。\n\n### 1. 什么是背压\n\n像`Flink`这样的流处理系统需要能够从容地处理背压。背压是指系统在一个临时负载峰值期间接收数据的速率大于其处理速率的一种场景(备注:就是处理速度慢，接收速度快，系统处理不了接收的数据)。许多日常情况都会导致背压。例如，垃圾回收卡顿可能导致流入的数据堆积起来，或者数据源可能出现发送数据过快的峰值。如果处理不当，背压会导致资源耗尽，甚至导致数据丢失。\n\n让我们看一个简单的例子。 假设一个数据流管道包含一个数据源，一个流作业和一个接收器sink，它稳定的以每秒500万个元素的速度处理数据，如下所示(一个黑条代表100万个元素，下图是系统某一秒的快照)：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-1.png?raw=true)\n\n在某些时候，流处理作业或sink有1秒的卡顿，导致500多万个元素的堆积。或者，数据源可能出现了一个峰值，在一秒内以双倍的速度产生数据。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-2.png?raw=true)\n\n我们如何处理这样的情况(如上数据源出现一个峰值，一秒内以双倍的速度产生数据)呢？ 当然，可以放弃这些元素(一秒内只能处理一半的数据)。但是，对于许多按`Exactly One`处理语义处理记录的流式应用程序来说，数据丢失是不可接受的。额外的数据可以缓存在某个地方。缓存也应该是可持久化的，因为在失败的情况下，这些数据需要被重新读取以防止数据丢失。理想情况下，这些数据应该被缓存在一个持久化的通道中(例如，如果数据源自己能保证持久性，`Apache Kafka` 就是这样的一种数据源)。理想状态下应对背压的措施是将整个管道从 `sink` 回压到数据源，并对源头进行限流，以将速度调整到管道最慢部分的速度，从而达到稳定状态:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-3.png?raw=true)\n\n### 2. Flink中的背压\n\n`Flink`运行时的构建组件是算子和流。每个算子消费中间数据流，并对其进行转换，并产生新的数据流。描述这种机制的最好比喻是`Flink`充分使用有界容量的分布式阻塞队列。与 `Java` 连接线程的常规阻塞队列一样，一旦队列的有效缓冲耗尽(有界容量)，较慢的接收者就会使发送者放慢发送速度。\n\n以两个任务之间的简单流程为例，说明 `Flink` 如何实现背压：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-4.jpg?raw=true)\n\n(1) 记录 `A` 进入`Flink`并由任务1处理。\n\n(2) 记录被序列化在缓冲区，\n\n(3) 缓冲区输送到任务2中，然后任务2从缓冲区中读取记录。\n\n为了使记录通过`Flink`进行处理，缓冲区必须是可用的。在`Flink`中，这些分布式队列被认为是逻辑数据流，通过生产流和消费流管理的缓冲池来实现有界容量。缓冲池是缓冲区的集合，它们在使用后会被回收。总体思路很简单：从缓冲池中取出一个缓冲区，填充数据，在数据消耗完后，将缓冲区放回缓冲池中，之后还可以再次使用它。\n\n缓冲池的大小在运行时会动态变化。网络堆栈中的内存缓冲区的数量(=队列的容量)决定了系统在不同发送/接收速度可以进行的缓冲量。`Flink`保证始终有足够的缓冲区来进行进程处理(enough buffers to make some progress)，但是这个进程的速度取决于用户程序和可用内存的数量。更多的内存意味着系统可以轻松地缓冲一定的瞬时背压(短时间段，短 GC)。越少的内存意味着需要对背压进行直接响应(没有足够的缓冲区进行缓存，只能响应处理)。\n\n从上面的简单例子可以看出：在任务1输出端和任务2输入端都有一个与其关联的缓冲池。如果有一个可用于序列化 `A` 的缓冲区，我们将其序列化并分配缓冲区。\n\n我们在这里有必要看两个case：\n\n(1) 本地交换：如果任务1和任务2在同一个工作节点(TaskManager)上运行，缓冲区可以直接交给下一个任务。一旦任务2消费完，它就会被回收。如果任务2比任务1慢，则缓冲区将以低于任务1填充的速度进行回收，从而导致任务1速度变慢。\n\n(2) 远程交换：如果任务1和任务2在不同的工作节点上运行，缓冲区一旦发送到线路中(TCP通道)就可以被回收。在接收端，数据从线路复制到输入缓冲池的缓冲区。如果没有缓冲区可用，从TCP连接读取操作将被中断。输出端通过一个简单的 `watermark` 机制保证不会在线上放置太多的数据。如果有足够的数据处在可发送状态，我们会一直复制更多的数据到线路中直到低于某个阈值时。这保证了没有太多的数据在传输途中。如果接收端没有消费新的数据(因为没有缓冲区可用)，这会减慢发送方的速度。\n\n这种简单的在固定大小缓冲池之间的缓冲区流使`Flink`能够拥有一个强大的背压机制，在这种机制下，任务生产数据速度不会比消费的快。\n\n我们描述的两个任务之间的数据传输的机制可以自然的推广到复杂管道上，保证背压在整个管道内传播。\n\n让我们看看一个简单的实验，展示了`Flink`在背压情况下的行为。我们运行一个简单的生产者-消费者流式拓扑，其中任务在本地交换数据，在这里我们可以变换任务产生记录的速度。对于这个测试，我们使用比默认更少的内存，以使得背压效果更明显。我们使用每个任务有2个大小为4096字节的缓冲区。在通常的`Flink`部署中，任务将具有更大更多缓冲区，这会提高性能。这个测试在单个JVM中运行，但使用完整的`Flink`代码堆栈。\n\n图中显示了生产者任务(黄色)和消费者任务(绿色)随着时间变化所达到的最大吞吐量(单个JVM中每秒达到800万个元素)的平均吞吐量占比(average throughput as a percentage of the maximum attained throughput  of the producing (yellow) and consuming (green) tasks)。为了衡量平均吞吐量，我们每5秒测量一次任务处理的记录数量。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-5.png?raw=true)\n\n首先，我们以60％的速度运行生产任务(我们通过调用Thread.sleep()来模拟减速)。消费者以相同的速度处理数据，不会产生延迟。然后我们把消费者任务放慢到全速的30％。在这里，背压效果产生作用，因为我们看到生产者也自然放缓到全速的30％。然后，我们取消消费者任务的人为减速，并且这两项任务都达到最大吞吐量。我们再次把消费者任务放慢到全速的30％，管道立即响应，生产者任务也全速下降到30％。最后，我们再次停止减速，两项任务都以100％的速度持续下去。总而言之，我们看到生产者和消费者在管道上相互跟随彼此的吞吐量，这是我们在流水线中期望的行为。\n\n\n### 3. 结论\n\n`Flink`与像Kafka这样的可持久化数据源，让你可以立即响应处理背压而不会丢失数据。`Flink`不需要专门的机制来处理背压，因为data shipping in `Flink` doubles as a backpressure mechanism。 因此，`Flink`实现了管道最慢部分允许的最大吞吐量。\n\n\n原文:https://data-artisans.com/blog/how-flink-handles-backpressure\n","source":"_posts/Flink/[Flink]Flink1.4 处理背压.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 处理背压\ndate: 2018-02-11 10:39:01\ntags:\n  - Flink\n\ncategories: Flink\npermalink: how-flink-handles-backpressure\n---\n\n人们经常会问`Flink`是如何处理背压(backpressure)效应的。 答案很简单：`Flink`不使用任何复杂的机制，因为它不需要任何处理机制。它只凭借数据流引擎，就可以从容地应对背压。在这篇博文中，我们介绍一下背压。然后，我们深入了解 `Flink` 运行时如何在任务之间传送缓冲区中的数据，并展示流数传输自然双倍下降的背压机制(how streaming data shipping naturally doubles down as a backpressure mechanism)。 我们最终通过一个小实验展示了这一点。\n\n### 1. 什么是背压\n\n像`Flink`这样的流处理系统需要能够从容地处理背压。背压是指系统在一个临时负载峰值期间接收数据的速率大于其处理速率的一种场景(备注:就是处理速度慢，接收速度快，系统处理不了接收的数据)。许多日常情况都会导致背压。例如，垃圾回收卡顿可能导致流入的数据堆积起来，或者数据源可能出现发送数据过快的峰值。如果处理不当，背压会导致资源耗尽，甚至导致数据丢失。\n\n让我们看一个简单的例子。 假设一个数据流管道包含一个数据源，一个流作业和一个接收器sink，它稳定的以每秒500万个元素的速度处理数据，如下所示(一个黑条代表100万个元素，下图是系统某一秒的快照)：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-1.png?raw=true)\n\n在某些时候，流处理作业或sink有1秒的卡顿，导致500多万个元素的堆积。或者，数据源可能出现了一个峰值，在一秒内以双倍的速度产生数据。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-2.png?raw=true)\n\n我们如何处理这样的情况(如上数据源出现一个峰值，一秒内以双倍的速度产生数据)呢？ 当然，可以放弃这些元素(一秒内只能处理一半的数据)。但是，对于许多按`Exactly One`处理语义处理记录的流式应用程序来说，数据丢失是不可接受的。额外的数据可以缓存在某个地方。缓存也应该是可持久化的，因为在失败的情况下，这些数据需要被重新读取以防止数据丢失。理想情况下，这些数据应该被缓存在一个持久化的通道中(例如，如果数据源自己能保证持久性，`Apache Kafka` 就是这样的一种数据源)。理想状态下应对背压的措施是将整个管道从 `sink` 回压到数据源，并对源头进行限流，以将速度调整到管道最慢部分的速度，从而达到稳定状态:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-3.png?raw=true)\n\n### 2. Flink中的背压\n\n`Flink`运行时的构建组件是算子和流。每个算子消费中间数据流，并对其进行转换，并产生新的数据流。描述这种机制的最好比喻是`Flink`充分使用有界容量的分布式阻塞队列。与 `Java` 连接线程的常规阻塞队列一样，一旦队列的有效缓冲耗尽(有界容量)，较慢的接收者就会使发送者放慢发送速度。\n\n以两个任务之间的简单流程为例，说明 `Flink` 如何实现背压：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-4.jpg?raw=true)\n\n(1) 记录 `A` 进入`Flink`并由任务1处理。\n\n(2) 记录被序列化在缓冲区，\n\n(3) 缓冲区输送到任务2中，然后任务2从缓冲区中读取记录。\n\n为了使记录通过`Flink`进行处理，缓冲区必须是可用的。在`Flink`中，这些分布式队列被认为是逻辑数据流，通过生产流和消费流管理的缓冲池来实现有界容量。缓冲池是缓冲区的集合，它们在使用后会被回收。总体思路很简单：从缓冲池中取出一个缓冲区，填充数据，在数据消耗完后，将缓冲区放回缓冲池中，之后还可以再次使用它。\n\n缓冲池的大小在运行时会动态变化。网络堆栈中的内存缓冲区的数量(=队列的容量)决定了系统在不同发送/接收速度可以进行的缓冲量。`Flink`保证始终有足够的缓冲区来进行进程处理(enough buffers to make some progress)，但是这个进程的速度取决于用户程序和可用内存的数量。更多的内存意味着系统可以轻松地缓冲一定的瞬时背压(短时间段，短 GC)。越少的内存意味着需要对背压进行直接响应(没有足够的缓冲区进行缓存，只能响应处理)。\n\n从上面的简单例子可以看出：在任务1输出端和任务2输入端都有一个与其关联的缓冲池。如果有一个可用于序列化 `A` 的缓冲区，我们将其序列化并分配缓冲区。\n\n我们在这里有必要看两个case：\n\n(1) 本地交换：如果任务1和任务2在同一个工作节点(TaskManager)上运行，缓冲区可以直接交给下一个任务。一旦任务2消费完，它就会被回收。如果任务2比任务1慢，则缓冲区将以低于任务1填充的速度进行回收，从而导致任务1速度变慢。\n\n(2) 远程交换：如果任务1和任务2在不同的工作节点上运行，缓冲区一旦发送到线路中(TCP通道)就可以被回收。在接收端，数据从线路复制到输入缓冲池的缓冲区。如果没有缓冲区可用，从TCP连接读取操作将被中断。输出端通过一个简单的 `watermark` 机制保证不会在线上放置太多的数据。如果有足够的数据处在可发送状态，我们会一直复制更多的数据到线路中直到低于某个阈值时。这保证了没有太多的数据在传输途中。如果接收端没有消费新的数据(因为没有缓冲区可用)，这会减慢发送方的速度。\n\n这种简单的在固定大小缓冲池之间的缓冲区流使`Flink`能够拥有一个强大的背压机制，在这种机制下，任务生产数据速度不会比消费的快。\n\n我们描述的两个任务之间的数据传输的机制可以自然的推广到复杂管道上，保证背压在整个管道内传播。\n\n让我们看看一个简单的实验，展示了`Flink`在背压情况下的行为。我们运行一个简单的生产者-消费者流式拓扑，其中任务在本地交换数据，在这里我们可以变换任务产生记录的速度。对于这个测试，我们使用比默认更少的内存，以使得背压效果更明显。我们使用每个任务有2个大小为4096字节的缓冲区。在通常的`Flink`部署中，任务将具有更大更多缓冲区，这会提高性能。这个测试在单个JVM中运行，但使用完整的`Flink`代码堆栈。\n\n图中显示了生产者任务(黄色)和消费者任务(绿色)随着时间变化所达到的最大吞吐量(单个JVM中每秒达到800万个元素)的平均吞吐量占比(average throughput as a percentage of the maximum attained throughput  of the producing (yellow) and consuming (green) tasks)。为了衡量平均吞吐量，我们每5秒测量一次任务处理的记录数量。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-5.png?raw=true)\n\n首先，我们以60％的速度运行生产任务(我们通过调用Thread.sleep()来模拟减速)。消费者以相同的速度处理数据，不会产生延迟。然后我们把消费者任务放慢到全速的30％。在这里，背压效果产生作用，因为我们看到生产者也自然放缓到全速的30％。然后，我们取消消费者任务的人为减速，并且这两项任务都达到最大吞吐量。我们再次把消费者任务放慢到全速的30％，管道立即响应，生产者任务也全速下降到30％。最后，我们再次停止减速，两项任务都以100％的速度持续下去。总而言之，我们看到生产者和消费者在管道上相互跟随彼此的吞吐量，这是我们在流水线中期望的行为。\n\n\n### 3. 结论\n\n`Flink`与像Kafka这样的可持久化数据源，让你可以立即响应处理背压而不会丢失数据。`Flink`不需要专门的机制来处理背压，因为data shipping in `Flink` doubles as a backpressure mechanism。 因此，`Flink`实现了管道最慢部分允许的最大吞吐量。\n\n\n原文:https://data-artisans.com/blog/how-flink-handles-backpressure\n","slug":"how-flink-handles-backpressure","published":1,"updated":"2018-02-12T03:47:24.113Z","comments":1,"photos":[],"link":"","_id":"cje58tir1001nordbtcxfz2qj","content":"<p>人们经常会问<code>Flink</code>是如何处理背压(backpressure)效应的。 答案很简单：<code>Flink</code>不使用任何复杂的机制，因为它不需要任何处理机制。它只凭借数据流引擎，就可以从容地应对背压。在这篇博文中，我们介绍一下背压。然后，我们深入了解 <code>Flink</code> 运行时如何在任务之间传送缓冲区中的数据，并展示流数传输自然双倍下降的背压机制(how streaming data shipping naturally doubles down as a backpressure mechanism)。 我们最终通过一个小实验展示了这一点。</p>\n<h3 id=\"1-什么是背压\"><a href=\"#1-什么是背压\" class=\"headerlink\" title=\"1. 什么是背压\"></a>1. 什么是背压</h3><p>像<code>Flink</code>这样的流处理系统需要能够从容地处理背压。背压是指系统在一个临时负载峰值期间接收数据的速率大于其处理速率的一种场景(备注:就是处理速度慢，接收速度快，系统处理不了接收的数据)。许多日常情况都会导致背压。例如，垃圾回收卡顿可能导致流入的数据堆积起来，或者数据源可能出现发送数据过快的峰值。如果处理不当，背压会导致资源耗尽，甚至导致数据丢失。</p>\n<p>让我们看一个简单的例子。 假设一个数据流管道包含一个数据源，一个流作业和一个接收器sink，它稳定的以每秒500万个元素的速度处理数据，如下所示(一个黑条代表100万个元素，下图是系统某一秒的快照)：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-1.png?raw=true\" alt=\"\"></p>\n<p>在某些时候，流处理作业或sink有1秒的卡顿，导致500多万个元素的堆积。或者，数据源可能出现了一个峰值，在一秒内以双倍的速度产生数据。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-2.png?raw=true\" alt=\"\"></p>\n<p>我们如何处理这样的情况(如上数据源出现一个峰值，一秒内以双倍的速度产生数据)呢？ 当然，可以放弃这些元素(一秒内只能处理一半的数据)。但是，对于许多按<code>Exactly One</code>处理语义处理记录的流式应用程序来说，数据丢失是不可接受的。额外的数据可以缓存在某个地方。缓存也应该是可持久化的，因为在失败的情况下，这些数据需要被重新读取以防止数据丢失。理想情况下，这些数据应该被缓存在一个持久化的通道中(例如，如果数据源自己能保证持久性，<code>Apache Kafka</code> 就是这样的一种数据源)。理想状态下应对背压的措施是将整个管道从 <code>sink</code> 回压到数据源，并对源头进行限流，以将速度调整到管道最慢部分的速度，从而达到稳定状态:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-Flink中的背压\"><a href=\"#2-Flink中的背压\" class=\"headerlink\" title=\"2. Flink中的背压\"></a>2. Flink中的背压</h3><p><code>Flink</code>运行时的构建组件是算子和流。每个算子消费中间数据流，并对其进行转换，并产生新的数据流。描述这种机制的最好比喻是<code>Flink</code>充分使用有界容量的分布式阻塞队列。与 <code>Java</code> 连接线程的常规阻塞队列一样，一旦队列的有效缓冲耗尽(有界容量)，较慢的接收者就会使发送者放慢发送速度。</p>\n<p>以两个任务之间的简单流程为例，说明 <code>Flink</code> 如何实现背压：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-4.jpg?raw=true\" alt=\"\"></p>\n<p>(1) 记录 <code>A</code> 进入<code>Flink</code>并由任务1处理。</p>\n<p>(2) 记录被序列化在缓冲区，</p>\n<p>(3) 缓冲区输送到任务2中，然后任务2从缓冲区中读取记录。</p>\n<p>为了使记录通过<code>Flink</code>进行处理，缓冲区必须是可用的。在<code>Flink</code>中，这些分布式队列被认为是逻辑数据流，通过生产流和消费流管理的缓冲池来实现有界容量。缓冲池是缓冲区的集合，它们在使用后会被回收。总体思路很简单：从缓冲池中取出一个缓冲区，填充数据，在数据消耗完后，将缓冲区放回缓冲池中，之后还可以再次使用它。</p>\n<p>缓冲池的大小在运行时会动态变化。网络堆栈中的内存缓冲区的数量(=队列的容量)决定了系统在不同发送/接收速度可以进行的缓冲量。<code>Flink</code>保证始终有足够的缓冲区来进行进程处理(enough buffers to make some progress)，但是这个进程的速度取决于用户程序和可用内存的数量。更多的内存意味着系统可以轻松地缓冲一定的瞬时背压(短时间段，短 GC)。越少的内存意味着需要对背压进行直接响应(没有足够的缓冲区进行缓存，只能响应处理)。</p>\n<p>从上面的简单例子可以看出：在任务1输出端和任务2输入端都有一个与其关联的缓冲池。如果有一个可用于序列化 <code>A</code> 的缓冲区，我们将其序列化并分配缓冲区。</p>\n<p>我们在这里有必要看两个case：</p>\n<p>(1) 本地交换：如果任务1和任务2在同一个工作节点(TaskManager)上运行，缓冲区可以直接交给下一个任务。一旦任务2消费完，它就会被回收。如果任务2比任务1慢，则缓冲区将以低于任务1填充的速度进行回收，从而导致任务1速度变慢。</p>\n<p>(2) 远程交换：如果任务1和任务2在不同的工作节点上运行，缓冲区一旦发送到线路中(TCP通道)就可以被回收。在接收端，数据从线路复制到输入缓冲池的缓冲区。如果没有缓冲区可用，从TCP连接读取操作将被中断。输出端通过一个简单的 <code>watermark</code> 机制保证不会在线上放置太多的数据。如果有足够的数据处在可发送状态，我们会一直复制更多的数据到线路中直到低于某个阈值时。这保证了没有太多的数据在传输途中。如果接收端没有消费新的数据(因为没有缓冲区可用)，这会减慢发送方的速度。</p>\n<p>这种简单的在固定大小缓冲池之间的缓冲区流使<code>Flink</code>能够拥有一个强大的背压机制，在这种机制下，任务生产数据速度不会比消费的快。</p>\n<p>我们描述的两个任务之间的数据传输的机制可以自然的推广到复杂管道上，保证背压在整个管道内传播。</p>\n<p>让我们看看一个简单的实验，展示了<code>Flink</code>在背压情况下的行为。我们运行一个简单的生产者-消费者流式拓扑，其中任务在本地交换数据，在这里我们可以变换任务产生记录的速度。对于这个测试，我们使用比默认更少的内存，以使得背压效果更明显。我们使用每个任务有2个大小为4096字节的缓冲区。在通常的<code>Flink</code>部署中，任务将具有更大更多缓冲区，这会提高性能。这个测试在单个JVM中运行，但使用完整的<code>Flink</code>代码堆栈。</p>\n<p>图中显示了生产者任务(黄色)和消费者任务(绿色)随着时间变化所达到的最大吞吐量(单个JVM中每秒达到800万个元素)的平均吞吐量占比(average throughput as a percentage of the maximum attained throughput  of the producing (yellow) and consuming (green) tasks)。为了衡量平均吞吐量，我们每5秒测量一次任务处理的记录数量。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-5.png?raw=true\" alt=\"\"></p>\n<p>首先，我们以60％的速度运行生产任务(我们通过调用Thread.sleep()来模拟减速)。消费者以相同的速度处理数据，不会产生延迟。然后我们把消费者任务放慢到全速的30％。在这里，背压效果产生作用，因为我们看到生产者也自然放缓到全速的30％。然后，我们取消消费者任务的人为减速，并且这两项任务都达到最大吞吐量。我们再次把消费者任务放慢到全速的30％，管道立即响应，生产者任务也全速下降到30％。最后，我们再次停止减速，两项任务都以100％的速度持续下去。总而言之，我们看到生产者和消费者在管道上相互跟随彼此的吞吐量，这是我们在流水线中期望的行为。</p>\n<h3 id=\"3-结论\"><a href=\"#3-结论\" class=\"headerlink\" title=\"3. 结论\"></a>3. 结论</h3><p><code>Flink</code>与像Kafka这样的可持久化数据源，让你可以立即响应处理背压而不会丢失数据。<code>Flink</code>不需要专门的机制来处理背压，因为data shipping in <code>Flink</code> doubles as a backpressure mechanism。 因此，<code>Flink</code>实现了管道最慢部分允许的最大吞吐量。</p>\n<p>原文:<a href=\"https://data-artisans.com/blog/how-flink-handles-backpressure\" target=\"_blank\" rel=\"noopener\">https://data-artisans.com/blog/how-flink-handles-backpressure</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>人们经常会问<code>Flink</code>是如何处理背压(backpressure)效应的。 答案很简单：<code>Flink</code>不使用任何复杂的机制，因为它不需要任何处理机制。它只凭借数据流引擎，就可以从容地应对背压。在这篇博文中，我们介绍一下背压。然后，我们深入了解 <code>Flink</code> 运行时如何在任务之间传送缓冲区中的数据，并展示流数传输自然双倍下降的背压机制(how streaming data shipping naturally doubles down as a backpressure mechanism)。 我们最终通过一个小实验展示了这一点。</p>\n<h3 id=\"1-什么是背压\"><a href=\"#1-什么是背压\" class=\"headerlink\" title=\"1. 什么是背压\"></a>1. 什么是背压</h3><p>像<code>Flink</code>这样的流处理系统需要能够从容地处理背压。背压是指系统在一个临时负载峰值期间接收数据的速率大于其处理速率的一种场景(备注:就是处理速度慢，接收速度快，系统处理不了接收的数据)。许多日常情况都会导致背压。例如，垃圾回收卡顿可能导致流入的数据堆积起来，或者数据源可能出现发送数据过快的峰值。如果处理不当，背压会导致资源耗尽，甚至导致数据丢失。</p>\n<p>让我们看一个简单的例子。 假设一个数据流管道包含一个数据源，一个流作业和一个接收器sink，它稳定的以每秒500万个元素的速度处理数据，如下所示(一个黑条代表100万个元素，下图是系统某一秒的快照)：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-1.png?raw=true\" alt=\"\"></p>\n<p>在某些时候，流处理作业或sink有1秒的卡顿，导致500多万个元素的堆积。或者，数据源可能出现了一个峰值，在一秒内以双倍的速度产生数据。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-2.png?raw=true\" alt=\"\"></p>\n<p>我们如何处理这样的情况(如上数据源出现一个峰值，一秒内以双倍的速度产生数据)呢？ 当然，可以放弃这些元素(一秒内只能处理一半的数据)。但是，对于许多按<code>Exactly One</code>处理语义处理记录的流式应用程序来说，数据丢失是不可接受的。额外的数据可以缓存在某个地方。缓存也应该是可持久化的，因为在失败的情况下，这些数据需要被重新读取以防止数据丢失。理想情况下，这些数据应该被缓存在一个持久化的通道中(例如，如果数据源自己能保证持久性，<code>Apache Kafka</code> 就是这样的一种数据源)。理想状态下应对背压的措施是将整个管道从 <code>sink</code> 回压到数据源，并对源头进行限流，以将速度调整到管道最慢部分的速度，从而达到稳定状态:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-Flink中的背压\"><a href=\"#2-Flink中的背压\" class=\"headerlink\" title=\"2. Flink中的背压\"></a>2. Flink中的背压</h3><p><code>Flink</code>运行时的构建组件是算子和流。每个算子消费中间数据流，并对其进行转换，并产生新的数据流。描述这种机制的最好比喻是<code>Flink</code>充分使用有界容量的分布式阻塞队列。与 <code>Java</code> 连接线程的常规阻塞队列一样，一旦队列的有效缓冲耗尽(有界容量)，较慢的接收者就会使发送者放慢发送速度。</p>\n<p>以两个任务之间的简单流程为例，说明 <code>Flink</code> 如何实现背压：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-4.jpg?raw=true\" alt=\"\"></p>\n<p>(1) 记录 <code>A</code> 进入<code>Flink</code>并由任务1处理。</p>\n<p>(2) 记录被序列化在缓冲区，</p>\n<p>(3) 缓冲区输送到任务2中，然后任务2从缓冲区中读取记录。</p>\n<p>为了使记录通过<code>Flink</code>进行处理，缓冲区必须是可用的。在<code>Flink</code>中，这些分布式队列被认为是逻辑数据流，通过生产流和消费流管理的缓冲池来实现有界容量。缓冲池是缓冲区的集合，它们在使用后会被回收。总体思路很简单：从缓冲池中取出一个缓冲区，填充数据，在数据消耗完后，将缓冲区放回缓冲池中，之后还可以再次使用它。</p>\n<p>缓冲池的大小在运行时会动态变化。网络堆栈中的内存缓冲区的数量(=队列的容量)决定了系统在不同发送/接收速度可以进行的缓冲量。<code>Flink</code>保证始终有足够的缓冲区来进行进程处理(enough buffers to make some progress)，但是这个进程的速度取决于用户程序和可用内存的数量。更多的内存意味着系统可以轻松地缓冲一定的瞬时背压(短时间段，短 GC)。越少的内存意味着需要对背压进行直接响应(没有足够的缓冲区进行缓存，只能响应处理)。</p>\n<p>从上面的简单例子可以看出：在任务1输出端和任务2输入端都有一个与其关联的缓冲池。如果有一个可用于序列化 <code>A</code> 的缓冲区，我们将其序列化并分配缓冲区。</p>\n<p>我们在这里有必要看两个case：</p>\n<p>(1) 本地交换：如果任务1和任务2在同一个工作节点(TaskManager)上运行，缓冲区可以直接交给下一个任务。一旦任务2消费完，它就会被回收。如果任务2比任务1慢，则缓冲区将以低于任务1填充的速度进行回收，从而导致任务1速度变慢。</p>\n<p>(2) 远程交换：如果任务1和任务2在不同的工作节点上运行，缓冲区一旦发送到线路中(TCP通道)就可以被回收。在接收端，数据从线路复制到输入缓冲池的缓冲区。如果没有缓冲区可用，从TCP连接读取操作将被中断。输出端通过一个简单的 <code>watermark</code> 机制保证不会在线上放置太多的数据。如果有足够的数据处在可发送状态，我们会一直复制更多的数据到线路中直到低于某个阈值时。这保证了没有太多的数据在传输途中。如果接收端没有消费新的数据(因为没有缓冲区可用)，这会减慢发送方的速度。</p>\n<p>这种简单的在固定大小缓冲池之间的缓冲区流使<code>Flink</code>能够拥有一个强大的背压机制，在这种机制下，任务生产数据速度不会比消费的快。</p>\n<p>我们描述的两个任务之间的数据传输的机制可以自然的推广到复杂管道上，保证背压在整个管道内传播。</p>\n<p>让我们看看一个简单的实验，展示了<code>Flink</code>在背压情况下的行为。我们运行一个简单的生产者-消费者流式拓扑，其中任务在本地交换数据，在这里我们可以变换任务产生记录的速度。对于这个测试，我们使用比默认更少的内存，以使得背压效果更明显。我们使用每个任务有2个大小为4096字节的缓冲区。在通常的<code>Flink</code>部署中，任务将具有更大更多缓冲区，这会提高性能。这个测试在单个JVM中运行，但使用完整的<code>Flink</code>代码堆栈。</p>\n<p>图中显示了生产者任务(黄色)和消费者任务(绿色)随着时间变化所达到的最大吞吐量(单个JVM中每秒达到800万个元素)的平均吞吐量占比(average throughput as a percentage of the maximum attained throughput  of the producing (yellow) and consuming (green) tasks)。为了衡量平均吞吐量，我们每5秒测量一次任务处理的记录数量。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/how-flink-handles-backpressure-5.png?raw=true\" alt=\"\"></p>\n<p>首先，我们以60％的速度运行生产任务(我们通过调用Thread.sleep()来模拟减速)。消费者以相同的速度处理数据，不会产生延迟。然后我们把消费者任务放慢到全速的30％。在这里，背压效果产生作用，因为我们看到生产者也自然放缓到全速的30％。然后，我们取消消费者任务的人为减速，并且这两项任务都达到最大吞吐量。我们再次把消费者任务放慢到全速的30％，管道立即响应，生产者任务也全速下降到30％。最后，我们再次停止减速，两项任务都以100％的速度持续下去。总而言之，我们看到生产者和消费者在管道上相互跟随彼此的吞吐量，这是我们在流水线中期望的行为。</p>\n<h3 id=\"3-结论\"><a href=\"#3-结论\" class=\"headerlink\" title=\"3. 结论\"></a>3. 结论</h3><p><code>Flink</code>与像Kafka这样的可持久化数据源，让你可以立即响应处理背压而不会丢失数据。<code>Flink</code>不需要专门的机制来处理背压，因为data shipping in <code>Flink</code> doubles as a backpressure mechanism。 因此，<code>Flink</code>实现了管道最慢部分允许的最大吞吐量。</p>\n<p>原文:<a href=\"https://data-artisans.com/blog/how-flink-handles-backpressure\" target=\"_blank\" rel=\"noopener\">https://data-artisans.com/blog/how-flink-handles-backpressure</a></p>\n"},{"layout":"post","author":"Jark","title":"Flink1.4 数据流类型与转换关系","date":"2018-01-04T07:47:01.000Z","_content":"\nFlink 为流处理和批处理分别提供了 DataStream API 和 DataSet API。正是这种高层的抽象和 flunent API 极大地便利了用户编写大数据应用。不过很多初学者在看到官方文档中那一大坨的转换时，常常会蒙了圈，文档中那些只言片语也很难讲清它们之间的关系。所以本文将介绍几种关键的数据流类型，它们之间是如何通过转换关联起来的。下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-1.png?raw=true)\n\n### 1. DataStream\n\n`DataStream` 是 `Flink` 流处理 API 中最核心的数据结构。它代表了一个运行在多个分区上的并行流。一个 `DataStream` 可以从 `StreamExecutionEnvironment` 通过 `env.addSource(SourceFunction)` 获得。\n\n`DataStream` 上的转换操作都是逐条的，比如 `map()`，`flatMap()`，`filter()`。`DataStream` 也可以执行 `rebalance`（再平衡，用来减轻数据倾斜）和 `broadcaseted`（广播）等分区转换。\n\n```\nval stream: DataStream[MyType] = env.addSource(new FlinkKafkaConsumer08[String](...))\nval str1: DataStream[(String, MyType)] = stream.flatMap { ... }\nval str2: DataStream[(String, MyType)] = stream.rebalance()\nval str3: DataStream[AnotherType] = stream.map { ... }\n```\n上述 DataStream 上的转换在运行时会转换成如下的执行图：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-2.png?raw=true)\n\n如上图的执行图所示，`DataStream` 各个算子会并行运行，算子之间是数据流分区。如 `Source` 的第一个并行实例（S1）和 `flatMap()` 的第一个并行实例（m1）之间就是一个数据流分区。而在 `flatMap()` 和 `map()` 之间由于加了 `rebalance()`，它们之间的数据流分区就有3个子分区（m1的数据流向3个map()实例）。这与 `Apache Kafka` 是很类似的，把流想象成 `Kafka Topic`，而一个流分区就表示一个 `Topic Partition`，流的目标并行算子实例就是 `Kafka Consumers`。\n\n### 2. KeyedStream\n\n`KeyedStream` 用来表示根据指定的 `key` 进行分组的数据流。一个 `KeyedStream `可以通过调用 `DataStream.keyBy()` 来获得。而在 `KeyedStream` 上进行任何 `transformation` 都将转变回 `DataStream`。在实现中，`KeyedStream` 是把 `key` 的信息写入到了 `transformation` 中。每条记录只能访问所属 `key` 的状态，其上的聚合函数可以方便地操作和保存对应 `key` 的状态。\n\n### 3. WindowedStream & AllWindowedStream\n\n`WindowedStream`代表了根据 `key` 分组，并且基于 `WindowAssigner` 切分窗口的数据流。所以 `WindowedStream` 都是从 `KeyedStream` 衍生而来的。而在 `WindowedStream` 上进行任何 `transformation` 也都将转变回 `DataStream`。\n\n```\nval stream: DataStream[MyType] = ...\nval windowed: WindowedDataStream[MyType] = stream\n        .keyBy(\"userId\")\n        .window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data\nval result: DataStream[ResultType] = windowed.reduce(myReducer)\n```\n\n上述 `WindowedStream` 的样例代码在运行时会转换成如下的执行图：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-3.png?raw=true)\n\n`Flink` 的窗口实现中会将到达的数据缓存在对应的窗口`buffer`中（一个数据可能会对应多个窗口）。当到达窗口发送的条件时（由`Trigger`控制），`Flink` 会对整个窗口中的数据进行处理。`Flink` 在聚合类窗口有一定的优化，即不会保存窗口中的所有值，而是每到一个元素执行一次聚合函数，最终只保存一份数据即可。\n\n在`key`分组的流上进行窗口切分是比较常用的场景，也能够很好地并行化（不同的 `key` 上的窗口聚合可以分配到不同的 `task` 去处理）。不过有时候我们也需要在普通流上进行窗口的操作，这就是  `AllWindowedStream`。`AllWindowedStream` 是直接在 `DataStream` 上进行 `windowAll(...)` 操作。`AllWindowedStream` 的实现是基于 `WindowedStream` 的（Flink 1.1.x 开始）。`Flink` 不推荐使用 `AllWindowedStream`，因为在普通流上进行窗口操作，就势必需要将所有分区的流都汇集到单个的 `Task` 中，而这个单个的 `Task` 很显然就会成为整个Job的瓶颈。\n\n### 4. JoinedStreams & CoGroupedStreams\n\n双流 `Join` 也是一个非常常见的应用场景。深入源码你可以发现，`JoinedStreams` 和 `CoGroupedStreams` 的代码实现有80%是一模一样的，`JoinedStreams` 在底层又调用了 `CoGroupedStreams` 来实现 `Join` 功能。除了名字不一样，一开始很难将它们区分开来，而且为什么要提供两个功能类似的接口呢？\n\n实际上这两者还是很点区别的。首先 `co-group` 侧重的是 `group`，是对同一个 `key` 上的两组集合进行操作，而 `join` 侧重的是 `pair`，是对同一个 `key` 上的每对元素进行操作。`co-group` 比 `join` 更通用一些，因为 `join` 只是 `co-group` 的一个特例，所以 `join` 是可以基于 `co-group` 来实现的（当然有优化的空间）。而在 `co-group` 之外又提供了 `join` 接口是因为用户更熟悉 `join`（源于数据库吧），而且能够跟 `DataSet API` 保持一致，降低用户的学习成本。\n\n`JoinedStreams` 和 `CoGroupedStreams` 是基于 `Window` 上实现的，所以 `CoGroupedStreams` 最终又调用了 `WindowedStream` 来实现。\n\n```\nval firstInput: DataStream[MyType] = ...\nval secondInput: DataStream[AnotherType] = ...\n\nval result: DataStream[(MyType, AnotherType)] = firstInput.join(secondInput)\n    .where(\"userId\").equalTo(\"id\")\n    .window(TumblingEventTimeWindows.of(Time.seconds(3)))\n    .apply (new JoinFunction () {...})\n```\n上述 `JoinedStreams` 的样例代码在运行时会转换成如下的执行图：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-4.png?raw=true)\n\n双流上的数据在同一个 `key` 的会被分别分配到同一个 `window` 窗口的左右两个篮子里，当 `window` 结束的时候，会对左右篮子进行笛卡尔积从而得到每一对 `pair`，对每一对 `pair` 应用 `JoinFunction`。不过目前（Flink 1.1.x） `JoinedStreams` 只是简单地实现了流上的 `join` 操作而已，距离真正的生产使用还是有些距离。因为目前 `join` 窗口的双流数据都是被缓存在内存中的，也就是说如果某个 `key` 上的窗口数据太多就会导致 `JVM OOM`（然而数据倾斜是常态）。双流 `join` 的难点也正是在这里，这也是社区后面对 `join` 操作的优化方向，例如可以借鉴 `Flink` 在批处理 `join` 中的优化方案，也可以用 `ManagedMemory` 来管理窗口中的数据，并当数据超过阈值时能spill到硬盘。\n\n### 5. ConnectedStreams\n\n在 `DataStream` 上有一个 `union` 的转换 `dataStream.union(otherStream1, otherStream2, ...)`，用来合并多个流，新的流会包含所有流中的数据。`union` 有一个限制，就是所有合并的流的类型必须是一致的。`ConnectedStreams` 提供了和 `union` 类似的功能，用来连接两个流，但是与 `union` 转换有以下几个区别：\n- `ConnectedStreams` 只能连接两个流，而 `union` 可以连接多于两个流。\n- `ConnectedStreams` 连接的两个流类型可以不一致，而 `union` 连接的流的类型必须一致。\n- `ConnectedStreams` 会对两个流的数据应用不同的处理方法，并且双流之间可以共享状态。这在第一个流的输入会影响第二个流时, 会非常有用。\n\n如下 `ConnectedStreams` 的样例，连接 `input` 和 `other` 流，并在 `input` 流上应用 `map1` 方法，在 `other` 上应用 `map2` 方法，双流可以共享状态（比如计数）。\n\n```\nval input: DataStream[MyType] = ...\nval other: DataStream[AnotherType] = ...\n\nval connected: ConnectedStreams[MyType, AnotherType] = input.connect(other)\n\nval result: DataStream[ResultType] =\n        connected.map(new CoMapFunction[MyType, AnotherType, ResultType]() {\n            override def map1(value: MyType): ResultType = { ... }\n            override def map2(value: AnotherType): ResultType = { ... }\n        })\n```\n\n当并行度为2时，其执行图如下所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-5.png?raw=true)\n\n### 6. 总结\n\n本文介绍通过不同数据流类型的转换图来解释每一种数据流的含义、转换关系。后面的文章会深入讲解 Window 机制的实现，双流 Join 的实现等。\n\n\n原文:http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/\n","source":"_posts/Flink/[Flink]Flink1.4 数据流类型和转换关系.md","raw":"---\nlayout: post\nauthor: Jark\ntitle: Flink1.4 数据流类型与转换关系\ndate: 2018-01-04 15:47:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\nFlink 为流处理和批处理分别提供了 DataStream API 和 DataSet API。正是这种高层的抽象和 flunent API 极大地便利了用户编写大数据应用。不过很多初学者在看到官方文档中那一大坨的转换时，常常会蒙了圈，文档中那些只言片语也很难讲清它们之间的关系。所以本文将介绍几种关键的数据流类型，它们之间是如何通过转换关联起来的。下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-1.png?raw=true)\n\n### 1. DataStream\n\n`DataStream` 是 `Flink` 流处理 API 中最核心的数据结构。它代表了一个运行在多个分区上的并行流。一个 `DataStream` 可以从 `StreamExecutionEnvironment` 通过 `env.addSource(SourceFunction)` 获得。\n\n`DataStream` 上的转换操作都是逐条的，比如 `map()`，`flatMap()`，`filter()`。`DataStream` 也可以执行 `rebalance`（再平衡，用来减轻数据倾斜）和 `broadcaseted`（广播）等分区转换。\n\n```\nval stream: DataStream[MyType] = env.addSource(new FlinkKafkaConsumer08[String](...))\nval str1: DataStream[(String, MyType)] = stream.flatMap { ... }\nval str2: DataStream[(String, MyType)] = stream.rebalance()\nval str3: DataStream[AnotherType] = stream.map { ... }\n```\n上述 DataStream 上的转换在运行时会转换成如下的执行图：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-2.png?raw=true)\n\n如上图的执行图所示，`DataStream` 各个算子会并行运行，算子之间是数据流分区。如 `Source` 的第一个并行实例（S1）和 `flatMap()` 的第一个并行实例（m1）之间就是一个数据流分区。而在 `flatMap()` 和 `map()` 之间由于加了 `rebalance()`，它们之间的数据流分区就有3个子分区（m1的数据流向3个map()实例）。这与 `Apache Kafka` 是很类似的，把流想象成 `Kafka Topic`，而一个流分区就表示一个 `Topic Partition`，流的目标并行算子实例就是 `Kafka Consumers`。\n\n### 2. KeyedStream\n\n`KeyedStream` 用来表示根据指定的 `key` 进行分组的数据流。一个 `KeyedStream `可以通过调用 `DataStream.keyBy()` 来获得。而在 `KeyedStream` 上进行任何 `transformation` 都将转变回 `DataStream`。在实现中，`KeyedStream` 是把 `key` 的信息写入到了 `transformation` 中。每条记录只能访问所属 `key` 的状态，其上的聚合函数可以方便地操作和保存对应 `key` 的状态。\n\n### 3. WindowedStream & AllWindowedStream\n\n`WindowedStream`代表了根据 `key` 分组，并且基于 `WindowAssigner` 切分窗口的数据流。所以 `WindowedStream` 都是从 `KeyedStream` 衍生而来的。而在 `WindowedStream` 上进行任何 `transformation` 也都将转变回 `DataStream`。\n\n```\nval stream: DataStream[MyType] = ...\nval windowed: WindowedDataStream[MyType] = stream\n        .keyBy(\"userId\")\n        .window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data\nval result: DataStream[ResultType] = windowed.reduce(myReducer)\n```\n\n上述 `WindowedStream` 的样例代码在运行时会转换成如下的执行图：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-3.png?raw=true)\n\n`Flink` 的窗口实现中会将到达的数据缓存在对应的窗口`buffer`中（一个数据可能会对应多个窗口）。当到达窗口发送的条件时（由`Trigger`控制），`Flink` 会对整个窗口中的数据进行处理。`Flink` 在聚合类窗口有一定的优化，即不会保存窗口中的所有值，而是每到一个元素执行一次聚合函数，最终只保存一份数据即可。\n\n在`key`分组的流上进行窗口切分是比较常用的场景，也能够很好地并行化（不同的 `key` 上的窗口聚合可以分配到不同的 `task` 去处理）。不过有时候我们也需要在普通流上进行窗口的操作，这就是  `AllWindowedStream`。`AllWindowedStream` 是直接在 `DataStream` 上进行 `windowAll(...)` 操作。`AllWindowedStream` 的实现是基于 `WindowedStream` 的（Flink 1.1.x 开始）。`Flink` 不推荐使用 `AllWindowedStream`，因为在普通流上进行窗口操作，就势必需要将所有分区的流都汇集到单个的 `Task` 中，而这个单个的 `Task` 很显然就会成为整个Job的瓶颈。\n\n### 4. JoinedStreams & CoGroupedStreams\n\n双流 `Join` 也是一个非常常见的应用场景。深入源码你可以发现，`JoinedStreams` 和 `CoGroupedStreams` 的代码实现有80%是一模一样的，`JoinedStreams` 在底层又调用了 `CoGroupedStreams` 来实现 `Join` 功能。除了名字不一样，一开始很难将它们区分开来，而且为什么要提供两个功能类似的接口呢？\n\n实际上这两者还是很点区别的。首先 `co-group` 侧重的是 `group`，是对同一个 `key` 上的两组集合进行操作，而 `join` 侧重的是 `pair`，是对同一个 `key` 上的每对元素进行操作。`co-group` 比 `join` 更通用一些，因为 `join` 只是 `co-group` 的一个特例，所以 `join` 是可以基于 `co-group` 来实现的（当然有优化的空间）。而在 `co-group` 之外又提供了 `join` 接口是因为用户更熟悉 `join`（源于数据库吧），而且能够跟 `DataSet API` 保持一致，降低用户的学习成本。\n\n`JoinedStreams` 和 `CoGroupedStreams` 是基于 `Window` 上实现的，所以 `CoGroupedStreams` 最终又调用了 `WindowedStream` 来实现。\n\n```\nval firstInput: DataStream[MyType] = ...\nval secondInput: DataStream[AnotherType] = ...\n\nval result: DataStream[(MyType, AnotherType)] = firstInput.join(secondInput)\n    .where(\"userId\").equalTo(\"id\")\n    .window(TumblingEventTimeWindows.of(Time.seconds(3)))\n    .apply (new JoinFunction () {...})\n```\n上述 `JoinedStreams` 的样例代码在运行时会转换成如下的执行图：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-4.png?raw=true)\n\n双流上的数据在同一个 `key` 的会被分别分配到同一个 `window` 窗口的左右两个篮子里，当 `window` 结束的时候，会对左右篮子进行笛卡尔积从而得到每一对 `pair`，对每一对 `pair` 应用 `JoinFunction`。不过目前（Flink 1.1.x） `JoinedStreams` 只是简单地实现了流上的 `join` 操作而已，距离真正的生产使用还是有些距离。因为目前 `join` 窗口的双流数据都是被缓存在内存中的，也就是说如果某个 `key` 上的窗口数据太多就会导致 `JVM OOM`（然而数据倾斜是常态）。双流 `join` 的难点也正是在这里，这也是社区后面对 `join` 操作的优化方向，例如可以借鉴 `Flink` 在批处理 `join` 中的优化方案，也可以用 `ManagedMemory` 来管理窗口中的数据，并当数据超过阈值时能spill到硬盘。\n\n### 5. ConnectedStreams\n\n在 `DataStream` 上有一个 `union` 的转换 `dataStream.union(otherStream1, otherStream2, ...)`，用来合并多个流，新的流会包含所有流中的数据。`union` 有一个限制，就是所有合并的流的类型必须是一致的。`ConnectedStreams` 提供了和 `union` 类似的功能，用来连接两个流，但是与 `union` 转换有以下几个区别：\n- `ConnectedStreams` 只能连接两个流，而 `union` 可以连接多于两个流。\n- `ConnectedStreams` 连接的两个流类型可以不一致，而 `union` 连接的流的类型必须一致。\n- `ConnectedStreams` 会对两个流的数据应用不同的处理方法，并且双流之间可以共享状态。这在第一个流的输入会影响第二个流时, 会非常有用。\n\n如下 `ConnectedStreams` 的样例，连接 `input` 和 `other` 流，并在 `input` 流上应用 `map1` 方法，在 `other` 上应用 `map2` 方法，双流可以共享状态（比如计数）。\n\n```\nval input: DataStream[MyType] = ...\nval other: DataStream[AnotherType] = ...\n\nval connected: ConnectedStreams[MyType, AnotherType] = input.connect(other)\n\nval result: DataStream[ResultType] =\n        connected.map(new CoMapFunction[MyType, AnotherType, ResultType]() {\n            override def map1(value: MyType): ResultType = { ... }\n            override def map2(value: AnotherType): ResultType = { ... }\n        })\n```\n\n当并行度为2时，其执行图如下所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-5.png?raw=true)\n\n### 6. 总结\n\n本文介绍通过不同数据流类型的转换图来解释每一种数据流的含义、转换关系。后面的文章会深入讲解 Window 机制的实现，双流 Join 的实现等。\n\n\n原文:http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/\n","slug":"Flink/[Flink]Flink1.4 数据流类型和转换关系","published":1,"updated":"2018-01-29T09:36:59.648Z","comments":1,"photos":[],"link":"","_id":"cje58tir6001qordbqdp1dmpx","content":"<p>Flink 为流处理和批处理分别提供了 DataStream API 和 DataSet API。正是这种高层的抽象和 flunent API 极大地便利了用户编写大数据应用。不过很多初学者在看到官方文档中那一大坨的转换时，常常会蒙了圈，文档中那些只言片语也很难讲清它们之间的关系。所以本文将介绍几种关键的数据流类型，它们之间是如何通过转换关联起来的。下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"1-DataStream\"><a href=\"#1-DataStream\" class=\"headerlink\" title=\"1. DataStream\"></a>1. DataStream</h3><p><code>DataStream</code> 是 <code>Flink</code> 流处理 API 中最核心的数据结构。它代表了一个运行在多个分区上的并行流。一个 <code>DataStream</code> 可以从 <code>StreamExecutionEnvironment</code> 通过 <code>env.addSource(SourceFunction)</code> 获得。</p>\n<p><code>DataStream</code> 上的转换操作都是逐条的，比如 <code>map()</code>，<code>flatMap()</code>，<code>filter()</code>。<code>DataStream</code> 也可以执行 <code>rebalance</code>（再平衡，用来减轻数据倾斜）和 <code>broadcaseted</code>（广播）等分区转换。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyType] = env.addSource(new FlinkKafkaConsumer08[String](...))</span><br><span class=\"line\">val str1: DataStream[(String, MyType)] = stream.flatMap &#123; ... &#125;</span><br><span class=\"line\">val str2: DataStream[(String, MyType)] = stream.rebalance()</span><br><span class=\"line\">val str3: DataStream[AnotherType] = stream.map &#123; ... &#125;</span><br></pre></td></tr></table></figure>\n<p>上述 DataStream 上的转换在运行时会转换成如下的执行图：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-2.png?raw=true\" alt=\"\"></p>\n<p>如上图的执行图所示，<code>DataStream</code> 各个算子会并行运行，算子之间是数据流分区。如 <code>Source</code> 的第一个并行实例（S1）和 <code>flatMap()</code> 的第一个并行实例（m1）之间就是一个数据流分区。而在 <code>flatMap()</code> 和 <code>map()</code> 之间由于加了 <code>rebalance()</code>，它们之间的数据流分区就有3个子分区（m1的数据流向3个map()实例）。这与 <code>Apache Kafka</code> 是很类似的，把流想象成 <code>Kafka Topic</code>，而一个流分区就表示一个 <code>Topic Partition</code>，流的目标并行算子实例就是 <code>Kafka Consumers</code>。</p>\n<h3 id=\"2-KeyedStream\"><a href=\"#2-KeyedStream\" class=\"headerlink\" title=\"2. KeyedStream\"></a>2. KeyedStream</h3><p><code>KeyedStream</code> 用来表示根据指定的 <code>key</code> 进行分组的数据流。一个 <code>KeyedStream</code>可以通过调用 <code>DataStream.keyBy()</code> 来获得。而在 <code>KeyedStream</code> 上进行任何 <code>transformation</code> 都将转变回 <code>DataStream</code>。在实现中，<code>KeyedStream</code> 是把 <code>key</code> 的信息写入到了 <code>transformation</code> 中。每条记录只能访问所属 <code>key</code> 的状态，其上的聚合函数可以方便地操作和保存对应 <code>key</code> 的状态。</p>\n<h3 id=\"3-WindowedStream-amp-AllWindowedStream\"><a href=\"#3-WindowedStream-amp-AllWindowedStream\" class=\"headerlink\" title=\"3. WindowedStream &amp; AllWindowedStream\"></a>3. WindowedStream &amp; AllWindowedStream</h3><p><code>WindowedStream</code>代表了根据 <code>key</code> 分组，并且基于 <code>WindowAssigner</code> 切分窗口的数据流。所以 <code>WindowedStream</code> 都是从 <code>KeyedStream</code> 衍生而来的。而在 <code>WindowedStream</code> 上进行任何 <code>transformation</code> 也都将转变回 <code>DataStream</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyType] = ...</span><br><span class=\"line\">val windowed: WindowedDataStream[MyType] = stream</span><br><span class=\"line\">        .keyBy(&quot;userId&quot;)</span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data</span><br><span class=\"line\">val result: DataStream[ResultType] = windowed.reduce(myReducer)</span><br></pre></td></tr></table></figure>\n<p>上述 <code>WindowedStream</code> 的样例代码在运行时会转换成如下的执行图：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-3.png?raw=true\" alt=\"\"></p>\n<p><code>Flink</code> 的窗口实现中会将到达的数据缓存在对应的窗口<code>buffer</code>中（一个数据可能会对应多个窗口）。当到达窗口发送的条件时（由<code>Trigger</code>控制），<code>Flink</code> 会对整个窗口中的数据进行处理。<code>Flink</code> 在聚合类窗口有一定的优化，即不会保存窗口中的所有值，而是每到一个元素执行一次聚合函数，最终只保存一份数据即可。</p>\n<p>在<code>key</code>分组的流上进行窗口切分是比较常用的场景，也能够很好地并行化（不同的 <code>key</code> 上的窗口聚合可以分配到不同的 <code>task</code> 去处理）。不过有时候我们也需要在普通流上进行窗口的操作，这就是  <code>AllWindowedStream</code>。<code>AllWindowedStream</code> 是直接在 <code>DataStream</code> 上进行 <code>windowAll(...)</code> 操作。<code>AllWindowedStream</code> 的实现是基于 <code>WindowedStream</code> 的（Flink 1.1.x 开始）。<code>Flink</code> 不推荐使用 <code>AllWindowedStream</code>，因为在普通流上进行窗口操作，就势必需要将所有分区的流都汇集到单个的 <code>Task</code> 中，而这个单个的 <code>Task</code> 很显然就会成为整个Job的瓶颈。</p>\n<h3 id=\"4-JoinedStreams-amp-CoGroupedStreams\"><a href=\"#4-JoinedStreams-amp-CoGroupedStreams\" class=\"headerlink\" title=\"4. JoinedStreams &amp; CoGroupedStreams\"></a>4. JoinedStreams &amp; CoGroupedStreams</h3><p>双流 <code>Join</code> 也是一个非常常见的应用场景。深入源码你可以发现，<code>JoinedStreams</code> 和 <code>CoGroupedStreams</code> 的代码实现有80%是一模一样的，<code>JoinedStreams</code> 在底层又调用了 <code>CoGroupedStreams</code> 来实现 <code>Join</code> 功能。除了名字不一样，一开始很难将它们区分开来，而且为什么要提供两个功能类似的接口呢？</p>\n<p>实际上这两者还是很点区别的。首先 <code>co-group</code> 侧重的是 <code>group</code>，是对同一个 <code>key</code> 上的两组集合进行操作，而 <code>join</code> 侧重的是 <code>pair</code>，是对同一个 <code>key</code> 上的每对元素进行操作。<code>co-group</code> 比 <code>join</code> 更通用一些，因为 <code>join</code> 只是 <code>co-group</code> 的一个特例，所以 <code>join</code> 是可以基于 <code>co-group</code> 来实现的（当然有优化的空间）。而在 <code>co-group</code> 之外又提供了 <code>join</code> 接口是因为用户更熟悉 <code>join</code>（源于数据库吧），而且能够跟 <code>DataSet API</code> 保持一致，降低用户的学习成本。</p>\n<p><code>JoinedStreams</code> 和 <code>CoGroupedStreams</code> 是基于 <code>Window</code> 上实现的，所以 <code>CoGroupedStreams</code> 最终又调用了 <code>WindowedStream</code> 来实现。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val firstInput: DataStream[MyType] = ...</span><br><span class=\"line\">val secondInput: DataStream[AnotherType] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val result: DataStream[(MyType, AnotherType)] = firstInput.join(secondInput)</span><br><span class=\"line\">    .where(&quot;userId&quot;).equalTo(&quot;id&quot;)</span><br><span class=\"line\">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class=\"line\">    .apply (new JoinFunction () &#123;...&#125;)</span><br></pre></td></tr></table></figure>\n<p>上述 <code>JoinedStreams</code> 的样例代码在运行时会转换成如下的执行图：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-4.png?raw=true\" alt=\"\"></p>\n<p>双流上的数据在同一个 <code>key</code> 的会被分别分配到同一个 <code>window</code> 窗口的左右两个篮子里，当 <code>window</code> 结束的时候，会对左右篮子进行笛卡尔积从而得到每一对 <code>pair</code>，对每一对 <code>pair</code> 应用 <code>JoinFunction</code>。不过目前（Flink 1.1.x） <code>JoinedStreams</code> 只是简单地实现了流上的 <code>join</code> 操作而已，距离真正的生产使用还是有些距离。因为目前 <code>join</code> 窗口的双流数据都是被缓存在内存中的，也就是说如果某个 <code>key</code> 上的窗口数据太多就会导致 <code>JVM OOM</code>（然而数据倾斜是常态）。双流 <code>join</code> 的难点也正是在这里，这也是社区后面对 <code>join</code> 操作的优化方向，例如可以借鉴 <code>Flink</code> 在批处理 <code>join</code> 中的优化方案，也可以用 <code>ManagedMemory</code> 来管理窗口中的数据，并当数据超过阈值时能spill到硬盘。</p>\n<h3 id=\"5-ConnectedStreams\"><a href=\"#5-ConnectedStreams\" class=\"headerlink\" title=\"5. ConnectedStreams\"></a>5. ConnectedStreams</h3><p>在 <code>DataStream</code> 上有一个 <code>union</code> 的转换 <code>dataStream.union(otherStream1, otherStream2, ...)</code>，用来合并多个流，新的流会包含所有流中的数据。<code>union</code> 有一个限制，就是所有合并的流的类型必须是一致的。<code>ConnectedStreams</code> 提供了和 <code>union</code> 类似的功能，用来连接两个流，但是与 <code>union</code> 转换有以下几个区别：</p>\n<ul>\n<li><code>ConnectedStreams</code> 只能连接两个流，而 <code>union</code> 可以连接多于两个流。</li>\n<li><code>ConnectedStreams</code> 连接的两个流类型可以不一致，而 <code>union</code> 连接的流的类型必须一致。</li>\n<li><code>ConnectedStreams</code> 会对两个流的数据应用不同的处理方法，并且双流之间可以共享状态。这在第一个流的输入会影响第二个流时, 会非常有用。</li>\n</ul>\n<p>如下 <code>ConnectedStreams</code> 的样例，连接 <code>input</code> 和 <code>other</code> 流，并在 <code>input</code> 流上应用 <code>map1</code> 方法，在 <code>other</code> 上应用 <code>map2</code> 方法，双流可以共享状态（比如计数）。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataStream[MyType] = ...</span><br><span class=\"line\">val other: DataStream[AnotherType] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val connected: ConnectedStreams[MyType, AnotherType] = input.connect(other)</span><br><span class=\"line\"></span><br><span class=\"line\">val result: DataStream[ResultType] =</span><br><span class=\"line\">        connected.map(new CoMapFunction[MyType, AnotherType, ResultType]() &#123;</span><br><span class=\"line\">            override def map1(value: MyType): ResultType = &#123; ... &#125;</span><br><span class=\"line\">            override def map2(value: AnotherType): ResultType = &#123; ... &#125;</span><br><span class=\"line\">        &#125;)</span><br></pre></td></tr></table></figure>\n<p>当并行度为2时，其执行图如下所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"6-总结\"><a href=\"#6-总结\" class=\"headerlink\" title=\"6. 总结\"></a>6. 总结</h3><p>本文介绍通过不同数据流类型的转换图来解释每一种数据流的含义、转换关系。后面的文章会深入讲解 Window 机制的实现，双流 Join 的实现等。</p>\n<p>原文:<a href=\"http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/\" target=\"_blank\" rel=\"noopener\">http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Flink 为流处理和批处理分别提供了 DataStream API 和 DataSet API。正是这种高层的抽象和 flunent API 极大地便利了用户编写大数据应用。不过很多初学者在看到官方文档中那一大坨的转换时，常常会蒙了圈，文档中那些只言片语也很难讲清它们之间的关系。所以本文将介绍几种关键的数据流类型，它们之间是如何通过转换关联起来的。下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"1-DataStream\"><a href=\"#1-DataStream\" class=\"headerlink\" title=\"1. DataStream\"></a>1. DataStream</h3><p><code>DataStream</code> 是 <code>Flink</code> 流处理 API 中最核心的数据结构。它代表了一个运行在多个分区上的并行流。一个 <code>DataStream</code> 可以从 <code>StreamExecutionEnvironment</code> 通过 <code>env.addSource(SourceFunction)</code> 获得。</p>\n<p><code>DataStream</code> 上的转换操作都是逐条的，比如 <code>map()</code>，<code>flatMap()</code>，<code>filter()</code>。<code>DataStream</code> 也可以执行 <code>rebalance</code>（再平衡，用来减轻数据倾斜）和 <code>broadcaseted</code>（广播）等分区转换。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyType] = env.addSource(new FlinkKafkaConsumer08[String](...))</span><br><span class=\"line\">val str1: DataStream[(String, MyType)] = stream.flatMap &#123; ... &#125;</span><br><span class=\"line\">val str2: DataStream[(String, MyType)] = stream.rebalance()</span><br><span class=\"line\">val str3: DataStream[AnotherType] = stream.map &#123; ... &#125;</span><br></pre></td></tr></table></figure>\n<p>上述 DataStream 上的转换在运行时会转换成如下的执行图：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-2.png?raw=true\" alt=\"\"></p>\n<p>如上图的执行图所示，<code>DataStream</code> 各个算子会并行运行，算子之间是数据流分区。如 <code>Source</code> 的第一个并行实例（S1）和 <code>flatMap()</code> 的第一个并行实例（m1）之间就是一个数据流分区。而在 <code>flatMap()</code> 和 <code>map()</code> 之间由于加了 <code>rebalance()</code>，它们之间的数据流分区就有3个子分区（m1的数据流向3个map()实例）。这与 <code>Apache Kafka</code> 是很类似的，把流想象成 <code>Kafka Topic</code>，而一个流分区就表示一个 <code>Topic Partition</code>，流的目标并行算子实例就是 <code>Kafka Consumers</code>。</p>\n<h3 id=\"2-KeyedStream\"><a href=\"#2-KeyedStream\" class=\"headerlink\" title=\"2. KeyedStream\"></a>2. KeyedStream</h3><p><code>KeyedStream</code> 用来表示根据指定的 <code>key</code> 进行分组的数据流。一个 <code>KeyedStream</code>可以通过调用 <code>DataStream.keyBy()</code> 来获得。而在 <code>KeyedStream</code> 上进行任何 <code>transformation</code> 都将转变回 <code>DataStream</code>。在实现中，<code>KeyedStream</code> 是把 <code>key</code> 的信息写入到了 <code>transformation</code> 中。每条记录只能访问所属 <code>key</code> 的状态，其上的聚合函数可以方便地操作和保存对应 <code>key</code> 的状态。</p>\n<h3 id=\"3-WindowedStream-amp-AllWindowedStream\"><a href=\"#3-WindowedStream-amp-AllWindowedStream\" class=\"headerlink\" title=\"3. WindowedStream &amp; AllWindowedStream\"></a>3. WindowedStream &amp; AllWindowedStream</h3><p><code>WindowedStream</code>代表了根据 <code>key</code> 分组，并且基于 <code>WindowAssigner</code> 切分窗口的数据流。所以 <code>WindowedStream</code> 都是从 <code>KeyedStream</code> 衍生而来的。而在 <code>WindowedStream</code> 上进行任何 <code>transformation</code> 也都将转变回 <code>DataStream</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[MyType] = ...</span><br><span class=\"line\">val windowed: WindowedDataStream[MyType] = stream</span><br><span class=\"line\">        .keyBy(&quot;userId&quot;)</span><br><span class=\"line\">        .window(TumblingEventTimeWindows.of(Time.seconds(5))) // Last 5 seconds of data</span><br><span class=\"line\">val result: DataStream[ResultType] = windowed.reduce(myReducer)</span><br></pre></td></tr></table></figure>\n<p>上述 <code>WindowedStream</code> 的样例代码在运行时会转换成如下的执行图：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-3.png?raw=true\" alt=\"\"></p>\n<p><code>Flink</code> 的窗口实现中会将到达的数据缓存在对应的窗口<code>buffer</code>中（一个数据可能会对应多个窗口）。当到达窗口发送的条件时（由<code>Trigger</code>控制），<code>Flink</code> 会对整个窗口中的数据进行处理。<code>Flink</code> 在聚合类窗口有一定的优化，即不会保存窗口中的所有值，而是每到一个元素执行一次聚合函数，最终只保存一份数据即可。</p>\n<p>在<code>key</code>分组的流上进行窗口切分是比较常用的场景，也能够很好地并行化（不同的 <code>key</code> 上的窗口聚合可以分配到不同的 <code>task</code> 去处理）。不过有时候我们也需要在普通流上进行窗口的操作，这就是  <code>AllWindowedStream</code>。<code>AllWindowedStream</code> 是直接在 <code>DataStream</code> 上进行 <code>windowAll(...)</code> 操作。<code>AllWindowedStream</code> 的实现是基于 <code>WindowedStream</code> 的（Flink 1.1.x 开始）。<code>Flink</code> 不推荐使用 <code>AllWindowedStream</code>，因为在普通流上进行窗口操作，就势必需要将所有分区的流都汇集到单个的 <code>Task</code> 中，而这个单个的 <code>Task</code> 很显然就会成为整个Job的瓶颈。</p>\n<h3 id=\"4-JoinedStreams-amp-CoGroupedStreams\"><a href=\"#4-JoinedStreams-amp-CoGroupedStreams\" class=\"headerlink\" title=\"4. JoinedStreams &amp; CoGroupedStreams\"></a>4. JoinedStreams &amp; CoGroupedStreams</h3><p>双流 <code>Join</code> 也是一个非常常见的应用场景。深入源码你可以发现，<code>JoinedStreams</code> 和 <code>CoGroupedStreams</code> 的代码实现有80%是一模一样的，<code>JoinedStreams</code> 在底层又调用了 <code>CoGroupedStreams</code> 来实现 <code>Join</code> 功能。除了名字不一样，一开始很难将它们区分开来，而且为什么要提供两个功能类似的接口呢？</p>\n<p>实际上这两者还是很点区别的。首先 <code>co-group</code> 侧重的是 <code>group</code>，是对同一个 <code>key</code> 上的两组集合进行操作，而 <code>join</code> 侧重的是 <code>pair</code>，是对同一个 <code>key</code> 上的每对元素进行操作。<code>co-group</code> 比 <code>join</code> 更通用一些，因为 <code>join</code> 只是 <code>co-group</code> 的一个特例，所以 <code>join</code> 是可以基于 <code>co-group</code> 来实现的（当然有优化的空间）。而在 <code>co-group</code> 之外又提供了 <code>join</code> 接口是因为用户更熟悉 <code>join</code>（源于数据库吧），而且能够跟 <code>DataSet API</code> 保持一致，降低用户的学习成本。</p>\n<p><code>JoinedStreams</code> 和 <code>CoGroupedStreams</code> 是基于 <code>Window</code> 上实现的，所以 <code>CoGroupedStreams</code> 最终又调用了 <code>WindowedStream</code> 来实现。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val firstInput: DataStream[MyType] = ...</span><br><span class=\"line\">val secondInput: DataStream[AnotherType] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val result: DataStream[(MyType, AnotherType)] = firstInput.join(secondInput)</span><br><span class=\"line\">    .where(&quot;userId&quot;).equalTo(&quot;id&quot;)</span><br><span class=\"line\">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class=\"line\">    .apply (new JoinFunction () &#123;...&#125;)</span><br></pre></td></tr></table></figure>\n<p>上述 <code>JoinedStreams</code> 的样例代码在运行时会转换成如下的执行图：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-4.png?raw=true\" alt=\"\"></p>\n<p>双流上的数据在同一个 <code>key</code> 的会被分别分配到同一个 <code>window</code> 窗口的左右两个篮子里，当 <code>window</code> 结束的时候，会对左右篮子进行笛卡尔积从而得到每一对 <code>pair</code>，对每一对 <code>pair</code> 应用 <code>JoinFunction</code>。不过目前（Flink 1.1.x） <code>JoinedStreams</code> 只是简单地实现了流上的 <code>join</code> 操作而已，距离真正的生产使用还是有些距离。因为目前 <code>join</code> 窗口的双流数据都是被缓存在内存中的，也就是说如果某个 <code>key</code> 上的窗口数据太多就会导致 <code>JVM OOM</code>（然而数据倾斜是常态）。双流 <code>join</code> 的难点也正是在这里，这也是社区后面对 <code>join</code> 操作的优化方向，例如可以借鉴 <code>Flink</code> 在批处理 <code>join</code> 中的优化方案，也可以用 <code>ManagedMemory</code> 来管理窗口中的数据，并当数据超过阈值时能spill到硬盘。</p>\n<h3 id=\"5-ConnectedStreams\"><a href=\"#5-ConnectedStreams\" class=\"headerlink\" title=\"5. ConnectedStreams\"></a>5. ConnectedStreams</h3><p>在 <code>DataStream</code> 上有一个 <code>union</code> 的转换 <code>dataStream.union(otherStream1, otherStream2, ...)</code>，用来合并多个流，新的流会包含所有流中的数据。<code>union</code> 有一个限制，就是所有合并的流的类型必须是一致的。<code>ConnectedStreams</code> 提供了和 <code>union</code> 类似的功能，用来连接两个流，但是与 <code>union</code> 转换有以下几个区别：</p>\n<ul>\n<li><code>ConnectedStreams</code> 只能连接两个流，而 <code>union</code> 可以连接多于两个流。</li>\n<li><code>ConnectedStreams</code> 连接的两个流类型可以不一致，而 <code>union</code> 连接的流的类型必须一致。</li>\n<li><code>ConnectedStreams</code> 会对两个流的数据应用不同的处理方法，并且双流之间可以共享状态。这在第一个流的输入会影响第二个流时, 会非常有用。</li>\n</ul>\n<p>如下 <code>ConnectedStreams</code> 的样例，连接 <code>input</code> 和 <code>other</code> 流，并在 <code>input</code> 流上应用 <code>map1</code> 方法，在 <code>other</code> 上应用 <code>map2</code> 方法，双流可以共享状态（比如计数）。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val input: DataStream[MyType] = ...</span><br><span class=\"line\">val other: DataStream[AnotherType] = ...</span><br><span class=\"line\"></span><br><span class=\"line\">val connected: ConnectedStreams[MyType, AnotherType] = input.connect(other)</span><br><span class=\"line\"></span><br><span class=\"line\">val result: DataStream[ResultType] =</span><br><span class=\"line\">        connected.map(new CoMapFunction[MyType, AnotherType, ResultType]() &#123;</span><br><span class=\"line\">            override def map1(value: MyType): ResultType = &#123; ... &#125;</span><br><span class=\"line\">            override def map2(value: AnotherType): ResultType = &#123; ... &#125;</span><br><span class=\"line\">        &#125;)</span><br></pre></td></tr></table></figure>\n<p>当并行度为2时，其执行图如下所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"6-总结\"><a href=\"#6-总结\" class=\"headerlink\" title=\"6. 总结\"></a>6. 总结</h3><p>本文介绍通过不同数据流类型的转换图来解释每一种数据流的含义、转换关系。后面的文章会深入讲解 Window 机制的实现，双流 Join 的实现等。</p>\n<p>原文:<a href=\"http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/\" target=\"_blank\" rel=\"noopener\">http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 执行计划","date":"2018-01-04T06:46:01.000Z","_content":"\n根据各种参数(如数据大小或集群中的机器数量)，`Flink`的优化器自动会为你的程序选择一个执行策略。很多情况下，准确的知道`Flink`如何执行你的程序是很有帮助的。\n\n### 1. 计划可视化工具\n\n`Flink`内置一个执行计划的可视化工具。包含可视化工具的`HTML`文档位于`tools/planVisualizer.html`下。用`JSON`表示作业执行计划，并将其可视化为具有执行策略完整注释的图(visualizes it as a graph with complete annotations of execution strategies)。\n\n备注:\n```\n打开可视化工具的方式有所改变:由本地文件 tools/planVisualizer.html 改为 url http://flink.apache.org/visualizer/index.html\n```\n\n以下代码显示了如何从程序中打印执行计划的`JSON`：\n\nJava版本:\n```java\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n...\nSystem.out.println(env.getExecutionPlan());\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment\n...\nprintln(env.getExecutionPlan())\n```\n\n要可视化执行计划，请执行以下操作：\n\n(1) 使用浏览器打开`planVisualizer.html`(或者直接在浏览器中输入http://flink.apache.org/visualizer/index.html 网址)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-2.png?raw=true)\n\n(2) 将`JSON`字符串粘贴到文本框中\n\n(3) 点击`Draw`按钮\n\n完成上面这些步骤后，将会显示详细的执行计划。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-1.png?raw=true)\n\n### 2. Web界面\n\n`Flink`提供了一个用于提交和执行作业的Web界面。这个界面是`JobManager Web`监控界面的一部分，默认情况下在端口`8081`上运行。通过这个界面提交作业需要你在`flink-conf.yaml`中设置`jobmanager.web.submit.enable：true`。\n\n你可以在作业执行之前指定程序参数。执行计划可视化器使你能够在执行Flink作业之前查看执行计划。\n\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html\n","source":"_posts/Flink/[Flink]Flink1.4 执行计划.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 执行计划\ndate: 2018-01-04 14:46:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n根据各种参数(如数据大小或集群中的机器数量)，`Flink`的优化器自动会为你的程序选择一个执行策略。很多情况下，准确的知道`Flink`如何执行你的程序是很有帮助的。\n\n### 1. 计划可视化工具\n\n`Flink`内置一个执行计划的可视化工具。包含可视化工具的`HTML`文档位于`tools/planVisualizer.html`下。用`JSON`表示作业执行计划，并将其可视化为具有执行策略完整注释的图(visualizes it as a graph with complete annotations of execution strategies)。\n\n备注:\n```\n打开可视化工具的方式有所改变:由本地文件 tools/planVisualizer.html 改为 url http://flink.apache.org/visualizer/index.html\n```\n\n以下代码显示了如何从程序中打印执行计划的`JSON`：\n\nJava版本:\n```java\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n...\nSystem.out.println(env.getExecutionPlan());\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment\n...\nprintln(env.getExecutionPlan())\n```\n\n要可视化执行计划，请执行以下操作：\n\n(1) 使用浏览器打开`planVisualizer.html`(或者直接在浏览器中输入http://flink.apache.org/visualizer/index.html 网址)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-2.png?raw=true)\n\n(2) 将`JSON`字符串粘贴到文本框中\n\n(3) 点击`Draw`按钮\n\n完成上面这些步骤后，将会显示详细的执行计划。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-1.png?raw=true)\n\n### 2. Web界面\n\n`Flink`提供了一个用于提交和执行作业的Web界面。这个界面是`JobManager Web`监控界面的一部分，默认情况下在端口`8081`上运行。通过这个界面提交作业需要你在`flink-conf.yaml`中设置`jobmanager.web.submit.enable：true`。\n\n你可以在作业执行之前指定程序参数。执行计划可视化器使你能够在执行Flink作业之前查看执行计划。\n\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html\n","slug":"Flink/[Flink]Flink1.4 执行计划","published":1,"updated":"2018-01-29T09:36:59.649Z","comments":1,"photos":[],"link":"","_id":"cje58tir9001tordbhchvxtcq","content":"<p>根据各种参数(如数据大小或集群中的机器数量)，<code>Flink</code>的优化器自动会为你的程序选择一个执行策略。很多情况下，准确的知道<code>Flink</code>如何执行你的程序是很有帮助的。</p>\n<h3 id=\"1-计划可视化工具\"><a href=\"#1-计划可视化工具\" class=\"headerlink\" title=\"1. 计划可视化工具\"></a>1. 计划可视化工具</h3><p><code>Flink</code>内置一个执行计划的可视化工具。包含可视化工具的<code>HTML</code>文档位于<code>tools/planVisualizer.html</code>下。用<code>JSON</code>表示作业执行计划，并将其可视化为具有执行策略完整注释的图(visualizes it as a graph with complete annotations of execution strategies)。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">打开可视化工具的方式有所改变:由本地文件 tools/planVisualizer.html 改为 url http://flink.apache.org/visualizer/index.html</span><br></pre></td></tr></table></figure></p>\n<p>以下代码显示了如何从程序中打印执行计划的<code>JSON</code>：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">...</span><br><span class=\"line\">System.out.println(env.getExecutionPlan());</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">...</span><br><span class=\"line\">println(env.getExecutionPlan())</span><br></pre></td></tr></table></figure></p>\n<p>要可视化执行计划，请执行以下操作：</p>\n<p>(1) 使用浏览器打开<code>planVisualizer.html</code>(或者直接在浏览器中输入<a href=\"http://flink.apache.org/visualizer/index.html\" target=\"_blank\" rel=\"noopener\">http://flink.apache.org/visualizer/index.html</a> 网址)</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-2.png?raw=true\" alt=\"\"></p>\n<p>(2) 将<code>JSON</code>字符串粘贴到文本框中</p>\n<p>(3) 点击<code>Draw</code>按钮</p>\n<p>完成上面这些步骤后，将会显示详细的执行计划。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-Web界面\"><a href=\"#2-Web界面\" class=\"headerlink\" title=\"2. Web界面\"></a>2. Web界面</h3><p><code>Flink</code>提供了一个用于提交和执行作业的Web界面。这个界面是<code>JobManager Web</code>监控界面的一部分，默认情况下在端口<code>8081</code>上运行。通过这个界面提交作业需要你在<code>flink-conf.yaml</code>中设置<code>jobmanager.web.submit.enable：true</code>。</p>\n<p>你可以在作业执行之前指定程序参数。执行计划可视化器使你能够在执行Flink作业之前查看执行计划。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>根据各种参数(如数据大小或集群中的机器数量)，<code>Flink</code>的优化器自动会为你的程序选择一个执行策略。很多情况下，准确的知道<code>Flink</code>如何执行你的程序是很有帮助的。</p>\n<h3 id=\"1-计划可视化工具\"><a href=\"#1-计划可视化工具\" class=\"headerlink\" title=\"1. 计划可视化工具\"></a>1. 计划可视化工具</h3><p><code>Flink</code>内置一个执行计划的可视化工具。包含可视化工具的<code>HTML</code>文档位于<code>tools/planVisualizer.html</code>下。用<code>JSON</code>表示作业执行计划，并将其可视化为具有执行策略完整注释的图(visualizes it as a graph with complete annotations of execution strategies)。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">打开可视化工具的方式有所改变:由本地文件 tools/planVisualizer.html 改为 url http://flink.apache.org/visualizer/index.html</span><br></pre></td></tr></table></figure></p>\n<p>以下代码显示了如何从程序中打印执行计划的<code>JSON</code>：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">...</span><br><span class=\"line\">System.out.println(env.getExecutionPlan());</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">...</span><br><span class=\"line\">println(env.getExecutionPlan())</span><br></pre></td></tr></table></figure></p>\n<p>要可视化执行计划，请执行以下操作：</p>\n<p>(1) 使用浏览器打开<code>planVisualizer.html</code>(或者直接在浏览器中输入<a href=\"http://flink.apache.org/visualizer/index.html\" target=\"_blank\" rel=\"noopener\">http://flink.apache.org/visualizer/index.html</a> 网址)</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-2.png?raw=true\" alt=\"\"></p>\n<p>(2) 将<code>JSON</code>字符串粘贴到文本框中</p>\n<p>(3) 点击<code>Draw</code>按钮</p>\n<p>完成上面这些步骤后，将会显示详细的执行计划。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/Flink%20%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-Web界面\"><a href=\"#2-Web界面\" class=\"headerlink\" title=\"2. Web界面\"></a>2. Web界面</h3><p><code>Flink</code>提供了一个用于提交和执行作业的Web界面。这个界面是<code>JobManager Web</code>监控界面的一部分，默认情况下在端口<code>8081</code>上运行。通过这个界面提交作业需要你在<code>flink-conf.yaml</code>中设置<code>jobmanager.web.submit.enable：true</code>。</p>\n<p>你可以在作业执行之前指定程序参数。执行计划可视化器使你能够在执行Flink作业之前查看执行计划。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/execution_plans.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 状态概述","date":"2018-01-16T11:30:17.000Z","_content":"\n有状态的函数和算子在处理单个元素/事件时存储数据，使得状态`state`成为任何精细操作的关键构件。\n\n例如：\n- 当应用程序搜索某些特定模式事件时，状态将存储迄今为止遇到的事件序列。\n- 当按每分钟/小时/天聚合事件时，状态保存待处理的聚合事件。\n- 在数据流上训练机器学习模型时，状态保存当前版本的模型参数。\n- 当需要管理历史数据时，状态允许访问过去发生的事件。\n\n`Flink` 需要了解状态，以便使用检查点进行状态容错，并允许流应用程序使用保存点。\n\n对状态进行了解有助于你对 `Flink` 应用程序进行扩展，这意味着 `Flink` 负责在并行实例之间进行重新分配状态。\n\n`Flink` 的可查询状态`queryable state`功能允许你在 `Flink` 运行时在外部访问状态。\n\n在使用状态时，阅读有关`Flink`的 `State Backends` 应该对你很有帮助。`Flink` 提供不同的 `State Backends`，并指定状态的存储方式和位置。状态可以位于`Java`的堆内或堆外。根据你的 `State Backends`，`Flink`也可以管理应用程序的状态，这意味着`Flink`进行内存管理(可能会溢写到磁盘，如果有必要)，以允许应用程序保持非常大的状态。`State Backends`可以在不更改应用程序逻辑的情况下进行配置。\n\n### 下一步\n\n- 使用状态：显示如何在`Flink`应用程序中使用状态，并解释不同类型的状态。\n- 检查点：描述如何启用和配置容错检查点。\n- 可查询状态：解释如何在`Flink`运行时从外部访问状态。\n- 为`Managed State`自定义序列化：讨论为状态自定义序列化逻辑及其升级。\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html\n","source":"_posts/Flink/[Flink]Flink1.4 状态概述.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 状态概述\ndate: 2018-01-16 19:30:17\ntags:\n  - Flink\n  - Flink 容错\n\ncategories: Flink\npermalink: flink_stream_state_overview\n---\n\n有状态的函数和算子在处理单个元素/事件时存储数据，使得状态`state`成为任何精细操作的关键构件。\n\n例如：\n- 当应用程序搜索某些特定模式事件时，状态将存储迄今为止遇到的事件序列。\n- 当按每分钟/小时/天聚合事件时，状态保存待处理的聚合事件。\n- 在数据流上训练机器学习模型时，状态保存当前版本的模型参数。\n- 当需要管理历史数据时，状态允许访问过去发生的事件。\n\n`Flink` 需要了解状态，以便使用检查点进行状态容错，并允许流应用程序使用保存点。\n\n对状态进行了解有助于你对 `Flink` 应用程序进行扩展，这意味着 `Flink` 负责在并行实例之间进行重新分配状态。\n\n`Flink` 的可查询状态`queryable state`功能允许你在 `Flink` 运行时在外部访问状态。\n\n在使用状态时，阅读有关`Flink`的 `State Backends` 应该对你很有帮助。`Flink` 提供不同的 `State Backends`，并指定状态的存储方式和位置。状态可以位于`Java`的堆内或堆外。根据你的 `State Backends`，`Flink`也可以管理应用程序的状态，这意味着`Flink`进行内存管理(可能会溢写到磁盘，如果有必要)，以允许应用程序保持非常大的状态。`State Backends`可以在不更改应用程序逻辑的情况下进行配置。\n\n### 下一步\n\n- 使用状态：显示如何在`Flink`应用程序中使用状态，并解释不同类型的状态。\n- 检查点：描述如何启用和配置容错检查点。\n- 可查询状态：解释如何在`Flink`运行时从外部访问状态。\n- 为`Managed State`自定义序列化：讨论为状态自定义序列化逻辑及其升级。\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html\n","slug":"flink_stream_state_overview","published":1,"updated":"2018-02-01T06:21:18.771Z","comments":1,"photos":[],"link":"","_id":"cje58tire001wordbwfd9i0ao","content":"<p>有状态的函数和算子在处理单个元素/事件时存储数据，使得状态<code>state</code>成为任何精细操作的关键构件。</p>\n<p>例如：</p>\n<ul>\n<li>当应用程序搜索某些特定模式事件时，状态将存储迄今为止遇到的事件序列。</li>\n<li>当按每分钟/小时/天聚合事件时，状态保存待处理的聚合事件。</li>\n<li>在数据流上训练机器学习模型时，状态保存当前版本的模型参数。</li>\n<li>当需要管理历史数据时，状态允许访问过去发生的事件。</li>\n</ul>\n<p><code>Flink</code> 需要了解状态，以便使用检查点进行状态容错，并允许流应用程序使用保存点。</p>\n<p>对状态进行了解有助于你对 <code>Flink</code> 应用程序进行扩展，这意味着 <code>Flink</code> 负责在并行实例之间进行重新分配状态。</p>\n<p><code>Flink</code> 的可查询状态<code>queryable state</code>功能允许你在 <code>Flink</code> 运行时在外部访问状态。</p>\n<p>在使用状态时，阅读有关<code>Flink</code>的 <code>State Backends</code> 应该对你很有帮助。<code>Flink</code> 提供不同的 <code>State Backends</code>，并指定状态的存储方式和位置。状态可以位于<code>Java</code>的堆内或堆外。根据你的 <code>State Backends</code>，<code>Flink</code>也可以管理应用程序的状态，这意味着<code>Flink</code>进行内存管理(可能会溢写到磁盘，如果有必要)，以允许应用程序保持非常大的状态。<code>State Backends</code>可以在不更改应用程序逻辑的情况下进行配置。</p>\n<h3 id=\"下一步\"><a href=\"#下一步\" class=\"headerlink\" title=\"下一步\"></a>下一步</h3><ul>\n<li>使用状态：显示如何在<code>Flink</code>应用程序中使用状态，并解释不同类型的状态。</li>\n<li>检查点：描述如何启用和配置容错检查点。</li>\n<li>可查询状态：解释如何在<code>Flink</code>运行时从外部访问状态。</li>\n<li>为<code>Managed State</code>自定义序列化：讨论为状态自定义序列化逻辑及其升级。</li>\n</ul>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>有状态的函数和算子在处理单个元素/事件时存储数据，使得状态<code>state</code>成为任何精细操作的关键构件。</p>\n<p>例如：</p>\n<ul>\n<li>当应用程序搜索某些特定模式事件时，状态将存储迄今为止遇到的事件序列。</li>\n<li>当按每分钟/小时/天聚合事件时，状态保存待处理的聚合事件。</li>\n<li>在数据流上训练机器学习模型时，状态保存当前版本的模型参数。</li>\n<li>当需要管理历史数据时，状态允许访问过去发生的事件。</li>\n</ul>\n<p><code>Flink</code> 需要了解状态，以便使用检查点进行状态容错，并允许流应用程序使用保存点。</p>\n<p>对状态进行了解有助于你对 <code>Flink</code> 应用程序进行扩展，这意味着 <code>Flink</code> 负责在并行实例之间进行重新分配状态。</p>\n<p><code>Flink</code> 的可查询状态<code>queryable state</code>功能允许你在 <code>Flink</code> 运行时在外部访问状态。</p>\n<p>在使用状态时，阅读有关<code>Flink</code>的 <code>State Backends</code> 应该对你很有帮助。<code>Flink</code> 提供不同的 <code>State Backends</code>，并指定状态的存储方式和位置。状态可以位于<code>Java</code>的堆内或堆外。根据你的 <code>State Backends</code>，<code>Flink</code>也可以管理应用程序的状态，这意味着<code>Flink</code>进行内存管理(可能会溢写到磁盘，如果有必要)，以允许应用程序保持非常大的状态。<code>State Backends</code>可以在不更改应用程序逻辑的情况下进行配置。</p>\n<h3 id=\"下一步\"><a href=\"#下一步\" class=\"headerlink\" title=\"下一步\"></a>下一步</h3><ul>\n<li>使用状态：显示如何在<code>Flink</code>应用程序中使用状态，并解释不同类型的状态。</li>\n<li>检查点：描述如何启用和配置容错检查点。</li>\n<li>可查询状态：解释如何在<code>Flink</code>运行时从外部访问状态。</li>\n<li>为<code>Managed State</code>自定义序列化：讨论为状态自定义序列化逻辑及其升级。</li>\n</ul>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/index.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 生成时间戳与Watermarks","date":"2018-01-15T01:47:01.000Z","_content":"\n本节适用于在事件时间上运行的程序。有关事件时间，处理时间和提取时间的介绍，请参阅[Flink1.4 事件时间与处理时间](http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4/)。\n\n为了处理事件时间，流处理程序需要相应地设置`TimeCharacteristic`。\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n```\n\nScala版本:\n```\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n```\n\n### 1. 分配时间戳\n\n为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配事件时间戳。这通常通过访问/提取元素中某个字段的时间戳来完成。时间戳分配与生成`watermarks`相结合，告诉系统有关事件时间的进度`progress`。分配时间戳和生成`watermarks`有两种方法：\n- 直接在数据流源中分配与生成\n- 通过时间戳分配器/`watermark`生成器：在`Flink`时间戳分配器中也会定义要发送的`watermarks`\n\n备注:\n```\n时间戳和watermarks都是从Java历元1970-01-01T00：00：00Z以来的毫秒数。\n```\n\n#### 1.1 带有时间戳和watermarks的数据源函数\n\n流数据源还可以直接为它们产生的元素分配时间戳，并且也可以发送`watermarks`。如果数据源分配了时间戳，那么就不需要时间戳分配器。\n\n备注:\n```\n如果继续使用时间戳分配器，将会覆盖数据源提供的时间戳和watermarks。\n```\n\n如果直接向数据源中的元素分配时间戳，数据源必须使用`SourceContext上`的`collectWithTimestamp()`方法。如果要生成`watermarks`，数据源必须调用`emitWatermark（Watermark）`函数。\n\n以下是分配时间戳并生成`watermarks`的源(non-checkpointed)的简单示例：\n\nJava版本:\n```java\n@Override\npublic void run(SourceContext<MyType> ctx) throws Exception {\n\twhile (/* condition */) {\n\t\tMyType next = getNext();\n\t\tctx.collectWithTimestamp(next, next.getEventTimestamp());\n\n\t\tif (next.hasWatermarkTime()) {\n\t\t\tctx.emitWatermark(new Watermark(next.getWatermarkTime()));\n\t\t}\n\t}\n}\n```\n\nScala版本:\n```\noverride def run(ctx: SourceContext[MyType]): Unit = {\n\twhile (/* condition */) {\n\t\tval next: MyType = getNext()\n\t\tctx.collectWithTimestamp(next, next.eventTimestamp)\n\n\t\tif (next.hasWatermarkTime) {\n\t\t\tctx.emitWatermark(new Watermark(next.getWatermarkTime))\n\t\t}\n\t}\n}\n```\n\n#### 1.2 时间戳分配器/Watermark生成器\n\n时间戳分配器接收数据流并产生一个新的数据流，包含带有时间戳的元素和`Watermark`。如果原始流已经拥有时间戳或`watermarks`，那么如果使用时间戳分配器将会覆盖它们。\n\n时间戳分配器通常在数据源之后立马指定，但也不是严格遵循这样的原则。例如，一个常见的模式是在时间戳分配器之前需要进行解析(`MapFunction`)和过滤(`FilterFunction`)。无论如何，时间戳分配器都需要在第一个基于事件时间的操作(例如第一个窗口操作)之前被指定。但也有特殊情况，当使用`Kafka`作为流作业的数据源时，`Flink`允许在数据源(消费者)内部定义时间戳分配器/`watermarks`生成器。有关如何执行此操作的更多信息，请参见[Kafka Connector文档](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html)。\n\n备注:\n```\n本节的其余部分介绍了程序员为了创建自己的时间戳提取器/watermarks生成器而必须实现的主要接口。如果要查看Flink内置的执行器，请参阅[Pre-defined Timestamp Extractors / Watermark Emitters](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html)\n```\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\nDataStream<MyEvent> stream = env.readFile(\n        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100,\n        FilePathFilter.createDefaultFilter(), typeInfo);\n\nDataStream<MyEvent> withTimestampsAndWatermarks = stream\n        .filter( event -> event.severity() == WARNING )\n        .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks());\n\nwithTimestampsAndWatermarks\n        .keyBy( (event) -> event.getGroup() )\n        .timeWindow(Time.seconds(10))\n        .reduce( (a, b) -> a.add(b) )\n        .addSink(...);\n```\n\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval stream: DataStream[MyEvent] = env.readFile(\n         myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100,\n         FilePathFilter.createDefaultFilter());\n\nval withTimestampsAndWatermarks: DataStream[MyEvent] = stream\n        .filter( _.severity == WARNING )\n        .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())\n\nwithTimestampsAndWatermarks\n        .keyBy( _.getGroup )\n        .timeWindow(Time.seconds(10))\n        .reduce( (a, b) => a.add(b) )\n        .addSink(...)\n```\n\n##### 1.2.1 Periodic Watermarks 分配器\n\n`AssignerWithPeriodicWatermarks`分配时间戳并定期生成`Watermarks`(可能取决于流元素，或纯粹基于处理时间)。\n\n通过`ExecutionConfig.setAutoWatermarkInterval()`定义`Watermarks`的时间间隔(每n毫秒)。每次调用分配器的`getCurrentWatermark()`方法，如果返回的`Watermark`非null，并且大于先前的`Watermark`，则会发送(emitted)这个新的`Watermarks`。\n\n以下是带有周期性`Watermark`的时间戳分配器的两个简单示例:\n\nJava版本:\n```java\n/**\n * This generator generates watermarks assuming that elements arrive out of order,\n * but only to a certain degree. The latest elements for a certain timestamp t will arrive\n * at most n milliseconds after the earliest elements for timestamp t.\n */\npublic class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks<MyEvent> {\n\n    private final long maxOutOfOrderness = 3500; // 3.5 seconds\n\n    private long currentMaxTimestamp;\n\n    @Override\n    public long extractTimestamp(MyEvent element, long previousElementTimestamp) {\n        long timestamp = element.getCreationTime();\n        currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp);\n        return timestamp;\n    }\n\n    @Override\n    public Watermark getCurrentWatermark() {\n        // return the watermark as current highest timestamp minus the out-of-orderness bound\n        return new Watermark(currentMaxTimestamp - maxOutOfOrderness);\n    }\n}\n\n/**\n * This generator generates watermarks that are lagging behind processing time by a fixed amount.\n * It assumes that elements arrive in Flink after a bounded delay.\n */\npublic class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks<MyEvent> {\n\n\tprivate final long maxTimeLag = 5000; // 5 seconds\n\n\t@Override\n\tpublic long extractTimestamp(MyEvent element, long previousElementTimestamp) {\n\t\treturn element.getCreationTime();\n\t}\n\n\t@Override\n\tpublic Watermark getCurrentWatermark() {\n\t\t// return the watermark as current time minus the maximum time lag\n\t\treturn new Watermark(System.currentTimeMillis() - maxTimeLag);\n\t}\n}\n```\n\nScala版本:\n```\n/**\n * This generator generates watermarks assuming that elements arrive out of order,\n * but only to a certain degree. The latest elements for a certain timestamp t will arrive\n * at most n milliseconds after the earliest elements for timestamp t.\n */\nclass BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] {\n\n    val maxOutOfOrderness = 3500L; // 3.5 seconds\n\n    var currentMaxTimestamp: Long;\n\n    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = {\n        val timestamp = element.getCreationTime()\n        currentMaxTimestamp = max(timestamp, currentMaxTimestamp)\n        timestamp;\n    }\n\n    override def getCurrentWatermark(): Watermark = {\n        // return the watermark as current highest timestamp minus the out-of-orderness bound\n        new Watermark(currentMaxTimestamp - maxOutOfOrderness);\n    }\n}\n\n/**\n * This generator generates watermarks that are lagging behind processing time by a fixed amount.\n * It assumes that elements arrive in Flink after a bounded delay.\n */\nclass TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] {\n\n    val maxTimeLag = 5000L; // 5 seconds\n\n    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = {\n        element.getCreationTime\n    }\n\n    override def getCurrentWatermark(): Watermark = {\n        // return the watermark as current time minus the maximum time lag\n        new Watermark(System.currentTimeMillis() - maxTimeLag)\n    }\n}\n```\n\n##### 1.2.2 Punctuated Watermarks 分配器\n\n每当某个事件表明一个新的`Watermarks`可能要生成时，需要调用`AssignerWithPunctuatedWatermarks`方法来生成`Watermarks`(To generate watermarks whenever a certain event indicates that a new watermark might be generated, use AssignerWithPunctuatedWatermarks)。对于这个类，`Flink`首先调用`extractTimestamp()`方法为元素分配时间戳，然后立即调用该元素上的`checkAndGetNextWatermark()`方法。\n\n把在`extractTimestamp()`方法中分配的时间戳传递给`checkAndGetNextWatermark()`方法，并且可以决定是否要生成`Watermarks`。只要`checkAndGetNextWatermark()`方法返回非null的`Watermark`，并且该`Watermark`比以前最新的`Watermark`都大，则会发送这个新的`Watermark`。\n\nJava版本:\n```java\npublic class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks<MyEvent> {\n\n\t@Override\n\tpublic long extractTimestamp(MyEvent element, long previousElementTimestamp) {\n\t\treturn element.getCreationTime();\n\t}\n\n\t@Override\n\tpublic Watermark checkAndGetNextWatermark(MyEvent lastElement, long extractedTimestamp) {\n\t\treturn lastElement.hasWatermarkMarker() ? new Watermark(extractedTimestamp) : null;\n\t}\n}\n```\n\nScala版本:\n```\nclass PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] {\n\n\toverride def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = {\n\t\telement.getCreationTime\n\t}\n\n\toverride def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = {\n\t\tif (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null\n\t}\n}\n```\n备注:\n```\n可以在每个单独的事件上生成Watermark。但是，由于每个Watermark在下游引起一些计算，所以过多的Watermark会降低性能。\n```\n\n### 2. 每个Kafka分区一个时间戳\n\n当使用`Apache Kafka`作为数据源时，每个`Kafka`分区都可能有一个简单的事件时间模式(时间戳按升序递增或有界无序)。然而，当消费`Kafka`中的流时，多个分区通常并行消费，来自多个分区的事件会交叉在一起，破坏每个分区模式。\n\n在这种情况下，你可以使用`Flink`的`Kafka`分区感知`Watermark`的生成(Kafka-partition-aware watermark generation)。使用该特性，在`Kafka`消费者中，每个`Kafka`分区都生成`watermark`，并且每个分区的`watermark`的合并方式与在数据流`shuffle`上合并方式相同(the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.)。\n\n例如，如果在每个`Kafka`分区中的事件时间戳严格递增，则使用[递增时间戳`watermark`](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html#assigners-with-ascending-timestamps)生成器生成每个分区的`watermark`，在整体`watermark`上产生的结果也非常好。\n\n下图显示了如何使用每个`Kafka`分区生成`watermark`，以及在这种情况下`watermark`如何通过流数据流进行传播:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks-1.png?raw=true)\n\nJava版本:\n```java\nFlinkKafkaConsumer09<MyType> kafkaSource = new FlinkKafkaConsumer09<>(\"myTopic\", schema, props);\nkafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor<MyType>() {\n\n    @Override\n    public long extractAscendingTimestamp(MyType element) {\n        return element.eventTimestamp();\n    }\n});\n\nDataStream<MyType> stream = env.addSource(kafkaSource);\n```\n\nScala版本:\n```\nval kafkaSource = new FlinkKafkaConsumer09[MyType](\"myTopic\", schema, props)\nkafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] {\n    def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp\n})\n\nval stream: DataStream[MyType] = env.addSource(kafkaSource)\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html\n","source":"_posts/Flink/[Flink]Flink1.4 生成时间戳与Watermarks.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 生成时间戳与Watermarks\ndate: 2018-01-15 09:47:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n本节适用于在事件时间上运行的程序。有关事件时间，处理时间和提取时间的介绍，请参阅[Flink1.4 事件时间与处理时间](http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4/)。\n\n为了处理事件时间，流处理程序需要相应地设置`TimeCharacteristic`。\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n```\n\nScala版本:\n```\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n```\n\n### 1. 分配时间戳\n\n为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配事件时间戳。这通常通过访问/提取元素中某个字段的时间戳来完成。时间戳分配与生成`watermarks`相结合，告诉系统有关事件时间的进度`progress`。分配时间戳和生成`watermarks`有两种方法：\n- 直接在数据流源中分配与生成\n- 通过时间戳分配器/`watermark`生成器：在`Flink`时间戳分配器中也会定义要发送的`watermarks`\n\n备注:\n```\n时间戳和watermarks都是从Java历元1970-01-01T00：00：00Z以来的毫秒数。\n```\n\n#### 1.1 带有时间戳和watermarks的数据源函数\n\n流数据源还可以直接为它们产生的元素分配时间戳，并且也可以发送`watermarks`。如果数据源分配了时间戳，那么就不需要时间戳分配器。\n\n备注:\n```\n如果继续使用时间戳分配器，将会覆盖数据源提供的时间戳和watermarks。\n```\n\n如果直接向数据源中的元素分配时间戳，数据源必须使用`SourceContext上`的`collectWithTimestamp()`方法。如果要生成`watermarks`，数据源必须调用`emitWatermark（Watermark）`函数。\n\n以下是分配时间戳并生成`watermarks`的源(non-checkpointed)的简单示例：\n\nJava版本:\n```java\n@Override\npublic void run(SourceContext<MyType> ctx) throws Exception {\n\twhile (/* condition */) {\n\t\tMyType next = getNext();\n\t\tctx.collectWithTimestamp(next, next.getEventTimestamp());\n\n\t\tif (next.hasWatermarkTime()) {\n\t\t\tctx.emitWatermark(new Watermark(next.getWatermarkTime()));\n\t\t}\n\t}\n}\n```\n\nScala版本:\n```\noverride def run(ctx: SourceContext[MyType]): Unit = {\n\twhile (/* condition */) {\n\t\tval next: MyType = getNext()\n\t\tctx.collectWithTimestamp(next, next.eventTimestamp)\n\n\t\tif (next.hasWatermarkTime) {\n\t\t\tctx.emitWatermark(new Watermark(next.getWatermarkTime))\n\t\t}\n\t}\n}\n```\n\n#### 1.2 时间戳分配器/Watermark生成器\n\n时间戳分配器接收数据流并产生一个新的数据流，包含带有时间戳的元素和`Watermark`。如果原始流已经拥有时间戳或`watermarks`，那么如果使用时间戳分配器将会覆盖它们。\n\n时间戳分配器通常在数据源之后立马指定，但也不是严格遵循这样的原则。例如，一个常见的模式是在时间戳分配器之前需要进行解析(`MapFunction`)和过滤(`FilterFunction`)。无论如何，时间戳分配器都需要在第一个基于事件时间的操作(例如第一个窗口操作)之前被指定。但也有特殊情况，当使用`Kafka`作为流作业的数据源时，`Flink`允许在数据源(消费者)内部定义时间戳分配器/`watermarks`生成器。有关如何执行此操作的更多信息，请参见[Kafka Connector文档](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html)。\n\n备注:\n```\n本节的其余部分介绍了程序员为了创建自己的时间戳提取器/watermarks生成器而必须实现的主要接口。如果要查看Flink内置的执行器，请参阅[Pre-defined Timestamp Extractors / Watermark Emitters](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html)\n```\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\nDataStream<MyEvent> stream = env.readFile(\n        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100,\n        FilePathFilter.createDefaultFilter(), typeInfo);\n\nDataStream<MyEvent> withTimestampsAndWatermarks = stream\n        .filter( event -> event.severity() == WARNING )\n        .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks());\n\nwithTimestampsAndWatermarks\n        .keyBy( (event) -> event.getGroup() )\n        .timeWindow(Time.seconds(10))\n        .reduce( (a, b) -> a.add(b) )\n        .addSink(...);\n```\n\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n\nval stream: DataStream[MyEvent] = env.readFile(\n         myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100,\n         FilePathFilter.createDefaultFilter());\n\nval withTimestampsAndWatermarks: DataStream[MyEvent] = stream\n        .filter( _.severity == WARNING )\n        .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())\n\nwithTimestampsAndWatermarks\n        .keyBy( _.getGroup )\n        .timeWindow(Time.seconds(10))\n        .reduce( (a, b) => a.add(b) )\n        .addSink(...)\n```\n\n##### 1.2.1 Periodic Watermarks 分配器\n\n`AssignerWithPeriodicWatermarks`分配时间戳并定期生成`Watermarks`(可能取决于流元素，或纯粹基于处理时间)。\n\n通过`ExecutionConfig.setAutoWatermarkInterval()`定义`Watermarks`的时间间隔(每n毫秒)。每次调用分配器的`getCurrentWatermark()`方法，如果返回的`Watermark`非null，并且大于先前的`Watermark`，则会发送(emitted)这个新的`Watermarks`。\n\n以下是带有周期性`Watermark`的时间戳分配器的两个简单示例:\n\nJava版本:\n```java\n/**\n * This generator generates watermarks assuming that elements arrive out of order,\n * but only to a certain degree. The latest elements for a certain timestamp t will arrive\n * at most n milliseconds after the earliest elements for timestamp t.\n */\npublic class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks<MyEvent> {\n\n    private final long maxOutOfOrderness = 3500; // 3.5 seconds\n\n    private long currentMaxTimestamp;\n\n    @Override\n    public long extractTimestamp(MyEvent element, long previousElementTimestamp) {\n        long timestamp = element.getCreationTime();\n        currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp);\n        return timestamp;\n    }\n\n    @Override\n    public Watermark getCurrentWatermark() {\n        // return the watermark as current highest timestamp minus the out-of-orderness bound\n        return new Watermark(currentMaxTimestamp - maxOutOfOrderness);\n    }\n}\n\n/**\n * This generator generates watermarks that are lagging behind processing time by a fixed amount.\n * It assumes that elements arrive in Flink after a bounded delay.\n */\npublic class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks<MyEvent> {\n\n\tprivate final long maxTimeLag = 5000; // 5 seconds\n\n\t@Override\n\tpublic long extractTimestamp(MyEvent element, long previousElementTimestamp) {\n\t\treturn element.getCreationTime();\n\t}\n\n\t@Override\n\tpublic Watermark getCurrentWatermark() {\n\t\t// return the watermark as current time minus the maximum time lag\n\t\treturn new Watermark(System.currentTimeMillis() - maxTimeLag);\n\t}\n}\n```\n\nScala版本:\n```\n/**\n * This generator generates watermarks assuming that elements arrive out of order,\n * but only to a certain degree. The latest elements for a certain timestamp t will arrive\n * at most n milliseconds after the earliest elements for timestamp t.\n */\nclass BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] {\n\n    val maxOutOfOrderness = 3500L; // 3.5 seconds\n\n    var currentMaxTimestamp: Long;\n\n    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = {\n        val timestamp = element.getCreationTime()\n        currentMaxTimestamp = max(timestamp, currentMaxTimestamp)\n        timestamp;\n    }\n\n    override def getCurrentWatermark(): Watermark = {\n        // return the watermark as current highest timestamp minus the out-of-orderness bound\n        new Watermark(currentMaxTimestamp - maxOutOfOrderness);\n    }\n}\n\n/**\n * This generator generates watermarks that are lagging behind processing time by a fixed amount.\n * It assumes that elements arrive in Flink after a bounded delay.\n */\nclass TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] {\n\n    val maxTimeLag = 5000L; // 5 seconds\n\n    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = {\n        element.getCreationTime\n    }\n\n    override def getCurrentWatermark(): Watermark = {\n        // return the watermark as current time minus the maximum time lag\n        new Watermark(System.currentTimeMillis() - maxTimeLag)\n    }\n}\n```\n\n##### 1.2.2 Punctuated Watermarks 分配器\n\n每当某个事件表明一个新的`Watermarks`可能要生成时，需要调用`AssignerWithPunctuatedWatermarks`方法来生成`Watermarks`(To generate watermarks whenever a certain event indicates that a new watermark might be generated, use AssignerWithPunctuatedWatermarks)。对于这个类，`Flink`首先调用`extractTimestamp()`方法为元素分配时间戳，然后立即调用该元素上的`checkAndGetNextWatermark()`方法。\n\n把在`extractTimestamp()`方法中分配的时间戳传递给`checkAndGetNextWatermark()`方法，并且可以决定是否要生成`Watermarks`。只要`checkAndGetNextWatermark()`方法返回非null的`Watermark`，并且该`Watermark`比以前最新的`Watermark`都大，则会发送这个新的`Watermark`。\n\nJava版本:\n```java\npublic class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks<MyEvent> {\n\n\t@Override\n\tpublic long extractTimestamp(MyEvent element, long previousElementTimestamp) {\n\t\treturn element.getCreationTime();\n\t}\n\n\t@Override\n\tpublic Watermark checkAndGetNextWatermark(MyEvent lastElement, long extractedTimestamp) {\n\t\treturn lastElement.hasWatermarkMarker() ? new Watermark(extractedTimestamp) : null;\n\t}\n}\n```\n\nScala版本:\n```\nclass PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] {\n\n\toverride def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = {\n\t\telement.getCreationTime\n\t}\n\n\toverride def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = {\n\t\tif (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null\n\t}\n}\n```\n备注:\n```\n可以在每个单独的事件上生成Watermark。但是，由于每个Watermark在下游引起一些计算，所以过多的Watermark会降低性能。\n```\n\n### 2. 每个Kafka分区一个时间戳\n\n当使用`Apache Kafka`作为数据源时，每个`Kafka`分区都可能有一个简单的事件时间模式(时间戳按升序递增或有界无序)。然而，当消费`Kafka`中的流时，多个分区通常并行消费，来自多个分区的事件会交叉在一起，破坏每个分区模式。\n\n在这种情况下，你可以使用`Flink`的`Kafka`分区感知`Watermark`的生成(Kafka-partition-aware watermark generation)。使用该特性，在`Kafka`消费者中，每个`Kafka`分区都生成`watermark`，并且每个分区的`watermark`的合并方式与在数据流`shuffle`上合并方式相同(the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.)。\n\n例如，如果在每个`Kafka`分区中的事件时间戳严格递增，则使用[递增时间戳`watermark`](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html#assigners-with-ascending-timestamps)生成器生成每个分区的`watermark`，在整体`watermark`上产生的结果也非常好。\n\n下图显示了如何使用每个`Kafka`分区生成`watermark`，以及在这种情况下`watermark`如何通过流数据流进行传播:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks-1.png?raw=true)\n\nJava版本:\n```java\nFlinkKafkaConsumer09<MyType> kafkaSource = new FlinkKafkaConsumer09<>(\"myTopic\", schema, props);\nkafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor<MyType>() {\n\n    @Override\n    public long extractAscendingTimestamp(MyType element) {\n        return element.eventTimestamp();\n    }\n});\n\nDataStream<MyType> stream = env.addSource(kafkaSource);\n```\n\nScala版本:\n```\nval kafkaSource = new FlinkKafkaConsumer09[MyType](\"myTopic\", schema, props)\nkafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] {\n    def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp\n})\n\nval stream: DataStream[MyType] = env.addSource(kafkaSource)\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html\n","slug":"Flink/[Flink]Flink1.4 生成时间戳与Watermarks","published":1,"updated":"2018-01-29T09:36:59.648Z","comments":1,"photos":[],"link":"","_id":"cje58tiri001zordbpi9gw85k","content":"<p>本节适用于在事件时间上运行的程序。有关事件时间，处理时间和提取时间的介绍，请参阅<a href=\"http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4/\" target=\"_blank\" rel=\"noopener\">Flink1.4 事件时间与处理时间</a>。</p>\n<p>为了处理事件时间，流处理程序需要相应地设置<code>TimeCharacteristic</code>。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-分配时间戳\"><a href=\"#1-分配时间戳\" class=\"headerlink\" title=\"1. 分配时间戳\"></a>1. 分配时间戳</h3><p>为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配事件时间戳。这通常通过访问/提取元素中某个字段的时间戳来完成。时间戳分配与生成<code>watermarks</code>相结合，告诉系统有关事件时间的进度<code>progress</code>。分配时间戳和生成<code>watermarks</code>有两种方法：</p>\n<ul>\n<li>直接在数据流源中分配与生成</li>\n<li>通过时间戳分配器/<code>watermark</code>生成器：在<code>Flink</code>时间戳分配器中也会定义要发送的<code>watermarks</code></li>\n</ul>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">时间戳和watermarks都是从Java历元1970-01-01T00：00：00Z以来的毫秒数。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-1-带有时间戳和watermarks的数据源函数\"><a href=\"#1-1-带有时间戳和watermarks的数据源函数\" class=\"headerlink\" title=\"1.1 带有时间戳和watermarks的数据源函数\"></a>1.1 带有时间戳和watermarks的数据源函数</h4><p>流数据源还可以直接为它们产生的元素分配时间戳，并且也可以发送<code>watermarks</code>。如果数据源分配了时间戳，那么就不需要时间戳分配器。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果继续使用时间戳分配器，将会覆盖数据源提供的时间戳和watermarks。</span><br></pre></td></tr></table></figure></p>\n<p>如果直接向数据源中的元素分配时间戳，数据源必须使用<code>SourceContext上</code>的<code>collectWithTimestamp()</code>方法。如果要生成<code>watermarks</code>，数据源必须调用<code>emitWatermark（Watermark）</code>函数。</p>\n<p>以下是分配时间戳并生成<code>watermarks</code>的源(non-checkpointed)的简单示例：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">(SourceContext&lt;MyType&gt; ctx)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"comment\">/* condition */</span>) &#123;</span><br><span class=\"line\">\t\tMyType next = getNext();</span><br><span class=\"line\">\t\tctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class=\"line\">\t\t\tctx.emitWatermark(<span class=\"keyword\">new</span> Watermark(next.getWatermarkTime()));</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">override def run(ctx: SourceContext[MyType]): Unit = &#123;</span><br><span class=\"line\">\twhile (/* condition */) &#123;</span><br><span class=\"line\">\t\tval next: MyType = getNext()</span><br><span class=\"line\">\t\tctx.collectWithTimestamp(next, next.eventTimestamp)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif (next.hasWatermarkTime) &#123;</span><br><span class=\"line\">\t\t\tctx.emitWatermark(new Watermark(next.getWatermarkTime))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-时间戳分配器-Watermark生成器\"><a href=\"#1-2-时间戳分配器-Watermark生成器\" class=\"headerlink\" title=\"1.2 时间戳分配器/Watermark生成器\"></a>1.2 时间戳分配器/Watermark生成器</h4><p>时间戳分配器接收数据流并产生一个新的数据流，包含带有时间戳的元素和<code>Watermark</code>。如果原始流已经拥有时间戳或<code>watermarks</code>，那么如果使用时间戳分配器将会覆盖它们。</p>\n<p>时间戳分配器通常在数据源之后立马指定，但也不是严格遵循这样的原则。例如，一个常见的模式是在时间戳分配器之前需要进行解析(<code>MapFunction</code>)和过滤(<code>FilterFunction</code>)。无论如何，时间戳分配器都需要在第一个基于事件时间的操作(例如第一个窗口操作)之前被指定。但也有特殊情况，当使用<code>Kafka</code>作为流作业的数据源时，<code>Flink</code>允许在数据源(消费者)内部定义时间戳分配器/<code>watermarks</code>生成器。有关如何执行此操作的更多信息，请参见<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html\" target=\"_blank\" rel=\"noopener\">Kafka Connector文档</a>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">本节的其余部分介绍了程序员为了创建自己的时间戳提取器/watermarks生成器而必须实现的主要接口。如果要查看Flink内置的执行器，请参阅[Pre-defined Timestamp Extractors / Watermark Emitters](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html)</span><br></pre></td></tr></table></figure></p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class=\"line\">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class=\"number\">100</span>,</span><br><span class=\"line\">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream</span><br><span class=\"line\">        .filter( event -&gt; event.severity() == WARNING )</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> MyTimestampsAndWatermarks());</span><br><span class=\"line\"></span><br><span class=\"line\">withTimestampsAndWatermarks</span><br><span class=\"line\">        .keyBy( (event) -&gt; event.getGroup() )</span><br><span class=\"line\">        .timeWindow(Time.seconds(<span class=\"number\">10</span>))</span><br><span class=\"line\">        .reduce( (a, b) -&gt; a.add(b) )</span><br><span class=\"line\">        .addSink(...);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">val stream: DataStream[MyEvent] = env.readFile(</span><br><span class=\"line\">         myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100,</span><br><span class=\"line\">         FilePathFilter.createDefaultFilter());</span><br><span class=\"line\"></span><br><span class=\"line\">val withTimestampsAndWatermarks: DataStream[MyEvent] = stream</span><br><span class=\"line\">        .filter( _.severity == WARNING )</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())</span><br><span class=\"line\"></span><br><span class=\"line\">withTimestampsAndWatermarks</span><br><span class=\"line\">        .keyBy( _.getGroup )</span><br><span class=\"line\">        .timeWindow(Time.seconds(10))</span><br><span class=\"line\">        .reduce( (a, b) =&gt; a.add(b) )</span><br><span class=\"line\">        .addSink(...)</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-2-1-Periodic-Watermarks-分配器\"><a href=\"#1-2-1-Periodic-Watermarks-分配器\" class=\"headerlink\" title=\"1.2.1 Periodic Watermarks 分配器\"></a>1.2.1 Periodic Watermarks 分配器</h5><p><code>AssignerWithPeriodicWatermarks</code>分配时间戳并定期生成<code>Watermarks</code>(可能取决于流元素，或纯粹基于处理时间)。</p>\n<p>通过<code>ExecutionConfig.setAutoWatermarkInterval()</code>定义<code>Watermarks</code>的时间间隔(每n毫秒)。每次调用分配器的<code>getCurrentWatermark()</code>方法，如果返回的<code>Watermark</code>非null，并且大于先前的<code>Watermark</code>，则会发送(emitted)这个新的<code>Watermarks</code>。</p>\n<p>以下是带有周期性<code>Watermark</code>的时间戳分配器的两个简单示例:</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * This generator generates watermarks assuming that elements arrive out of order,</span></span><br><span class=\"line\"><span class=\"comment\"> * but only to a certain degree. The latest elements for a certain timestamp t will arrive</span></span><br><span class=\"line\"><span class=\"comment\"> * at most n milliseconds after the earliest elements for timestamp t.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BoundedOutOfOrdernessGenerator</span> <span class=\"keyword\">extends</span> <span class=\"title\">AssignerWithPeriodicWatermarks</span>&lt;<span class=\"title\">MyEvent</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> maxOutOfOrderness = <span class=\"number\">3500</span>; <span class=\"comment\">// 3.5 seconds</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> currentMaxTimestamp;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element, <span class=\"keyword\">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">long</span> timestamp = element.getCreationTime();</span><br><span class=\"line\">        currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> timestamp;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Watermark <span class=\"title\">getCurrentWatermark</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// return the watermark as current highest timestamp minus the out-of-orderness bound</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Watermark(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * This generator generates watermarks that are lagging behind processing time by a fixed amount.</span></span><br><span class=\"line\"><span class=\"comment\"> * It assumes that elements arrive in Flink after a bounded delay.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TimeLagWatermarkGenerator</span> <span class=\"keyword\">extends</span> <span class=\"title\">AssignerWithPeriodicWatermarks</span>&lt;<span class=\"title\">MyEvent</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> maxTimeLag = <span class=\"number\">5000</span>; <span class=\"comment\">// 5 seconds</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element, <span class=\"keyword\">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> Watermark <span class=\"title\">getCurrentWatermark</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// return the watermark as current time minus the maximum time lag</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Watermark(System.currentTimeMillis() - maxTimeLag);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * This generator generates watermarks assuming that elements arrive out of order,</span><br><span class=\"line\"> * but only to a certain degree. The latest elements for a certain timestamp t will arrive</span><br><span class=\"line\"> * at most n milliseconds after the earliest elements for timestamp t.</span><br><span class=\"line\"> */</span><br><span class=\"line\">class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val maxOutOfOrderness = 3500L; // 3.5 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">    var currentMaxTimestamp: Long;</span><br><span class=\"line\"></span><br><span class=\"line\">    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class=\"line\">        val timestamp = element.getCreationTime()</span><br><span class=\"line\">        currentMaxTimestamp = max(timestamp, currentMaxTimestamp)</span><br><span class=\"line\">        timestamp;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">        // return the watermark as current highest timestamp minus the out-of-orderness bound</span><br><span class=\"line\">        new Watermark(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * This generator generates watermarks that are lagging behind processing time by a fixed amount.</span><br><span class=\"line\"> * It assumes that elements arrive in Flink after a bounded delay.</span><br><span class=\"line\"> */</span><br><span class=\"line\">class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val maxTimeLag = 5000L; // 5 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class=\"line\">        element.getCreationTime</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">        // return the watermark as current time minus the maximum time lag</span><br><span class=\"line\">        new Watermark(System.currentTimeMillis() - maxTimeLag)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-2-2-Punctuated-Watermarks-分配器\"><a href=\"#1-2-2-Punctuated-Watermarks-分配器\" class=\"headerlink\" title=\"1.2.2 Punctuated Watermarks 分配器\"></a>1.2.2 Punctuated Watermarks 分配器</h5><p>每当某个事件表明一个新的<code>Watermarks</code>可能要生成时，需要调用<code>AssignerWithPunctuatedWatermarks</code>方法来生成<code>Watermarks</code>(To generate watermarks whenever a certain event indicates that a new watermark might be generated, use AssignerWithPunctuatedWatermarks)。对于这个类，<code>Flink</code>首先调用<code>extractTimestamp()</code>方法为元素分配时间戳，然后立即调用该元素上的<code>checkAndGetNextWatermark()</code>方法。</p>\n<p>把在<code>extractTimestamp()</code>方法中分配的时间戳传递给<code>checkAndGetNextWatermark()</code>方法，并且可以决定是否要生成<code>Watermarks</code>。只要<code>checkAndGetNextWatermark()</code>方法返回非null的<code>Watermark</code>，并且该<code>Watermark</code>比以前最新的<code>Watermark</code>都大，则会发送这个新的<code>Watermark</code>。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PunctuatedAssigner</span> <span class=\"keyword\">extends</span> <span class=\"title\">AssignerWithPunctuatedWatermarks</span>&lt;<span class=\"title\">MyEvent</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element, <span class=\"keyword\">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> Watermark <span class=\"title\">checkAndGetNextWatermark</span><span class=\"params\">(MyEvent lastElement, <span class=\"keyword\">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lastElement.hasWatermarkMarker() ? <span class=\"keyword\">new</span> Watermark(extractedTimestamp) : <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\toverride def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class=\"line\">\t\telement.getCreationTime</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\toverride def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123;</span><br><span class=\"line\">\t\tif (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">可以在每个单独的事件上生成Watermark。但是，由于每个Watermark在下游引起一些计算，所以过多的Watermark会降低性能。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-每个Kafka分区一个时间戳\"><a href=\"#2-每个Kafka分区一个时间戳\" class=\"headerlink\" title=\"2. 每个Kafka分区一个时间戳\"></a>2. 每个Kafka分区一个时间戳</h3><p>当使用<code>Apache Kafka</code>作为数据源时，每个<code>Kafka</code>分区都可能有一个简单的事件时间模式(时间戳按升序递增或有界无序)。然而，当消费<code>Kafka</code>中的流时，多个分区通常并行消费，来自多个分区的事件会交叉在一起，破坏每个分区模式。</p>\n<p>在这种情况下，你可以使用<code>Flink</code>的<code>Kafka</code>分区感知<code>Watermark</code>的生成(Kafka-partition-aware watermark generation)。使用该特性，在<code>Kafka</code>消费者中，每个<code>Kafka</code>分区都生成<code>watermark</code>，并且每个分区的<code>watermark</code>的合并方式与在数据流<code>shuffle</code>上合并方式相同(the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.)。</p>\n<p>例如，如果在每个<code>Kafka</code>分区中的事件时间戳严格递增，则使用<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html#assigners-with-ascending-timestamps\" target=\"_blank\" rel=\"noopener\">递增时间戳<code>watermark</code></a>生成器生成每个分区的<code>watermark</code>，在整体<code>watermark</code>上产生的结果也非常好。</p>\n<p>下图显示了如何使用每个<code>Kafka</code>分区生成<code>watermark</code>，以及在这种情况下<code>watermark</code>如何通过流数据流进行传播:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks-1.png?raw=true\" alt=\"\"></p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">FlinkKafkaConsumer09&lt;MyType&gt; kafkaSource = <span class=\"keyword\">new</span> FlinkKafkaConsumer09&lt;&gt;(<span class=\"string\">\"myTopic\"</span>, schema, props);</span><br><span class=\"line\">kafkaSource.assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> AscendingTimestampExtractor&lt;MyType&gt;() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractAscendingTimestamp</span><span class=\"params\">(MyType element)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> element.eventTimestamp();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyType&gt; stream = env.addSource(kafkaSource);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val kafkaSource = new FlinkKafkaConsumer09[MyType](&quot;myTopic&quot;, schema, props)</span><br><span class=\"line\">kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123;</span><br><span class=\"line\">    def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp</span><br><span class=\"line\">&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">val stream: DataStream[MyType] = env.addSource(kafkaSource)</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>本节适用于在事件时间上运行的程序。有关事件时间，处理时间和提取时间的介绍，请参阅<a href=\"http://smartsi.club/2018/01/04/Flink/[Flink]Flink1.4%20%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%E4%B8%8E%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4/\" target=\"_blank\" rel=\"noopener\">Flink1.4 事件时间与处理时间</a>。</p>\n<p>为了处理事件时间，流处理程序需要相应地设置<code>TimeCharacteristic</code>。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-分配时间戳\"><a href=\"#1-分配时间戳\" class=\"headerlink\" title=\"1. 分配时间戳\"></a>1. 分配时间戳</h3><p>为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配事件时间戳。这通常通过访问/提取元素中某个字段的时间戳来完成。时间戳分配与生成<code>watermarks</code>相结合，告诉系统有关事件时间的进度<code>progress</code>。分配时间戳和生成<code>watermarks</code>有两种方法：</p>\n<ul>\n<li>直接在数据流源中分配与生成</li>\n<li>通过时间戳分配器/<code>watermark</code>生成器：在<code>Flink</code>时间戳分配器中也会定义要发送的<code>watermarks</code></li>\n</ul>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">时间戳和watermarks都是从Java历元1970-01-01T00：00：00Z以来的毫秒数。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-1-带有时间戳和watermarks的数据源函数\"><a href=\"#1-1-带有时间戳和watermarks的数据源函数\" class=\"headerlink\" title=\"1.1 带有时间戳和watermarks的数据源函数\"></a>1.1 带有时间戳和watermarks的数据源函数</h4><p>流数据源还可以直接为它们产生的元素分配时间戳，并且也可以发送<code>watermarks</code>。如果数据源分配了时间戳，那么就不需要时间戳分配器。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果继续使用时间戳分配器，将会覆盖数据源提供的时间戳和watermarks。</span><br></pre></td></tr></table></figure></p>\n<p>如果直接向数据源中的元素分配时间戳，数据源必须使用<code>SourceContext上</code>的<code>collectWithTimestamp()</code>方法。如果要生成<code>watermarks</code>，数据源必须调用<code>emitWatermark（Watermark）</code>函数。</p>\n<p>以下是分配时间戳并生成<code>watermarks</code>的源(non-checkpointed)的简单示例：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">(SourceContext&lt;MyType&gt; ctx)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"comment\">/* condition */</span>) &#123;</span><br><span class=\"line\">\t\tMyType next = getNext();</span><br><span class=\"line\">\t\tctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class=\"line\">\t\t\tctx.emitWatermark(<span class=\"keyword\">new</span> Watermark(next.getWatermarkTime()));</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">override def run(ctx: SourceContext[MyType]): Unit = &#123;</span><br><span class=\"line\">\twhile (/* condition */) &#123;</span><br><span class=\"line\">\t\tval next: MyType = getNext()</span><br><span class=\"line\">\t\tctx.collectWithTimestamp(next, next.eventTimestamp)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif (next.hasWatermarkTime) &#123;</span><br><span class=\"line\">\t\t\tctx.emitWatermark(new Watermark(next.getWatermarkTime))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-时间戳分配器-Watermark生成器\"><a href=\"#1-2-时间戳分配器-Watermark生成器\" class=\"headerlink\" title=\"1.2 时间戳分配器/Watermark生成器\"></a>1.2 时间戳分配器/Watermark生成器</h4><p>时间戳分配器接收数据流并产生一个新的数据流，包含带有时间戳的元素和<code>Watermark</code>。如果原始流已经拥有时间戳或<code>watermarks</code>，那么如果使用时间戳分配器将会覆盖它们。</p>\n<p>时间戳分配器通常在数据源之后立马指定，但也不是严格遵循这样的原则。例如，一个常见的模式是在时间戳分配器之前需要进行解析(<code>MapFunction</code>)和过滤(<code>FilterFunction</code>)。无论如何，时间戳分配器都需要在第一个基于事件时间的操作(例如第一个窗口操作)之前被指定。但也有特殊情况，当使用<code>Kafka</code>作为流作业的数据源时，<code>Flink</code>允许在数据源(消费者)内部定义时间戳分配器/<code>watermarks</code>生成器。有关如何执行此操作的更多信息，请参见<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/kafka.html\" target=\"_blank\" rel=\"noopener\">Kafka Connector文档</a>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">本节的其余部分介绍了程序员为了创建自己的时间戳提取器/watermarks生成器而必须实现的主要接口。如果要查看Flink内置的执行器，请参阅[Pre-defined Timestamp Extractors / Watermark Emitters](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html)</span><br></pre></td></tr></table></figure></p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class=\"line\">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class=\"number\">100</span>,</span><br><span class=\"line\">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream</span><br><span class=\"line\">        .filter( event -&gt; event.severity() == WARNING )</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> MyTimestampsAndWatermarks());</span><br><span class=\"line\"></span><br><span class=\"line\">withTimestampsAndWatermarks</span><br><span class=\"line\">        .keyBy( (event) -&gt; event.getGroup() )</span><br><span class=\"line\">        .timeWindow(Time.seconds(<span class=\"number\">10</span>))</span><br><span class=\"line\">        .reduce( (a, b) -&gt; a.add(b) )</span><br><span class=\"line\">        .addSink(...);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class=\"line\"></span><br><span class=\"line\">val stream: DataStream[MyEvent] = env.readFile(</span><br><span class=\"line\">         myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100,</span><br><span class=\"line\">         FilePathFilter.createDefaultFilter());</span><br><span class=\"line\"></span><br><span class=\"line\">val withTimestampsAndWatermarks: DataStream[MyEvent] = stream</span><br><span class=\"line\">        .filter( _.severity == WARNING )</span><br><span class=\"line\">        .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())</span><br><span class=\"line\"></span><br><span class=\"line\">withTimestampsAndWatermarks</span><br><span class=\"line\">        .keyBy( _.getGroup )</span><br><span class=\"line\">        .timeWindow(Time.seconds(10))</span><br><span class=\"line\">        .reduce( (a, b) =&gt; a.add(b) )</span><br><span class=\"line\">        .addSink(...)</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-2-1-Periodic-Watermarks-分配器\"><a href=\"#1-2-1-Periodic-Watermarks-分配器\" class=\"headerlink\" title=\"1.2.1 Periodic Watermarks 分配器\"></a>1.2.1 Periodic Watermarks 分配器</h5><p><code>AssignerWithPeriodicWatermarks</code>分配时间戳并定期生成<code>Watermarks</code>(可能取决于流元素，或纯粹基于处理时间)。</p>\n<p>通过<code>ExecutionConfig.setAutoWatermarkInterval()</code>定义<code>Watermarks</code>的时间间隔(每n毫秒)。每次调用分配器的<code>getCurrentWatermark()</code>方法，如果返回的<code>Watermark</code>非null，并且大于先前的<code>Watermark</code>，则会发送(emitted)这个新的<code>Watermarks</code>。</p>\n<p>以下是带有周期性<code>Watermark</code>的时间戳分配器的两个简单示例:</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * This generator generates watermarks assuming that elements arrive out of order,</span></span><br><span class=\"line\"><span class=\"comment\"> * but only to a certain degree. The latest elements for a certain timestamp t will arrive</span></span><br><span class=\"line\"><span class=\"comment\"> * at most n milliseconds after the earliest elements for timestamp t.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BoundedOutOfOrdernessGenerator</span> <span class=\"keyword\">extends</span> <span class=\"title\">AssignerWithPeriodicWatermarks</span>&lt;<span class=\"title\">MyEvent</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> maxOutOfOrderness = <span class=\"number\">3500</span>; <span class=\"comment\">// 3.5 seconds</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">long</span> currentMaxTimestamp;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element, <span class=\"keyword\">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">long</span> timestamp = element.getCreationTime();</span><br><span class=\"line\">        currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> timestamp;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Watermark <span class=\"title\">getCurrentWatermark</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// return the watermark as current highest timestamp minus the out-of-orderness bound</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Watermark(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * This generator generates watermarks that are lagging behind processing time by a fixed amount.</span></span><br><span class=\"line\"><span class=\"comment\"> * It assumes that elements arrive in Flink after a bounded delay.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TimeLagWatermarkGenerator</span> <span class=\"keyword\">extends</span> <span class=\"title\">AssignerWithPeriodicWatermarks</span>&lt;<span class=\"title\">MyEvent</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> maxTimeLag = <span class=\"number\">5000</span>; <span class=\"comment\">// 5 seconds</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element, <span class=\"keyword\">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> Watermark <span class=\"title\">getCurrentWatermark</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// return the watermark as current time minus the maximum time lag</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Watermark(System.currentTimeMillis() - maxTimeLag);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * This generator generates watermarks assuming that elements arrive out of order,</span><br><span class=\"line\"> * but only to a certain degree. The latest elements for a certain timestamp t will arrive</span><br><span class=\"line\"> * at most n milliseconds after the earliest elements for timestamp t.</span><br><span class=\"line\"> */</span><br><span class=\"line\">class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val maxOutOfOrderness = 3500L; // 3.5 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">    var currentMaxTimestamp: Long;</span><br><span class=\"line\"></span><br><span class=\"line\">    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class=\"line\">        val timestamp = element.getCreationTime()</span><br><span class=\"line\">        currentMaxTimestamp = max(timestamp, currentMaxTimestamp)</span><br><span class=\"line\">        timestamp;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">        // return the watermark as current highest timestamp minus the out-of-orderness bound</span><br><span class=\"line\">        new Watermark(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * This generator generates watermarks that are lagging behind processing time by a fixed amount.</span><br><span class=\"line\"> * It assumes that elements arrive in Flink after a bounded delay.</span><br><span class=\"line\"> */</span><br><span class=\"line\">class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val maxTimeLag = 5000L; // 5 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class=\"line\">        element.getCreationTime</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    override def getCurrentWatermark(): Watermark = &#123;</span><br><span class=\"line\">        // return the watermark as current time minus the maximum time lag</span><br><span class=\"line\">        new Watermark(System.currentTimeMillis() - maxTimeLag)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-2-2-Punctuated-Watermarks-分配器\"><a href=\"#1-2-2-Punctuated-Watermarks-分配器\" class=\"headerlink\" title=\"1.2.2 Punctuated Watermarks 分配器\"></a>1.2.2 Punctuated Watermarks 分配器</h5><p>每当某个事件表明一个新的<code>Watermarks</code>可能要生成时，需要调用<code>AssignerWithPunctuatedWatermarks</code>方法来生成<code>Watermarks</code>(To generate watermarks whenever a certain event indicates that a new watermark might be generated, use AssignerWithPunctuatedWatermarks)。对于这个类，<code>Flink</code>首先调用<code>extractTimestamp()</code>方法为元素分配时间戳，然后立即调用该元素上的<code>checkAndGetNextWatermark()</code>方法。</p>\n<p>把在<code>extractTimestamp()</code>方法中分配的时间戳传递给<code>checkAndGetNextWatermark()</code>方法，并且可以决定是否要生成<code>Watermarks</code>。只要<code>checkAndGetNextWatermark()</code>方法返回非null的<code>Watermark</code>，并且该<code>Watermark</code>比以前最新的<code>Watermark</code>都大，则会发送这个新的<code>Watermark</code>。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PunctuatedAssigner</span> <span class=\"keyword\">extends</span> <span class=\"title\">AssignerWithPunctuatedWatermarks</span>&lt;<span class=\"title\">MyEvent</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractTimestamp</span><span class=\"params\">(MyEvent element, <span class=\"keyword\">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> element.getCreationTime();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> Watermark <span class=\"title\">checkAndGetNextWatermark</span><span class=\"params\">(MyEvent lastElement, <span class=\"keyword\">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> lastElement.hasWatermarkMarker() ? <span class=\"keyword\">new</span> Watermark(extractedTimestamp) : <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\toverride def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = &#123;</span><br><span class=\"line\">\t\telement.getCreationTime</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\toverride def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = &#123;</span><br><span class=\"line\">\t\tif (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">可以在每个单独的事件上生成Watermark。但是，由于每个Watermark在下游引起一些计算，所以过多的Watermark会降低性能。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-每个Kafka分区一个时间戳\"><a href=\"#2-每个Kafka分区一个时间戳\" class=\"headerlink\" title=\"2. 每个Kafka分区一个时间戳\"></a>2. 每个Kafka分区一个时间戳</h3><p>当使用<code>Apache Kafka</code>作为数据源时，每个<code>Kafka</code>分区都可能有一个简单的事件时间模式(时间戳按升序递增或有界无序)。然而，当消费<code>Kafka</code>中的流时，多个分区通常并行消费，来自多个分区的事件会交叉在一起，破坏每个分区模式。</p>\n<p>在这种情况下，你可以使用<code>Flink</code>的<code>Kafka</code>分区感知<code>Watermark</code>的生成(Kafka-partition-aware watermark generation)。使用该特性，在<code>Kafka</code>消费者中，每个<code>Kafka</code>分区都生成<code>watermark</code>，并且每个分区的<code>watermark</code>的合并方式与在数据流<code>shuffle</code>上合并方式相同(the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.)。</p>\n<p>例如，如果在每个<code>Kafka</code>分区中的事件时间戳严格递增，则使用<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamp_extractors.html#assigners-with-ascending-timestamps\" target=\"_blank\" rel=\"noopener\">递增时间戳<code>watermark</code></a>生成器生成每个分区的<code>watermark</code>，在整体<code>watermark</code>上产生的结果也非常好。</p>\n<p>下图显示了如何使用每个<code>Kafka</code>分区生成<code>watermark</code>，以及在这种情况下<code>watermark</code>如何通过流数据流进行传播:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Flink/%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8EWatermarks-1.png?raw=true\" alt=\"\"></p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">FlinkKafkaConsumer09&lt;MyType&gt; kafkaSource = <span class=\"keyword\">new</span> FlinkKafkaConsumer09&lt;&gt;(<span class=\"string\">\"myTopic\"</span>, schema, props);</span><br><span class=\"line\">kafkaSource.assignTimestampsAndWatermarks(<span class=\"keyword\">new</span> AscendingTimestampExtractor&lt;MyType&gt;() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extractAscendingTimestamp</span><span class=\"params\">(MyType element)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> element.eventTimestamp();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;MyType&gt; stream = env.addSource(kafkaSource);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val kafkaSource = new FlinkKafkaConsumer09[MyType](&quot;myTopic&quot;, schema, props)</span><br><span class=\"line\">kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] &#123;</span><br><span class=\"line\">    def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp</span><br><span class=\"line\">&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">val stream: DataStream[MyType] = env.addSource(kafkaSource)</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_timestamps_watermarks.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 累加器与计数器","date":"2018-01-04T03:46:01.000Z","_content":"\n### 1. 概述\n\n累加器(`Accumulators`)是一个简单的构造器，具有加法操作和获取最终累加结果操作，在作业结束后可以使用。\n\n最直接的累加器是一个计数器(`counter`)：你可以使用`Accumulator.add()`方法对其进行累加。在作业结束时，`Flink`将合并所有部分结果并将最终结果发送给客户端。在调试过程中，或者你快速想要了解有关数据的更多信息，累加器很有用。\n\n目前`Flink`拥有以下内置累加器。它们中的每一个都实现了累加器接口：\n\n(1) `IntCounter`, `LongCounter` 以及 `DoubleCounter`: 参阅下面示例中使用的计数器。\n\n(2) `Histogram`：为离散数据的直方图(A histogram implementation for a discrete number of bins.)。内部它只是一个整数到整数的映射。你可以用它来计算值的分布，例如 单词计数程序的每行单词分配。\n\n### 2. 如何使用\n\n首先，你必须在你要使用的用户自定义转换函数中创建一个累加器(`accumulator`)对象(这里是一个计数器):\n```java\nprivate IntCounter numLines = new IntCounter();\n```\n\n其次，你必须注册累加器(`accumulator`)对象，通常在`rich`函数的`open()`方法中注册。在这里你也可以自定义累加器的名字:\n```java\ngetRuntimeContext().addAccumulator(\"num-lines\", this.numLines);\n```\n现在你就可以在算子函数中的任何位置使用累加器，包括在`open()`和`close()`方法中:\n```java\nthis.numLines.add(1);\n```\n最后结果将存储在`JobExecutionResult`对象中，该对象从执行环境的`execute()`方法返回(当前仅当执行等待作业完成时才起作用):\n```java\nJobExecutionResult result = env.execute();\nlong lineCounter = result.getAccumulatorResult(\"num-lines\");\nSystem.out.println(lineCounter);\n```\n每个作业的所有累加器共享一个命名空间。因此，你可以在作业的不同算子函数中使用同一个累加器。`Flink`在内部合并所有具有相同名称的累加器。\n\n备注:\n```\n目前累加器的结果只有在整个工作结束之后才可以使用。我们还计划在下一次迭代中可以使用前一次迭代的结果。你可以使用聚合器来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。\n```\n\n### 3. Example\n\n```java\nimport com.google.gson.Gson;\nimport com.google.gson.GsonBuilder;\nimport com.qunar.innovation.data.bean.AdsPushBehavior;\nimport com.qunar.innovation.data.utils.ConstantUtil;\nimport org.apache.flink.api.common.accumulators.LongCounter;\nimport org.apache.flink.api.common.functions.RichMapFunction;\nimport org.apache.flink.configuration.Configuration;\n\npublic class AdsPushParseMap extends RichMapFunction<String, AdsPushBehavior> {\n\n    private static Gson gson = new GsonBuilder().setDateFormat(\"yyyy-MM-dd HH:mm:ss\").create();\n    private final LongCounter behaviorCounter = new LongCounter();\n\n    @Override\n    public void open(Configuration parameters) throws Exception {\n        super.open(parameters);\n        getRuntimeContext().addAccumulator(ConstantUtil.ADS_PUSH_APP_CODE, behaviorCounter);\n    }\n\n    @Override\n    public AdsPushBehavior map(String content) throws Exception {\n\n        try{\n            // 解析\n            AdsPushBehavior adsPushBehavior = gson.fromJson(content, AdsPushBehavior.class);\n            this.behaviorCounter.add(1);\n            return adsPushBehavior;\n        }\n        catch (Exception e){\n            e.printStackTrace();\n        }\n        return null;\n\n    }\n}\n```\n\n```java\nimport com.qunar.innovation.data.TestFlink;\nimport com.qunar.innovation.data.functions.*;\nimport com.qunar.innovation.data.utils.ConstantUtil;\nimport org.apache.flink.api.common.JobExecutionResult;\nimport org.apache.flink.api.java.DataSet;\nimport org.apache.flink.api.java.ExecutionEnvironment;\nimport org.apache.flink.core.fs.FileSystem;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class AdsPushLocalStream {\n\n    private final static Logger LOGGER = LoggerFactory.getLogger(TestFlink.class);\n\n    public static void main(String[] args) {\n\n        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n        DataSet<String> dataSet = env.readTextFile(\"file:///home/xiaosi/input.txt\");\n\n        // 处理数据\n        DataSet<String> adsPushDataSet = dataSet.map(new ContentMap()).name(\"contentMap\").setParallelism(1).\n                map(new AdsPushParseMap()).name(\"behaviorMap\").setParallelism(1)\n                .map(new AdsPushFeatureMap()).name(\"featureMap\").setParallelism(1)\n                .filter(new AdsPushFeatureFilter()).name(\"featureFilter\").setParallelism(1);\n\n        adsPushDataSet.writeAsText(\"file:///home/xiaosi/output\", FileSystem.WriteMode.OVERWRITE);\n\n        try {\n            JobExecutionResult result = env.execute();\n            long behaviorCounter = result.getAccumulatorResult(ConstantUtil.ADS_PUSH_APP_CODE);\n            System.out.println(behaviorCounter);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage(), e);\n        }\n\n    }\n}\n```\n\n### 3. 自定义累加器\n\n为了实现你自己的累加器，你只需要编写你的`Accumulator`接口的实现。如果你认为你的自定义累加器应与`Flink`一起传输，请随意创建一个拉取请求(Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.)。\n\n你可以选择实现[Accumulator](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java)或[SimpleAccumulator](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java)。\n\n`Accumulator<V，R>`非常灵活：它为要添加的值定义一个类型`V`，并为最终结果定义一个结果类型`R`。例如，对于直方图，`V`是数字，`R`是直方图。`SimpleAccumulator`适用于两种类型相同的情况，例如，计数器。\n\n备注:\n```\nFlink版本:1.4\n```\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters\n","source":"_posts/Flink/[Flink]Flink1.4 累加器与计数器.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 累加器与计数器\ndate: 2018-01-04 11:46:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n### 1. 概述\n\n累加器(`Accumulators`)是一个简单的构造器，具有加法操作和获取最终累加结果操作，在作业结束后可以使用。\n\n最直接的累加器是一个计数器(`counter`)：你可以使用`Accumulator.add()`方法对其进行累加。在作业结束时，`Flink`将合并所有部分结果并将最终结果发送给客户端。在调试过程中，或者你快速想要了解有关数据的更多信息，累加器很有用。\n\n目前`Flink`拥有以下内置累加器。它们中的每一个都实现了累加器接口：\n\n(1) `IntCounter`, `LongCounter` 以及 `DoubleCounter`: 参阅下面示例中使用的计数器。\n\n(2) `Histogram`：为离散数据的直方图(A histogram implementation for a discrete number of bins.)。内部它只是一个整数到整数的映射。你可以用它来计算值的分布，例如 单词计数程序的每行单词分配。\n\n### 2. 如何使用\n\n首先，你必须在你要使用的用户自定义转换函数中创建一个累加器(`accumulator`)对象(这里是一个计数器):\n```java\nprivate IntCounter numLines = new IntCounter();\n```\n\n其次，你必须注册累加器(`accumulator`)对象，通常在`rich`函数的`open()`方法中注册。在这里你也可以自定义累加器的名字:\n```java\ngetRuntimeContext().addAccumulator(\"num-lines\", this.numLines);\n```\n现在你就可以在算子函数中的任何位置使用累加器，包括在`open()`和`close()`方法中:\n```java\nthis.numLines.add(1);\n```\n最后结果将存储在`JobExecutionResult`对象中，该对象从执行环境的`execute()`方法返回(当前仅当执行等待作业完成时才起作用):\n```java\nJobExecutionResult result = env.execute();\nlong lineCounter = result.getAccumulatorResult(\"num-lines\");\nSystem.out.println(lineCounter);\n```\n每个作业的所有累加器共享一个命名空间。因此，你可以在作业的不同算子函数中使用同一个累加器。`Flink`在内部合并所有具有相同名称的累加器。\n\n备注:\n```\n目前累加器的结果只有在整个工作结束之后才可以使用。我们还计划在下一次迭代中可以使用前一次迭代的结果。你可以使用聚合器来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。\n```\n\n### 3. Example\n\n```java\nimport com.google.gson.Gson;\nimport com.google.gson.GsonBuilder;\nimport com.qunar.innovation.data.bean.AdsPushBehavior;\nimport com.qunar.innovation.data.utils.ConstantUtil;\nimport org.apache.flink.api.common.accumulators.LongCounter;\nimport org.apache.flink.api.common.functions.RichMapFunction;\nimport org.apache.flink.configuration.Configuration;\n\npublic class AdsPushParseMap extends RichMapFunction<String, AdsPushBehavior> {\n\n    private static Gson gson = new GsonBuilder().setDateFormat(\"yyyy-MM-dd HH:mm:ss\").create();\n    private final LongCounter behaviorCounter = new LongCounter();\n\n    @Override\n    public void open(Configuration parameters) throws Exception {\n        super.open(parameters);\n        getRuntimeContext().addAccumulator(ConstantUtil.ADS_PUSH_APP_CODE, behaviorCounter);\n    }\n\n    @Override\n    public AdsPushBehavior map(String content) throws Exception {\n\n        try{\n            // 解析\n            AdsPushBehavior adsPushBehavior = gson.fromJson(content, AdsPushBehavior.class);\n            this.behaviorCounter.add(1);\n            return adsPushBehavior;\n        }\n        catch (Exception e){\n            e.printStackTrace();\n        }\n        return null;\n\n    }\n}\n```\n\n```java\nimport com.qunar.innovation.data.TestFlink;\nimport com.qunar.innovation.data.functions.*;\nimport com.qunar.innovation.data.utils.ConstantUtil;\nimport org.apache.flink.api.common.JobExecutionResult;\nimport org.apache.flink.api.java.DataSet;\nimport org.apache.flink.api.java.ExecutionEnvironment;\nimport org.apache.flink.core.fs.FileSystem;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class AdsPushLocalStream {\n\n    private final static Logger LOGGER = LoggerFactory.getLogger(TestFlink.class);\n\n    public static void main(String[] args) {\n\n        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n        DataSet<String> dataSet = env.readTextFile(\"file:///home/xiaosi/input.txt\");\n\n        // 处理数据\n        DataSet<String> adsPushDataSet = dataSet.map(new ContentMap()).name(\"contentMap\").setParallelism(1).\n                map(new AdsPushParseMap()).name(\"behaviorMap\").setParallelism(1)\n                .map(new AdsPushFeatureMap()).name(\"featureMap\").setParallelism(1)\n                .filter(new AdsPushFeatureFilter()).name(\"featureFilter\").setParallelism(1);\n\n        adsPushDataSet.writeAsText(\"file:///home/xiaosi/output\", FileSystem.WriteMode.OVERWRITE);\n\n        try {\n            JobExecutionResult result = env.execute();\n            long behaviorCounter = result.getAccumulatorResult(ConstantUtil.ADS_PUSH_APP_CODE);\n            System.out.println(behaviorCounter);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage(), e);\n        }\n\n    }\n}\n```\n\n### 3. 自定义累加器\n\n为了实现你自己的累加器，你只需要编写你的`Accumulator`接口的实现。如果你认为你的自定义累加器应与`Flink`一起传输，请随意创建一个拉取请求(Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.)。\n\n你可以选择实现[Accumulator](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java)或[SimpleAccumulator](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java)。\n\n`Accumulator<V，R>`非常灵活：它为要添加的值定义一个类型`V`，并为最终结果定义一个结果类型`R`。例如，对于直方图，`V`是数字，`R`是直方图。`SimpleAccumulator`适用于两种类型相同的情况，例如，计数器。\n\n备注:\n```\nFlink版本:1.4\n```\n\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters\n","slug":"Flink/[Flink]Flink1.4 累加器与计数器","published":1,"updated":"2018-01-29T09:36:59.647Z","comments":1,"photos":[],"link":"","_id":"cje58tirn0022ordb9miemp4y","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>累加器(<code>Accumulators</code>)是一个简单的构造器，具有加法操作和获取最终累加结果操作，在作业结束后可以使用。</p>\n<p>最直接的累加器是一个计数器(<code>counter</code>)：你可以使用<code>Accumulator.add()</code>方法对其进行累加。在作业结束时，<code>Flink</code>将合并所有部分结果并将最终结果发送给客户端。在调试过程中，或者你快速想要了解有关数据的更多信息，累加器很有用。</p>\n<p>目前<code>Flink</code>拥有以下内置累加器。它们中的每一个都实现了累加器接口：</p>\n<p>(1) <code>IntCounter</code>, <code>LongCounter</code> 以及 <code>DoubleCounter</code>: 参阅下面示例中使用的计数器。</p>\n<p>(2) <code>Histogram</code>：为离散数据的直方图(A histogram implementation for a discrete number of bins.)。内部它只是一个整数到整数的映射。你可以用它来计算值的分布，例如 单词计数程序的每行单词分配。</p>\n<h3 id=\"2-如何使用\"><a href=\"#2-如何使用\" class=\"headerlink\" title=\"2. 如何使用\"></a>2. 如何使用</h3><p>首先，你必须在你要使用的用户自定义转换函数中创建一个累加器(<code>accumulator</code>)对象(这里是一个计数器):<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> IntCounter numLines = <span class=\"keyword\">new</span> IntCounter();</span><br></pre></td></tr></table></figure></p>\n<p>其次，你必须注册累加器(<code>accumulator</code>)对象，通常在<code>rich</code>函数的<code>open()</code>方法中注册。在这里你也可以自定义累加器的名字:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">getRuntimeContext().addAccumulator(<span class=\"string\">\"num-lines\"</span>, <span class=\"keyword\">this</span>.numLines);</span><br></pre></td></tr></table></figure></p>\n<p>现在你就可以在算子函数中的任何位置使用累加器，包括在<code>open()</code>和<code>close()</code>方法中:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">this</span>.numLines.add(<span class=\"number\">1</span>);</span><br></pre></td></tr></table></figure></p>\n<p>最后结果将存储在<code>JobExecutionResult</code>对象中，该对象从执行环境的<code>execute()</code>方法返回(当前仅当执行等待作业完成时才起作用):<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">JobExecutionResult result = env.execute();</span><br><span class=\"line\"><span class=\"keyword\">long</span> lineCounter = result.getAccumulatorResult(<span class=\"string\">\"num-lines\"</span>);</span><br><span class=\"line\">System.out.println(lineCounter);</span><br></pre></td></tr></table></figure></p>\n<p>每个作业的所有累加器共享一个命名空间。因此，你可以在作业的不同算子函数中使用同一个累加器。<code>Flink</code>在内部合并所有具有相同名称的累加器。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">目前累加器的结果只有在整个工作结束之后才可以使用。我们还计划在下一次迭代中可以使用前一次迭代的结果。你可以使用聚合器来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-Example\"><a href=\"#3-Example\" class=\"headerlink\" title=\"3. Example\"></a>3. Example</h3><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> com.google.gson.Gson;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.google.gson.GsonBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.bean.AdsPushBehavior;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.utils.ConstantUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.accumulators.LongCounter;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.configuration.Configuration;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AdsPushParseMap</span> <span class=\"keyword\">extends</span> <span class=\"title\">RichMapFunction</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">AdsPushBehavior</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Gson gson = <span class=\"keyword\">new</span> GsonBuilder().setDateFormat(<span class=\"string\">\"yyyy-MM-dd HH:mm:ss\"</span>).create();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> LongCounter behaviorCounter = <span class=\"keyword\">new</span> LongCounter();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">open</span><span class=\"params\">(Configuration parameters)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.open(parameters);</span><br><span class=\"line\">        getRuntimeContext().addAccumulator(ConstantUtil.ADS_PUSH_APP_CODE, behaviorCounter);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> AdsPushBehavior <span class=\"title\">map</span><span class=\"params\">(String content)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// 解析</span></span><br><span class=\"line\">            AdsPushBehavior adsPushBehavior = gson.fromJson(content, AdsPushBehavior.class);</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.behaviorCounter.add(<span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> adsPushBehavior;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)&#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.TestFlink;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.functions.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.utils.ConstantUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.JobExecutionResult;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.DataSet;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.core.fs.FileSystem;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.Logger;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.LoggerFactory;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AdsPushLocalStream</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> Logger LOGGER = LoggerFactory.getLogger(TestFlink.class);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        DataSet&lt;String&gt; dataSet = env.readTextFile(<span class=\"string\">\"file:///home/xiaosi/input.txt\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 处理数据</span></span><br><span class=\"line\">        DataSet&lt;String&gt; adsPushDataSet = dataSet.map(<span class=\"keyword\">new</span> ContentMap()).name(<span class=\"string\">\"contentMap\"</span>).setParallelism(<span class=\"number\">1</span>).</span><br><span class=\"line\">                map(<span class=\"keyword\">new</span> AdsPushParseMap()).name(<span class=\"string\">\"behaviorMap\"</span>).setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\">                .map(<span class=\"keyword\">new</span> AdsPushFeatureMap()).name(<span class=\"string\">\"featureMap\"</span>).setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\">                .filter(<span class=\"keyword\">new</span> AdsPushFeatureFilter()).name(<span class=\"string\">\"featureFilter\"</span>).setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        adsPushDataSet.writeAsText(<span class=\"string\">\"file:///home/xiaosi/output\"</span>, FileSystem.WriteMode.OVERWRITE);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JobExecutionResult result = env.execute();</span><br><span class=\"line\">            <span class=\"keyword\">long</span> behaviorCounter = result.getAccumulatorResult(ConstantUtil.ADS_PUSH_APP_CODE);</span><br><span class=\"line\">            System.out.println(behaviorCounter);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">            LOGGER.error(e.getMessage(), e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-自定义累加器\"><a href=\"#3-自定义累加器\" class=\"headerlink\" title=\"3. 自定义累加器\"></a>3. 自定义累加器</h3><p>为了实现你自己的累加器，你只需要编写你的<code>Accumulator</code>接口的实现。如果你认为你的自定义累加器应与<code>Flink</code>一起传输，请随意创建一个拉取请求(Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.)。</p>\n<p>你可以选择实现<a href=\"https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java\" target=\"_blank\" rel=\"noopener\">Accumulator</a>或<a href=\"https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java\" target=\"_blank\" rel=\"noopener\">SimpleAccumulator</a>。</p>\n<p><code>Accumulator&lt;V，R&gt;</code>非常灵活：它为要添加的值定义一个类型<code>V</code>，并为最终结果定义一个结果类型<code>R</code>。例如，对于直方图，<code>V</code>是数字，<code>R</code>是直方图。<code>SimpleAccumulator</code>适用于两种类型相同的情况，例如，计数器。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>累加器(<code>Accumulators</code>)是一个简单的构造器，具有加法操作和获取最终累加结果操作，在作业结束后可以使用。</p>\n<p>最直接的累加器是一个计数器(<code>counter</code>)：你可以使用<code>Accumulator.add()</code>方法对其进行累加。在作业结束时，<code>Flink</code>将合并所有部分结果并将最终结果发送给客户端。在调试过程中，或者你快速想要了解有关数据的更多信息，累加器很有用。</p>\n<p>目前<code>Flink</code>拥有以下内置累加器。它们中的每一个都实现了累加器接口：</p>\n<p>(1) <code>IntCounter</code>, <code>LongCounter</code> 以及 <code>DoubleCounter</code>: 参阅下面示例中使用的计数器。</p>\n<p>(2) <code>Histogram</code>：为离散数据的直方图(A histogram implementation for a discrete number of bins.)。内部它只是一个整数到整数的映射。你可以用它来计算值的分布，例如 单词计数程序的每行单词分配。</p>\n<h3 id=\"2-如何使用\"><a href=\"#2-如何使用\" class=\"headerlink\" title=\"2. 如何使用\"></a>2. 如何使用</h3><p>首先，你必须在你要使用的用户自定义转换函数中创建一个累加器(<code>accumulator</code>)对象(这里是一个计数器):<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> IntCounter numLines = <span class=\"keyword\">new</span> IntCounter();</span><br></pre></td></tr></table></figure></p>\n<p>其次，你必须注册累加器(<code>accumulator</code>)对象，通常在<code>rich</code>函数的<code>open()</code>方法中注册。在这里你也可以自定义累加器的名字:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">getRuntimeContext().addAccumulator(<span class=\"string\">\"num-lines\"</span>, <span class=\"keyword\">this</span>.numLines);</span><br></pre></td></tr></table></figure></p>\n<p>现在你就可以在算子函数中的任何位置使用累加器，包括在<code>open()</code>和<code>close()</code>方法中:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">this</span>.numLines.add(<span class=\"number\">1</span>);</span><br></pre></td></tr></table></figure></p>\n<p>最后结果将存储在<code>JobExecutionResult</code>对象中，该对象从执行环境的<code>execute()</code>方法返回(当前仅当执行等待作业完成时才起作用):<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">JobExecutionResult result = env.execute();</span><br><span class=\"line\"><span class=\"keyword\">long</span> lineCounter = result.getAccumulatorResult(<span class=\"string\">\"num-lines\"</span>);</span><br><span class=\"line\">System.out.println(lineCounter);</span><br></pre></td></tr></table></figure></p>\n<p>每个作业的所有累加器共享一个命名空间。因此，你可以在作业的不同算子函数中使用同一个累加器。<code>Flink</code>在内部合并所有具有相同名称的累加器。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">目前累加器的结果只有在整个工作结束之后才可以使用。我们还计划在下一次迭代中可以使用前一次迭代的结果。你可以使用聚合器来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-Example\"><a href=\"#3-Example\" class=\"headerlink\" title=\"3. Example\"></a>3. Example</h3><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> com.google.gson.Gson;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.google.gson.GsonBuilder;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.bean.AdsPushBehavior;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.utils.ConstantUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.accumulators.LongCounter;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.configuration.Configuration;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AdsPushParseMap</span> <span class=\"keyword\">extends</span> <span class=\"title\">RichMapFunction</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">AdsPushBehavior</span>&gt; </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> Gson gson = <span class=\"keyword\">new</span> GsonBuilder().setDateFormat(<span class=\"string\">\"yyyy-MM-dd HH:mm:ss\"</span>).create();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> LongCounter behaviorCounter = <span class=\"keyword\">new</span> LongCounter();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">open</span><span class=\"params\">(Configuration parameters)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.open(parameters);</span><br><span class=\"line\">        getRuntimeContext().addAccumulator(ConstantUtil.ADS_PUSH_APP_CODE, behaviorCounter);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> AdsPushBehavior <span class=\"title\">map</span><span class=\"params\">(String content)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">// 解析</span></span><br><span class=\"line\">            AdsPushBehavior adsPushBehavior = gson.fromJson(content, AdsPushBehavior.class);</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.behaviorCounter.add(<span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> adsPushBehavior;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)&#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.TestFlink;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.functions.*;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.qunar.innovation.data.utils.ConstantUtil;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.common.JobExecutionResult;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.DataSet;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.flink.core.fs.FileSystem;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.Logger;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.slf4j.LoggerFactory;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AdsPushLocalStream</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> Logger LOGGER = LoggerFactory.getLogger(TestFlink.class);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\"></span><br><span class=\"line\">        DataSet&lt;String&gt; dataSet = env.readTextFile(<span class=\"string\">\"file:///home/xiaosi/input.txt\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 处理数据</span></span><br><span class=\"line\">        DataSet&lt;String&gt; adsPushDataSet = dataSet.map(<span class=\"keyword\">new</span> ContentMap()).name(<span class=\"string\">\"contentMap\"</span>).setParallelism(<span class=\"number\">1</span>).</span><br><span class=\"line\">                map(<span class=\"keyword\">new</span> AdsPushParseMap()).name(<span class=\"string\">\"behaviorMap\"</span>).setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\">                .map(<span class=\"keyword\">new</span> AdsPushFeatureMap()).name(<span class=\"string\">\"featureMap\"</span>).setParallelism(<span class=\"number\">1</span>)</span><br><span class=\"line\">                .filter(<span class=\"keyword\">new</span> AdsPushFeatureFilter()).name(<span class=\"string\">\"featureFilter\"</span>).setParallelism(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        adsPushDataSet.writeAsText(<span class=\"string\">\"file:///home/xiaosi/output\"</span>, FileSystem.WriteMode.OVERWRITE);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JobExecutionResult result = env.execute();</span><br><span class=\"line\">            <span class=\"keyword\">long</span> behaviorCounter = result.getAccumulatorResult(ConstantUtil.ADS_PUSH_APP_CODE);</span><br><span class=\"line\">            System.out.println(behaviorCounter);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">            LOGGER.error(e.getMessage(), e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-自定义累加器\"><a href=\"#3-自定义累加器\" class=\"headerlink\" title=\"3. 自定义累加器\"></a>3. 自定义累加器</h3><p>为了实现你自己的累加器，你只需要编写你的<code>Accumulator</code>接口的实现。如果你认为你的自定义累加器应与<code>Flink</code>一起传输，请随意创建一个拉取请求(Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.)。</p>\n<p>你可以选择实现<a href=\"https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/Accumulator.java\" target=\"_blank\" rel=\"noopener\">Accumulator</a>或<a href=\"https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/accumulators/SimpleAccumulator.java\" target=\"_blank\" rel=\"noopener\">SimpleAccumulator</a>。</p>\n<p><code>Accumulator&lt;V，R&gt;</code>非常灵活：它为要添加的值定义一个类型<code>V</code>，并为最终结果定义一个结果类型<code>R</code>。例如，对于直方图，<code>V</code>是数字，<code>R</code>是直方图。<code>SimpleAccumulator</code>适用于两种类型相同的情况，例如，计数器。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#accumulators--counters</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop Hadoop中的推测执行","date":"2017-12-07T11:49:01.000Z","_content":"\n### 1. 概述\n\nHadoop不会去诊断或修复执行慢的任务，相反，它试图检测任务的运行速度是否比预期慢，并启动另一个等效任务作为备份(备份任务称为`推测任务`)。这个过程在Hadoop中被称为`推测执行`。\n\n在这篇文章中，我们将讨论`推测执行` - `Hadoop`中提高效率的一个重要功能，我们有必要去了解`Hadoop`中的推测执行是否总是有帮助的，或者我们需要关闭它时如何禁用。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%AD%E7%9A%84%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C-Speculative-Execution-in-Spark.gif?raw=true)\n\n### 2. 什么是推测执行\n\n在`Hadoop`中，`MapReduce`将作业分解为任务，并且这些任务并行而不是顺序地运行，从而缩短了总体执行时间。这种执行模式对缓慢的任务很敏感(即使他们的数量很少)，因为它们减慢了整个工作的执行速度。\n\n任务执行缓慢的原因可能有各种，包括硬件退化或软件错误配置等，尽管花费的时间超过了预期的时间，但是由于任务仍然有可能成功完成，因此很难检测缓慢原因。`Hadoop`不会尝试诊断和修复运行缓慢的任务，而是尝试检测并为其运行一个备份任务。这在`Hadoop`中被称为推测执行。这些备份任务在`Hadoop`中被称为推测任务。\n\n### 3. 推测执行如何工作\n\n现在让我们看看`Hadoop`的推测执行过程。\n\n首先，在`Hadoop MapReduce`中启动所有任务。为那些已经运行了一段时间(至少一分钟)且比作业中其他任务平均进度慢的任务启动推测任务。如果原始任务在推测性任务之前完成，那么推测任务将被终止，相反，如果推测性任务在原始任务之前完成，那么原始任务被终止。一个任务成功完成之后，任何正在运行的重复任务都将被终止。\n\n### 4. 推测执行的优势\n\n`Hadoop MapReduce`推测执行在某些情况下是很有帮助的，因为在具有100个节点的`Hadoop`集群中，硬件故障或网络拥塞等问题很常见，并行或重复运行任务会更好一些，因为我们不必等到有问题的任务执行之后。\n\n但是如果两个重复的任务同时启动，就会造成集群资源的浪费。\n\n### 5. 配置推测执行\n\n推测执行是`Hadoop MapReduce`作业中的一种优化技术，默认情况下启用的。你可以在`mapred-site.xml`中禁用`mappers`和`reducer`的推测执行，如下所示：\n```xml\n<property>\n  <name>mapred.map.tasks.speculative.execution</name>\n  <value>false</value>\n</property>\n<property>\n  <name>mapred.reduce.tasks.speculative.execution</name>\n  <value>false</value>\n</property>\n```\n\n### 6. 有没有必要关闭推测执行\n\n推测执行的主要目的是减少工作执行时间，但是，由于重复的任务，集群效率受到影响。由于在推测执行中正在执行冗余任务，因此这可能降低整体吞吐量。出于这个原因，一些集群管理员喜欢关闭`Hadoop`中的推测执行。\n\n对于Reduce任务，关闭推测执行是有益的，因为任意重复的reduce任务都必须将取得map任务输出作为最先的任务，这可能会大幅度的增加集群上的网络传输。\n\n关闭推测执行的另一种情况是考虑到非幂等任务。然而在很多情况下，将任务写成幂等的并使用`OutputCommitter`来提升任务成功时输出到最后位置的速度，这是可行的。\n\n\n\n\n原文:https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/\n","source":"_posts/Hadoop/Hadoop Hadoop中的推测执行.md","raw":"\n---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop Hadoop中的推测执行\ndate: 2017-12-07 19:49:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. 概述\n\nHadoop不会去诊断或修复执行慢的任务，相反，它试图检测任务的运行速度是否比预期慢，并启动另一个等效任务作为备份(备份任务称为`推测任务`)。这个过程在Hadoop中被称为`推测执行`。\n\n在这篇文章中，我们将讨论`推测执行` - `Hadoop`中提高效率的一个重要功能，我们有必要去了解`Hadoop`中的推测执行是否总是有帮助的，或者我们需要关闭它时如何禁用。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%AD%E7%9A%84%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C-Speculative-Execution-in-Spark.gif?raw=true)\n\n### 2. 什么是推测执行\n\n在`Hadoop`中，`MapReduce`将作业分解为任务，并且这些任务并行而不是顺序地运行，从而缩短了总体执行时间。这种执行模式对缓慢的任务很敏感(即使他们的数量很少)，因为它们减慢了整个工作的执行速度。\n\n任务执行缓慢的原因可能有各种，包括硬件退化或软件错误配置等，尽管花费的时间超过了预期的时间，但是由于任务仍然有可能成功完成，因此很难检测缓慢原因。`Hadoop`不会尝试诊断和修复运行缓慢的任务，而是尝试检测并为其运行一个备份任务。这在`Hadoop`中被称为推测执行。这些备份任务在`Hadoop`中被称为推测任务。\n\n### 3. 推测执行如何工作\n\n现在让我们看看`Hadoop`的推测执行过程。\n\n首先，在`Hadoop MapReduce`中启动所有任务。为那些已经运行了一段时间(至少一分钟)且比作业中其他任务平均进度慢的任务启动推测任务。如果原始任务在推测性任务之前完成，那么推测任务将被终止，相反，如果推测性任务在原始任务之前完成，那么原始任务被终止。一个任务成功完成之后，任何正在运行的重复任务都将被终止。\n\n### 4. 推测执行的优势\n\n`Hadoop MapReduce`推测执行在某些情况下是很有帮助的，因为在具有100个节点的`Hadoop`集群中，硬件故障或网络拥塞等问题很常见，并行或重复运行任务会更好一些，因为我们不必等到有问题的任务执行之后。\n\n但是如果两个重复的任务同时启动，就会造成集群资源的浪费。\n\n### 5. 配置推测执行\n\n推测执行是`Hadoop MapReduce`作业中的一种优化技术，默认情况下启用的。你可以在`mapred-site.xml`中禁用`mappers`和`reducer`的推测执行，如下所示：\n```xml\n<property>\n  <name>mapred.map.tasks.speculative.execution</name>\n  <value>false</value>\n</property>\n<property>\n  <name>mapred.reduce.tasks.speculative.execution</name>\n  <value>false</value>\n</property>\n```\n\n### 6. 有没有必要关闭推测执行\n\n推测执行的主要目的是减少工作执行时间，但是，由于重复的任务，集群效率受到影响。由于在推测执行中正在执行冗余任务，因此这可能降低整体吞吐量。出于这个原因，一些集群管理员喜欢关闭`Hadoop`中的推测执行。\n\n对于Reduce任务，关闭推测执行是有益的，因为任意重复的reduce任务都必须将取得map任务输出作为最先的任务，这可能会大幅度的增加集群上的网络传输。\n\n关闭推测执行的另一种情况是考虑到非幂等任务。然而在很多情况下，将任务写成幂等的并使用`OutputCommitter`来提升任务成功时输出到最后位置的速度，这是可行的。\n\n\n\n\n原文:https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/\n","slug":"Hadoop/Hadoop Hadoop中的推测执行","published":1,"updated":"2018-01-29T09:36:59.638Z","comments":1,"photos":[],"link":"","_id":"cje58tirr0025ordbqx49f6i3","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>Hadoop不会去诊断或修复执行慢的任务，相反，它试图检测任务的运行速度是否比预期慢，并启动另一个等效任务作为备份(备份任务称为<code>推测任务</code>)。这个过程在Hadoop中被称为<code>推测执行</code>。</p>\n<p>在这篇文章中，我们将讨论<code>推测执行</code> - <code>Hadoop</code>中提高效率的一个重要功能，我们有必要去了解<code>Hadoop</code>中的推测执行是否总是有帮助的，或者我们需要关闭它时如何禁用。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%AD%E7%9A%84%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C-Speculative-Execution-in-Spark.gif?raw=true\" alt=\"\"></p>\n<h3 id=\"2-什么是推测执行\"><a href=\"#2-什么是推测执行\" class=\"headerlink\" title=\"2. 什么是推测执行\"></a>2. 什么是推测执行</h3><p>在<code>Hadoop</code>中，<code>MapReduce</code>将作业分解为任务，并且这些任务并行而不是顺序地运行，从而缩短了总体执行时间。这种执行模式对缓慢的任务很敏感(即使他们的数量很少)，因为它们减慢了整个工作的执行速度。</p>\n<p>任务执行缓慢的原因可能有各种，包括硬件退化或软件错误配置等，尽管花费的时间超过了预期的时间，但是由于任务仍然有可能成功完成，因此很难检测缓慢原因。<code>Hadoop</code>不会尝试诊断和修复运行缓慢的任务，而是尝试检测并为其运行一个备份任务。这在<code>Hadoop</code>中被称为推测执行。这些备份任务在<code>Hadoop</code>中被称为推测任务。</p>\n<h3 id=\"3-推测执行如何工作\"><a href=\"#3-推测执行如何工作\" class=\"headerlink\" title=\"3. 推测执行如何工作\"></a>3. 推测执行如何工作</h3><p>现在让我们看看<code>Hadoop</code>的推测执行过程。</p>\n<p>首先，在<code>Hadoop MapReduce</code>中启动所有任务。为那些已经运行了一段时间(至少一分钟)且比作业中其他任务平均进度慢的任务启动推测任务。如果原始任务在推测性任务之前完成，那么推测任务将被终止，相反，如果推测性任务在原始任务之前完成，那么原始任务被终止。一个任务成功完成之后，任何正在运行的重复任务都将被终止。</p>\n<h3 id=\"4-推测执行的优势\"><a href=\"#4-推测执行的优势\" class=\"headerlink\" title=\"4. 推测执行的优势\"></a>4. 推测执行的优势</h3><p><code>Hadoop MapReduce</code>推测执行在某些情况下是很有帮助的，因为在具有100个节点的<code>Hadoop</code>集群中，硬件故障或网络拥塞等问题很常见，并行或重复运行任务会更好一些，因为我们不必等到有问题的任务执行之后。</p>\n<p>但是如果两个重复的任务同时启动，就会造成集群资源的浪费。</p>\n<h3 id=\"5-配置推测执行\"><a href=\"#5-配置推测执行\" class=\"headerlink\" title=\"5. 配置推测执行\"></a>5. 配置推测执行</h3><p>推测执行是<code>Hadoop MapReduce</code>作业中的一种优化技术，默认情况下启用的。你可以在<code>mapred-site.xml</code>中禁用<code>mappers</code>和<code>reducer</code>的推测执行，如下所示：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.map.tasks.speculative.execution<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.reduce.tasks.speculative.execution<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-有没有必要关闭推测执行\"><a href=\"#6-有没有必要关闭推测执行\" class=\"headerlink\" title=\"6. 有没有必要关闭推测执行\"></a>6. 有没有必要关闭推测执行</h3><p>推测执行的主要目的是减少工作执行时间，但是，由于重复的任务，集群效率受到影响。由于在推测执行中正在执行冗余任务，因此这可能降低整体吞吐量。出于这个原因，一些集群管理员喜欢关闭<code>Hadoop</code>中的推测执行。</p>\n<p>对于Reduce任务，关闭推测执行是有益的，因为任意重复的reduce任务都必须将取得map任务输出作为最先的任务，这可能会大幅度的增加集群上的网络传输。</p>\n<p>关闭推测执行的另一种情况是考虑到非幂等任务。然而在很多情况下，将任务写成幂等的并使用<code>OutputCommitter</code>来提升任务成功时输出到最后位置的速度，这是可行的。</p>\n<p>原文:<a href=\"https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/\" target=\"_blank\" rel=\"noopener\">https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>Hadoop不会去诊断或修复执行慢的任务，相反，它试图检测任务的运行速度是否比预期慢，并启动另一个等效任务作为备份(备份任务称为<code>推测任务</code>)。这个过程在Hadoop中被称为<code>推测执行</code>。</p>\n<p>在这篇文章中，我们将讨论<code>推测执行</code> - <code>Hadoop</code>中提高效率的一个重要功能，我们有必要去了解<code>Hadoop</code>中的推测执行是否总是有帮助的，或者我们需要关闭它时如何禁用。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%AD%E7%9A%84%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C-Speculative-Execution-in-Spark.gif?raw=true\" alt=\"\"></p>\n<h3 id=\"2-什么是推测执行\"><a href=\"#2-什么是推测执行\" class=\"headerlink\" title=\"2. 什么是推测执行\"></a>2. 什么是推测执行</h3><p>在<code>Hadoop</code>中，<code>MapReduce</code>将作业分解为任务，并且这些任务并行而不是顺序地运行，从而缩短了总体执行时间。这种执行模式对缓慢的任务很敏感(即使他们的数量很少)，因为它们减慢了整个工作的执行速度。</p>\n<p>任务执行缓慢的原因可能有各种，包括硬件退化或软件错误配置等，尽管花费的时间超过了预期的时间，但是由于任务仍然有可能成功完成，因此很难检测缓慢原因。<code>Hadoop</code>不会尝试诊断和修复运行缓慢的任务，而是尝试检测并为其运行一个备份任务。这在<code>Hadoop</code>中被称为推测执行。这些备份任务在<code>Hadoop</code>中被称为推测任务。</p>\n<h3 id=\"3-推测执行如何工作\"><a href=\"#3-推测执行如何工作\" class=\"headerlink\" title=\"3. 推测执行如何工作\"></a>3. 推测执行如何工作</h3><p>现在让我们看看<code>Hadoop</code>的推测执行过程。</p>\n<p>首先，在<code>Hadoop MapReduce</code>中启动所有任务。为那些已经运行了一段时间(至少一分钟)且比作业中其他任务平均进度慢的任务启动推测任务。如果原始任务在推测性任务之前完成，那么推测任务将被终止，相反，如果推测性任务在原始任务之前完成，那么原始任务被终止。一个任务成功完成之后，任何正在运行的重复任务都将被终止。</p>\n<h3 id=\"4-推测执行的优势\"><a href=\"#4-推测执行的优势\" class=\"headerlink\" title=\"4. 推测执行的优势\"></a>4. 推测执行的优势</h3><p><code>Hadoop MapReduce</code>推测执行在某些情况下是很有帮助的，因为在具有100个节点的<code>Hadoop</code>集群中，硬件故障或网络拥塞等问题很常见，并行或重复运行任务会更好一些，因为我们不必等到有问题的任务执行之后。</p>\n<p>但是如果两个重复的任务同时启动，就会造成集群资源的浪费。</p>\n<h3 id=\"5-配置推测执行\"><a href=\"#5-配置推测执行\" class=\"headerlink\" title=\"5. 配置推测执行\"></a>5. 配置推测执行</h3><p>推测执行是<code>Hadoop MapReduce</code>作业中的一种优化技术，默认情况下启用的。你可以在<code>mapred-site.xml</code>中禁用<code>mappers</code>和<code>reducer</code>的推测执行，如下所示：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.map.tasks.speculative.execution<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.reduce.tasks.speculative.execution<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-有没有必要关闭推测执行\"><a href=\"#6-有没有必要关闭推测执行\" class=\"headerlink\" title=\"6. 有没有必要关闭推测执行\"></a>6. 有没有必要关闭推测执行</h3><p>推测执行的主要目的是减少工作执行时间，但是，由于重复的任务，集群效率受到影响。由于在推测执行中正在执行冗余任务，因此这可能降低整体吞吐量。出于这个原因，一些集群管理员喜欢关闭<code>Hadoop</code>中的推测执行。</p>\n<p>对于Reduce任务，关闭推测执行是有益的，因为任意重复的reduce任务都必须将取得map任务输出作为最先的任务，这可能会大幅度的增加集群上的网络传输。</p>\n<p>关闭推测执行的另一种情况是考虑到非幂等任务。然而在很多情况下，将任务写成幂等的并使用<code>OutputCommitter</code>来提升任务成功时输出到最后位置的速度，这是可行的。</p>\n<p>原文:<a href=\"https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/\" target=\"_blank\" rel=\"noopener\">https://data-flair.training/blogs/speculative-execution-in-hadoop-mapreduce/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 重启策略","date":"2018-01-04T08:36:01.000Z","_content":"\n`Flink`支持不同的重启策略，重启策略控制在作业失败后如何重启。可以使用默认的重启策略启动集群，这个默认策略在作业没有特别指定重启策略时使用。如果在提交作业时指定了重启策略，那么此策略将覆盖集群的默认配置策略。\n\n### 1. 概述\n\n默认的重启策略通过`Flink`的配置文件`flink-conf.yaml`进行设置。配置参数`restart-strategy`定义了采取哪种策略。如果未启用检查点，那么将使用`不重启`策略。如果启用检查点且重启策略尚未配置，则固定延迟重启策略与`Integer.MAX_VALUE`一起使用进行尝试重启。请参阅下面可用的重启策略列表以了解支持哪些值。\n\n每个重启策略都有自己的一套控制其行为的参数。这些值也在配置文件中配置。每个重启策略的描述都包含有关各个配置值的更多信息。\n\n重启策略|值\n---|---\n固定延迟重启策略|fixed-delay\n失败率重启策略|failure-rate\n不重启策略|none\n\n除了定义一个默认的重启策略之外，还可以为每个`Flink`作业定义一个指定的重启策略。此重启策略通过调用`ExecutionEnvironment`上的`setRestartStrategy`方法以编程的方式进行设置。请注意，这也适用于`StreamExecutionEnvironment`。\n\n以下示例显示了如何为作业设置固定延迟重启策略。如果发生故障，系统将尝试每10s重新启动一次作业，最多重启3次。\n\nJava版本:\n```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n));\n```\n\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n))\n```\n\n### 2. 重启策略\n\n下面介绍几种重启策略的配置选项。\n\n#### 2.1 固定延迟重启策略\n\n固定延迟重启策略尝试一定次数来重新启动作业。如果超过最大尝试次数，那么作业最终将失败。在两次连续的尝试重启之间，重启策略会等待一段固定的时间(译者注:连续重启时间间隔)。\n\n通过在`flink-conf.yaml`中设置以下配置参数，可以将此策略默认启用：\n```\nrestart-strategy: fixed-delay\n```\n\n配置参数|描述|默认值\n---|---|---\n`restart-strategy.fixed-delay.attempts`|在声明作业失败之前，`Flink`重试执行的次数|1或者如果启用检查点，则为`Integer.MAX_VALUE`\n`restart-strategy.fixed-delay.delay`|延迟重试意味着在执行失败后，重新执行不会立即开始，而只会在某个延迟之后开始。当程序与外部系统进行交互时，延迟重试会很有帮助|`akka.ask.timeout`，或10s(如果通过检查点激活)\n\nExample:\n```\nrestart-strategy.fixed-delay.attempts: 3\nrestart-strategy.fixed-delay.delay: 10 s\n```\n固定延迟重启策略也可以通过编程来设置：\n\nJava版本:\n```Java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n));\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n))\n```\n\n#### 2.2 失败率重启策略\n\n失败率重启策略在失败后重新启动作业，但当超过失败率(每个时间间隔的失败)时，作业最终会失败。在两次连续的重启尝试之间，重启策略会等待一段固定的时间。\n\n通过在`flink-conf.yaml`中设置以下配置参数，可以将此策略默认启用:\n\n配置参数|描述|默认值\n---|---|---\n`restart-strategy.failure-rate.max-failures-per-interval`|在一个作业声明失败之前，在给定时间间隔内最大的重启次数|1\n`restart-strategy.failure-rate.failure-rate-interval`|计算失败率的时间间隔|1分钟\n`restart-strategy.failure-rate.delay`|两次连续重启尝试之间的时间间隔|`akka.ask.timeout`\n\nExample:\n```\nrestart-strategy.failure-rate.max-failures-per-interval: 3\nrestart-strategy.failure-rate.failure-rate-interval: 5 min\nrestart-strategy.failure-rate.delay: 10 s\n```\n失败率重新启动策略也可以通过编程来设置：\n\nJava版本:\n```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.failureRateRestart(\n  3, // max failures per interval\n  Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate\n  Time.of(10, TimeUnit.SECONDS) // delay\n));\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.failureRateRestart(\n  3, // max failures per unit\n  Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate\n  Time.of(10, TimeUnit.SECONDS) // delay\n))\n```\n\n#### 2.3 不重启策略\n\n作业直接失败，不会尝试重新启动:\n```\nrestart-strategy: none\n```\n不重启策略也可以通过编程来设置：\n\nJava版本:\n```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.noRestart());\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.noRestart())\n```\n\n#### 2.4 回退重启策略\n\n使用集群定义的重启策略(The cluster defined restart strategy is used. )。这有助于启用检查点的流式传输程序。默认情况下，如果没有定义其他重启策略，则选择固定延时重启策略。\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html\n","source":"_posts/Flink/[Flink]Flink1.4 重启策略.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 重启策略\ndate: 2018-01-04 16:36:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n`Flink`支持不同的重启策略，重启策略控制在作业失败后如何重启。可以使用默认的重启策略启动集群，这个默认策略在作业没有特别指定重启策略时使用。如果在提交作业时指定了重启策略，那么此策略将覆盖集群的默认配置策略。\n\n### 1. 概述\n\n默认的重启策略通过`Flink`的配置文件`flink-conf.yaml`进行设置。配置参数`restart-strategy`定义了采取哪种策略。如果未启用检查点，那么将使用`不重启`策略。如果启用检查点且重启策略尚未配置，则固定延迟重启策略与`Integer.MAX_VALUE`一起使用进行尝试重启。请参阅下面可用的重启策略列表以了解支持哪些值。\n\n每个重启策略都有自己的一套控制其行为的参数。这些值也在配置文件中配置。每个重启策略的描述都包含有关各个配置值的更多信息。\n\n重启策略|值\n---|---\n固定延迟重启策略|fixed-delay\n失败率重启策略|failure-rate\n不重启策略|none\n\n除了定义一个默认的重启策略之外，还可以为每个`Flink`作业定义一个指定的重启策略。此重启策略通过调用`ExecutionEnvironment`上的`setRestartStrategy`方法以编程的方式进行设置。请注意，这也适用于`StreamExecutionEnvironment`。\n\n以下示例显示了如何为作业设置固定延迟重启策略。如果发生故障，系统将尝试每10s重新启动一次作业，最多重启3次。\n\nJava版本:\n```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n));\n```\n\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n))\n```\n\n### 2. 重启策略\n\n下面介绍几种重启策略的配置选项。\n\n#### 2.1 固定延迟重启策略\n\n固定延迟重启策略尝试一定次数来重新启动作业。如果超过最大尝试次数，那么作业最终将失败。在两次连续的尝试重启之间，重启策略会等待一段固定的时间(译者注:连续重启时间间隔)。\n\n通过在`flink-conf.yaml`中设置以下配置参数，可以将此策略默认启用：\n```\nrestart-strategy: fixed-delay\n```\n\n配置参数|描述|默认值\n---|---|---\n`restart-strategy.fixed-delay.attempts`|在声明作业失败之前，`Flink`重试执行的次数|1或者如果启用检查点，则为`Integer.MAX_VALUE`\n`restart-strategy.fixed-delay.delay`|延迟重试意味着在执行失败后，重新执行不会立即开始，而只会在某个延迟之后开始。当程序与外部系统进行交互时，延迟重试会很有帮助|`akka.ask.timeout`，或10s(如果通过检查点激活)\n\nExample:\n```\nrestart-strategy.fixed-delay.attempts: 3\nrestart-strategy.fixed-delay.delay: 10 s\n```\n固定延迟重启策略也可以通过编程来设置：\n\nJava版本:\n```Java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n));\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n  3, // number of restart attempts\n  Time.of(10, TimeUnit.SECONDS) // delay\n))\n```\n\n#### 2.2 失败率重启策略\n\n失败率重启策略在失败后重新启动作业，但当超过失败率(每个时间间隔的失败)时，作业最终会失败。在两次连续的重启尝试之间，重启策略会等待一段固定的时间。\n\n通过在`flink-conf.yaml`中设置以下配置参数，可以将此策略默认启用:\n\n配置参数|描述|默认值\n---|---|---\n`restart-strategy.failure-rate.max-failures-per-interval`|在一个作业声明失败之前，在给定时间间隔内最大的重启次数|1\n`restart-strategy.failure-rate.failure-rate-interval`|计算失败率的时间间隔|1分钟\n`restart-strategy.failure-rate.delay`|两次连续重启尝试之间的时间间隔|`akka.ask.timeout`\n\nExample:\n```\nrestart-strategy.failure-rate.max-failures-per-interval: 3\nrestart-strategy.failure-rate.failure-rate-interval: 5 min\nrestart-strategy.failure-rate.delay: 10 s\n```\n失败率重新启动策略也可以通过编程来设置：\n\nJava版本:\n```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.failureRateRestart(\n  3, // max failures per interval\n  Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate\n  Time.of(10, TimeUnit.SECONDS) // delay\n));\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.failureRateRestart(\n  3, // max failures per unit\n  Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate\n  Time.of(10, TimeUnit.SECONDS) // delay\n))\n```\n\n#### 2.3 不重启策略\n\n作业直接失败，不会尝试重新启动:\n```\nrestart-strategy: none\n```\n不重启策略也可以通过编程来设置：\n\nJava版本:\n```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\nenv.setRestartStrategy(RestartStrategies.noRestart());\n```\nScala版本:\n```\nval env = ExecutionEnvironment.getExecutionEnvironment()\nenv.setRestartStrategy(RestartStrategies.noRestart())\n```\n\n#### 2.4 回退重启策略\n\n使用集群定义的重启策略(The cluster defined restart strategy is used. )。这有助于启用检查点的流式传输程序。默认情况下，如果没有定义其他重启策略，则选择固定延时重启策略。\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html\n","slug":"Flink/[Flink]Flink1.4 重启策略","published":1,"updated":"2018-01-29T09:36:59.638Z","comments":1,"photos":[],"link":"","_id":"cje58tirv0028ordb3f4uzpox","content":"<p><code>Flink</code>支持不同的重启策略，重启策略控制在作业失败后如何重启。可以使用默认的重启策略启动集群，这个默认策略在作业没有特别指定重启策略时使用。如果在提交作业时指定了重启策略，那么此策略将覆盖集群的默认配置策略。</p>\n<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>默认的重启策略通过<code>Flink</code>的配置文件<code>flink-conf.yaml</code>进行设置。配置参数<code>restart-strategy</code>定义了采取哪种策略。如果未启用检查点，那么将使用<code>不重启</code>策略。如果启用检查点且重启策略尚未配置，则固定延迟重启策略与<code>Integer.MAX_VALUE</code>一起使用进行尝试重启。请参阅下面可用的重启策略列表以了解支持哪些值。</p>\n<p>每个重启策略都有自己的一套控制其行为的参数。这些值也在配置文件中配置。每个重启策略的描述都包含有关各个配置值的更多信息。</p>\n<table>\n<thead>\n<tr>\n<th>重启策略</th>\n<th>值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>固定延迟重启策略</td>\n<td>fixed-delay</td>\n</tr>\n<tr>\n<td>失败率重启策略</td>\n<td>failure-rate</td>\n</tr>\n<tr>\n<td>不重启策略</td>\n<td>none</td>\n</tr>\n</tbody>\n</table>\n<p>除了定义一个默认的重启策略之外，还可以为每个<code>Flink</code>作业定义一个指定的重启策略。此重启策略通过调用<code>ExecutionEnvironment</code>上的<code>setRestartStrategy</code>方法以编程的方式进行设置。请注意，这也适用于<code>StreamExecutionEnvironment</code>。</p>\n<p>以下示例显示了如何为作业设置固定延迟重启策略。如果发生故障，系统将尝试每10s重新启动一次作业，最多重启3次。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  <span class=\"number\">3</span>, <span class=\"comment\">// number of restart attempts</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">10</span>, TimeUnit.SECONDS) <span class=\"comment\">// delay</span></span><br><span class=\"line\">));</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  3, // number of restart attempts</span><br><span class=\"line\">  Time.of(10, TimeUnit.SECONDS) // delay</span><br><span class=\"line\">))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-重启策略\"><a href=\"#2-重启策略\" class=\"headerlink\" title=\"2. 重启策略\"></a>2. 重启策略</h3><p>下面介绍几种重启策略的配置选项。</p>\n<h4 id=\"2-1-固定延迟重启策略\"><a href=\"#2-1-固定延迟重启策略\" class=\"headerlink\" title=\"2.1 固定延迟重启策略\"></a>2.1 固定延迟重启策略</h4><p>固定延迟重启策略尝试一定次数来重新启动作业。如果超过最大尝试次数，那么作业最终将失败。在两次连续的尝试重启之间，重启策略会等待一段固定的时间(译者注:连续重启时间间隔)。</p>\n<p>通过在<code>flink-conf.yaml</code>中设置以下配置参数，可以将此策略默认启用：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy: fixed-delay</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>配置参数</th>\n<th>描述</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>restart-strategy.fixed-delay.attempts</code></td>\n<td>在声明作业失败之前，<code>Flink</code>重试执行的次数</td>\n<td>1或者如果启用检查点，则为<code>Integer.MAX_VALUE</code></td>\n</tr>\n<tr>\n<td><code>restart-strategy.fixed-delay.delay</code></td>\n<td>延迟重试意味着在执行失败后，重新执行不会立即开始，而只会在某个延迟之后开始。当程序与外部系统进行交互时，延迟重试会很有帮助</td>\n<td><code>akka.ask.timeout</code>，或10s(如果通过检查点激活)</td>\n</tr>\n</tbody>\n</table>\n<p>Example:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy.fixed-delay.attempts: 3</span><br><span class=\"line\">restart-strategy.fixed-delay.delay: 10 s</span><br></pre></td></tr></table></figure></p>\n<p>固定延迟重启策略也可以通过编程来设置：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  <span class=\"number\">3</span>, <span class=\"comment\">// number of restart attempts</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">10</span>, TimeUnit.SECONDS) <span class=\"comment\">// delay</span></span><br><span class=\"line\">));</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  3, // number of restart attempts</span><br><span class=\"line\">  Time.of(10, TimeUnit.SECONDS) // delay</span><br><span class=\"line\">))</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-2-失败率重启策略\"><a href=\"#2-2-失败率重启策略\" class=\"headerlink\" title=\"2.2 失败率重启策略\"></a>2.2 失败率重启策略</h4><p>失败率重启策略在失败后重新启动作业，但当超过失败率(每个时间间隔的失败)时，作业最终会失败。在两次连续的重启尝试之间，重启策略会等待一段固定的时间。</p>\n<p>通过在<code>flink-conf.yaml</code>中设置以下配置参数，可以将此策略默认启用:</p>\n<table>\n<thead>\n<tr>\n<th>配置参数</th>\n<th>描述</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>restart-strategy.failure-rate.max-failures-per-interval</code></td>\n<td>在一个作业声明失败之前，在给定时间间隔内最大的重启次数</td>\n<td>1</td>\n</tr>\n<tr>\n<td><code>restart-strategy.failure-rate.failure-rate-interval</code></td>\n<td>计算失败率的时间间隔</td>\n<td>1分钟</td>\n</tr>\n<tr>\n<td><code>restart-strategy.failure-rate.delay</code></td>\n<td>两次连续重启尝试之间的时间间隔</td>\n<td><code>akka.ask.timeout</code></td>\n</tr>\n</tbody>\n</table>\n<p>Example:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy.failure-rate.max-failures-per-interval: 3</span><br><span class=\"line\">restart-strategy.failure-rate.failure-rate-interval: 5 min</span><br><span class=\"line\">restart-strategy.failure-rate.delay: 10 s</span><br></pre></td></tr></table></figure></p>\n<p>失败率重新启动策略也可以通过编程来设置：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.failureRateRestart(</span><br><span class=\"line\">  <span class=\"number\">3</span>, <span class=\"comment\">// max failures per interval</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">5</span>, TimeUnit.MINUTES), <span class=\"comment\">//time interval for measuring failure rate</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">10</span>, TimeUnit.SECONDS) <span class=\"comment\">// delay</span></span><br><span class=\"line\">));</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.failureRateRestart(</span><br><span class=\"line\">  3, // max failures per unit</span><br><span class=\"line\">  Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate</span><br><span class=\"line\">  Time.of(10, TimeUnit.SECONDS) // delay</span><br><span class=\"line\">))</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-不重启策略\"><a href=\"#2-3-不重启策略\" class=\"headerlink\" title=\"2.3 不重启策略\"></a>2.3 不重启策略</h4><p>作业直接失败，不会尝试重新启动:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy: none</span><br></pre></td></tr></table></figure></p>\n<p>不重启策略也可以通过编程来设置：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.noRestart());</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.noRestart())</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-4-回退重启策略\"><a href=\"#2-4-回退重启策略\" class=\"headerlink\" title=\"2.4 回退重启策略\"></a>2.4 回退重启策略</h4><p>使用集群定义的重启策略(The cluster defined restart strategy is used. )。这有助于启用检查点的流式传输程序。默认情况下，如果没有定义其他重启策略，则选择固定延时重启策略。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>Flink</code>支持不同的重启策略，重启策略控制在作业失败后如何重启。可以使用默认的重启策略启动集群，这个默认策略在作业没有特别指定重启策略时使用。如果在提交作业时指定了重启策略，那么此策略将覆盖集群的默认配置策略。</p>\n<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>默认的重启策略通过<code>Flink</code>的配置文件<code>flink-conf.yaml</code>进行设置。配置参数<code>restart-strategy</code>定义了采取哪种策略。如果未启用检查点，那么将使用<code>不重启</code>策略。如果启用检查点且重启策略尚未配置，则固定延迟重启策略与<code>Integer.MAX_VALUE</code>一起使用进行尝试重启。请参阅下面可用的重启策略列表以了解支持哪些值。</p>\n<p>每个重启策略都有自己的一套控制其行为的参数。这些值也在配置文件中配置。每个重启策略的描述都包含有关各个配置值的更多信息。</p>\n<table>\n<thead>\n<tr>\n<th>重启策略</th>\n<th>值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>固定延迟重启策略</td>\n<td>fixed-delay</td>\n</tr>\n<tr>\n<td>失败率重启策略</td>\n<td>failure-rate</td>\n</tr>\n<tr>\n<td>不重启策略</td>\n<td>none</td>\n</tr>\n</tbody>\n</table>\n<p>除了定义一个默认的重启策略之外，还可以为每个<code>Flink</code>作业定义一个指定的重启策略。此重启策略通过调用<code>ExecutionEnvironment</code>上的<code>setRestartStrategy</code>方法以编程的方式进行设置。请注意，这也适用于<code>StreamExecutionEnvironment</code>。</p>\n<p>以下示例显示了如何为作业设置固定延迟重启策略。如果发生故障，系统将尝试每10s重新启动一次作业，最多重启3次。</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  <span class=\"number\">3</span>, <span class=\"comment\">// number of restart attempts</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">10</span>, TimeUnit.SECONDS) <span class=\"comment\">// delay</span></span><br><span class=\"line\">));</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  3, // number of restart attempts</span><br><span class=\"line\">  Time.of(10, TimeUnit.SECONDS) // delay</span><br><span class=\"line\">))</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-重启策略\"><a href=\"#2-重启策略\" class=\"headerlink\" title=\"2. 重启策略\"></a>2. 重启策略</h3><p>下面介绍几种重启策略的配置选项。</p>\n<h4 id=\"2-1-固定延迟重启策略\"><a href=\"#2-1-固定延迟重启策略\" class=\"headerlink\" title=\"2.1 固定延迟重启策略\"></a>2.1 固定延迟重启策略</h4><p>固定延迟重启策略尝试一定次数来重新启动作业。如果超过最大尝试次数，那么作业最终将失败。在两次连续的尝试重启之间，重启策略会等待一段固定的时间(译者注:连续重启时间间隔)。</p>\n<p>通过在<code>flink-conf.yaml</code>中设置以下配置参数，可以将此策略默认启用：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy: fixed-delay</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>配置参数</th>\n<th>描述</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>restart-strategy.fixed-delay.attempts</code></td>\n<td>在声明作业失败之前，<code>Flink</code>重试执行的次数</td>\n<td>1或者如果启用检查点，则为<code>Integer.MAX_VALUE</code></td>\n</tr>\n<tr>\n<td><code>restart-strategy.fixed-delay.delay</code></td>\n<td>延迟重试意味着在执行失败后，重新执行不会立即开始，而只会在某个延迟之后开始。当程序与外部系统进行交互时，延迟重试会很有帮助</td>\n<td><code>akka.ask.timeout</code>，或10s(如果通过检查点激活)</td>\n</tr>\n</tbody>\n</table>\n<p>Example:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy.fixed-delay.attempts: 3</span><br><span class=\"line\">restart-strategy.fixed-delay.delay: 10 s</span><br></pre></td></tr></table></figure></p>\n<p>固定延迟重启策略也可以通过编程来设置：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  <span class=\"number\">3</span>, <span class=\"comment\">// number of restart attempts</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">10</span>, TimeUnit.SECONDS) <span class=\"comment\">// delay</span></span><br><span class=\"line\">));</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class=\"line\">  3, // number of restart attempts</span><br><span class=\"line\">  Time.of(10, TimeUnit.SECONDS) // delay</span><br><span class=\"line\">))</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-2-失败率重启策略\"><a href=\"#2-2-失败率重启策略\" class=\"headerlink\" title=\"2.2 失败率重启策略\"></a>2.2 失败率重启策略</h4><p>失败率重启策略在失败后重新启动作业，但当超过失败率(每个时间间隔的失败)时，作业最终会失败。在两次连续的重启尝试之间，重启策略会等待一段固定的时间。</p>\n<p>通过在<code>flink-conf.yaml</code>中设置以下配置参数，可以将此策略默认启用:</p>\n<table>\n<thead>\n<tr>\n<th>配置参数</th>\n<th>描述</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>restart-strategy.failure-rate.max-failures-per-interval</code></td>\n<td>在一个作业声明失败之前，在给定时间间隔内最大的重启次数</td>\n<td>1</td>\n</tr>\n<tr>\n<td><code>restart-strategy.failure-rate.failure-rate-interval</code></td>\n<td>计算失败率的时间间隔</td>\n<td>1分钟</td>\n</tr>\n<tr>\n<td><code>restart-strategy.failure-rate.delay</code></td>\n<td>两次连续重启尝试之间的时间间隔</td>\n<td><code>akka.ask.timeout</code></td>\n</tr>\n</tbody>\n</table>\n<p>Example:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy.failure-rate.max-failures-per-interval: 3</span><br><span class=\"line\">restart-strategy.failure-rate.failure-rate-interval: 5 min</span><br><span class=\"line\">restart-strategy.failure-rate.delay: 10 s</span><br></pre></td></tr></table></figure></p>\n<p>失败率重新启动策略也可以通过编程来设置：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.failureRateRestart(</span><br><span class=\"line\">  <span class=\"number\">3</span>, <span class=\"comment\">// max failures per interval</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">5</span>, TimeUnit.MINUTES), <span class=\"comment\">//time interval for measuring failure rate</span></span><br><span class=\"line\">  Time.of(<span class=\"number\">10</span>, TimeUnit.SECONDS) <span class=\"comment\">// delay</span></span><br><span class=\"line\">));</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.failureRateRestart(</span><br><span class=\"line\">  3, // max failures per unit</span><br><span class=\"line\">  Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate</span><br><span class=\"line\">  Time.of(10, TimeUnit.SECONDS) // delay</span><br><span class=\"line\">))</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-不重启策略\"><a href=\"#2-3-不重启策略\" class=\"headerlink\" title=\"2.3 不重启策略\"></a>2.3 不重启策略</h4><p>作业直接失败，不会尝试重新启动:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">restart-strategy: none</span><br></pre></td></tr></table></figure></p>\n<p>不重启策略也可以通过编程来设置：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.noRestart());</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\">env.setRestartStrategy(RestartStrategies.noRestart())</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-4-回退重启策略\"><a href=\"#2-4-回退重启策略\" class=\"headerlink\" title=\"2.4 回退重启策略\"></a>2.4 回退重启策略</h4><p>使用集群定义的重启策略(The cluster defined restart strategy is used. )。这有助于启用检查点的流式传输程序。默认情况下，如果没有定义其他重启策略，则选择固定延时重启策略。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/restart_strategies.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop MapReduce 2 工作原理","date":"2017-12-14T11:58:01.000Z","_content":"\n### 1. 概述\n\n对于节点数超出4000的大型集群，`MapReduce 1`的系统开始面领着扩展性的瓶颈。在2010年雅虎的一个团队开始设计下一代`MapReduce`.由此，`YARN`(`Yet Another Resource Negotiator`的缩写或者为`YARN Application Resource Neforiator`的缩写)应运而生。\n\n`YARN` 将 `Jobtracker` 的职能划分为多个独立的实体，从而改善了'经典的'`MapReduce`面临的扩展瓶颈问题。`Jobtracker`负责作业调度和任务进度监视、追踪任务、重启失败或过慢的任务和进行任务登记，例如维护计数器总数。\n\n`YARN`将这两种角色划分为两个独立的守护进程：管理集群上资源使用的资源管理器(`ResourceManager`)和管理集群上运行任务生命周期的应用管理器(`ApplicationMaster`)。基本思路是：应用服务器与资源管理器协商集群的计算资源：容器(每个容器都有特定的内存上限)，在这些容器上运行特定应用程序的进程。容器由集群节点上运行的节点管理器(`NodeManager`)监视，以确保应用程序使用的资源不会超过分配给它的资源。\n\n与`jobtracker`不同，应用的每个实例（这里指一个`MapReduce`作业）有一个专用的应用`master`(`ApplicationMaster`)，它运行在应用的运行期间。这种方式实际上和最初的`Google`的`MapReduce`论文里介绍的方法很相似，该论文描述了`master`进程如何协调在一组`worker`上运行的`map`任务和`reduce`任务。\n\n如前所述，`YARN`比`MapReduce`更具一般性，实际上`MapReduce`只是`YARN`应用的一种形式。有很多其他的`YARN`应用(例如能够在集群中的一组节点上运行脚本的分布式shell)以及其他正在开发的程序。`YARN`设计的精妙之处在于不同的`YARN`应用可以在同一个集群上共存。例如，一个`MapReduce`应用可以同时作为MPI应用运行，这大大提高了可管理性和集群的利用率。\n\n此外，用户甚至有可能在同一个`YARN`集群上运行多个不同版本的`MapReduce`，这使得`MapReduce`升级过程更容易管理。注意，`MapReduce`的某些部分(比如作业历史服务器和shuffle处理器)以及`YARN`本身仍然需要在整个集群上升级。\n\n`YARN`上的`MapReduce`比经典的`MapReduce`包括更多的实体：\n- 提交`MapReduce`作业的客户端\n- `YARN`资源管理器(`ResourceManager`)，负责协调集群上计算资源的分配\n- `YARN`节点管理器(`NodeManager`)，负责启动和监视集群中机器上的计算容器(`container`)\n- `MapReduce`应用程序`master`(`ApplicationMaster`)，负责协调运行`MapReduce`作业的任务。它和`MapReduce`任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理\n- 分布式文件系统（一般为`HDFS`），用来与其他实体见共享作业文件\n\n作业运行过程如下图所示:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce%202%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true)\n\n\n### 2. 作业提交\n\n`MapReduce 2` 中的作业提交是使用与`MapReduce 1`相同的用户API(参见上图步骤1)。`MapReduce 2`实现了`ClientProtocol`，当`mapreduce.framework.name`设置为`yarn`时启动。提交的过程与经典的非常相似。从资源管理器`ResourceManager`(而不是`jobtracker`)获取新的作业ID，在`YARN`命名法中它是一个应用程序ID(参见上图步骤2)。作业客户端检查作业的输出说明，计算输入分片(虽然有选项`yarn.app.mapreduce.am.compute-splits-in-cluster`在集群上来产生分片，这可以使具有多个分片的作业从中受益)并将作业资源(包括作业`JAR`、配置和分片信息)复制到`HDFS`(参见上图步骤3）。最后，通过调用资源管理器上的`submitApplication()`方法提交作业(参见上图步骤4)。          \n\n### 3. 作业初始化\n\n资源管理器收到调用它的`submitApplication()`消息后，便将请求传递给调度器(`scheduler`)。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动应用程序的master进程(`ApplicationMaster`)(参见上图步骤5a和5b)。\n\n`MapReduce`作业的`ApplicationMaster`是一个`Java`应用程序,它的主类是`MRAppMaster`。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告(参见上图步骤6)。接下来，它接受来自共享文件系统的在客户端计算的输入分片(参见上图步骤7)。对每一个分片创建一个`map`任务对象以及由`mapreduce.job.reduces`属性确定的多个`reduce`任务对象。\n\n接下来，`ApplicationMaster`决定如何运行构成`MapReduce`作业的各个任务。如果作业很小，就选择在与它同一个`JVM`上运行任务。\n\n相对于在一个节点上顺序运行它们，判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这不同于`MapReduce 1`，`MapReduce 1`从不在单个`tasktracker`上运行小作业。这样的作业称为`uberized`，或者作为`uber`任务运行。\n\n哪些任务是小任务呢？ 默认情况下，小任务就是小于10个`mapper`且只有1个`reducer`且输入大小小于一个`HDFS`块的任务。(通过设置`mapreduce.job.ubertask.maxmaps`、`mapreduce.job.ubertask.maxreduces`和` mapreduce.job.ubertask.maxbytes`可以改变一个作业的上述值。)将`mapreduce.job.ubertask.enable`设置为`false`也可以完全使`uber`任务不可用。\n\n在任何任务运行之前，作业的`setup`方法为了设置作业的`OutputCommitter`被调用来建立作业的输出目录。在`MapReduce 1`中，它在一个由`tasktracker`运行的特殊任务中被调用，而在`YARN`执行框架中，该方法由应用程序master直接调用。\n\n### 4. 任务分配\n\n如果作业不适合作为`uber`任务运行，那么`ApplicationMaster`就会为该作业中的所有`map`任务和`reduce`任务向资源管理器请求容器(参见上图步骤8)。附着心跳信息的请求包括每个`map`任务的数据本地化信息，特别是输入分片所在的主机和相应机架信息。调度器使用这些信息来做调度策略(像`jobtracker`的调度器一样)。理想情况下，它将任务分配到数据本地化的节点，但如果不可能这样做，调度器就会相对于非本地化的分配有限使用机架本地化的分配。\n\n请求也为任务指定了内存需求。在默认情况下，`map`任务和`reduce`任务都分配到`1024MB`的内存，但这可以通过`mapreduce.map.memory.mb`和`mapreduce.reduce.memory.mb`来设置。\n\n内存的分配方式不同于`MapReduce 1`，后者中`tasktrackers`有在集群配置时设置的固定数量的槽，每个任务在一个槽上运行。槽有最大内存分配限制，这对集群是固定的，导致当任务使用较少内存时无法充分利用内存(因为其他等待的任务不能使用这些未使用的内存)以及由于任务不能获取足够内存而导致作业失败。\n\n在`YARN`中，资源划分更细的粒度，所以可以避免上述问题。具体而言，应用程序可以请求最小到最大限制范围内的任意最小值倍数的内存容量。默认的内存分配容量是调度器特定的，对于容量调度器，它的默认值最小值是`1024MB`(由 `yarn.sheduler.capacity.minimum-allocation-mb`设置)，默认的最大值是`10240MB`(由`yarn.sheduler.capacity.maximum-allocation-mb`设置)。因此，任务可以通过适当设置`mapreduce.map.memory.mb`和 `mapreduce.reduce.memory.mb`来请求`1GB`到`10GB`间的任务`1GB`倍数的内存容量(调度器在需要的时候使用最接近的倍数)。\n\n### 5. 任务执行\n\n一旦资源管理器的调度器为任务分配了容器，`ApplicationMaster`就通过与节点管理器通信来启动容器(参见上图步骤9a和9b)。该任务由主类`YarnChild`的`Java`应用程序执行，在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、`JAR`文件和所有来自分布式缓存的文件(参见上图步骤10)。最后，运行`map`任务或`reduce`任务(参见上图步骤11)。\n\n### 6. 进度和状态更新\n\n在`YARN`下运行时，任务每三秒钟通过`umbilical`接口向`ApplicationMaster`汇报进度和状态(包含计数器)，作为作业的汇聚视图(`aggregate view`)。相比之下，`MapReduce 1`通过`tasktracker`到`jobtracker`来实现进度更新。\n\n客户端每秒钟(通过`mapreduce.client.progressmonitor.pollinterval`设置)查询一次`ApplicationMaster`以接收进度更新，通常都会向用户显示。\n\n### 7. 作业完成\n\n除了向`ApplicationMaster`查询进度外，客户端每5秒钟通过调用`Job`的`waitForCompletion()`来检查作业是否完成。查询的间隔可以通过`mapreduce.client.completion.pollinterval`属性进行设置。\n\n作业完成后，`ApplicationMaster`和任务容器清理其工作状态，`OutputCommitter`的作业清理方法会被调用。作业历史服务器保存作业的信息供用户需要时查询。\n\n\n来源于: `Hadoop 权威指南`\n","source":"_posts/Hadoop/Hadoop MapReduce 2 工作原理.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop MapReduce 2 工作原理\ndate: 2017-12-14 19:58:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. 概述\n\n对于节点数超出4000的大型集群，`MapReduce 1`的系统开始面领着扩展性的瓶颈。在2010年雅虎的一个团队开始设计下一代`MapReduce`.由此，`YARN`(`Yet Another Resource Negotiator`的缩写或者为`YARN Application Resource Neforiator`的缩写)应运而生。\n\n`YARN` 将 `Jobtracker` 的职能划分为多个独立的实体，从而改善了'经典的'`MapReduce`面临的扩展瓶颈问题。`Jobtracker`负责作业调度和任务进度监视、追踪任务、重启失败或过慢的任务和进行任务登记，例如维护计数器总数。\n\n`YARN`将这两种角色划分为两个独立的守护进程：管理集群上资源使用的资源管理器(`ResourceManager`)和管理集群上运行任务生命周期的应用管理器(`ApplicationMaster`)。基本思路是：应用服务器与资源管理器协商集群的计算资源：容器(每个容器都有特定的内存上限)，在这些容器上运行特定应用程序的进程。容器由集群节点上运行的节点管理器(`NodeManager`)监视，以确保应用程序使用的资源不会超过分配给它的资源。\n\n与`jobtracker`不同，应用的每个实例（这里指一个`MapReduce`作业）有一个专用的应用`master`(`ApplicationMaster`)，它运行在应用的运行期间。这种方式实际上和最初的`Google`的`MapReduce`论文里介绍的方法很相似，该论文描述了`master`进程如何协调在一组`worker`上运行的`map`任务和`reduce`任务。\n\n如前所述，`YARN`比`MapReduce`更具一般性，实际上`MapReduce`只是`YARN`应用的一种形式。有很多其他的`YARN`应用(例如能够在集群中的一组节点上运行脚本的分布式shell)以及其他正在开发的程序。`YARN`设计的精妙之处在于不同的`YARN`应用可以在同一个集群上共存。例如，一个`MapReduce`应用可以同时作为MPI应用运行，这大大提高了可管理性和集群的利用率。\n\n此外，用户甚至有可能在同一个`YARN`集群上运行多个不同版本的`MapReduce`，这使得`MapReduce`升级过程更容易管理。注意，`MapReduce`的某些部分(比如作业历史服务器和shuffle处理器)以及`YARN`本身仍然需要在整个集群上升级。\n\n`YARN`上的`MapReduce`比经典的`MapReduce`包括更多的实体：\n- 提交`MapReduce`作业的客户端\n- `YARN`资源管理器(`ResourceManager`)，负责协调集群上计算资源的分配\n- `YARN`节点管理器(`NodeManager`)，负责启动和监视集群中机器上的计算容器(`container`)\n- `MapReduce`应用程序`master`(`ApplicationMaster`)，负责协调运行`MapReduce`作业的任务。它和`MapReduce`任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理\n- 分布式文件系统（一般为`HDFS`），用来与其他实体见共享作业文件\n\n作业运行过程如下图所示:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce%202%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true)\n\n\n### 2. 作业提交\n\n`MapReduce 2` 中的作业提交是使用与`MapReduce 1`相同的用户API(参见上图步骤1)。`MapReduce 2`实现了`ClientProtocol`，当`mapreduce.framework.name`设置为`yarn`时启动。提交的过程与经典的非常相似。从资源管理器`ResourceManager`(而不是`jobtracker`)获取新的作业ID，在`YARN`命名法中它是一个应用程序ID(参见上图步骤2)。作业客户端检查作业的输出说明，计算输入分片(虽然有选项`yarn.app.mapreduce.am.compute-splits-in-cluster`在集群上来产生分片，这可以使具有多个分片的作业从中受益)并将作业资源(包括作业`JAR`、配置和分片信息)复制到`HDFS`(参见上图步骤3）。最后，通过调用资源管理器上的`submitApplication()`方法提交作业(参见上图步骤4)。          \n\n### 3. 作业初始化\n\n资源管理器收到调用它的`submitApplication()`消息后，便将请求传递给调度器(`scheduler`)。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动应用程序的master进程(`ApplicationMaster`)(参见上图步骤5a和5b)。\n\n`MapReduce`作业的`ApplicationMaster`是一个`Java`应用程序,它的主类是`MRAppMaster`。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告(参见上图步骤6)。接下来，它接受来自共享文件系统的在客户端计算的输入分片(参见上图步骤7)。对每一个分片创建一个`map`任务对象以及由`mapreduce.job.reduces`属性确定的多个`reduce`任务对象。\n\n接下来，`ApplicationMaster`决定如何运行构成`MapReduce`作业的各个任务。如果作业很小，就选择在与它同一个`JVM`上运行任务。\n\n相对于在一个节点上顺序运行它们，判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这不同于`MapReduce 1`，`MapReduce 1`从不在单个`tasktracker`上运行小作业。这样的作业称为`uberized`，或者作为`uber`任务运行。\n\n哪些任务是小任务呢？ 默认情况下，小任务就是小于10个`mapper`且只有1个`reducer`且输入大小小于一个`HDFS`块的任务。(通过设置`mapreduce.job.ubertask.maxmaps`、`mapreduce.job.ubertask.maxreduces`和` mapreduce.job.ubertask.maxbytes`可以改变一个作业的上述值。)将`mapreduce.job.ubertask.enable`设置为`false`也可以完全使`uber`任务不可用。\n\n在任何任务运行之前，作业的`setup`方法为了设置作业的`OutputCommitter`被调用来建立作业的输出目录。在`MapReduce 1`中，它在一个由`tasktracker`运行的特殊任务中被调用，而在`YARN`执行框架中，该方法由应用程序master直接调用。\n\n### 4. 任务分配\n\n如果作业不适合作为`uber`任务运行，那么`ApplicationMaster`就会为该作业中的所有`map`任务和`reduce`任务向资源管理器请求容器(参见上图步骤8)。附着心跳信息的请求包括每个`map`任务的数据本地化信息，特别是输入分片所在的主机和相应机架信息。调度器使用这些信息来做调度策略(像`jobtracker`的调度器一样)。理想情况下，它将任务分配到数据本地化的节点，但如果不可能这样做，调度器就会相对于非本地化的分配有限使用机架本地化的分配。\n\n请求也为任务指定了内存需求。在默认情况下，`map`任务和`reduce`任务都分配到`1024MB`的内存，但这可以通过`mapreduce.map.memory.mb`和`mapreduce.reduce.memory.mb`来设置。\n\n内存的分配方式不同于`MapReduce 1`，后者中`tasktrackers`有在集群配置时设置的固定数量的槽，每个任务在一个槽上运行。槽有最大内存分配限制，这对集群是固定的，导致当任务使用较少内存时无法充分利用内存(因为其他等待的任务不能使用这些未使用的内存)以及由于任务不能获取足够内存而导致作业失败。\n\n在`YARN`中，资源划分更细的粒度，所以可以避免上述问题。具体而言，应用程序可以请求最小到最大限制范围内的任意最小值倍数的内存容量。默认的内存分配容量是调度器特定的，对于容量调度器，它的默认值最小值是`1024MB`(由 `yarn.sheduler.capacity.minimum-allocation-mb`设置)，默认的最大值是`10240MB`(由`yarn.sheduler.capacity.maximum-allocation-mb`设置)。因此，任务可以通过适当设置`mapreduce.map.memory.mb`和 `mapreduce.reduce.memory.mb`来请求`1GB`到`10GB`间的任务`1GB`倍数的内存容量(调度器在需要的时候使用最接近的倍数)。\n\n### 5. 任务执行\n\n一旦资源管理器的调度器为任务分配了容器，`ApplicationMaster`就通过与节点管理器通信来启动容器(参见上图步骤9a和9b)。该任务由主类`YarnChild`的`Java`应用程序执行，在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、`JAR`文件和所有来自分布式缓存的文件(参见上图步骤10)。最后，运行`map`任务或`reduce`任务(参见上图步骤11)。\n\n### 6. 进度和状态更新\n\n在`YARN`下运行时，任务每三秒钟通过`umbilical`接口向`ApplicationMaster`汇报进度和状态(包含计数器)，作为作业的汇聚视图(`aggregate view`)。相比之下，`MapReduce 1`通过`tasktracker`到`jobtracker`来实现进度更新。\n\n客户端每秒钟(通过`mapreduce.client.progressmonitor.pollinterval`设置)查询一次`ApplicationMaster`以接收进度更新，通常都会向用户显示。\n\n### 7. 作业完成\n\n除了向`ApplicationMaster`查询进度外，客户端每5秒钟通过调用`Job`的`waitForCompletion()`来检查作业是否完成。查询的间隔可以通过`mapreduce.client.completion.pollinterval`属性进行设置。\n\n作业完成后，`ApplicationMaster`和任务容器清理其工作状态，`OutputCommitter`的作业清理方法会被调用。作业历史服务器保存作业的信息供用户需要时查询。\n\n\n来源于: `Hadoop 权威指南`\n","slug":"Hadoop/Hadoop MapReduce 2 工作原理","published":1,"updated":"2018-01-29T09:36:59.637Z","comments":1,"photos":[],"link":"","_id":"cje58tirx002dordbaowcte91","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>对于节点数超出4000的大型集群，<code>MapReduce 1</code>的系统开始面领着扩展性的瓶颈。在2010年雅虎的一个团队开始设计下一代<code>MapReduce</code>.由此，<code>YARN</code>(<code>Yet Another Resource Negotiator</code>的缩写或者为<code>YARN Application Resource Neforiator</code>的缩写)应运而生。</p>\n<p><code>YARN</code> 将 <code>Jobtracker</code> 的职能划分为多个独立的实体，从而改善了’经典的’<code>MapReduce</code>面临的扩展瓶颈问题。<code>Jobtracker</code>负责作业调度和任务进度监视、追踪任务、重启失败或过慢的任务和进行任务登记，例如维护计数器总数。</p>\n<p><code>YARN</code>将这两种角色划分为两个独立的守护进程：管理集群上资源使用的资源管理器(<code>ResourceManager</code>)和管理集群上运行任务生命周期的应用管理器(<code>ApplicationMaster</code>)。基本思路是：应用服务器与资源管理器协商集群的计算资源：容器(每个容器都有特定的内存上限)，在这些容器上运行特定应用程序的进程。容器由集群节点上运行的节点管理器(<code>NodeManager</code>)监视，以确保应用程序使用的资源不会超过分配给它的资源。</p>\n<p>与<code>jobtracker</code>不同，应用的每个实例（这里指一个<code>MapReduce</code>作业）有一个专用的应用<code>master</code>(<code>ApplicationMaster</code>)，它运行在应用的运行期间。这种方式实际上和最初的<code>Google</code>的<code>MapReduce</code>论文里介绍的方法很相似，该论文描述了<code>master</code>进程如何协调在一组<code>worker</code>上运行的<code>map</code>任务和<code>reduce</code>任务。</p>\n<p>如前所述，<code>YARN</code>比<code>MapReduce</code>更具一般性，实际上<code>MapReduce</code>只是<code>YARN</code>应用的一种形式。有很多其他的<code>YARN</code>应用(例如能够在集群中的一组节点上运行脚本的分布式shell)以及其他正在开发的程序。<code>YARN</code>设计的精妙之处在于不同的<code>YARN</code>应用可以在同一个集群上共存。例如，一个<code>MapReduce</code>应用可以同时作为MPI应用运行，这大大提高了可管理性和集群的利用率。</p>\n<p>此外，用户甚至有可能在同一个<code>YARN</code>集群上运行多个不同版本的<code>MapReduce</code>，这使得<code>MapReduce</code>升级过程更容易管理。注意，<code>MapReduce</code>的某些部分(比如作业历史服务器和shuffle处理器)以及<code>YARN</code>本身仍然需要在整个集群上升级。</p>\n<p><code>YARN</code>上的<code>MapReduce</code>比经典的<code>MapReduce</code>包括更多的实体：</p>\n<ul>\n<li>提交<code>MapReduce</code>作业的客户端</li>\n<li><code>YARN</code>资源管理器(<code>ResourceManager</code>)，负责协调集群上计算资源的分配</li>\n<li><code>YARN</code>节点管理器(<code>NodeManager</code>)，负责启动和监视集群中机器上的计算容器(<code>container</code>)</li>\n<li><code>MapReduce</code>应用程序<code>master</code>(<code>ApplicationMaster</code>)，负责协调运行<code>MapReduce</code>作业的任务。它和<code>MapReduce</code>任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理</li>\n<li>分布式文件系统（一般为<code>HDFS</code>），用来与其他实体见共享作业文件</li>\n</ul>\n<p>作业运行过程如下图所示:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce%202%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-作业提交\"><a href=\"#2-作业提交\" class=\"headerlink\" title=\"2. 作业提交\"></a>2. 作业提交</h3><p><code>MapReduce 2</code> 中的作业提交是使用与<code>MapReduce 1</code>相同的用户API(参见上图步骤1)。<code>MapReduce 2</code>实现了<code>ClientProtocol</code>，当<code>mapreduce.framework.name</code>设置为<code>yarn</code>时启动。提交的过程与经典的非常相似。从资源管理器<code>ResourceManager</code>(而不是<code>jobtracker</code>)获取新的作业ID，在<code>YARN</code>命名法中它是一个应用程序ID(参见上图步骤2)。作业客户端检查作业的输出说明，计算输入分片(虽然有选项<code>yarn.app.mapreduce.am.compute-splits-in-cluster</code>在集群上来产生分片，这可以使具有多个分片的作业从中受益)并将作业资源(包括作业<code>JAR</code>、配置和分片信息)复制到<code>HDFS</code>(参见上图步骤3）。最后，通过调用资源管理器上的<code>submitApplication()</code>方法提交作业(参见上图步骤4)。          </p>\n<h3 id=\"3-作业初始化\"><a href=\"#3-作业初始化\" class=\"headerlink\" title=\"3. 作业初始化\"></a>3. 作业初始化</h3><p>资源管理器收到调用它的<code>submitApplication()</code>消息后，便将请求传递给调度器(<code>scheduler</code>)。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动应用程序的master进程(<code>ApplicationMaster</code>)(参见上图步骤5a和5b)。</p>\n<p><code>MapReduce</code>作业的<code>ApplicationMaster</code>是一个<code>Java</code>应用程序,它的主类是<code>MRAppMaster</code>。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告(参见上图步骤6)。接下来，它接受来自共享文件系统的在客户端计算的输入分片(参见上图步骤7)。对每一个分片创建一个<code>map</code>任务对象以及由<code>mapreduce.job.reduces</code>属性确定的多个<code>reduce</code>任务对象。</p>\n<p>接下来，<code>ApplicationMaster</code>决定如何运行构成<code>MapReduce</code>作业的各个任务。如果作业很小，就选择在与它同一个<code>JVM</code>上运行任务。</p>\n<p>相对于在一个节点上顺序运行它们，判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这不同于<code>MapReduce 1</code>，<code>MapReduce 1</code>从不在单个<code>tasktracker</code>上运行小作业。这样的作业称为<code>uberized</code>，或者作为<code>uber</code>任务运行。</p>\n<p>哪些任务是小任务呢？ 默认情况下，小任务就是小于10个<code>mapper</code>且只有1个<code>reducer</code>且输入大小小于一个<code>HDFS</code>块的任务。(通过设置<code>mapreduce.job.ubertask.maxmaps</code>、<code>mapreduce.job.ubertask.maxreduces</code>和<code>mapreduce.job.ubertask.maxbytes</code>可以改变一个作业的上述值。)将<code>mapreduce.job.ubertask.enable</code>设置为<code>false</code>也可以完全使<code>uber</code>任务不可用。</p>\n<p>在任何任务运行之前，作业的<code>setup</code>方法为了设置作业的<code>OutputCommitter</code>被调用来建立作业的输出目录。在<code>MapReduce 1</code>中，它在一个由<code>tasktracker</code>运行的特殊任务中被调用，而在<code>YARN</code>执行框架中，该方法由应用程序master直接调用。</p>\n<h3 id=\"4-任务分配\"><a href=\"#4-任务分配\" class=\"headerlink\" title=\"4. 任务分配\"></a>4. 任务分配</h3><p>如果作业不适合作为<code>uber</code>任务运行，那么<code>ApplicationMaster</code>就会为该作业中的所有<code>map</code>任务和<code>reduce</code>任务向资源管理器请求容器(参见上图步骤8)。附着心跳信息的请求包括每个<code>map</code>任务的数据本地化信息，特别是输入分片所在的主机和相应机架信息。调度器使用这些信息来做调度策略(像<code>jobtracker</code>的调度器一样)。理想情况下，它将任务分配到数据本地化的节点，但如果不可能这样做，调度器就会相对于非本地化的分配有限使用机架本地化的分配。</p>\n<p>请求也为任务指定了内存需求。在默认情况下，<code>map</code>任务和<code>reduce</code>任务都分配到<code>1024MB</code>的内存，但这可以通过<code>mapreduce.map.memory.mb</code>和<code>mapreduce.reduce.memory.mb</code>来设置。</p>\n<p>内存的分配方式不同于<code>MapReduce 1</code>，后者中<code>tasktrackers</code>有在集群配置时设置的固定数量的槽，每个任务在一个槽上运行。槽有最大内存分配限制，这对集群是固定的，导致当任务使用较少内存时无法充分利用内存(因为其他等待的任务不能使用这些未使用的内存)以及由于任务不能获取足够内存而导致作业失败。</p>\n<p>在<code>YARN</code>中，资源划分更细的粒度，所以可以避免上述问题。具体而言，应用程序可以请求最小到最大限制范围内的任意最小值倍数的内存容量。默认的内存分配容量是调度器特定的，对于容量调度器，它的默认值最小值是<code>1024MB</code>(由 <code>yarn.sheduler.capacity.minimum-allocation-mb</code>设置)，默认的最大值是<code>10240MB</code>(由<code>yarn.sheduler.capacity.maximum-allocation-mb</code>设置)。因此，任务可以通过适当设置<code>mapreduce.map.memory.mb</code>和 <code>mapreduce.reduce.memory.mb</code>来请求<code>1GB</code>到<code>10GB</code>间的任务<code>1GB</code>倍数的内存容量(调度器在需要的时候使用最接近的倍数)。</p>\n<h3 id=\"5-任务执行\"><a href=\"#5-任务执行\" class=\"headerlink\" title=\"5. 任务执行\"></a>5. 任务执行</h3><p>一旦资源管理器的调度器为任务分配了容器，<code>ApplicationMaster</code>就通过与节点管理器通信来启动容器(参见上图步骤9a和9b)。该任务由主类<code>YarnChild</code>的<code>Java</code>应用程序执行，在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、<code>JAR</code>文件和所有来自分布式缓存的文件(参见上图步骤10)。最后，运行<code>map</code>任务或<code>reduce</code>任务(参见上图步骤11)。</p>\n<h3 id=\"6-进度和状态更新\"><a href=\"#6-进度和状态更新\" class=\"headerlink\" title=\"6. 进度和状态更新\"></a>6. 进度和状态更新</h3><p>在<code>YARN</code>下运行时，任务每三秒钟通过<code>umbilical</code>接口向<code>ApplicationMaster</code>汇报进度和状态(包含计数器)，作为作业的汇聚视图(<code>aggregate view</code>)。相比之下，<code>MapReduce 1</code>通过<code>tasktracker</code>到<code>jobtracker</code>来实现进度更新。</p>\n<p>客户端每秒钟(通过<code>mapreduce.client.progressmonitor.pollinterval</code>设置)查询一次<code>ApplicationMaster</code>以接收进度更新，通常都会向用户显示。</p>\n<h3 id=\"7-作业完成\"><a href=\"#7-作业完成\" class=\"headerlink\" title=\"7. 作业完成\"></a>7. 作业完成</h3><p>除了向<code>ApplicationMaster</code>查询进度外，客户端每5秒钟通过调用<code>Job</code>的<code>waitForCompletion()</code>来检查作业是否完成。查询的间隔可以通过<code>mapreduce.client.completion.pollinterval</code>属性进行设置。</p>\n<p>作业完成后，<code>ApplicationMaster</code>和任务容器清理其工作状态，<code>OutputCommitter</code>的作业清理方法会被调用。作业历史服务器保存作业的信息供用户需要时查询。</p>\n<p>来源于: <code>Hadoop 权威指南</code></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>对于节点数超出4000的大型集群，<code>MapReduce 1</code>的系统开始面领着扩展性的瓶颈。在2010年雅虎的一个团队开始设计下一代<code>MapReduce</code>.由此，<code>YARN</code>(<code>Yet Another Resource Negotiator</code>的缩写或者为<code>YARN Application Resource Neforiator</code>的缩写)应运而生。</p>\n<p><code>YARN</code> 将 <code>Jobtracker</code> 的职能划分为多个独立的实体，从而改善了’经典的’<code>MapReduce</code>面临的扩展瓶颈问题。<code>Jobtracker</code>负责作业调度和任务进度监视、追踪任务、重启失败或过慢的任务和进行任务登记，例如维护计数器总数。</p>\n<p><code>YARN</code>将这两种角色划分为两个独立的守护进程：管理集群上资源使用的资源管理器(<code>ResourceManager</code>)和管理集群上运行任务生命周期的应用管理器(<code>ApplicationMaster</code>)。基本思路是：应用服务器与资源管理器协商集群的计算资源：容器(每个容器都有特定的内存上限)，在这些容器上运行特定应用程序的进程。容器由集群节点上运行的节点管理器(<code>NodeManager</code>)监视，以确保应用程序使用的资源不会超过分配给它的资源。</p>\n<p>与<code>jobtracker</code>不同，应用的每个实例（这里指一个<code>MapReduce</code>作业）有一个专用的应用<code>master</code>(<code>ApplicationMaster</code>)，它运行在应用的运行期间。这种方式实际上和最初的<code>Google</code>的<code>MapReduce</code>论文里介绍的方法很相似，该论文描述了<code>master</code>进程如何协调在一组<code>worker</code>上运行的<code>map</code>任务和<code>reduce</code>任务。</p>\n<p>如前所述，<code>YARN</code>比<code>MapReduce</code>更具一般性，实际上<code>MapReduce</code>只是<code>YARN</code>应用的一种形式。有很多其他的<code>YARN</code>应用(例如能够在集群中的一组节点上运行脚本的分布式shell)以及其他正在开发的程序。<code>YARN</code>设计的精妙之处在于不同的<code>YARN</code>应用可以在同一个集群上共存。例如，一个<code>MapReduce</code>应用可以同时作为MPI应用运行，这大大提高了可管理性和集群的利用率。</p>\n<p>此外，用户甚至有可能在同一个<code>YARN</code>集群上运行多个不同版本的<code>MapReduce</code>，这使得<code>MapReduce</code>升级过程更容易管理。注意，<code>MapReduce</code>的某些部分(比如作业历史服务器和shuffle处理器)以及<code>YARN</code>本身仍然需要在整个集群上升级。</p>\n<p><code>YARN</code>上的<code>MapReduce</code>比经典的<code>MapReduce</code>包括更多的实体：</p>\n<ul>\n<li>提交<code>MapReduce</code>作业的客户端</li>\n<li><code>YARN</code>资源管理器(<code>ResourceManager</code>)，负责协调集群上计算资源的分配</li>\n<li><code>YARN</code>节点管理器(<code>NodeManager</code>)，负责启动和监视集群中机器上的计算容器(<code>container</code>)</li>\n<li><code>MapReduce</code>应用程序<code>master</code>(<code>ApplicationMaster</code>)，负责协调运行<code>MapReduce</code>作业的任务。它和<code>MapReduce</code>任务在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理</li>\n<li>分布式文件系统（一般为<code>HDFS</code>），用来与其他实体见共享作业文件</li>\n</ul>\n<p>作业运行过程如下图所示:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce%202%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-作业提交\"><a href=\"#2-作业提交\" class=\"headerlink\" title=\"2. 作业提交\"></a>2. 作业提交</h3><p><code>MapReduce 2</code> 中的作业提交是使用与<code>MapReduce 1</code>相同的用户API(参见上图步骤1)。<code>MapReduce 2</code>实现了<code>ClientProtocol</code>，当<code>mapreduce.framework.name</code>设置为<code>yarn</code>时启动。提交的过程与经典的非常相似。从资源管理器<code>ResourceManager</code>(而不是<code>jobtracker</code>)获取新的作业ID，在<code>YARN</code>命名法中它是一个应用程序ID(参见上图步骤2)。作业客户端检查作业的输出说明，计算输入分片(虽然有选项<code>yarn.app.mapreduce.am.compute-splits-in-cluster</code>在集群上来产生分片，这可以使具有多个分片的作业从中受益)并将作业资源(包括作业<code>JAR</code>、配置和分片信息)复制到<code>HDFS</code>(参见上图步骤3）。最后，通过调用资源管理器上的<code>submitApplication()</code>方法提交作业(参见上图步骤4)。          </p>\n<h3 id=\"3-作业初始化\"><a href=\"#3-作业初始化\" class=\"headerlink\" title=\"3. 作业初始化\"></a>3. 作业初始化</h3><p>资源管理器收到调用它的<code>submitApplication()</code>消息后，便将请求传递给调度器(<code>scheduler</code>)。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动应用程序的master进程(<code>ApplicationMaster</code>)(参见上图步骤5a和5b)。</p>\n<p><code>MapReduce</code>作业的<code>ApplicationMaster</code>是一个<code>Java</code>应用程序,它的主类是<code>MRAppMaster</code>。它对作业进行初始化：通过创建多个簿记对象以保持对作业进度的跟踪，因为它将接受来自任务的进度和完成报告(参见上图步骤6)。接下来，它接受来自共享文件系统的在客户端计算的输入分片(参见上图步骤7)。对每一个分片创建一个<code>map</code>任务对象以及由<code>mapreduce.job.reduces</code>属性确定的多个<code>reduce</code>任务对象。</p>\n<p>接下来，<code>ApplicationMaster</code>决定如何运行构成<code>MapReduce</code>作业的各个任务。如果作业很小，就选择在与它同一个<code>JVM</code>上运行任务。</p>\n<p>相对于在一个节点上顺序运行它们，判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这不同于<code>MapReduce 1</code>，<code>MapReduce 1</code>从不在单个<code>tasktracker</code>上运行小作业。这样的作业称为<code>uberized</code>，或者作为<code>uber</code>任务运行。</p>\n<p>哪些任务是小任务呢？ 默认情况下，小任务就是小于10个<code>mapper</code>且只有1个<code>reducer</code>且输入大小小于一个<code>HDFS</code>块的任务。(通过设置<code>mapreduce.job.ubertask.maxmaps</code>、<code>mapreduce.job.ubertask.maxreduces</code>和<code>mapreduce.job.ubertask.maxbytes</code>可以改变一个作业的上述值。)将<code>mapreduce.job.ubertask.enable</code>设置为<code>false</code>也可以完全使<code>uber</code>任务不可用。</p>\n<p>在任何任务运行之前，作业的<code>setup</code>方法为了设置作业的<code>OutputCommitter</code>被调用来建立作业的输出目录。在<code>MapReduce 1</code>中，它在一个由<code>tasktracker</code>运行的特殊任务中被调用，而在<code>YARN</code>执行框架中，该方法由应用程序master直接调用。</p>\n<h3 id=\"4-任务分配\"><a href=\"#4-任务分配\" class=\"headerlink\" title=\"4. 任务分配\"></a>4. 任务分配</h3><p>如果作业不适合作为<code>uber</code>任务运行，那么<code>ApplicationMaster</code>就会为该作业中的所有<code>map</code>任务和<code>reduce</code>任务向资源管理器请求容器(参见上图步骤8)。附着心跳信息的请求包括每个<code>map</code>任务的数据本地化信息，特别是输入分片所在的主机和相应机架信息。调度器使用这些信息来做调度策略(像<code>jobtracker</code>的调度器一样)。理想情况下，它将任务分配到数据本地化的节点，但如果不可能这样做，调度器就会相对于非本地化的分配有限使用机架本地化的分配。</p>\n<p>请求也为任务指定了内存需求。在默认情况下，<code>map</code>任务和<code>reduce</code>任务都分配到<code>1024MB</code>的内存，但这可以通过<code>mapreduce.map.memory.mb</code>和<code>mapreduce.reduce.memory.mb</code>来设置。</p>\n<p>内存的分配方式不同于<code>MapReduce 1</code>，后者中<code>tasktrackers</code>有在集群配置时设置的固定数量的槽，每个任务在一个槽上运行。槽有最大内存分配限制，这对集群是固定的，导致当任务使用较少内存时无法充分利用内存(因为其他等待的任务不能使用这些未使用的内存)以及由于任务不能获取足够内存而导致作业失败。</p>\n<p>在<code>YARN</code>中，资源划分更细的粒度，所以可以避免上述问题。具体而言，应用程序可以请求最小到最大限制范围内的任意最小值倍数的内存容量。默认的内存分配容量是调度器特定的，对于容量调度器，它的默认值最小值是<code>1024MB</code>(由 <code>yarn.sheduler.capacity.minimum-allocation-mb</code>设置)，默认的最大值是<code>10240MB</code>(由<code>yarn.sheduler.capacity.maximum-allocation-mb</code>设置)。因此，任务可以通过适当设置<code>mapreduce.map.memory.mb</code>和 <code>mapreduce.reduce.memory.mb</code>来请求<code>1GB</code>到<code>10GB</code>间的任务<code>1GB</code>倍数的内存容量(调度器在需要的时候使用最接近的倍数)。</p>\n<h3 id=\"5-任务执行\"><a href=\"#5-任务执行\" class=\"headerlink\" title=\"5. 任务执行\"></a>5. 任务执行</h3><p>一旦资源管理器的调度器为任务分配了容器，<code>ApplicationMaster</code>就通过与节点管理器通信来启动容器(参见上图步骤9a和9b)。该任务由主类<code>YarnChild</code>的<code>Java</code>应用程序执行，在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、<code>JAR</code>文件和所有来自分布式缓存的文件(参见上图步骤10)。最后，运行<code>map</code>任务或<code>reduce</code>任务(参见上图步骤11)。</p>\n<h3 id=\"6-进度和状态更新\"><a href=\"#6-进度和状态更新\" class=\"headerlink\" title=\"6. 进度和状态更新\"></a>6. 进度和状态更新</h3><p>在<code>YARN</code>下运行时，任务每三秒钟通过<code>umbilical</code>接口向<code>ApplicationMaster</code>汇报进度和状态(包含计数器)，作为作业的汇聚视图(<code>aggregate view</code>)。相比之下，<code>MapReduce 1</code>通过<code>tasktracker</code>到<code>jobtracker</code>来实现进度更新。</p>\n<p>客户端每秒钟(通过<code>mapreduce.client.progressmonitor.pollinterval</code>设置)查询一次<code>ApplicationMaster</code>以接收进度更新，通常都会向用户显示。</p>\n<h3 id=\"7-作业完成\"><a href=\"#7-作业完成\" class=\"headerlink\" title=\"7. 作业完成\"></a>7. 作业完成</h3><p>除了向<code>ApplicationMaster</code>查询进度外，客户端每5秒钟通过调用<code>Job</code>的<code>waitForCompletion()</code>来检查作业是否完成。查询的间隔可以通过<code>mapreduce.client.completion.pollinterval</code>属性进行设置。</p>\n<p>作业完成后，<code>ApplicationMaster</code>和任务容器清理其工作状态，<code>OutputCommitter</code>的作业清理方法会被调用。作业历史服务器保存作业的信息供用户需要时查询。</p>\n<p>来源于: <code>Hadoop 权威指南</code></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop MapReduce1 工作原理","date":"2017-12-14T05:03:01.000Z","head-img":"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce1.0%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true","_content":"\n下面解释一下作业在经典的`MapReduce 1.0`中运行的工作原理。最顶层包含4个独立的实体:\n\n- 客户端，提交`MapReduce`作业。\n- `JobTracker`，协调作业的运行。`JobTracker`是一个Java应用程序，它的主类是`JobTracker`。\n- `TaskTracker`，运行作业划分后的任务。`TaskTracker`是一个Java应用程序，它的主类是`TaskTracker`。\n- 分布式文件系统(一般为`HDFS`)，用来在其他实体间共享作业文件。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce1.0%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true)\n\n### 1. 作业提交\n\n`Job`的`submit()`方法创建一个内部的`JobSunmmiter`实例，并且调用其`submitJobInternal()`方法。提交作业后，`waitForCompletion()`每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功，就显示作业计数器。如果失败，导致作业失败的错误被记录到控制台。\n\n`JobSunmmiter`所实现的作业提交过程如下:\n\n(1) 向`jobtracker`请求一个新的作业ID(通过调用`JobTracker`的`getNewJobId()`方法获取）。参见上图步骤2。\n\n(2) 检查作业的输出说明。例如，如果没有指定输出目录或输出目录已经存在，作业就不提交，错误抛回给`MapReduce`程序。\n\n(3) 计算作业的输入分片。如果分片无法计算，比如因为输入路径不存在，作业不提交，错误返回给`MapReduce`程序。\n\n(4) 将运行作业所需要的资源(包括作业`JAR`文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的目录下`jobtracker`的文件系统中。作业`JAR`中副本较多(由`mapred.submit.replication`属性控制，默认值为10)，因此在运行作业的任务时，集群中有很多个副本可供`tasktracker`访问。参见上图步骤3.\n\n(5) 告知`jobtracker`作业准备执行(通过调用`JobTracker`的`submitJob()`方法实现)。参见上图步骤4.\n\n### 2. 作业初始化\n\n当`JobTracker`接收到其`submitJob()`方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度，并对其进行初始化。初始化包括创建一个表示正在运行作业的对象，用于封装任务和记录信息，以便跟踪任务的状态和进程(参见上图步骤 5)。\n\n为了创建任务运行列表，作业调度器首先从共享文件系统中获取`JobClient`已计算好的输入分片信息(参见上图步骤6)。然后为每个分片创建一个`Map`任务。创建的`Reduce`任务的数量由`JobConf`的`mapred.reduce.task`属性决定，它是用`setNumReduceTasks()`方法来设置的，然后调度器创建相应数量的要运行的`reduce`任务。任务在此时被指定ID。\n\n除了`Map`任务和`Reduce`任务，还会创建两个任务:作业创建和作业清理。这两个任务在`TaskTracker`中执行，在`Map`任务之前运行代码来创建作业，并且在所有`Reduce`任务完成之后完成清理工作。配置项`OutputCommitter`属性能设置运行的代码。默认值是`FileOutputCommitter`。作业创建为作业创建输出路径和临时工作空间。作业清理清除作业运行过程中的临时目录。\n\n### 3. 任务分配\n\n`tasktracker`运行一个简单的循环来定期发送`心跳`(`heartbeat`)给`jobtracker`。`心跳`向`jobtracker`表明`tasktracker`是否还存活，同时也充当两者之间的消息通道。作为`心跳`的一部分，`tasktracker`会指明它是否已经准备好运行新的任务，如果是，`jobtracker`会为它分配一个任务，并使用`心跳`的返回值与`tasktracker`进行通信(参见上图步骤7)。\n\n在`jobtracker`为`tasktracker`选择任务之前，`jobtracker`必须先选定任务所在的作业。一旦选择好作业，`jobtracker`就可以为该作业选定一个任务。\n\n对于`Map`任务和`Reduce`任务，`tasktracker`有固定数据的任务槽。例如，一个`tasktracker`可能可以同时运行两个`Map`任务和两个`Reduce`任务。准确数据由`tasktracker`核的数据和内存大小来决定。默认调度器在处理`Reduce`任务槽之前，会填满空闲的`Map`任务槽，因此，如果`tasktracker`至少有一个空闲的`Map`任务槽，`jobtracker`会为它选择一个`Map`任务，否则选择一个`Reduce`任务。\n\n为了选择一个`Reduce`任务，`jobtracker`简单地从待运行的`Reduce`任务列表中选取下一个来执行，用不着考虑数据的本地化。然而，对于一个`Map`任务，`jobtracker`会考虑`tasktracker`的网络设置，并选取一个距离其输入分片最近的`tasktracker`。在最理想的情况下， 任务是数据本地化的(`data-local`)，也就是任务运行在输入分片所在的节点上。同样，任务也可能是机架本地化的(`rack-local`)：任务和输入分片在所同一机架，但不在同一节点上。一些任务即不是数据本地化的，也不是机架本地化的，而是从与它们自身运行的不同机架上检索数据。可以通过查看作业的计数器得知每类任务的比例。\n\n### 4. 任务执行\n\n现在，`tasktracker`已经被分配了一个任务，下一步是运行该任务。第一步，通过从共享文件系统把作业的`JAR`文件复制到`tasktracker`所在的文件系统，从而实现作业的`JAR`文件本地化。同时，`tasktracker`将应用程序所需要的全部文件从分布式缓存复制到本地磁盘(参见上图步骤8)。第二步，`tasktracker`为任务新建一个本地工作目录，并把`JAR`文件中的内容解压到这个文件夹下。第三步，`tasktracker`新建一个`TaskRunner`实例来运行该任务。\n\n`TaskRunner`启动一个新的`JVM`(参见上图步骤9)来运行每个任务(参见上图步骤10)，以便用户定义的`map`和`reduce`函数的任务软件问题都不会影响到`tasktracker`(例如导致崩溃或挂起等）。但在不同的任务之前重用`JVM`还是可能的。\n\n### 5. 进度和状态的更新\n\nMapReduce作业是长时间运行的批处理作业，运行时间范围从数分钟到数小时。这是一个很长的时间段，所以对于用户而言，能够的制作也进展是很重要的。一个作业和它的每个任务都有一个状态(status)，包括:作业或任务的状态(比如，运行状态，成功完成，失败状态)，`map`和`reduce`的进度，作业计数器的值，状态消息或描述。\n\n### 6. 作业完成\n\n当`jobtracker`收到作业最后一个任务已完成的通知后(这是一个特定的作业清理任务)，便把作业的状态设置为'成功'。然后，在`Job`查询状态时，便知道任务以成功完成，于是`Job`打印一条消息告知用户，然后从`waitForCompletion()`方法返回。`Job`的统计信息和计数值也在这是输出到控制台。\n\n最后，`jobtracker`清空作业的工作状态，只是`tasktracker`也清空作业的工作状态(如删除中间输出)。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n来源于: `Hadoop 权威指南`\n","source":"_posts/Hadoop/Hadoop MapReduce1.0 工作原理.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop MapReduce1 工作原理\ndate: 2017-12-14 13:03:01\nhead-img: https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce1.0%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n下面解释一下作业在经典的`MapReduce 1.0`中运行的工作原理。最顶层包含4个独立的实体:\n\n- 客户端，提交`MapReduce`作业。\n- `JobTracker`，协调作业的运行。`JobTracker`是一个Java应用程序，它的主类是`JobTracker`。\n- `TaskTracker`，运行作业划分后的任务。`TaskTracker`是一个Java应用程序，它的主类是`TaskTracker`。\n- 分布式文件系统(一般为`HDFS`)，用来在其他实体间共享作业文件。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce1.0%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true)\n\n### 1. 作业提交\n\n`Job`的`submit()`方法创建一个内部的`JobSunmmiter`实例，并且调用其`submitJobInternal()`方法。提交作业后，`waitForCompletion()`每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功，就显示作业计数器。如果失败，导致作业失败的错误被记录到控制台。\n\n`JobSunmmiter`所实现的作业提交过程如下:\n\n(1) 向`jobtracker`请求一个新的作业ID(通过调用`JobTracker`的`getNewJobId()`方法获取）。参见上图步骤2。\n\n(2) 检查作业的输出说明。例如，如果没有指定输出目录或输出目录已经存在，作业就不提交，错误抛回给`MapReduce`程序。\n\n(3) 计算作业的输入分片。如果分片无法计算，比如因为输入路径不存在，作业不提交，错误返回给`MapReduce`程序。\n\n(4) 将运行作业所需要的资源(包括作业`JAR`文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的目录下`jobtracker`的文件系统中。作业`JAR`中副本较多(由`mapred.submit.replication`属性控制，默认值为10)，因此在运行作业的任务时，集群中有很多个副本可供`tasktracker`访问。参见上图步骤3.\n\n(5) 告知`jobtracker`作业准备执行(通过调用`JobTracker`的`submitJob()`方法实现)。参见上图步骤4.\n\n### 2. 作业初始化\n\n当`JobTracker`接收到其`submitJob()`方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度，并对其进行初始化。初始化包括创建一个表示正在运行作业的对象，用于封装任务和记录信息，以便跟踪任务的状态和进程(参见上图步骤 5)。\n\n为了创建任务运行列表，作业调度器首先从共享文件系统中获取`JobClient`已计算好的输入分片信息(参见上图步骤6)。然后为每个分片创建一个`Map`任务。创建的`Reduce`任务的数量由`JobConf`的`mapred.reduce.task`属性决定，它是用`setNumReduceTasks()`方法来设置的，然后调度器创建相应数量的要运行的`reduce`任务。任务在此时被指定ID。\n\n除了`Map`任务和`Reduce`任务，还会创建两个任务:作业创建和作业清理。这两个任务在`TaskTracker`中执行，在`Map`任务之前运行代码来创建作业，并且在所有`Reduce`任务完成之后完成清理工作。配置项`OutputCommitter`属性能设置运行的代码。默认值是`FileOutputCommitter`。作业创建为作业创建输出路径和临时工作空间。作业清理清除作业运行过程中的临时目录。\n\n### 3. 任务分配\n\n`tasktracker`运行一个简单的循环来定期发送`心跳`(`heartbeat`)给`jobtracker`。`心跳`向`jobtracker`表明`tasktracker`是否还存活，同时也充当两者之间的消息通道。作为`心跳`的一部分，`tasktracker`会指明它是否已经准备好运行新的任务，如果是，`jobtracker`会为它分配一个任务，并使用`心跳`的返回值与`tasktracker`进行通信(参见上图步骤7)。\n\n在`jobtracker`为`tasktracker`选择任务之前，`jobtracker`必须先选定任务所在的作业。一旦选择好作业，`jobtracker`就可以为该作业选定一个任务。\n\n对于`Map`任务和`Reduce`任务，`tasktracker`有固定数据的任务槽。例如，一个`tasktracker`可能可以同时运行两个`Map`任务和两个`Reduce`任务。准确数据由`tasktracker`核的数据和内存大小来决定。默认调度器在处理`Reduce`任务槽之前，会填满空闲的`Map`任务槽，因此，如果`tasktracker`至少有一个空闲的`Map`任务槽，`jobtracker`会为它选择一个`Map`任务，否则选择一个`Reduce`任务。\n\n为了选择一个`Reduce`任务，`jobtracker`简单地从待运行的`Reduce`任务列表中选取下一个来执行，用不着考虑数据的本地化。然而，对于一个`Map`任务，`jobtracker`会考虑`tasktracker`的网络设置，并选取一个距离其输入分片最近的`tasktracker`。在最理想的情况下， 任务是数据本地化的(`data-local`)，也就是任务运行在输入分片所在的节点上。同样，任务也可能是机架本地化的(`rack-local`)：任务和输入分片在所同一机架，但不在同一节点上。一些任务即不是数据本地化的，也不是机架本地化的，而是从与它们自身运行的不同机架上检索数据。可以通过查看作业的计数器得知每类任务的比例。\n\n### 4. 任务执行\n\n现在，`tasktracker`已经被分配了一个任务，下一步是运行该任务。第一步，通过从共享文件系统把作业的`JAR`文件复制到`tasktracker`所在的文件系统，从而实现作业的`JAR`文件本地化。同时，`tasktracker`将应用程序所需要的全部文件从分布式缓存复制到本地磁盘(参见上图步骤8)。第二步，`tasktracker`为任务新建一个本地工作目录，并把`JAR`文件中的内容解压到这个文件夹下。第三步，`tasktracker`新建一个`TaskRunner`实例来运行该任务。\n\n`TaskRunner`启动一个新的`JVM`(参见上图步骤9)来运行每个任务(参见上图步骤10)，以便用户定义的`map`和`reduce`函数的任务软件问题都不会影响到`tasktracker`(例如导致崩溃或挂起等）。但在不同的任务之前重用`JVM`还是可能的。\n\n### 5. 进度和状态的更新\n\nMapReduce作业是长时间运行的批处理作业，运行时间范围从数分钟到数小时。这是一个很长的时间段，所以对于用户而言，能够的制作也进展是很重要的。一个作业和它的每个任务都有一个状态(status)，包括:作业或任务的状态(比如，运行状态，成功完成，失败状态)，`map`和`reduce`的进度，作业计数器的值，状态消息或描述。\n\n### 6. 作业完成\n\n当`jobtracker`收到作业最后一个任务已完成的通知后(这是一个特定的作业清理任务)，便把作业的状态设置为'成功'。然后，在`Job`查询状态时，便知道任务以成功完成，于是`Job`打印一条消息告知用户，然后从`waitForCompletion()`方法返回。`Job`的统计信息和计数值也在这是输出到控制台。\n\n最后，`jobtracker`清空作业的工作状态，只是`tasktracker`也清空作业的工作状态(如删除中间输出)。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n来源于: `Hadoop 权威指南`\n","slug":"Hadoop/Hadoop MapReduce1.0 工作原理","published":1,"updated":"2018-01-29T09:36:59.637Z","comments":1,"photos":[],"link":"","_id":"cje58tis1002gordbqh7saefo","content":"<p>下面解释一下作业在经典的<code>MapReduce 1.0</code>中运行的工作原理。最顶层包含4个独立的实体:</p>\n<ul>\n<li>客户端，提交<code>MapReduce</code>作业。</li>\n<li><code>JobTracker</code>，协调作业的运行。<code>JobTracker</code>是一个Java应用程序，它的主类是<code>JobTracker</code>。</li>\n<li><code>TaskTracker</code>，运行作业划分后的任务。<code>TaskTracker</code>是一个Java应用程序，它的主类是<code>TaskTracker</code>。</li>\n<li>分布式文件系统(一般为<code>HDFS</code>)，用来在其他实体间共享作业文件。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce1.0%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"1-作业提交\"><a href=\"#1-作业提交\" class=\"headerlink\" title=\"1. 作业提交\"></a>1. 作业提交</h3><p><code>Job</code>的<code>submit()</code>方法创建一个内部的<code>JobSunmmiter</code>实例，并且调用其<code>submitJobInternal()</code>方法。提交作业后，<code>waitForCompletion()</code>每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功，就显示作业计数器。如果失败，导致作业失败的错误被记录到控制台。</p>\n<p><code>JobSunmmiter</code>所实现的作业提交过程如下:</p>\n<p>(1) 向<code>jobtracker</code>请求一个新的作业ID(通过调用<code>JobTracker</code>的<code>getNewJobId()</code>方法获取）。参见上图步骤2。</p>\n<p>(2) 检查作业的输出说明。例如，如果没有指定输出目录或输出目录已经存在，作业就不提交，错误抛回给<code>MapReduce</code>程序。</p>\n<p>(3) 计算作业的输入分片。如果分片无法计算，比如因为输入路径不存在，作业不提交，错误返回给<code>MapReduce</code>程序。</p>\n<p>(4) 将运行作业所需要的资源(包括作业<code>JAR</code>文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的目录下<code>jobtracker</code>的文件系统中。作业<code>JAR</code>中副本较多(由<code>mapred.submit.replication</code>属性控制，默认值为10)，因此在运行作业的任务时，集群中有很多个副本可供<code>tasktracker</code>访问。参见上图步骤3.</p>\n<p>(5) 告知<code>jobtracker</code>作业准备执行(通过调用<code>JobTracker</code>的<code>submitJob()</code>方法实现)。参见上图步骤4.</p>\n<h3 id=\"2-作业初始化\"><a href=\"#2-作业初始化\" class=\"headerlink\" title=\"2. 作业初始化\"></a>2. 作业初始化</h3><p>当<code>JobTracker</code>接收到其<code>submitJob()</code>方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度，并对其进行初始化。初始化包括创建一个表示正在运行作业的对象，用于封装任务和记录信息，以便跟踪任务的状态和进程(参见上图步骤 5)。</p>\n<p>为了创建任务运行列表，作业调度器首先从共享文件系统中获取<code>JobClient</code>已计算好的输入分片信息(参见上图步骤6)。然后为每个分片创建一个<code>Map</code>任务。创建的<code>Reduce</code>任务的数量由<code>JobConf</code>的<code>mapred.reduce.task</code>属性决定，它是用<code>setNumReduceTasks()</code>方法来设置的，然后调度器创建相应数量的要运行的<code>reduce</code>任务。任务在此时被指定ID。</p>\n<p>除了<code>Map</code>任务和<code>Reduce</code>任务，还会创建两个任务:作业创建和作业清理。这两个任务在<code>TaskTracker</code>中执行，在<code>Map</code>任务之前运行代码来创建作业，并且在所有<code>Reduce</code>任务完成之后完成清理工作。配置项<code>OutputCommitter</code>属性能设置运行的代码。默认值是<code>FileOutputCommitter</code>。作业创建为作业创建输出路径和临时工作空间。作业清理清除作业运行过程中的临时目录。</p>\n<h3 id=\"3-任务分配\"><a href=\"#3-任务分配\" class=\"headerlink\" title=\"3. 任务分配\"></a>3. 任务分配</h3><p><code>tasktracker</code>运行一个简单的循环来定期发送<code>心跳</code>(<code>heartbeat</code>)给<code>jobtracker</code>。<code>心跳</code>向<code>jobtracker</code>表明<code>tasktracker</code>是否还存活，同时也充当两者之间的消息通道。作为<code>心跳</code>的一部分，<code>tasktracker</code>会指明它是否已经准备好运行新的任务，如果是，<code>jobtracker</code>会为它分配一个任务，并使用<code>心跳</code>的返回值与<code>tasktracker</code>进行通信(参见上图步骤7)。</p>\n<p>在<code>jobtracker</code>为<code>tasktracker</code>选择任务之前，<code>jobtracker</code>必须先选定任务所在的作业。一旦选择好作业，<code>jobtracker</code>就可以为该作业选定一个任务。</p>\n<p>对于<code>Map</code>任务和<code>Reduce</code>任务，<code>tasktracker</code>有固定数据的任务槽。例如，一个<code>tasktracker</code>可能可以同时运行两个<code>Map</code>任务和两个<code>Reduce</code>任务。准确数据由<code>tasktracker</code>核的数据和内存大小来决定。默认调度器在处理<code>Reduce</code>任务槽之前，会填满空闲的<code>Map</code>任务槽，因此，如果<code>tasktracker</code>至少有一个空闲的<code>Map</code>任务槽，<code>jobtracker</code>会为它选择一个<code>Map</code>任务，否则选择一个<code>Reduce</code>任务。</p>\n<p>为了选择一个<code>Reduce</code>任务，<code>jobtracker</code>简单地从待运行的<code>Reduce</code>任务列表中选取下一个来执行，用不着考虑数据的本地化。然而，对于一个<code>Map</code>任务，<code>jobtracker</code>会考虑<code>tasktracker</code>的网络设置，并选取一个距离其输入分片最近的<code>tasktracker</code>。在最理想的情况下， 任务是数据本地化的(<code>data-local</code>)，也就是任务运行在输入分片所在的节点上。同样，任务也可能是机架本地化的(<code>rack-local</code>)：任务和输入分片在所同一机架，但不在同一节点上。一些任务即不是数据本地化的，也不是机架本地化的，而是从与它们自身运行的不同机架上检索数据。可以通过查看作业的计数器得知每类任务的比例。</p>\n<h3 id=\"4-任务执行\"><a href=\"#4-任务执行\" class=\"headerlink\" title=\"4. 任务执行\"></a>4. 任务执行</h3><p>现在，<code>tasktracker</code>已经被分配了一个任务，下一步是运行该任务。第一步，通过从共享文件系统把作业的<code>JAR</code>文件复制到<code>tasktracker</code>所在的文件系统，从而实现作业的<code>JAR</code>文件本地化。同时，<code>tasktracker</code>将应用程序所需要的全部文件从分布式缓存复制到本地磁盘(参见上图步骤8)。第二步，<code>tasktracker</code>为任务新建一个本地工作目录，并把<code>JAR</code>文件中的内容解压到这个文件夹下。第三步，<code>tasktracker</code>新建一个<code>TaskRunner</code>实例来运行该任务。</p>\n<p><code>TaskRunner</code>启动一个新的<code>JVM</code>(参见上图步骤9)来运行每个任务(参见上图步骤10)，以便用户定义的<code>map</code>和<code>reduce</code>函数的任务软件问题都不会影响到<code>tasktracker</code>(例如导致崩溃或挂起等）。但在不同的任务之前重用<code>JVM</code>还是可能的。</p>\n<h3 id=\"5-进度和状态的更新\"><a href=\"#5-进度和状态的更新\" class=\"headerlink\" title=\"5. 进度和状态的更新\"></a>5. 进度和状态的更新</h3><p>MapReduce作业是长时间运行的批处理作业，运行时间范围从数分钟到数小时。这是一个很长的时间段，所以对于用户而言，能够的制作也进展是很重要的。一个作业和它的每个任务都有一个状态(status)，包括:作业或任务的状态(比如，运行状态，成功完成，失败状态)，<code>map</code>和<code>reduce</code>的进度，作业计数器的值，状态消息或描述。</p>\n<h3 id=\"6-作业完成\"><a href=\"#6-作业完成\" class=\"headerlink\" title=\"6. 作业完成\"></a>6. 作业完成</h3><p>当<code>jobtracker</code>收到作业最后一个任务已完成的通知后(这是一个特定的作业清理任务)，便把作业的状态设置为’成功’。然后，在<code>Job</code>查询状态时，便知道任务以成功完成，于是<code>Job</code>打印一条消息告知用户，然后从<code>waitForCompletion()</code>方法返回。<code>Job</code>的统计信息和计数值也在这是输出到控制台。</p>\n<p>最后，<code>jobtracker</code>清空作业的工作状态，只是<code>tasktracker</code>也清空作业的工作状态(如删除中间输出)。</p>\n<p>来源于: <code>Hadoop 权威指南</code></p>\n","site":{"data":{}},"excerpt":"","more":"<p>下面解释一下作业在经典的<code>MapReduce 1.0</code>中运行的工作原理。最顶层包含4个独立的实体:</p>\n<ul>\n<li>客户端，提交<code>MapReduce</code>作业。</li>\n<li><code>JobTracker</code>，协调作业的运行。<code>JobTracker</code>是一个Java应用程序，它的主类是<code>JobTracker</code>。</li>\n<li><code>TaskTracker</code>，运行作业划分后的任务。<code>TaskTracker</code>是一个Java应用程序，它的主类是<code>TaskTracker</code>。</li>\n<li>分布式文件系统(一般为<code>HDFS</code>)，用来在其他实体间共享作业文件。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce1.0%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"1-作业提交\"><a href=\"#1-作业提交\" class=\"headerlink\" title=\"1. 作业提交\"></a>1. 作业提交</h3><p><code>Job</code>的<code>submit()</code>方法创建一个内部的<code>JobSunmmiter</code>实例，并且调用其<code>submitJobInternal()</code>方法。提交作业后，<code>waitForCompletion()</code>每秒轮询作业的进度，如果发现自上次报告后有改变，便把进度报告到控制台。作业完成后，如果成功，就显示作业计数器。如果失败，导致作业失败的错误被记录到控制台。</p>\n<p><code>JobSunmmiter</code>所实现的作业提交过程如下:</p>\n<p>(1) 向<code>jobtracker</code>请求一个新的作业ID(通过调用<code>JobTracker</code>的<code>getNewJobId()</code>方法获取）。参见上图步骤2。</p>\n<p>(2) 检查作业的输出说明。例如，如果没有指定输出目录或输出目录已经存在，作业就不提交，错误抛回给<code>MapReduce</code>程序。</p>\n<p>(3) 计算作业的输入分片。如果分片无法计算，比如因为输入路径不存在，作业不提交，错误返回给<code>MapReduce</code>程序。</p>\n<p>(4) 将运行作业所需要的资源(包括作业<code>JAR</code>文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的目录下<code>jobtracker</code>的文件系统中。作业<code>JAR</code>中副本较多(由<code>mapred.submit.replication</code>属性控制，默认值为10)，因此在运行作业的任务时，集群中有很多个副本可供<code>tasktracker</code>访问。参见上图步骤3.</p>\n<p>(5) 告知<code>jobtracker</code>作业准备执行(通过调用<code>JobTracker</code>的<code>submitJob()</code>方法实现)。参见上图步骤4.</p>\n<h3 id=\"2-作业初始化\"><a href=\"#2-作业初始化\" class=\"headerlink\" title=\"2. 作业初始化\"></a>2. 作业初始化</h3><p>当<code>JobTracker</code>接收到其<code>submitJob()</code>方法的调用后，会把此调用放入一个内部队列中，交由作业调度器(job scheduler)进行调度，并对其进行初始化。初始化包括创建一个表示正在运行作业的对象，用于封装任务和记录信息，以便跟踪任务的状态和进程(参见上图步骤 5)。</p>\n<p>为了创建任务运行列表，作业调度器首先从共享文件系统中获取<code>JobClient</code>已计算好的输入分片信息(参见上图步骤6)。然后为每个分片创建一个<code>Map</code>任务。创建的<code>Reduce</code>任务的数量由<code>JobConf</code>的<code>mapred.reduce.task</code>属性决定，它是用<code>setNumReduceTasks()</code>方法来设置的，然后调度器创建相应数量的要运行的<code>reduce</code>任务。任务在此时被指定ID。</p>\n<p>除了<code>Map</code>任务和<code>Reduce</code>任务，还会创建两个任务:作业创建和作业清理。这两个任务在<code>TaskTracker</code>中执行，在<code>Map</code>任务之前运行代码来创建作业，并且在所有<code>Reduce</code>任务完成之后完成清理工作。配置项<code>OutputCommitter</code>属性能设置运行的代码。默认值是<code>FileOutputCommitter</code>。作业创建为作业创建输出路径和临时工作空间。作业清理清除作业运行过程中的临时目录。</p>\n<h3 id=\"3-任务分配\"><a href=\"#3-任务分配\" class=\"headerlink\" title=\"3. 任务分配\"></a>3. 任务分配</h3><p><code>tasktracker</code>运行一个简单的循环来定期发送<code>心跳</code>(<code>heartbeat</code>)给<code>jobtracker</code>。<code>心跳</code>向<code>jobtracker</code>表明<code>tasktracker</code>是否还存活，同时也充当两者之间的消息通道。作为<code>心跳</code>的一部分，<code>tasktracker</code>会指明它是否已经准备好运行新的任务，如果是，<code>jobtracker</code>会为它分配一个任务，并使用<code>心跳</code>的返回值与<code>tasktracker</code>进行通信(参见上图步骤7)。</p>\n<p>在<code>jobtracker</code>为<code>tasktracker</code>选择任务之前，<code>jobtracker</code>必须先选定任务所在的作业。一旦选择好作业，<code>jobtracker</code>就可以为该作业选定一个任务。</p>\n<p>对于<code>Map</code>任务和<code>Reduce</code>任务，<code>tasktracker</code>有固定数据的任务槽。例如，一个<code>tasktracker</code>可能可以同时运行两个<code>Map</code>任务和两个<code>Reduce</code>任务。准确数据由<code>tasktracker</code>核的数据和内存大小来决定。默认调度器在处理<code>Reduce</code>任务槽之前，会填满空闲的<code>Map</code>任务槽，因此，如果<code>tasktracker</code>至少有一个空闲的<code>Map</code>任务槽，<code>jobtracker</code>会为它选择一个<code>Map</code>任务，否则选择一个<code>Reduce</code>任务。</p>\n<p>为了选择一个<code>Reduce</code>任务，<code>jobtracker</code>简单地从待运行的<code>Reduce</code>任务列表中选取下一个来执行，用不着考虑数据的本地化。然而，对于一个<code>Map</code>任务，<code>jobtracker</code>会考虑<code>tasktracker</code>的网络设置，并选取一个距离其输入分片最近的<code>tasktracker</code>。在最理想的情况下， 任务是数据本地化的(<code>data-local</code>)，也就是任务运行在输入分片所在的节点上。同样，任务也可能是机架本地化的(<code>rack-local</code>)：任务和输入分片在所同一机架，但不在同一节点上。一些任务即不是数据本地化的，也不是机架本地化的，而是从与它们自身运行的不同机架上检索数据。可以通过查看作业的计数器得知每类任务的比例。</p>\n<h3 id=\"4-任务执行\"><a href=\"#4-任务执行\" class=\"headerlink\" title=\"4. 任务执行\"></a>4. 任务执行</h3><p>现在，<code>tasktracker</code>已经被分配了一个任务，下一步是运行该任务。第一步，通过从共享文件系统把作业的<code>JAR</code>文件复制到<code>tasktracker</code>所在的文件系统，从而实现作业的<code>JAR</code>文件本地化。同时，<code>tasktracker</code>将应用程序所需要的全部文件从分布式缓存复制到本地磁盘(参见上图步骤8)。第二步，<code>tasktracker</code>为任务新建一个本地工作目录，并把<code>JAR</code>文件中的内容解压到这个文件夹下。第三步，<code>tasktracker</code>新建一个<code>TaskRunner</code>实例来运行该任务。</p>\n<p><code>TaskRunner</code>启动一个新的<code>JVM</code>(参见上图步骤9)来运行每个任务(参见上图步骤10)，以便用户定义的<code>map</code>和<code>reduce</code>函数的任务软件问题都不会影响到<code>tasktracker</code>(例如导致崩溃或挂起等）。但在不同的任务之前重用<code>JVM</code>还是可能的。</p>\n<h3 id=\"5-进度和状态的更新\"><a href=\"#5-进度和状态的更新\" class=\"headerlink\" title=\"5. 进度和状态的更新\"></a>5. 进度和状态的更新</h3><p>MapReduce作业是长时间运行的批处理作业，运行时间范围从数分钟到数小时。这是一个很长的时间段，所以对于用户而言，能够的制作也进展是很重要的。一个作业和它的每个任务都有一个状态(status)，包括:作业或任务的状态(比如，运行状态，成功完成，失败状态)，<code>map</code>和<code>reduce</code>的进度，作业计数器的值，状态消息或描述。</p>\n<h3 id=\"6-作业完成\"><a href=\"#6-作业完成\" class=\"headerlink\" title=\"6. 作业完成\"></a>6. 作业完成</h3><p>当<code>jobtracker</code>收到作业最后一个任务已完成的通知后(这是一个特定的作业清理任务)，便把作业的状态设置为’成功’。然后，在<code>Job</code>查询状态时，便知道任务以成功完成，于是<code>Job</code>打印一条消息告知用户，然后从<code>waitForCompletion()</code>方法返回。<code>Job</code>的统计信息和计数值也在这是输出到控制台。</p>\n<p>最后，<code>jobtracker</code>清空作业的工作状态，只是<code>tasktracker</code>也清空作业的工作状态(如删除中间输出)。</p>\n<p>来源于: <code>Hadoop 权威指南</code></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop MapReduce2.0 架构详解","date":"2017-12-12T12:21:01.000Z","_content":"\n### 1. MapReduce 2.0 概述\n\n`Apache Hadoop 0.23`比以前的版本有了很大的改进。以下是`MapReduce`的一些亮点；请注意，`HDFS`也有一些主要的改进，这些都不在本文的讨论范围之内。\n\n`MapReduce 2.0`(又名`MRv2`或`YARN`)。新的架构将`JobTracker`的两个主要功能 - 资源管理和作业生命周期管理 - 分解成单独的组件：\n\n- 管理计算资源到应用程序的全局分配的`ResourceManager`(`RM`)。\n- 每个应用程序的`ApplicationMaster`(`AM`)，用于管理应用程序的生命周期。\n\n在`Hadoop 0.23`中，`MapReduce`应用程序是由`MapReduce` `ApplicationMaster`执行的`MapReduce`传统意义上的单一作业。\n\n每个节点上还有一个`NodeManager`(`NM`)，用于管理该节点上的用户进程。`RM`和`NM`构成集群的计算框架。该设计还允许在`NM`中运行长时间的辅助服务(The design also allows plugging long-running auxiliary services to the NM)；这些都是特定于应用程序的服务，作为配置的一部分进行指定，并在启动期间由`NM`加载。对于`YARN`上的`MapReduce`应用，`shuffle`是由`NM`加载的典型的辅助服务。请注意，在`Hadoop 0.23`版本之前，`shuffle`是`TaskTracker`的一部分。\n\n每个应用程序的`ApplicationMaster`是一个特定框架库，负责与`ResourceManager`协商资源，并与`NodeManager`一起来执行和监视这些任务。在`YARN`设计中，`MapReduce`只是一个应用程序框架， 该设计也可以允许使用其他框架来构建和部署分布式应用程序。例如，`Hadoop 0.23`附带了一个分布式`Shell`应用程序，允许在`YARN`集群上的多个节点上运行一个`shell`脚本。\n\n\n### 2. MapReduce 2.0 设计\n\n下图显示了一个`YARN`集群。只有一个资源管理器，它有两个主要的服务：\n- 可插拔的调度器，用于管理和实施集群中的资源调度策略。请注意，在编写本博文时，`Hadoop 0.23`中支持两个调度器，默认的`FIFO`调度器和`Capacity`调度器; `Fair`调度器尚未支持(译者注:博文2012编写，现在已经支持)。\n- `Applications Manager`(`AsM`)，负责管理集群中运行的`Application Masters`，例如，负责启动`Application Masters`，以及在发生故障时在不同节点上监视和重新启动`Application Masters`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-1.png?raw=true)\n\n上图还显示了在集群上的每个节点上都运行一个`NM`服务。该图还显示了有两个`AM`(`AM1`和`AM2`)。对于给定的任意`YARN`集群中，有多少个应用程序(作业)，就运行多少个`Application Masters`。每个`AM`管理应用程序的各个任务(启动，监视，以及在发生故障时重新启动任务)。该图还显示了`AM1`管理三个任务(容器1.1,1.2和1.3)，`AM2`管理四个任务(容器2.1,2.2,2.3和2.4)。每个任务运行在每个节点的`Container`中。在`AM`联系对应的`NM`来启动应用程序的各个任务之前，从`RM`的调度器中获取这些容器。这些容器可以大致与以前的`Hadoop`版本中的`Map`/`Reduce`插槽进行比较。然而，从集群利用角度来看，`Hadoop-0.23`中的资源分配模型更加优化。\n\n### 3. MapReduce 2.0 资源分配模型\n\n在较早的`Hadoop`版本中，集群中的每个节点都静态分配运行预定义数量的`Map`插槽和预定义数量的`Reduce`插槽的资源容量。插槽无法在`Map`和`Reduce`之间共享。这种静态分配槽的方式并不是最佳选择，因为在`MR`作业生命周期中槽的需求是不同的(典型地，当作业开始时对`Map`槽有需求，相反，对`Reduce`槽的需求是在最后)。实际上，在一个真正的集群中，作业是随机提交的，每个集群都有自己的`Map`/`Reduce`槽需求，对集群充分利用不是不可能，是非常难。\n\n`Hadoop 0.23`中的资源分配模型通过提供更灵活的资源建模来解决此类缺陷。以容器的形式请求资源，其中每个容器具有许多非静态属性。在写本博客时(2012年)，唯一支持的属性是内存(`RAM`)。然而，该模型是通用的，并且有意在将来的版本中添加更多的属性(例如CPU和网络带宽)。在这个新的资源管理模型中，每个属性只定义了最小值和最大值，并且`AM`可以请求具有这些最小值的倍数的属性值的容器。\n\n### 4. MapReduce 2.0 主要组件\n\n我们将详细介绍MapReduce架构的主要组件，以了解这些组件的功能以及它们如何交互的。\n\n#### 2.1 Client – Resource Manager\n\n下展示了在`YARN`集群上运行应用程序的初始步骤。通常，客户端与`RM`(特别是与`RM`的`Applications Manager`组件)通信来开启此步骤。图中标记为(1)的第一步是让客户端告诉`Applications Manager`我们提交应用程序的意愿，这是通过创建应用程序请求(`New Application Request`)完成的。标记为(2)的`RM`响应通常包含一个新生成的唯一应用程序ID，以及有关客户端在请求资源以运行应用程序`AM`时所需要的集群资源容量的信息。\n\n使用从`RM`接收到的响应信息，客户端可以构建并提交标记为(3)的应用程序提交上下文(`Application Submission Context`)，除了`RM`所需要来启动`AM`的信息之外，通常还包含诸如调度器队列，优先级和用户信息之类的信息。这些信息包含在容器启动上下文(`Container Launch Context`)中，还包含应用程序的`jar`，作业文件，安全令牌和任何需要的资源等。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-2.png?raw=true)\n\n在提交申请之后，客户端可以向`RM`查询应用程序报告，并接收返回的报告，并且如果需要，客户端也可以要求`RM`终止该应用程序。这三个步骤在下图中展示:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-3.png?raw=true)\n\n#### 2.2 Resource Manager – Application Master\n\n当`RM`从客户端接收到应用程序提交上下文时，会找到一个满足运行`AM`资源需求的可用容器，并与该容器的`NM`联系，以便在该节点上启动`AM`进程。下图描述了`AM`和`RM`(特别是与`RM`的调度器)之间的通信步骤。图中标记为(1)的第一步是`AM`将自己注册到`RM`中。这一步由一个握手过程组成，同时还传递了`AM`将要监听的`RPC`端口，监视应用程序状态和进度的跟踪URL等信息。\n\n标记为(2)的`RM`注册响应为`AM`传递一些基本信息，比如集群的最小和最大资源容量。`AM`将使用这些信息为各个任务的任何资源请求来计算和请求资源。标记为(3)的从`AM`到`RM`的资源分配请求主要包含所请求的容器列表，并且还可能包含该`AM`所释放的容器列表。心跳和进度信息也可以通过资源分配请求进行传达，如箭头(4)所示。\n\n当`RM`的调度器接收到资源分配请求时，它基于调度策略计算满足该请求的容器列表，并且返回分配响应，标记为(5)，其中包含分配的资源列表。使用资源列表，`AM`开始联系相关联的`NM`(很快就会看到)，最后，如箭头(6)所示，当作业完成时，`AM`向`RM`发送应用完成的消息并退出。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-4.png?raw=true)\n\n#### 2.3 Application Master – Container Manager\n\n下图描述了`AM`和`Node Manager`之间的通信。`AM`为每个容器请求`NM`来启动它，如图中箭头(1)所示。在容器运行时，`AM`可以分别请求并接收容器状态报告，如步骤(2)和(3)所示。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-5.png?raw=true)\n\n基于以上讨论，编写`YARN`应用程序的开发人员应主要关注以下接口：\n- `ClientRMProtocol`：`Client` `RM`(图3)。这是客户端与`RM`进行通信以启动新的应用程序(即AM)，检查应用程序状态或终止应用程序的协议。\n- `AMRMProtocol`：`AM` `RM`(图4)。这是`AM`用来向`RM`注册或取消注册，以及从`RM`调度器请求资源来运行任务的协议。\n- `ContainerManager`：`AM` `NM`(图5)。这是`AM`用来与`NM`进行通信以启动或停止容器以及获取容器状态更新的协议。\n\n\n\n原文:http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/\n","source":"_posts/Hadoop/Hadoop MapReduce2.0 架构详解.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop MapReduce2.0 架构详解\ndate: 2017-12-12 20:21:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. MapReduce 2.0 概述\n\n`Apache Hadoop 0.23`比以前的版本有了很大的改进。以下是`MapReduce`的一些亮点；请注意，`HDFS`也有一些主要的改进，这些都不在本文的讨论范围之内。\n\n`MapReduce 2.0`(又名`MRv2`或`YARN`)。新的架构将`JobTracker`的两个主要功能 - 资源管理和作业生命周期管理 - 分解成单独的组件：\n\n- 管理计算资源到应用程序的全局分配的`ResourceManager`(`RM`)。\n- 每个应用程序的`ApplicationMaster`(`AM`)，用于管理应用程序的生命周期。\n\n在`Hadoop 0.23`中，`MapReduce`应用程序是由`MapReduce` `ApplicationMaster`执行的`MapReduce`传统意义上的单一作业。\n\n每个节点上还有一个`NodeManager`(`NM`)，用于管理该节点上的用户进程。`RM`和`NM`构成集群的计算框架。该设计还允许在`NM`中运行长时间的辅助服务(The design also allows plugging long-running auxiliary services to the NM)；这些都是特定于应用程序的服务，作为配置的一部分进行指定，并在启动期间由`NM`加载。对于`YARN`上的`MapReduce`应用，`shuffle`是由`NM`加载的典型的辅助服务。请注意，在`Hadoop 0.23`版本之前，`shuffle`是`TaskTracker`的一部分。\n\n每个应用程序的`ApplicationMaster`是一个特定框架库，负责与`ResourceManager`协商资源，并与`NodeManager`一起来执行和监视这些任务。在`YARN`设计中，`MapReduce`只是一个应用程序框架， 该设计也可以允许使用其他框架来构建和部署分布式应用程序。例如，`Hadoop 0.23`附带了一个分布式`Shell`应用程序，允许在`YARN`集群上的多个节点上运行一个`shell`脚本。\n\n\n### 2. MapReduce 2.0 设计\n\n下图显示了一个`YARN`集群。只有一个资源管理器，它有两个主要的服务：\n- 可插拔的调度器，用于管理和实施集群中的资源调度策略。请注意，在编写本博文时，`Hadoop 0.23`中支持两个调度器，默认的`FIFO`调度器和`Capacity`调度器; `Fair`调度器尚未支持(译者注:博文2012编写，现在已经支持)。\n- `Applications Manager`(`AsM`)，负责管理集群中运行的`Application Masters`，例如，负责启动`Application Masters`，以及在发生故障时在不同节点上监视和重新启动`Application Masters`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-1.png?raw=true)\n\n上图还显示了在集群上的每个节点上都运行一个`NM`服务。该图还显示了有两个`AM`(`AM1`和`AM2`)。对于给定的任意`YARN`集群中，有多少个应用程序(作业)，就运行多少个`Application Masters`。每个`AM`管理应用程序的各个任务(启动，监视，以及在发生故障时重新启动任务)。该图还显示了`AM1`管理三个任务(容器1.1,1.2和1.3)，`AM2`管理四个任务(容器2.1,2.2,2.3和2.4)。每个任务运行在每个节点的`Container`中。在`AM`联系对应的`NM`来启动应用程序的各个任务之前，从`RM`的调度器中获取这些容器。这些容器可以大致与以前的`Hadoop`版本中的`Map`/`Reduce`插槽进行比较。然而，从集群利用角度来看，`Hadoop-0.23`中的资源分配模型更加优化。\n\n### 3. MapReduce 2.0 资源分配模型\n\n在较早的`Hadoop`版本中，集群中的每个节点都静态分配运行预定义数量的`Map`插槽和预定义数量的`Reduce`插槽的资源容量。插槽无法在`Map`和`Reduce`之间共享。这种静态分配槽的方式并不是最佳选择，因为在`MR`作业生命周期中槽的需求是不同的(典型地，当作业开始时对`Map`槽有需求，相反，对`Reduce`槽的需求是在最后)。实际上，在一个真正的集群中，作业是随机提交的，每个集群都有自己的`Map`/`Reduce`槽需求，对集群充分利用不是不可能，是非常难。\n\n`Hadoop 0.23`中的资源分配模型通过提供更灵活的资源建模来解决此类缺陷。以容器的形式请求资源，其中每个容器具有许多非静态属性。在写本博客时(2012年)，唯一支持的属性是内存(`RAM`)。然而，该模型是通用的，并且有意在将来的版本中添加更多的属性(例如CPU和网络带宽)。在这个新的资源管理模型中，每个属性只定义了最小值和最大值，并且`AM`可以请求具有这些最小值的倍数的属性值的容器。\n\n### 4. MapReduce 2.0 主要组件\n\n我们将详细介绍MapReduce架构的主要组件，以了解这些组件的功能以及它们如何交互的。\n\n#### 2.1 Client – Resource Manager\n\n下展示了在`YARN`集群上运行应用程序的初始步骤。通常，客户端与`RM`(特别是与`RM`的`Applications Manager`组件)通信来开启此步骤。图中标记为(1)的第一步是让客户端告诉`Applications Manager`我们提交应用程序的意愿，这是通过创建应用程序请求(`New Application Request`)完成的。标记为(2)的`RM`响应通常包含一个新生成的唯一应用程序ID，以及有关客户端在请求资源以运行应用程序`AM`时所需要的集群资源容量的信息。\n\n使用从`RM`接收到的响应信息，客户端可以构建并提交标记为(3)的应用程序提交上下文(`Application Submission Context`)，除了`RM`所需要来启动`AM`的信息之外，通常还包含诸如调度器队列，优先级和用户信息之类的信息。这些信息包含在容器启动上下文(`Container Launch Context`)中，还包含应用程序的`jar`，作业文件，安全令牌和任何需要的资源等。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-2.png?raw=true)\n\n在提交申请之后，客户端可以向`RM`查询应用程序报告，并接收返回的报告，并且如果需要，客户端也可以要求`RM`终止该应用程序。这三个步骤在下图中展示:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-3.png?raw=true)\n\n#### 2.2 Resource Manager – Application Master\n\n当`RM`从客户端接收到应用程序提交上下文时，会找到一个满足运行`AM`资源需求的可用容器，并与该容器的`NM`联系，以便在该节点上启动`AM`进程。下图描述了`AM`和`RM`(特别是与`RM`的调度器)之间的通信步骤。图中标记为(1)的第一步是`AM`将自己注册到`RM`中。这一步由一个握手过程组成，同时还传递了`AM`将要监听的`RPC`端口，监视应用程序状态和进度的跟踪URL等信息。\n\n标记为(2)的`RM`注册响应为`AM`传递一些基本信息，比如集群的最小和最大资源容量。`AM`将使用这些信息为各个任务的任何资源请求来计算和请求资源。标记为(3)的从`AM`到`RM`的资源分配请求主要包含所请求的容器列表，并且还可能包含该`AM`所释放的容器列表。心跳和进度信息也可以通过资源分配请求进行传达，如箭头(4)所示。\n\n当`RM`的调度器接收到资源分配请求时，它基于调度策略计算满足该请求的容器列表，并且返回分配响应，标记为(5)，其中包含分配的资源列表。使用资源列表，`AM`开始联系相关联的`NM`(很快就会看到)，最后，如箭头(6)所示，当作业完成时，`AM`向`RM`发送应用完成的消息并退出。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-4.png?raw=true)\n\n#### 2.3 Application Master – Container Manager\n\n下图描述了`AM`和`Node Manager`之间的通信。`AM`为每个容器请求`NM`来启动它，如图中箭头(1)所示。在容器运行时，`AM`可以分别请求并接收容器状态报告，如步骤(2)和(3)所示。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-5.png?raw=true)\n\n基于以上讨论，编写`YARN`应用程序的开发人员应主要关注以下接口：\n- `ClientRMProtocol`：`Client` `RM`(图3)。这是客户端与`RM`进行通信以启动新的应用程序(即AM)，检查应用程序状态或终止应用程序的协议。\n- `AMRMProtocol`：`AM` `RM`(图4)。这是`AM`用来向`RM`注册或取消注册，以及从`RM`调度器请求资源来运行任务的协议。\n- `ContainerManager`：`AM` `NM`(图5)。这是`AM`用来与`NM`进行通信以启动或停止容器以及获取容器状态更新的协议。\n\n\n\n原文:http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/\n","slug":"Hadoop/Hadoop MapReduce2.0 架构详解","published":1,"updated":"2018-01-29T09:36:59.636Z","comments":1,"photos":[],"link":"","_id":"cje58tis9002jordbjs4n3jjw","content":"<h3 id=\"1-MapReduce-2-0-概述\"><a href=\"#1-MapReduce-2-0-概述\" class=\"headerlink\" title=\"1. MapReduce 2.0 概述\"></a>1. MapReduce 2.0 概述</h3><p><code>Apache Hadoop 0.23</code>比以前的版本有了很大的改进。以下是<code>MapReduce</code>的一些亮点；请注意，<code>HDFS</code>也有一些主要的改进，这些都不在本文的讨论范围之内。</p>\n<p><code>MapReduce 2.0</code>(又名<code>MRv2</code>或<code>YARN</code>)。新的架构将<code>JobTracker</code>的两个主要功能 - 资源管理和作业生命周期管理 - 分解成单独的组件：</p>\n<ul>\n<li>管理计算资源到应用程序的全局分配的<code>ResourceManager</code>(<code>RM</code>)。</li>\n<li>每个应用程序的<code>ApplicationMaster</code>(<code>AM</code>)，用于管理应用程序的生命周期。</li>\n</ul>\n<p>在<code>Hadoop 0.23</code>中，<code>MapReduce</code>应用程序是由<code>MapReduce</code> <code>ApplicationMaster</code>执行的<code>MapReduce</code>传统意义上的单一作业。</p>\n<p>每个节点上还有一个<code>NodeManager</code>(<code>NM</code>)，用于管理该节点上的用户进程。<code>RM</code>和<code>NM</code>构成集群的计算框架。该设计还允许在<code>NM</code>中运行长时间的辅助服务(The design also allows plugging long-running auxiliary services to the NM)；这些都是特定于应用程序的服务，作为配置的一部分进行指定，并在启动期间由<code>NM</code>加载。对于<code>YARN</code>上的<code>MapReduce</code>应用，<code>shuffle</code>是由<code>NM</code>加载的典型的辅助服务。请注意，在<code>Hadoop 0.23</code>版本之前，<code>shuffle</code>是<code>TaskTracker</code>的一部分。</p>\n<p>每个应用程序的<code>ApplicationMaster</code>是一个特定框架库，负责与<code>ResourceManager</code>协商资源，并与<code>NodeManager</code>一起来执行和监视这些任务。在<code>YARN</code>设计中，<code>MapReduce</code>只是一个应用程序框架， 该设计也可以允许使用其他框架来构建和部署分布式应用程序。例如，<code>Hadoop 0.23</code>附带了一个分布式<code>Shell</code>应用程序，允许在<code>YARN</code>集群上的多个节点上运行一个<code>shell</code>脚本。</p>\n<h3 id=\"2-MapReduce-2-0-设计\"><a href=\"#2-MapReduce-2-0-设计\" class=\"headerlink\" title=\"2. MapReduce 2.0 设计\"></a>2. MapReduce 2.0 设计</h3><p>下图显示了一个<code>YARN</code>集群。只有一个资源管理器，它有两个主要的服务：</p>\n<ul>\n<li>可插拔的调度器，用于管理和实施集群中的资源调度策略。请注意，在编写本博文时，<code>Hadoop 0.23</code>中支持两个调度器，默认的<code>FIFO</code>调度器和<code>Capacity</code>调度器; <code>Fair</code>调度器尚未支持(译者注:博文2012编写，现在已经支持)。</li>\n<li><code>Applications Manager</code>(<code>AsM</code>)，负责管理集群中运行的<code>Application Masters</code>，例如，负责启动<code>Application Masters</code>，以及在发生故障时在不同节点上监视和重新启动<code>Application Masters</code>。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-1.png?raw=true\" alt=\"\"></p>\n<p>上图还显示了在集群上的每个节点上都运行一个<code>NM</code>服务。该图还显示了有两个<code>AM</code>(<code>AM1</code>和<code>AM2</code>)。对于给定的任意<code>YARN</code>集群中，有多少个应用程序(作业)，就运行多少个<code>Application Masters</code>。每个<code>AM</code>管理应用程序的各个任务(启动，监视，以及在发生故障时重新启动任务)。该图还显示了<code>AM1</code>管理三个任务(容器1.1,1.2和1.3)，<code>AM2</code>管理四个任务(容器2.1,2.2,2.3和2.4)。每个任务运行在每个节点的<code>Container</code>中。在<code>AM</code>联系对应的<code>NM</code>来启动应用程序的各个任务之前，从<code>RM</code>的调度器中获取这些容器。这些容器可以大致与以前的<code>Hadoop</code>版本中的<code>Map</code>/<code>Reduce</code>插槽进行比较。然而，从集群利用角度来看，<code>Hadoop-0.23</code>中的资源分配模型更加优化。</p>\n<h3 id=\"3-MapReduce-2-0-资源分配模型\"><a href=\"#3-MapReduce-2-0-资源分配模型\" class=\"headerlink\" title=\"3. MapReduce 2.0 资源分配模型\"></a>3. MapReduce 2.0 资源分配模型</h3><p>在较早的<code>Hadoop</code>版本中，集群中的每个节点都静态分配运行预定义数量的<code>Map</code>插槽和预定义数量的<code>Reduce</code>插槽的资源容量。插槽无法在<code>Map</code>和<code>Reduce</code>之间共享。这种静态分配槽的方式并不是最佳选择，因为在<code>MR</code>作业生命周期中槽的需求是不同的(典型地，当作业开始时对<code>Map</code>槽有需求，相反，对<code>Reduce</code>槽的需求是在最后)。实际上，在一个真正的集群中，作业是随机提交的，每个集群都有自己的<code>Map</code>/<code>Reduce</code>槽需求，对集群充分利用不是不可能，是非常难。</p>\n<p><code>Hadoop 0.23</code>中的资源分配模型通过提供更灵活的资源建模来解决此类缺陷。以容器的形式请求资源，其中每个容器具有许多非静态属性。在写本博客时(2012年)，唯一支持的属性是内存(<code>RAM</code>)。然而，该模型是通用的，并且有意在将来的版本中添加更多的属性(例如CPU和网络带宽)。在这个新的资源管理模型中，每个属性只定义了最小值和最大值，并且<code>AM</code>可以请求具有这些最小值的倍数的属性值的容器。</p>\n<h3 id=\"4-MapReduce-2-0-主要组件\"><a href=\"#4-MapReduce-2-0-主要组件\" class=\"headerlink\" title=\"4. MapReduce 2.0 主要组件\"></a>4. MapReduce 2.0 主要组件</h3><p>我们将详细介绍MapReduce架构的主要组件，以了解这些组件的功能以及它们如何交互的。</p>\n<h4 id=\"2-1-Client-–-Resource-Manager\"><a href=\"#2-1-Client-–-Resource-Manager\" class=\"headerlink\" title=\"2.1 Client – Resource Manager\"></a>2.1 Client – Resource Manager</h4><p>下展示了在<code>YARN</code>集群上运行应用程序的初始步骤。通常，客户端与<code>RM</code>(特别是与<code>RM</code>的<code>Applications Manager</code>组件)通信来开启此步骤。图中标记为(1)的第一步是让客户端告诉<code>Applications Manager</code>我们提交应用程序的意愿，这是通过创建应用程序请求(<code>New Application Request</code>)完成的。标记为(2)的<code>RM</code>响应通常包含一个新生成的唯一应用程序ID，以及有关客户端在请求资源以运行应用程序<code>AM</code>时所需要的集群资源容量的信息。</p>\n<p>使用从<code>RM</code>接收到的响应信息，客户端可以构建并提交标记为(3)的应用程序提交上下文(<code>Application Submission Context</code>)，除了<code>RM</code>所需要来启动<code>AM</code>的信息之外，通常还包含诸如调度器队列，优先级和用户信息之类的信息。这些信息包含在容器启动上下文(<code>Container Launch Context</code>)中，还包含应用程序的<code>jar</code>，作业文件，安全令牌和任何需要的资源等。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-2.png?raw=true\" alt=\"\"></p>\n<p>在提交申请之后，客户端可以向<code>RM</code>查询应用程序报告，并接收返回的报告，并且如果需要，客户端也可以要求<code>RM</code>终止该应用程序。这三个步骤在下图中展示:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-2-Resource-Manager-–-Application-Master\"><a href=\"#2-2-Resource-Manager-–-Application-Master\" class=\"headerlink\" title=\"2.2 Resource Manager – Application Master\"></a>2.2 Resource Manager – Application Master</h4><p>当<code>RM</code>从客户端接收到应用程序提交上下文时，会找到一个满足运行<code>AM</code>资源需求的可用容器，并与该容器的<code>NM</code>联系，以便在该节点上启动<code>AM</code>进程。下图描述了<code>AM</code>和<code>RM</code>(特别是与<code>RM</code>的调度器)之间的通信步骤。图中标记为(1)的第一步是<code>AM</code>将自己注册到<code>RM</code>中。这一步由一个握手过程组成，同时还传递了<code>AM</code>将要监听的<code>RPC</code>端口，监视应用程序状态和进度的跟踪URL等信息。</p>\n<p>标记为(2)的<code>RM</code>注册响应为<code>AM</code>传递一些基本信息，比如集群的最小和最大资源容量。<code>AM</code>将使用这些信息为各个任务的任何资源请求来计算和请求资源。标记为(3)的从<code>AM</code>到<code>RM</code>的资源分配请求主要包含所请求的容器列表，并且还可能包含该<code>AM</code>所释放的容器列表。心跳和进度信息也可以通过资源分配请求进行传达，如箭头(4)所示。</p>\n<p>当<code>RM</code>的调度器接收到资源分配请求时，它基于调度策略计算满足该请求的容器列表，并且返回分配响应，标记为(5)，其中包含分配的资源列表。使用资源列表，<code>AM</code>开始联系相关联的<code>NM</code>(很快就会看到)，最后，如箭头(6)所示，当作业完成时，<code>AM</code>向<code>RM</code>发送应用完成的消息并退出。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-4.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-Application-Master-–-Container-Manager\"><a href=\"#2-3-Application-Master-–-Container-Manager\" class=\"headerlink\" title=\"2.3 Application Master – Container Manager\"></a>2.3 Application Master – Container Manager</h4><p>下图描述了<code>AM</code>和<code>Node Manager</code>之间的通信。<code>AM</code>为每个容器请求<code>NM</code>来启动它，如图中箭头(1)所示。在容器运行时，<code>AM</code>可以分别请求并接收容器状态报告，如步骤(2)和(3)所示。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-5.png?raw=true\" alt=\"\"></p>\n<p>基于以上讨论，编写<code>YARN</code>应用程序的开发人员应主要关注以下接口：</p>\n<ul>\n<li><code>ClientRMProtocol</code>：<code>Client</code> <code>RM</code>(图3)。这是客户端与<code>RM</code>进行通信以启动新的应用程序(即AM)，检查应用程序状态或终止应用程序的协议。</li>\n<li><code>AMRMProtocol</code>：<code>AM</code> <code>RM</code>(图4)。这是<code>AM</code>用来向<code>RM</code>注册或取消注册，以及从<code>RM</code>调度器请求资源来运行任务的协议。</li>\n<li><code>ContainerManager</code>：<code>AM</code> <code>NM</code>(图5)。这是<code>AM</code>用来与<code>NM</code>进行通信以启动或停止容器以及获取容器状态更新的协议。</li>\n</ul>\n<p>原文:<a href=\"http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/\" target=\"_blank\" rel=\"noopener\">http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-MapReduce-2-0-概述\"><a href=\"#1-MapReduce-2-0-概述\" class=\"headerlink\" title=\"1. MapReduce 2.0 概述\"></a>1. MapReduce 2.0 概述</h3><p><code>Apache Hadoop 0.23</code>比以前的版本有了很大的改进。以下是<code>MapReduce</code>的一些亮点；请注意，<code>HDFS</code>也有一些主要的改进，这些都不在本文的讨论范围之内。</p>\n<p><code>MapReduce 2.0</code>(又名<code>MRv2</code>或<code>YARN</code>)。新的架构将<code>JobTracker</code>的两个主要功能 - 资源管理和作业生命周期管理 - 分解成单独的组件：</p>\n<ul>\n<li>管理计算资源到应用程序的全局分配的<code>ResourceManager</code>(<code>RM</code>)。</li>\n<li>每个应用程序的<code>ApplicationMaster</code>(<code>AM</code>)，用于管理应用程序的生命周期。</li>\n</ul>\n<p>在<code>Hadoop 0.23</code>中，<code>MapReduce</code>应用程序是由<code>MapReduce</code> <code>ApplicationMaster</code>执行的<code>MapReduce</code>传统意义上的单一作业。</p>\n<p>每个节点上还有一个<code>NodeManager</code>(<code>NM</code>)，用于管理该节点上的用户进程。<code>RM</code>和<code>NM</code>构成集群的计算框架。该设计还允许在<code>NM</code>中运行长时间的辅助服务(The design also allows plugging long-running auxiliary services to the NM)；这些都是特定于应用程序的服务，作为配置的一部分进行指定，并在启动期间由<code>NM</code>加载。对于<code>YARN</code>上的<code>MapReduce</code>应用，<code>shuffle</code>是由<code>NM</code>加载的典型的辅助服务。请注意，在<code>Hadoop 0.23</code>版本之前，<code>shuffle</code>是<code>TaskTracker</code>的一部分。</p>\n<p>每个应用程序的<code>ApplicationMaster</code>是一个特定框架库，负责与<code>ResourceManager</code>协商资源，并与<code>NodeManager</code>一起来执行和监视这些任务。在<code>YARN</code>设计中，<code>MapReduce</code>只是一个应用程序框架， 该设计也可以允许使用其他框架来构建和部署分布式应用程序。例如，<code>Hadoop 0.23</code>附带了一个分布式<code>Shell</code>应用程序，允许在<code>YARN</code>集群上的多个节点上运行一个<code>shell</code>脚本。</p>\n<h3 id=\"2-MapReduce-2-0-设计\"><a href=\"#2-MapReduce-2-0-设计\" class=\"headerlink\" title=\"2. MapReduce 2.0 设计\"></a>2. MapReduce 2.0 设计</h3><p>下图显示了一个<code>YARN</code>集群。只有一个资源管理器，它有两个主要的服务：</p>\n<ul>\n<li>可插拔的调度器，用于管理和实施集群中的资源调度策略。请注意，在编写本博文时，<code>Hadoop 0.23</code>中支持两个调度器，默认的<code>FIFO</code>调度器和<code>Capacity</code>调度器; <code>Fair</code>调度器尚未支持(译者注:博文2012编写，现在已经支持)。</li>\n<li><code>Applications Manager</code>(<code>AsM</code>)，负责管理集群中运行的<code>Application Masters</code>，例如，负责启动<code>Application Masters</code>，以及在发生故障时在不同节点上监视和重新启动<code>Application Masters</code>。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-1.png?raw=true\" alt=\"\"></p>\n<p>上图还显示了在集群上的每个节点上都运行一个<code>NM</code>服务。该图还显示了有两个<code>AM</code>(<code>AM1</code>和<code>AM2</code>)。对于给定的任意<code>YARN</code>集群中，有多少个应用程序(作业)，就运行多少个<code>Application Masters</code>。每个<code>AM</code>管理应用程序的各个任务(启动，监视，以及在发生故障时重新启动任务)。该图还显示了<code>AM1</code>管理三个任务(容器1.1,1.2和1.3)，<code>AM2</code>管理四个任务(容器2.1,2.2,2.3和2.4)。每个任务运行在每个节点的<code>Container</code>中。在<code>AM</code>联系对应的<code>NM</code>来启动应用程序的各个任务之前，从<code>RM</code>的调度器中获取这些容器。这些容器可以大致与以前的<code>Hadoop</code>版本中的<code>Map</code>/<code>Reduce</code>插槽进行比较。然而，从集群利用角度来看，<code>Hadoop-0.23</code>中的资源分配模型更加优化。</p>\n<h3 id=\"3-MapReduce-2-0-资源分配模型\"><a href=\"#3-MapReduce-2-0-资源分配模型\" class=\"headerlink\" title=\"3. MapReduce 2.0 资源分配模型\"></a>3. MapReduce 2.0 资源分配模型</h3><p>在较早的<code>Hadoop</code>版本中，集群中的每个节点都静态分配运行预定义数量的<code>Map</code>插槽和预定义数量的<code>Reduce</code>插槽的资源容量。插槽无法在<code>Map</code>和<code>Reduce</code>之间共享。这种静态分配槽的方式并不是最佳选择，因为在<code>MR</code>作业生命周期中槽的需求是不同的(典型地，当作业开始时对<code>Map</code>槽有需求，相反，对<code>Reduce</code>槽的需求是在最后)。实际上，在一个真正的集群中，作业是随机提交的，每个集群都有自己的<code>Map</code>/<code>Reduce</code>槽需求，对集群充分利用不是不可能，是非常难。</p>\n<p><code>Hadoop 0.23</code>中的资源分配模型通过提供更灵活的资源建模来解决此类缺陷。以容器的形式请求资源，其中每个容器具有许多非静态属性。在写本博客时(2012年)，唯一支持的属性是内存(<code>RAM</code>)。然而，该模型是通用的，并且有意在将来的版本中添加更多的属性(例如CPU和网络带宽)。在这个新的资源管理模型中，每个属性只定义了最小值和最大值，并且<code>AM</code>可以请求具有这些最小值的倍数的属性值的容器。</p>\n<h3 id=\"4-MapReduce-2-0-主要组件\"><a href=\"#4-MapReduce-2-0-主要组件\" class=\"headerlink\" title=\"4. MapReduce 2.0 主要组件\"></a>4. MapReduce 2.0 主要组件</h3><p>我们将详细介绍MapReduce架构的主要组件，以了解这些组件的功能以及它们如何交互的。</p>\n<h4 id=\"2-1-Client-–-Resource-Manager\"><a href=\"#2-1-Client-–-Resource-Manager\" class=\"headerlink\" title=\"2.1 Client – Resource Manager\"></a>2.1 Client – Resource Manager</h4><p>下展示了在<code>YARN</code>集群上运行应用程序的初始步骤。通常，客户端与<code>RM</code>(特别是与<code>RM</code>的<code>Applications Manager</code>组件)通信来开启此步骤。图中标记为(1)的第一步是让客户端告诉<code>Applications Manager</code>我们提交应用程序的意愿，这是通过创建应用程序请求(<code>New Application Request</code>)完成的。标记为(2)的<code>RM</code>响应通常包含一个新生成的唯一应用程序ID，以及有关客户端在请求资源以运行应用程序<code>AM</code>时所需要的集群资源容量的信息。</p>\n<p>使用从<code>RM</code>接收到的响应信息，客户端可以构建并提交标记为(3)的应用程序提交上下文(<code>Application Submission Context</code>)，除了<code>RM</code>所需要来启动<code>AM</code>的信息之外，通常还包含诸如调度器队列，优先级和用户信息之类的信息。这些信息包含在容器启动上下文(<code>Container Launch Context</code>)中，还包含应用程序的<code>jar</code>，作业文件，安全令牌和任何需要的资源等。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-2.png?raw=true\" alt=\"\"></p>\n<p>在提交申请之后，客户端可以向<code>RM</code>查询应用程序报告，并接收返回的报告，并且如果需要，客户端也可以要求<code>RM</code>终止该应用程序。这三个步骤在下图中展示:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-2-Resource-Manager-–-Application-Master\"><a href=\"#2-2-Resource-Manager-–-Application-Master\" class=\"headerlink\" title=\"2.2 Resource Manager – Application Master\"></a>2.2 Resource Manager – Application Master</h4><p>当<code>RM</code>从客户端接收到应用程序提交上下文时，会找到一个满足运行<code>AM</code>资源需求的可用容器，并与该容器的<code>NM</code>联系，以便在该节点上启动<code>AM</code>进程。下图描述了<code>AM</code>和<code>RM</code>(特别是与<code>RM</code>的调度器)之间的通信步骤。图中标记为(1)的第一步是<code>AM</code>将自己注册到<code>RM</code>中。这一步由一个握手过程组成，同时还传递了<code>AM</code>将要监听的<code>RPC</code>端口，监视应用程序状态和进度的跟踪URL等信息。</p>\n<p>标记为(2)的<code>RM</code>注册响应为<code>AM</code>传递一些基本信息，比如集群的最小和最大资源容量。<code>AM</code>将使用这些信息为各个任务的任何资源请求来计算和请求资源。标记为(3)的从<code>AM</code>到<code>RM</code>的资源分配请求主要包含所请求的容器列表，并且还可能包含该<code>AM</code>所释放的容器列表。心跳和进度信息也可以通过资源分配请求进行传达，如箭头(4)所示。</p>\n<p>当<code>RM</code>的调度器接收到资源分配请求时，它基于调度策略计算满足该请求的容器列表，并且返回分配响应，标记为(5)，其中包含分配的资源列表。使用资源列表，<code>AM</code>开始联系相关联的<code>NM</code>(很快就会看到)，最后，如箭头(6)所示，当作业完成时，<code>AM</code>向<code>RM</code>发送应用完成的消息并退出。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-4.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-Application-Master-–-Container-Manager\"><a href=\"#2-3-Application-Master-–-Container-Manager\" class=\"headerlink\" title=\"2.3 Application Master – Container Manager\"></a>2.3 Application Master – Container Manager</h4><p>下图描述了<code>AM</code>和<code>Node Manager</code>之间的通信。<code>AM</code>为每个容器请求<code>NM</code>来启动它，如图中箭头(1)所示。在容器运行时，<code>AM</code>可以分别请求并接收容器状态报告，如步骤(2)和(3)所示。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20MapReduce2.0%20%E6%9E%B6%E6%9E%84-5.png?raw=true\" alt=\"\"></p>\n<p>基于以上讨论，编写<code>YARN</code>应用程序的开发人员应主要关注以下接口：</p>\n<ul>\n<li><code>ClientRMProtocol</code>：<code>Client</code> <code>RM</code>(图3)。这是客户端与<code>RM</code>进行通信以启动新的应用程序(即AM)，检查应用程序状态或终止应用程序的协议。</li>\n<li><code>AMRMProtocol</code>：<code>AM</code> <code>RM</code>(图4)。这是<code>AM</code>用来向<code>RM</code>注册或取消注册，以及从<code>RM</code>调度器请求资源来运行任务的协议。</li>\n<li><code>ContainerManager</code>：<code>AM</code> <code>NM</code>(图5)。这是<code>AM</code>用来与<code>NM</code>进行通信以启动或停止容器以及获取容器状态更新的协议。</li>\n</ul>\n<p>原文:<a href=\"http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/\" target=\"_blank\" rel=\"noopener\">http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop MapReduce中的InputSplit","date":"2017-12-06T05:20:17.000Z","_content":"\n\nHadoop的初学者经常会有这样两个问题：\n\n(1) Hadoop的一个`Block`默认是128M(或者64M)，那么对于一条记录来说，会不会造成一条记录被分到两个`Block`中？\n\n(2) 从`Block`中读取数据进行切分时，会不会造成一条记录被分到两个`InputSplit`中？\n\n对于上面的两个问题，首先要明确两个概念：`Block`和`InputSplit`。在Hadoop中，文件由一个一个的记录组成，最终由mapper任务一个一个的处理。\n例如，示例数据集包含有关1987至2008年间美国境内已完成航班的信息。如果要下载数据集可以打开如下网址： http://stat-computing.org/dataexpo/2009/the-data.html 。每一年都会生成一个大文件（例如：2008年文件大小为108M），在每个文件中每单独的一行都代表一次航班信息。换句话说，一行代表一个记录。\n`HDFS`以固定大小的`Block`为基本单位存储数据，而对于`MapReduce`而言，其处理单位是`InputSplit`。\n\n### 1. Block\n\n块是以`block size`进行划分数据。因此，如果集群的`block size`为128MB，则数据集的每个块将为128MB，除非最后一个块小于`block size`（文件大小不能被 block size 完全整除）。例如下图中文件大小为513MB，513%128=1，最后一个块`e`小于`block size`，大小为1MB。因此，块是以`block size`进行切割，并且块甚至可以在到逻辑记录结束之前结束(blocks can end even before a logical record ends)。\n\n假设我们的集群中`block size`是128MB，每个逻辑记录大约100MB（假设为巨大的记录）。所以第一个记录将完全在一个块中，因为记录大小为100MB小于块大小128 MB。但是，第二个记录不能完全在一个块中，第二条记录将出现在两个块中，从块1开始，溢出到块2中。\n\n### 2.InputSplit\n\n但是如果每个`Map`任务都处理特定数据块中的所有记录，那怎么处理这种跨越块边界的记录呢？如果分配一个`Mapper`给块1，在这种情况下，`Mapper`不能处理第二条记录，因为块1中没有完整的第二条记录。因为`HDFS`对文件块内部并不清楚，它不知道一个记录会什么时候可能溢出到另一个块(because HDFS has no conception of what’s inside the file blocks, it can’t gauge when a record might spill over into another block)。`InputSplit`就是解决这种跨越块边界记录问题的，Hadoop使用逻辑表示存储在文件块中的数据，称为输入拆分`InputSplit`。`InputSplit`是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等。它的划分方法完全取决于用户自己。但是需要注意的是`InputSplit`的多少决定了`MapTask`的数目，因为每个`InputSplit`会交由一个`MapTask`处理。\n\n当`MapReduce`作业客户端计算`InputSplit`时，它会计算出块中第一个记录的开始位置和最后一个记录的结束位置。在最后一个记录不完整的情况下，`InputSplit`包括下一个块的位置信息和完成该记录所需的数据的字节偏移（In cases where the last record in a block is incomplete, the input split includes location information for the next block and the byte offset of the data needed to complete the record）。下图显示了数据块和InputSplit之间的关系：\n\n![image](http://img.blog.csdn.net/20170929115859522?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n块是磁盘中的数据存储的物理块，其中`InputSplit`不是物理数据块。它只是一个逻辑概念，并没有对实际文件进行切分，指向块中的开始和结束位置。因此，当`Mapper`尝试读取数据时，它清楚地知道从何处开始读取以及在哪里停止读取。`InputSplit`的开始位置可以在一个块中开始，在另一个块中结束。`InputSplit`代表了逻辑记录边界，在`MapReduce`执行期间，`Hadoop`扫描块并创建`InputSplits`，并且每个`InputSplit`将被分配给一个`Mapper`进行处理。\n\n\n原文：http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/\nhttp://hadoopinrealworld.com/inputsplit-vs-block/\n\nhttp://hadoopinrealworld.com/inputsplit-vs-block/\n","source":"_posts/Hadoop/Hadoop MapReduce中的InputSplit.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop MapReduce中的InputSplit\ndate: 2017-12-06 13:20:17\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n\nHadoop的初学者经常会有这样两个问题：\n\n(1) Hadoop的一个`Block`默认是128M(或者64M)，那么对于一条记录来说，会不会造成一条记录被分到两个`Block`中？\n\n(2) 从`Block`中读取数据进行切分时，会不会造成一条记录被分到两个`InputSplit`中？\n\n对于上面的两个问题，首先要明确两个概念：`Block`和`InputSplit`。在Hadoop中，文件由一个一个的记录组成，最终由mapper任务一个一个的处理。\n例如，示例数据集包含有关1987至2008年间美国境内已完成航班的信息。如果要下载数据集可以打开如下网址： http://stat-computing.org/dataexpo/2009/the-data.html 。每一年都会生成一个大文件（例如：2008年文件大小为108M），在每个文件中每单独的一行都代表一次航班信息。换句话说，一行代表一个记录。\n`HDFS`以固定大小的`Block`为基本单位存储数据，而对于`MapReduce`而言，其处理单位是`InputSplit`。\n\n### 1. Block\n\n块是以`block size`进行划分数据。因此，如果集群的`block size`为128MB，则数据集的每个块将为128MB，除非最后一个块小于`block size`（文件大小不能被 block size 完全整除）。例如下图中文件大小为513MB，513%128=1，最后一个块`e`小于`block size`，大小为1MB。因此，块是以`block size`进行切割，并且块甚至可以在到逻辑记录结束之前结束(blocks can end even before a logical record ends)。\n\n假设我们的集群中`block size`是128MB，每个逻辑记录大约100MB（假设为巨大的记录）。所以第一个记录将完全在一个块中，因为记录大小为100MB小于块大小128 MB。但是，第二个记录不能完全在一个块中，第二条记录将出现在两个块中，从块1开始，溢出到块2中。\n\n### 2.InputSplit\n\n但是如果每个`Map`任务都处理特定数据块中的所有记录，那怎么处理这种跨越块边界的记录呢？如果分配一个`Mapper`给块1，在这种情况下，`Mapper`不能处理第二条记录，因为块1中没有完整的第二条记录。因为`HDFS`对文件块内部并不清楚，它不知道一个记录会什么时候可能溢出到另一个块(because HDFS has no conception of what’s inside the file blocks, it can’t gauge when a record might spill over into another block)。`InputSplit`就是解决这种跨越块边界记录问题的，Hadoop使用逻辑表示存储在文件块中的数据，称为输入拆分`InputSplit`。`InputSplit`是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等。它的划分方法完全取决于用户自己。但是需要注意的是`InputSplit`的多少决定了`MapTask`的数目，因为每个`InputSplit`会交由一个`MapTask`处理。\n\n当`MapReduce`作业客户端计算`InputSplit`时，它会计算出块中第一个记录的开始位置和最后一个记录的结束位置。在最后一个记录不完整的情况下，`InputSplit`包括下一个块的位置信息和完成该记录所需的数据的字节偏移（In cases where the last record in a block is incomplete, the input split includes location information for the next block and the byte offset of the data needed to complete the record）。下图显示了数据块和InputSplit之间的关系：\n\n![image](http://img.blog.csdn.net/20170929115859522?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n块是磁盘中的数据存储的物理块，其中`InputSplit`不是物理数据块。它只是一个逻辑概念，并没有对实际文件进行切分，指向块中的开始和结束位置。因此，当`Mapper`尝试读取数据时，它清楚地知道从何处开始读取以及在哪里停止读取。`InputSplit`的开始位置可以在一个块中开始，在另一个块中结束。`InputSplit`代表了逻辑记录边界，在`MapReduce`执行期间，`Hadoop`扫描块并创建`InputSplits`，并且每个`InputSplit`将被分配给一个`Mapper`进行处理。\n\n\n原文：http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/\nhttp://hadoopinrealworld.com/inputsplit-vs-block/\n\nhttp://hadoopinrealworld.com/inputsplit-vs-block/\n","slug":"Hadoop/Hadoop MapReduce中的InputSplit","published":1,"updated":"2018-01-29T09:36:59.635Z","comments":1,"photos":[],"link":"","_id":"cje58tisc002oordb1x4bnbxp","content":"<p>Hadoop的初学者经常会有这样两个问题：</p>\n<p>(1) Hadoop的一个<code>Block</code>默认是128M(或者64M)，那么对于一条记录来说，会不会造成一条记录被分到两个<code>Block</code>中？</p>\n<p>(2) 从<code>Block</code>中读取数据进行切分时，会不会造成一条记录被分到两个<code>InputSplit</code>中？</p>\n<p>对于上面的两个问题，首先要明确两个概念：<code>Block</code>和<code>InputSplit</code>。在Hadoop中，文件由一个一个的记录组成，最终由mapper任务一个一个的处理。<br>例如，示例数据集包含有关1987至2008年间美国境内已完成航班的信息。如果要下载数据集可以打开如下网址： <a href=\"http://stat-computing.org/dataexpo/2009/the-data.html\" target=\"_blank\" rel=\"noopener\">http://stat-computing.org/dataexpo/2009/the-data.html</a> 。每一年都会生成一个大文件（例如：2008年文件大小为108M），在每个文件中每单独的一行都代表一次航班信息。换句话说，一行代表一个记录。<br><code>HDFS</code>以固定大小的<code>Block</code>为基本单位存储数据，而对于<code>MapReduce</code>而言，其处理单位是<code>InputSplit</code>。</p>\n<h3 id=\"1-Block\"><a href=\"#1-Block\" class=\"headerlink\" title=\"1. Block\"></a>1. Block</h3><p>块是以<code>block size</code>进行划分数据。因此，如果集群的<code>block size</code>为128MB，则数据集的每个块将为128MB，除非最后一个块小于<code>block size</code>（文件大小不能被 block size 完全整除）。例如下图中文件大小为513MB，513%128=1，最后一个块<code>e</code>小于<code>block size</code>，大小为1MB。因此，块是以<code>block size</code>进行切割，并且块甚至可以在到逻辑记录结束之前结束(blocks can end even before a logical record ends)。</p>\n<p>假设我们的集群中<code>block size</code>是128MB，每个逻辑记录大约100MB（假设为巨大的记录）。所以第一个记录将完全在一个块中，因为记录大小为100MB小于块大小128 MB。但是，第二个记录不能完全在一个块中，第二条记录将出现在两个块中，从块1开始，溢出到块2中。</p>\n<h3 id=\"2-InputSplit\"><a href=\"#2-InputSplit\" class=\"headerlink\" title=\"2.InputSplit\"></a>2.InputSplit</h3><p>但是如果每个<code>Map</code>任务都处理特定数据块中的所有记录，那怎么处理这种跨越块边界的记录呢？如果分配一个<code>Mapper</code>给块1，在这种情况下，<code>Mapper</code>不能处理第二条记录，因为块1中没有完整的第二条记录。因为<code>HDFS</code>对文件块内部并不清楚，它不知道一个记录会什么时候可能溢出到另一个块(because HDFS has no conception of what’s inside the file blocks, it can’t gauge when a record might spill over into another block)。<code>InputSplit</code>就是解决这种跨越块边界记录问题的，Hadoop使用逻辑表示存储在文件块中的数据，称为输入拆分<code>InputSplit</code>。<code>InputSplit</code>是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等。它的划分方法完全取决于用户自己。但是需要注意的是<code>InputSplit</code>的多少决定了<code>MapTask</code>的数目，因为每个<code>InputSplit</code>会交由一个<code>MapTask</code>处理。</p>\n<p>当<code>MapReduce</code>作业客户端计算<code>InputSplit</code>时，它会计算出块中第一个记录的开始位置和最后一个记录的结束位置。在最后一个记录不完整的情况下，<code>InputSplit</code>包括下一个块的位置信息和完成该记录所需的数据的字节偏移（In cases where the last record in a block is incomplete, the input split includes location information for the next block and the byte offset of the data needed to complete the record）。下图显示了数据块和InputSplit之间的关系：</p>\n<p><img src=\"http://img.blog.csdn.net/20170929115859522?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>块是磁盘中的数据存储的物理块，其中<code>InputSplit</code>不是物理数据块。它只是一个逻辑概念，并没有对实际文件进行切分，指向块中的开始和结束位置。因此，当<code>Mapper</code>尝试读取数据时，它清楚地知道从何处开始读取以及在哪里停止读取。<code>InputSplit</code>的开始位置可以在一个块中开始，在另一个块中结束。<code>InputSplit</code>代表了逻辑记录边界，在<code>MapReduce</code>执行期间，<code>Hadoop</code>扫描块并创建<code>InputSplits</code>，并且每个<code>InputSplit</code>将被分配给一个<code>Mapper</code>进行处理。</p>\n<p>原文：<a href=\"http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/\" target=\"_blank\" rel=\"noopener\">http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/</a><br><a href=\"http://hadoopinrealworld.com/inputsplit-vs-block/\" target=\"_blank\" rel=\"noopener\">http://hadoopinrealworld.com/inputsplit-vs-block/</a></p>\n<p><a href=\"http://hadoopinrealworld.com/inputsplit-vs-block/\" target=\"_blank\" rel=\"noopener\">http://hadoopinrealworld.com/inputsplit-vs-block/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Hadoop的初学者经常会有这样两个问题：</p>\n<p>(1) Hadoop的一个<code>Block</code>默认是128M(或者64M)，那么对于一条记录来说，会不会造成一条记录被分到两个<code>Block</code>中？</p>\n<p>(2) 从<code>Block</code>中读取数据进行切分时，会不会造成一条记录被分到两个<code>InputSplit</code>中？</p>\n<p>对于上面的两个问题，首先要明确两个概念：<code>Block</code>和<code>InputSplit</code>。在Hadoop中，文件由一个一个的记录组成，最终由mapper任务一个一个的处理。<br>例如，示例数据集包含有关1987至2008年间美国境内已完成航班的信息。如果要下载数据集可以打开如下网址： <a href=\"http://stat-computing.org/dataexpo/2009/the-data.html\" target=\"_blank\" rel=\"noopener\">http://stat-computing.org/dataexpo/2009/the-data.html</a> 。每一年都会生成一个大文件（例如：2008年文件大小为108M），在每个文件中每单独的一行都代表一次航班信息。换句话说，一行代表一个记录。<br><code>HDFS</code>以固定大小的<code>Block</code>为基本单位存储数据，而对于<code>MapReduce</code>而言，其处理单位是<code>InputSplit</code>。</p>\n<h3 id=\"1-Block\"><a href=\"#1-Block\" class=\"headerlink\" title=\"1. Block\"></a>1. Block</h3><p>块是以<code>block size</code>进行划分数据。因此，如果集群的<code>block size</code>为128MB，则数据集的每个块将为128MB，除非最后一个块小于<code>block size</code>（文件大小不能被 block size 完全整除）。例如下图中文件大小为513MB，513%128=1，最后一个块<code>e</code>小于<code>block size</code>，大小为1MB。因此，块是以<code>block size</code>进行切割，并且块甚至可以在到逻辑记录结束之前结束(blocks can end even before a logical record ends)。</p>\n<p>假设我们的集群中<code>block size</code>是128MB，每个逻辑记录大约100MB（假设为巨大的记录）。所以第一个记录将完全在一个块中，因为记录大小为100MB小于块大小128 MB。但是，第二个记录不能完全在一个块中，第二条记录将出现在两个块中，从块1开始，溢出到块2中。</p>\n<h3 id=\"2-InputSplit\"><a href=\"#2-InputSplit\" class=\"headerlink\" title=\"2.InputSplit\"></a>2.InputSplit</h3><p>但是如果每个<code>Map</code>任务都处理特定数据块中的所有记录，那怎么处理这种跨越块边界的记录呢？如果分配一个<code>Mapper</code>给块1，在这种情况下，<code>Mapper</code>不能处理第二条记录，因为块1中没有完整的第二条记录。因为<code>HDFS</code>对文件块内部并不清楚，它不知道一个记录会什么时候可能溢出到另一个块(because HDFS has no conception of what’s inside the file blocks, it can’t gauge when a record might spill over into another block)。<code>InputSplit</code>就是解决这种跨越块边界记录问题的，Hadoop使用逻辑表示存储在文件块中的数据，称为输入拆分<code>InputSplit</code>。<code>InputSplit</code>是一个逻辑概念，并没有对实际文件进行切分，它只包含一些元数据信息，比如数据的起始位置，数据长度，数据所在的节点等。它的划分方法完全取决于用户自己。但是需要注意的是<code>InputSplit</code>的多少决定了<code>MapTask</code>的数目，因为每个<code>InputSplit</code>会交由一个<code>MapTask</code>处理。</p>\n<p>当<code>MapReduce</code>作业客户端计算<code>InputSplit</code>时，它会计算出块中第一个记录的开始位置和最后一个记录的结束位置。在最后一个记录不完整的情况下，<code>InputSplit</code>包括下一个块的位置信息和完成该记录所需的数据的字节偏移（In cases where the last record in a block is incomplete, the input split includes location information for the next block and the byte offset of the data needed to complete the record）。下图显示了数据块和InputSplit之间的关系：</p>\n<p><img src=\"http://img.blog.csdn.net/20170929115859522?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>块是磁盘中的数据存储的物理块，其中<code>InputSplit</code>不是物理数据块。它只是一个逻辑概念，并没有对实际文件进行切分，指向块中的开始和结束位置。因此，当<code>Mapper</code>尝试读取数据时，它清楚地知道从何处开始读取以及在哪里停止读取。<code>InputSplit</code>的开始位置可以在一个块中开始，在另一个块中结束。<code>InputSplit</code>代表了逻辑记录边界，在<code>MapReduce</code>执行期间，<code>Hadoop</code>扫描块并创建<code>InputSplits</code>，并且每个<code>InputSplit</code>将被分配给一个<code>Mapper</code>进行处理。</p>\n<p>原文：<a href=\"http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/\" target=\"_blank\" rel=\"noopener\">http://www.dummies.com/programming/big-data/hadoop/input-splits-in-hadoops-mapreduce/</a><br><a href=\"http://hadoopinrealworld.com/inputsplit-vs-block/\" target=\"_blank\" rel=\"noopener\">http://hadoopinrealworld.com/inputsplit-vs-block/</a></p>\n<p><a href=\"http://hadoopinrealworld.com/inputsplit-vs-block/\" target=\"_blank\" rel=\"noopener\">http://hadoopinrealworld.com/inputsplit-vs-block/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop MapReducer工作过程","date":"2016-12-30T02:28:17.000Z","updated":"2016-12-30T02:28:17.000Z","_content":"\n### 1. 从输入到输出\n\n一个MapReducer作业经过了input，map，combine，reduce，output五个阶段，其中combine阶段并不一定发生，map输出的中间结果被分到reduce的过程成为shuffle（数据清洗）。\n![image](http://img.blog.csdn.net/20161230102351523?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n在shuffle阶段还会发生copy（复制）和sort（排序）。\n\n在MapReduce的过程中，一个作业被分成Map和Reducer两个计算阶段，它们由一个或者多个Map任务和Reduce任务组成。如下图所示，一个MapReduce作业从数据的流向可以分为Map任务和Reduce任务。当用户向Hadoop提交一个MapReduce作业时，JobTracker则会根据各个TaskTracker周期性发送过来的心跳信息综合考虑TaskTracker的资源剩余量，作业优先级，作业提交时间等因素，为TaskTracker分配合适的任务。Reduce任务默认会在Map任务数量完成5%后才开始启动。\n![image](http://img.blog.csdn.net/20161230102431790?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\nMap任务的执行过程可以概括为：首先通过用户指定的InputFormat类中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。然后map函数经过处理之后将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。最后map函数将中间结果保存到本地。\n\nReduce任务的执行过程可以概括为：首先需要将已经完成Map任务的中间结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。\n\n### 2. input\n\n如果使用HDFS上的文件作为MapReduce的输入，MapReduce计算框架首先会用org.apache.hadoop.mapreduce.InputFomat类的子类FileInputFormat类将作为输入HDFS上的文件切分形成输入分片(InputSplit)，每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MaoReduce作业的性能有非常大的影响。\n\nInputSplit只是逻辑上对输入数据进行分片，并不会将文件在磁盘上分成分片进行存储。InputSplit只是记录了分片的元数据节点信息，例如起始位置，长度以及所在的节点列表等。数据切分的算法需要确定InputSplit的个数，对于HDFS上的文件，FileInputFormat类使用computeSplitSize方法计算出InputSplit的大小，代码如下：\n```java\nprotected long computeSplitSize(long blockSize, long minSize, long maxSize) {\n    return Math.max(minSize, Math.min(maxSize, blockSize));\n}\n```\n其中 minSize 由mapred-site.xml文件中的配置项mapred.min.split.size决定，默认为1；maxSize 由mapred-site.xml文件中的配置项mapred.max.split.size决定，默认为9223 372 036 854 775 807；而blockSize是由hdfs-site.xml文件中的配置项dfs.block.size决定，默认为67 108 864字节（64M）。所以InputSplit的大小确定公式为：\n```java\nmax(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size));\n```\n一般来说，dfs.block.size的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size 和 mapred.max.split.size 的大小即可。InputSplit的数量为文件大小除以InputSplitSize。InputSplit的原数据信息会通过一下代码取得：\n```java\nsplits.add(new FileSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts()));\n```\n从上面的代码可以发现，元数据的信息由四部分组成：文件路径，文件开始位置，文件结束位置，数据块所在的host。\n\n对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念，InputSplit所包含的数据是仍然存储在HDFS的块里面，它们之间的关系如下图所示：\n![image](http://img.blog.csdn.net/20161230102503415?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n当输入文件切分为InputSplit后，由FileInputFormat的子类（如TextInputFormat）的createRecordReader方法将InputSplit解析为键值对，代码如下：\n```java\n  public RecordReader<LongWritable, Text>\n    createRecordReader(InputSplit split,\n                       TaskAttemptContext context) {\n    String delimiter = context.getConfiguration().get(\n        \"textinputformat.record.delimiter\");\n    byte[] recordDelimiterBytes = null;\n    if (null != delimiter)\n      recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);\n    return new LineRecordReader(recordDelimiterBytes);\n  }\n```\n此处默认是将行号作为键。解析出来的键值对将被用来作为map函数的输入。至此input阶段结束。\n\n### 3. map及中间结果的输出\n\nInputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和 sort（排序）的操作。\n\nmap函数产生输出时，并不是简单的刷写磁盘。为了保证I/O效率，采取了先写到内存的环形内存缓冲区，并做一次预排序，如下图所示：\n![image](http://img.blog.csdn.net/20161230102521915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲区大小是100M，该值可以通过mapred-site.xml文件中的io.sort.mb的配置项配置。一旦缓冲区内容达到阈值（由mapred-site.xml文件的io.sort.spill.percent的值决定，默认为0.80 或者 80%），一个后台线程便会将缓冲区的内容溢写到磁盘中。再写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到mapred.local.dir（mapred-site.xml文件的配置项）配置的作业特定目录下。\n\n在写磁盘之前，线程会根据数据最终要传入到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程按照建进行内排序，此时如果有一个Combiner，它会在排序后的输出上运行。\n\n一旦内存缓冲区达到溢出的阈值，就会新建一个溢出写文件，因此在Map任务完成最后一个输出记录之后，会有若干个溢出写文件。在Map任务完成之前，溢出写文件被合并成一个已分区且已排序的输出文件作为map输出的中间结果，这也是Map任务的输出结果。\n\n如果已经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner的意义在于使map输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。\n\n为了提高磁盘IO性能，可以考虑压缩map的输出，这样会写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将mapred-site.xml文件的配置项mapred.compress.map.output设为true即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项mapred.map.output.compression.codec\n\n指定，如下列出了目前hadoop支持的压缩格式：\n\nmap输出的中间结果存储的格式为IFile，IFile是一种支持航压缩的存储格式，支持上述压缩算法。\n\nReducer通过Http方式得到输出文件的分区。将map输出的中间结果发送到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threds配置项决定，此配置针对每个节点，而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。\n\n### 4. shuffle\n\nshuffle，也叫数据清洗。在某些语境下，代表map函数产生输出到reduce的消化输入的整个过程。\n\n#### 4.1 copy阶段\n\nMap任务输出的结果位于Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是，Reduce任务可能需要多个Map任务的输出作为其特殊的分区文件。每个Map任务的完成时间可能不同，当只要有一个任务完成，Reduce任务就开始复制其输出。这就是shuffle的copy阶段。如下图所示，Reduce任务有少量复制线程，可以并行取得Map任务的输出，默认值为5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies的配置项来改变。\n![image](http://img.blog.csdn.net/20161230102548036?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n如果map输出相当小，则会被复制到Reduce所在TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffle.input.buffer.percent配置项指定。否则，map输出将会被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapred-site.xml文件mapred.job.shuffle.merge.percent配置项决定）或缓冲区的文件数达到阈值大小（由mapred-site.xml文件mapred.inmem.merge.threshold配置项决定），则合并后溢写到磁盘中。\n\n#### 4.2 sort阶段\n\n随着溢写到磁盘的文件增多，shuffle进行sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的io.sort.factor配置项决定，默认为10）为10，合并操作将进行5次，每次将10个文件合并成一个文件，最后有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合并，将会直接把5个文件交给Reduce函数处理。到此shuffle阶段完成。\n\n从shuffle的过程可以看出，Map任务处理的是一个InputSplit，而Reduce任务处理的是所有Map任务同一个分区的中间结果。\n\n### 5. reduce及最后结果的输出\n\nreduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。\n\n与map的中间结果不同的是，reduce的输出一般为HDFS。\n\n### 6. sort\n\n排序贯穿于Map任务和Reduce任务，排序操作属于MapReduce计算框架的默认行为，不管流程是否需要，都会进行排序。在MapReduce计算框架中，主要用到了两种排序算法：快速排序和归并排序。\n\n在Map任务和Reduce任务的过程中，一共发生了3次排序操作。\n\n(1) 当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阈值，在刷写磁盘之前，后台线程会将缓冲区的数据划分相应的分区。在每个分区中，后台线程按键进行内排序。如下图所示。\n\n(2) 在Map任务完成之前，磁盘上存在多个已经分好区，并排好序，大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。如下图所示。\n\n![image](http://img.blog.csdn.net/20161230102655183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n(3) 在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需在做一次排序就可以使输出文件整体有序。\n![image](http://img.blog.csdn.net/20161230102708631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序；第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。\n\n### 7. 作业的进度组成\n\n一个MapReduce作业在Hadoop上运行时，客户端的屏幕通常会打印作业日志，如下：\n\n![image](http://img.blog.csdn.net/20161230102726615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n对于一个大型的MapReduce作业来说，执行时间可能会比较比较长，通过日志了解作业的运行状态和作业进度是非常重要的。对于Map来说，进度代表实际处理输入所占比例，例如 map 60% reduce 0% 表示Map任务已经处理了作业输入文件的60%，而Reduce任务还没有开始。而对于Reduce的进度来说，情况比较复杂，从前面得知，reduce阶段分为copy，sort 和 reduce，这三个步骤共同组成了reduce的进度，各占1/3。如果reduce已经处理了2/3的输入，那么整个reduce的进度应该为1/3 + 1/3 + 1/3 * (2/3) = 5/9 ，因为reduce开始处理时，copy和sort已经完成。\n","source":"_posts/Hadoop/Hadoop MapReducer工作过程.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop MapReducer工作过程\ndate: 2016-12-30 10:28:17\nupdated: 2016-12-30 10:28:17\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. 从输入到输出\n\n一个MapReducer作业经过了input，map，combine，reduce，output五个阶段，其中combine阶段并不一定发生，map输出的中间结果被分到reduce的过程成为shuffle（数据清洗）。\n![image](http://img.blog.csdn.net/20161230102351523?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n在shuffle阶段还会发生copy（复制）和sort（排序）。\n\n在MapReduce的过程中，一个作业被分成Map和Reducer两个计算阶段，它们由一个或者多个Map任务和Reduce任务组成。如下图所示，一个MapReduce作业从数据的流向可以分为Map任务和Reduce任务。当用户向Hadoop提交一个MapReduce作业时，JobTracker则会根据各个TaskTracker周期性发送过来的心跳信息综合考虑TaskTracker的资源剩余量，作业优先级，作业提交时间等因素，为TaskTracker分配合适的任务。Reduce任务默认会在Map任务数量完成5%后才开始启动。\n![image](http://img.blog.csdn.net/20161230102431790?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\nMap任务的执行过程可以概括为：首先通过用户指定的InputFormat类中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。然后map函数经过处理之后将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。最后map函数将中间结果保存到本地。\n\nReduce任务的执行过程可以概括为：首先需要将已经完成Map任务的中间结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。\n\n### 2. input\n\n如果使用HDFS上的文件作为MapReduce的输入，MapReduce计算框架首先会用org.apache.hadoop.mapreduce.InputFomat类的子类FileInputFormat类将作为输入HDFS上的文件切分形成输入分片(InputSplit)，每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MaoReduce作业的性能有非常大的影响。\n\nInputSplit只是逻辑上对输入数据进行分片，并不会将文件在磁盘上分成分片进行存储。InputSplit只是记录了分片的元数据节点信息，例如起始位置，长度以及所在的节点列表等。数据切分的算法需要确定InputSplit的个数，对于HDFS上的文件，FileInputFormat类使用computeSplitSize方法计算出InputSplit的大小，代码如下：\n```java\nprotected long computeSplitSize(long blockSize, long minSize, long maxSize) {\n    return Math.max(minSize, Math.min(maxSize, blockSize));\n}\n```\n其中 minSize 由mapred-site.xml文件中的配置项mapred.min.split.size决定，默认为1；maxSize 由mapred-site.xml文件中的配置项mapred.max.split.size决定，默认为9223 372 036 854 775 807；而blockSize是由hdfs-site.xml文件中的配置项dfs.block.size决定，默认为67 108 864字节（64M）。所以InputSplit的大小确定公式为：\n```java\nmax(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size));\n```\n一般来说，dfs.block.size的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size 和 mapred.max.split.size 的大小即可。InputSplit的数量为文件大小除以InputSplitSize。InputSplit的原数据信息会通过一下代码取得：\n```java\nsplits.add(new FileSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts()));\n```\n从上面的代码可以发现，元数据的信息由四部分组成：文件路径，文件开始位置，文件结束位置，数据块所在的host。\n\n对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念，InputSplit所包含的数据是仍然存储在HDFS的块里面，它们之间的关系如下图所示：\n![image](http://img.blog.csdn.net/20161230102503415?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n当输入文件切分为InputSplit后，由FileInputFormat的子类（如TextInputFormat）的createRecordReader方法将InputSplit解析为键值对，代码如下：\n```java\n  public RecordReader<LongWritable, Text>\n    createRecordReader(InputSplit split,\n                       TaskAttemptContext context) {\n    String delimiter = context.getConfiguration().get(\n        \"textinputformat.record.delimiter\");\n    byte[] recordDelimiterBytes = null;\n    if (null != delimiter)\n      recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);\n    return new LineRecordReader(recordDelimiterBytes);\n  }\n```\n此处默认是将行号作为键。解析出来的键值对将被用来作为map函数的输入。至此input阶段结束。\n\n### 3. map及中间结果的输出\n\nInputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和 sort（排序）的操作。\n\nmap函数产生输出时，并不是简单的刷写磁盘。为了保证I/O效率，采取了先写到内存的环形内存缓冲区，并做一次预排序，如下图所示：\n![image](http://img.blog.csdn.net/20161230102521915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲区大小是100M，该值可以通过mapred-site.xml文件中的io.sort.mb的配置项配置。一旦缓冲区内容达到阈值（由mapred-site.xml文件的io.sort.spill.percent的值决定，默认为0.80 或者 80%），一个后台线程便会将缓冲区的内容溢写到磁盘中。再写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到mapred.local.dir（mapred-site.xml文件的配置项）配置的作业特定目录下。\n\n在写磁盘之前，线程会根据数据最终要传入到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程按照建进行内排序，此时如果有一个Combiner，它会在排序后的输出上运行。\n\n一旦内存缓冲区达到溢出的阈值，就会新建一个溢出写文件，因此在Map任务完成最后一个输出记录之后，会有若干个溢出写文件。在Map任务完成之前，溢出写文件被合并成一个已分区且已排序的输出文件作为map输出的中间结果，这也是Map任务的输出结果。\n\n如果已经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner的意义在于使map输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。\n\n为了提高磁盘IO性能，可以考虑压缩map的输出，这样会写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将mapred-site.xml文件的配置项mapred.compress.map.output设为true即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项mapred.map.output.compression.codec\n\n指定，如下列出了目前hadoop支持的压缩格式：\n\nmap输出的中间结果存储的格式为IFile，IFile是一种支持航压缩的存储格式，支持上述压缩算法。\n\nReducer通过Http方式得到输出文件的分区。将map输出的中间结果发送到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threds配置项决定，此配置针对每个节点，而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。\n\n### 4. shuffle\n\nshuffle，也叫数据清洗。在某些语境下，代表map函数产生输出到reduce的消化输入的整个过程。\n\n#### 4.1 copy阶段\n\nMap任务输出的结果位于Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是，Reduce任务可能需要多个Map任务的输出作为其特殊的分区文件。每个Map任务的完成时间可能不同，当只要有一个任务完成，Reduce任务就开始复制其输出。这就是shuffle的copy阶段。如下图所示，Reduce任务有少量复制线程，可以并行取得Map任务的输出，默认值为5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies的配置项来改变。\n![image](http://img.blog.csdn.net/20161230102548036?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n如果map输出相当小，则会被复制到Reduce所在TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffle.input.buffer.percent配置项指定。否则，map输出将会被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapred-site.xml文件mapred.job.shuffle.merge.percent配置项决定）或缓冲区的文件数达到阈值大小（由mapred-site.xml文件mapred.inmem.merge.threshold配置项决定），则合并后溢写到磁盘中。\n\n#### 4.2 sort阶段\n\n随着溢写到磁盘的文件增多，shuffle进行sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的io.sort.factor配置项决定，默认为10）为10，合并操作将进行5次，每次将10个文件合并成一个文件，最后有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合并，将会直接把5个文件交给Reduce函数处理。到此shuffle阶段完成。\n\n从shuffle的过程可以看出，Map任务处理的是一个InputSplit，而Reduce任务处理的是所有Map任务同一个分区的中间结果。\n\n### 5. reduce及最后结果的输出\n\nreduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。\n\n与map的中间结果不同的是，reduce的输出一般为HDFS。\n\n### 6. sort\n\n排序贯穿于Map任务和Reduce任务，排序操作属于MapReduce计算框架的默认行为，不管流程是否需要，都会进行排序。在MapReduce计算框架中，主要用到了两种排序算法：快速排序和归并排序。\n\n在Map任务和Reduce任务的过程中，一共发生了3次排序操作。\n\n(1) 当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阈值，在刷写磁盘之前，后台线程会将缓冲区的数据划分相应的分区。在每个分区中，后台线程按键进行内排序。如下图所示。\n\n(2) 在Map任务完成之前，磁盘上存在多个已经分好区，并排好序，大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。如下图所示。\n\n![image](http://img.blog.csdn.net/20161230102655183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n(3) 在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需在做一次排序就可以使输出文件整体有序。\n![image](http://img.blog.csdn.net/20161230102708631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序；第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。\n\n### 7. 作业的进度组成\n\n一个MapReduce作业在Hadoop上运行时，客户端的屏幕通常会打印作业日志，如下：\n\n![image](http://img.blog.csdn.net/20161230102726615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n对于一个大型的MapReduce作业来说，执行时间可能会比较比较长，通过日志了解作业的运行状态和作业进度是非常重要的。对于Map来说，进度代表实际处理输入所占比例，例如 map 60% reduce 0% 表示Map任务已经处理了作业输入文件的60%，而Reduce任务还没有开始。而对于Reduce的进度来说，情况比较复杂，从前面得知，reduce阶段分为copy，sort 和 reduce，这三个步骤共同组成了reduce的进度，各占1/3。如果reduce已经处理了2/3的输入，那么整个reduce的进度应该为1/3 + 1/3 + 1/3 * (2/3) = 5/9 ，因为reduce开始处理时，copy和sort已经完成。\n","slug":"Hadoop/Hadoop MapReducer工作过程","published":1,"comments":1,"photos":[],"link":"","_id":"cje58tisf002rordbw5aqdh54","content":"<h3 id=\"1-从输入到输出\"><a href=\"#1-从输入到输出\" class=\"headerlink\" title=\"1. 从输入到输出\"></a>1. 从输入到输出</h3><p>一个MapReducer作业经过了input，map，combine，reduce，output五个阶段，其中combine阶段并不一定发生，map输出的中间结果被分到reduce的过程成为shuffle（数据清洗）。<br><img src=\"http://img.blog.csdn.net/20161230102351523?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>在shuffle阶段还会发生copy（复制）和sort（排序）。</p>\n<p>在MapReduce的过程中，一个作业被分成Map和Reducer两个计算阶段，它们由一个或者多个Map任务和Reduce任务组成。如下图所示，一个MapReduce作业从数据的流向可以分为Map任务和Reduce任务。当用户向Hadoop提交一个MapReduce作业时，JobTracker则会根据各个TaskTracker周期性发送过来的心跳信息综合考虑TaskTracker的资源剩余量，作业优先级，作业提交时间等因素，为TaskTracker分配合适的任务。Reduce任务默认会在Map任务数量完成5%后才开始启动。<br><img src=\"http://img.blog.csdn.net/20161230102431790?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>Map任务的执行过程可以概括为：首先通过用户指定的InputFormat类中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。然后map函数经过处理之后将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。最后map函数将中间结果保存到本地。</p>\n<p>Reduce任务的执行过程可以概括为：首先需要将已经完成Map任务的中间结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。</p>\n<h3 id=\"2-input\"><a href=\"#2-input\" class=\"headerlink\" title=\"2. input\"></a>2. input</h3><p>如果使用HDFS上的文件作为MapReduce的输入，MapReduce计算框架首先会用org.apache.hadoop.mapreduce.InputFomat类的子类FileInputFormat类将作为输入HDFS上的文件切分形成输入分片(InputSplit)，每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MaoReduce作业的性能有非常大的影响。</p>\n<p>InputSplit只是逻辑上对输入数据进行分片，并不会将文件在磁盘上分成分片进行存储。InputSplit只是记录了分片的元数据节点信息，例如起始位置，长度以及所在的节点列表等。数据切分的算法需要确定InputSplit的个数，对于HDFS上的文件，FileInputFormat类使用computeSplitSize方法计算出InputSplit的大小，代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">long</span> <span class=\"title\">computeSplitSize</span><span class=\"params\">(<span class=\"keyword\">long</span> blockSize, <span class=\"keyword\">long</span> minSize, <span class=\"keyword\">long</span> maxSize)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>其中 minSize 由mapred-site.xml文件中的配置项mapred.min.split.size决定，默认为1；maxSize 由mapred-site.xml文件中的配置项mapred.max.split.size决定，默认为9223 372 036 854 775 807；而blockSize是由hdfs-site.xml文件中的配置项dfs.block.size决定，默认为67 108 864字节（64M）。所以InputSplit的大小确定公式为：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">max(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size));</span><br></pre></td></tr></table></figure></p>\n<p>一般来说，dfs.block.size的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size 和 mapred.max.split.size 的大小即可。InputSplit的数量为文件大小除以InputSplitSize。InputSplit的原数据信息会通过一下代码取得：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">splits.add(<span class=\"keyword\">new</span> FileSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts()));</span><br></pre></td></tr></table></figure></p>\n<p>从上面的代码可以发现，元数据的信息由四部分组成：文件路径，文件开始位置，文件结束位置，数据块所在的host。</p>\n<p>对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念，InputSplit所包含的数据是仍然存储在HDFS的块里面，它们之间的关系如下图所示：<br><img src=\"http://img.blog.csdn.net/20161230102503415?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>当输入文件切分为InputSplit后，由FileInputFormat的子类（如TextInputFormat）的createRecordReader方法将InputSplit解析为键值对，代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> RecordReader&lt;LongWritable, Text&gt;</span><br><span class=\"line\">  createRecordReader(InputSplit split,</span><br><span class=\"line\">                     TaskAttemptContext context) &#123;</span><br><span class=\"line\">  String delimiter = context.getConfiguration().get(</span><br><span class=\"line\">      <span class=\"string\">\"textinputformat.record.delimiter\"</span>);</span><br><span class=\"line\">  <span class=\"keyword\">byte</span>[] recordDelimiterBytes = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"keyword\">null</span> != delimiter)</span><br><span class=\"line\">    recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> LineRecordReader(recordDelimiterBytes);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>此处默认是将行号作为键。解析出来的键值对将被用来作为map函数的输入。至此input阶段结束。</p>\n<h3 id=\"3-map及中间结果的输出\"><a href=\"#3-map及中间结果的输出\" class=\"headerlink\" title=\"3. map及中间结果的输出\"></a>3. map及中间结果的输出</h3><p>InputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和 sort（排序）的操作。</p>\n<p>map函数产生输出时，并不是简单的刷写磁盘。为了保证I/O效率，采取了先写到内存的环形内存缓冲区，并做一次预排序，如下图所示：<br><img src=\"http://img.blog.csdn.net/20161230102521915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲区大小是100M，该值可以通过mapred-site.xml文件中的io.sort.mb的配置项配置。一旦缓冲区内容达到阈值（由mapred-site.xml文件的io.sort.spill.percent的值决定，默认为0.80 或者 80%），一个后台线程便会将缓冲区的内容溢写到磁盘中。再写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到mapred.local.dir（mapred-site.xml文件的配置项）配置的作业特定目录下。</p>\n<p>在写磁盘之前，线程会根据数据最终要传入到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程按照建进行内排序，此时如果有一个Combiner，它会在排序后的输出上运行。</p>\n<p>一旦内存缓冲区达到溢出的阈值，就会新建一个溢出写文件，因此在Map任务完成最后一个输出记录之后，会有若干个溢出写文件。在Map任务完成之前，溢出写文件被合并成一个已分区且已排序的输出文件作为map输出的中间结果，这也是Map任务的输出结果。</p>\n<p>如果已经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner的意义在于使map输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。</p>\n<p>为了提高磁盘IO性能，可以考虑压缩map的输出，这样会写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将mapred-site.xml文件的配置项mapred.compress.map.output设为true即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项mapred.map.output.compression.codec</p>\n<p>指定，如下列出了目前hadoop支持的压缩格式：</p>\n<p>map输出的中间结果存储的格式为IFile，IFile是一种支持航压缩的存储格式，支持上述压缩算法。</p>\n<p>Reducer通过Http方式得到输出文件的分区。将map输出的中间结果发送到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threds配置项决定，此配置针对每个节点，而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。</p>\n<h3 id=\"4-shuffle\"><a href=\"#4-shuffle\" class=\"headerlink\" title=\"4. shuffle\"></a>4. shuffle</h3><p>shuffle，也叫数据清洗。在某些语境下，代表map函数产生输出到reduce的消化输入的整个过程。</p>\n<h4 id=\"4-1-copy阶段\"><a href=\"#4-1-copy阶段\" class=\"headerlink\" title=\"4.1 copy阶段\"></a>4.1 copy阶段</h4><p>Map任务输出的结果位于Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是，Reduce任务可能需要多个Map任务的输出作为其特殊的分区文件。每个Map任务的完成时间可能不同，当只要有一个任务完成，Reduce任务就开始复制其输出。这就是shuffle的copy阶段。如下图所示，Reduce任务有少量复制线程，可以并行取得Map任务的输出，默认值为5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies的配置项来改变。<br><img src=\"http://img.blog.csdn.net/20161230102548036?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>如果map输出相当小，则会被复制到Reduce所在TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffle.input.buffer.percent配置项指定。否则，map输出将会被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapred-site.xml文件mapred.job.shuffle.merge.percent配置项决定）或缓冲区的文件数达到阈值大小（由mapred-site.xml文件mapred.inmem.merge.threshold配置项决定），则合并后溢写到磁盘中。</p>\n<h4 id=\"4-2-sort阶段\"><a href=\"#4-2-sort阶段\" class=\"headerlink\" title=\"4.2 sort阶段\"></a>4.2 sort阶段</h4><p>随着溢写到磁盘的文件增多，shuffle进行sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的io.sort.factor配置项决定，默认为10）为10，合并操作将进行5次，每次将10个文件合并成一个文件，最后有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合并，将会直接把5个文件交给Reduce函数处理。到此shuffle阶段完成。</p>\n<p>从shuffle的过程可以看出，Map任务处理的是一个InputSplit，而Reduce任务处理的是所有Map任务同一个分区的中间结果。</p>\n<h3 id=\"5-reduce及最后结果的输出\"><a href=\"#5-reduce及最后结果的输出\" class=\"headerlink\" title=\"5. reduce及最后结果的输出\"></a>5. reduce及最后结果的输出</h3><p>reduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。</p>\n<p>与map的中间结果不同的是，reduce的输出一般为HDFS。</p>\n<h3 id=\"6-sort\"><a href=\"#6-sort\" class=\"headerlink\" title=\"6. sort\"></a>6. sort</h3><p>排序贯穿于Map任务和Reduce任务，排序操作属于MapReduce计算框架的默认行为，不管流程是否需要，都会进行排序。在MapReduce计算框架中，主要用到了两种排序算法：快速排序和归并排序。</p>\n<p>在Map任务和Reduce任务的过程中，一共发生了3次排序操作。</p>\n<p>(1) 当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阈值，在刷写磁盘之前，后台线程会将缓冲区的数据划分相应的分区。在每个分区中，后台线程按键进行内排序。如下图所示。</p>\n<p>(2) 在Map任务完成之前，磁盘上存在多个已经分好区，并排好序，大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。如下图所示。</p>\n<p><img src=\"http://img.blog.csdn.net/20161230102655183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>(3) 在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需在做一次排序就可以使输出文件整体有序。<br><img src=\"http://img.blog.csdn.net/20161230102708631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序；第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。</p>\n<h3 id=\"7-作业的进度组成\"><a href=\"#7-作业的进度组成\" class=\"headerlink\" title=\"7. 作业的进度组成\"></a>7. 作业的进度组成</h3><p>一个MapReduce作业在Hadoop上运行时，客户端的屏幕通常会打印作业日志，如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20161230102726615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>对于一个大型的MapReduce作业来说，执行时间可能会比较比较长，通过日志了解作业的运行状态和作业进度是非常重要的。对于Map来说，进度代表实际处理输入所占比例，例如 map 60% reduce 0% 表示Map任务已经处理了作业输入文件的60%，而Reduce任务还没有开始。而对于Reduce的进度来说，情况比较复杂，从前面得知，reduce阶段分为copy，sort 和 reduce，这三个步骤共同组成了reduce的进度，各占1/3。如果reduce已经处理了2/3的输入，那么整个reduce的进度应该为1/3 + 1/3 + 1/3 * (2/3) = 5/9 ，因为reduce开始处理时，copy和sort已经完成。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-从输入到输出\"><a href=\"#1-从输入到输出\" class=\"headerlink\" title=\"1. 从输入到输出\"></a>1. 从输入到输出</h3><p>一个MapReducer作业经过了input，map，combine，reduce，output五个阶段，其中combine阶段并不一定发生，map输出的中间结果被分到reduce的过程成为shuffle（数据清洗）。<br><img src=\"http://img.blog.csdn.net/20161230102351523?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>在shuffle阶段还会发生copy（复制）和sort（排序）。</p>\n<p>在MapReduce的过程中，一个作业被分成Map和Reducer两个计算阶段，它们由一个或者多个Map任务和Reduce任务组成。如下图所示，一个MapReduce作业从数据的流向可以分为Map任务和Reduce任务。当用户向Hadoop提交一个MapReduce作业时，JobTracker则会根据各个TaskTracker周期性发送过来的心跳信息综合考虑TaskTracker的资源剩余量，作业优先级，作业提交时间等因素，为TaskTracker分配合适的任务。Reduce任务默认会在Map任务数量完成5%后才开始启动。<br><img src=\"http://img.blog.csdn.net/20161230102431790?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>Map任务的执行过程可以概括为：首先通过用户指定的InputFormat类中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。然后map函数经过处理之后将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。最后map函数将中间结果保存到本地。</p>\n<p>Reduce任务的执行过程可以概括为：首先需要将已经完成Map任务的中间结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。</p>\n<h3 id=\"2-input\"><a href=\"#2-input\" class=\"headerlink\" title=\"2. input\"></a>2. input</h3><p>如果使用HDFS上的文件作为MapReduce的输入，MapReduce计算框架首先会用org.apache.hadoop.mapreduce.InputFomat类的子类FileInputFormat类将作为输入HDFS上的文件切分形成输入分片(InputSplit)，每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MaoReduce作业的性能有非常大的影响。</p>\n<p>InputSplit只是逻辑上对输入数据进行分片，并不会将文件在磁盘上分成分片进行存储。InputSplit只是记录了分片的元数据节点信息，例如起始位置，长度以及所在的节点列表等。数据切分的算法需要确定InputSplit的个数，对于HDFS上的文件，FileInputFormat类使用computeSplitSize方法计算出InputSplit的大小，代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">long</span> <span class=\"title\">computeSplitSize</span><span class=\"params\">(<span class=\"keyword\">long</span> blockSize, <span class=\"keyword\">long</span> minSize, <span class=\"keyword\">long</span> maxSize)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>其中 minSize 由mapred-site.xml文件中的配置项mapred.min.split.size决定，默认为1；maxSize 由mapred-site.xml文件中的配置项mapred.max.split.size决定，默认为9223 372 036 854 775 807；而blockSize是由hdfs-site.xml文件中的配置项dfs.block.size决定，默认为67 108 864字节（64M）。所以InputSplit的大小确定公式为：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">max(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size));</span><br></pre></td></tr></table></figure></p>\n<p>一般来说，dfs.block.size的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size 和 mapred.max.split.size 的大小即可。InputSplit的数量为文件大小除以InputSplitSize。InputSplit的原数据信息会通过一下代码取得：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">splits.add(<span class=\"keyword\">new</span> FileSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts()));</span><br></pre></td></tr></table></figure></p>\n<p>从上面的代码可以发现，元数据的信息由四部分组成：文件路径，文件开始位置，文件结束位置，数据块所在的host。</p>\n<p>对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念，InputSplit所包含的数据是仍然存储在HDFS的块里面，它们之间的关系如下图所示：<br><img src=\"http://img.blog.csdn.net/20161230102503415?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>当输入文件切分为InputSplit后，由FileInputFormat的子类（如TextInputFormat）的createRecordReader方法将InputSplit解析为键值对，代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> RecordReader&lt;LongWritable, Text&gt;</span><br><span class=\"line\">  createRecordReader(InputSplit split,</span><br><span class=\"line\">                     TaskAttemptContext context) &#123;</span><br><span class=\"line\">  String delimiter = context.getConfiguration().get(</span><br><span class=\"line\">      <span class=\"string\">\"textinputformat.record.delimiter\"</span>);</span><br><span class=\"line\">  <span class=\"keyword\">byte</span>[] recordDelimiterBytes = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"keyword\">null</span> != delimiter)</span><br><span class=\"line\">    recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> LineRecordReader(recordDelimiterBytes);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>此处默认是将行号作为键。解析出来的键值对将被用来作为map函数的输入。至此input阶段结束。</p>\n<h3 id=\"3-map及中间结果的输出\"><a href=\"#3-map及中间结果的输出\" class=\"headerlink\" title=\"3. map及中间结果的输出\"></a>3. map及中间结果的输出</h3><p>InputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和 sort（排序）的操作。</p>\n<p>map函数产生输出时，并不是简单的刷写磁盘。为了保证I/O效率，采取了先写到内存的环形内存缓冲区，并做一次预排序，如下图所示：<br><img src=\"http://img.blog.csdn.net/20161230102521915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲区大小是100M，该值可以通过mapred-site.xml文件中的io.sort.mb的配置项配置。一旦缓冲区内容达到阈值（由mapred-site.xml文件的io.sort.spill.percent的值决定，默认为0.80 或者 80%），一个后台线程便会将缓冲区的内容溢写到磁盘中。再写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满，map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到mapred.local.dir（mapred-site.xml文件的配置项）配置的作业特定目录下。</p>\n<p>在写磁盘之前，线程会根据数据最终要传入到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程按照建进行内排序，此时如果有一个Combiner，它会在排序后的输出上运行。</p>\n<p>一旦内存缓冲区达到溢出的阈值，就会新建一个溢出写文件，因此在Map任务完成最后一个输出记录之后，会有若干个溢出写文件。在Map任务完成之前，溢出写文件被合并成一个已分区且已排序的输出文件作为map输出的中间结果，这也是Map任务的输出结果。</p>\n<p>如果已经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner的意义在于使map输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。</p>\n<p>为了提高磁盘IO性能，可以考虑压缩map的输出，这样会写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将mapred-site.xml文件的配置项mapred.compress.map.output设为true即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项mapred.map.output.compression.codec</p>\n<p>指定，如下列出了目前hadoop支持的压缩格式：</p>\n<p>map输出的中间结果存储的格式为IFile，IFile是一种支持航压缩的存储格式，支持上述压缩算法。</p>\n<p>Reducer通过Http方式得到输出文件的分区。将map输出的中间结果发送到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threds配置项决定，此配置针对每个节点，而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。</p>\n<h3 id=\"4-shuffle\"><a href=\"#4-shuffle\" class=\"headerlink\" title=\"4. shuffle\"></a>4. shuffle</h3><p>shuffle，也叫数据清洗。在某些语境下，代表map函数产生输出到reduce的消化输入的整个过程。</p>\n<h4 id=\"4-1-copy阶段\"><a href=\"#4-1-copy阶段\" class=\"headerlink\" title=\"4.1 copy阶段\"></a>4.1 copy阶段</h4><p>Map任务输出的结果位于Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是，Reduce任务可能需要多个Map任务的输出作为其特殊的分区文件。每个Map任务的完成时间可能不同，当只要有一个任务完成，Reduce任务就开始复制其输出。这就是shuffle的copy阶段。如下图所示，Reduce任务有少量复制线程，可以并行取得Map任务的输出，默认值为5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies的配置项来改变。<br><img src=\"http://img.blog.csdn.net/20161230102548036?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"><br>如果map输出相当小，则会被复制到Reduce所在TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffle.input.buffer.percent配置项指定。否则，map输出将会被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapred-site.xml文件mapred.job.shuffle.merge.percent配置项决定）或缓冲区的文件数达到阈值大小（由mapred-site.xml文件mapred.inmem.merge.threshold配置项决定），则合并后溢写到磁盘中。</p>\n<h4 id=\"4-2-sort阶段\"><a href=\"#4-2-sort阶段\" class=\"headerlink\" title=\"4.2 sort阶段\"></a>4.2 sort阶段</h4><p>随着溢写到磁盘的文件增多，shuffle进行sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的io.sort.factor配置项决定，默认为10）为10，合并操作将进行5次，每次将10个文件合并成一个文件，最后有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合并，将会直接把5个文件交给Reduce函数处理。到此shuffle阶段完成。</p>\n<p>从shuffle的过程可以看出，Map任务处理的是一个InputSplit，而Reduce任务处理的是所有Map任务同一个分区的中间结果。</p>\n<h3 id=\"5-reduce及最后结果的输出\"><a href=\"#5-reduce及最后结果的输出\" class=\"headerlink\" title=\"5. reduce及最后结果的输出\"></a>5. reduce及最后结果的输出</h3><p>reduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。</p>\n<p>与map的中间结果不同的是，reduce的输出一般为HDFS。</p>\n<h3 id=\"6-sort\"><a href=\"#6-sort\" class=\"headerlink\" title=\"6. sort\"></a>6. sort</h3><p>排序贯穿于Map任务和Reduce任务，排序操作属于MapReduce计算框架的默认行为，不管流程是否需要，都会进行排序。在MapReduce计算框架中，主要用到了两种排序算法：快速排序和归并排序。</p>\n<p>在Map任务和Reduce任务的过程中，一共发生了3次排序操作。</p>\n<p>(1) 当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阈值，在刷写磁盘之前，后台线程会将缓冲区的数据划分相应的分区。在每个分区中，后台线程按键进行内排序。如下图所示。</p>\n<p>(2) 在Map任务完成之前，磁盘上存在多个已经分好区，并排好序，大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。如下图所示。</p>\n<p><img src=\"http://img.blog.csdn.net/20161230102655183?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>(3) 在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需在做一次排序就可以使输出文件整体有序。<br><img src=\"http://img.blog.csdn.net/20161230102708631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序；第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。</p>\n<h3 id=\"7-作业的进度组成\"><a href=\"#7-作业的进度组成\" class=\"headerlink\" title=\"7. 作业的进度组成\"></a>7. 作业的进度组成</h3><p>一个MapReduce作业在Hadoop上运行时，客户端的屏幕通常会打印作业日志，如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20161230102726615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU3VubnlZb29uYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"image\"></p>\n<p>对于一个大型的MapReduce作业来说，执行时间可能会比较比较长，通过日志了解作业的运行状态和作业进度是非常重要的。对于Map来说，进度代表实际处理输入所占比例，例如 map 60% reduce 0% 表示Map任务已经处理了作业输入文件的60%，而Reduce任务还没有开始。而对于Reduce的进度来说，情况比较复杂，从前面得知，reduce阶段分为copy，sort 和 reduce，这三个步骤共同组成了reduce的进度，各占1/3。如果reduce已经处理了2/3的输入，那么整个reduce的进度应该为1/3 + 1/3 + 1/3 * (2/3) = 5/9 ，因为reduce开始处理时，copy和sort已经完成。</p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop MapReduce新一代架构MRv2","date":"2017-12-08T05:49:01.000Z","_content":"\n`MapReduce`在`hadoop-0.23`中经历了彻底的改变，现在我们称之为`MapReduce 2.0`(MRv2)或者`YARN`。\n\n`MRv2`的基本思想是将`JobTracker`的两个主要功能，资源管理和作业调度/监视的功能拆分为独立的守护进程。设计思想是将`MRv1`中的`JobTracker`拆分成了两个独立的服务：一个全局的资源管理器`ResourceManager`(`RM`)和每个应用程序特有的`ApplicationMaster`(`AM`)。每个应用程序要么是单个作业，要么是`DAG`作业。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%8B%E4%B8%80%E4%BB%A3MapReduce-yarn-architecture.gif?raw=true)\n\n### 1. ResourceManager\n\n`ResourceManager`(`RM`)和每个从节点以及`NodeManager`(`NM`)构成了数据计算框架。`ResourceManager`是系统中所有应用程序资源分配的最终决策者。\n\n`ResourceManager`有两个主要组件:`Scheduler`(调度器) 和 `ApplicationsManager`。\n\n#### 1.1 Scheduler\n\n`Scheduler`根据容量，队列等限制条件将资源分配给各种正在运行的应用程序。`Scheduler`是'纯调度器'，因为它负责监视或跟踪应用程序的状态。此外，它也不保证会重启由于应用程序错误或硬件故障原因导致失败的任务。`Scheduler`仅根据应用程序的资源请求来执行调度。它基于'资源容器'(Resource Container)这一抽象概念来实现的，资源容器包括如内存，cpu，磁盘，网络等。\n\n`Scheduler`是一个可插拔的组件，它负责将集群资源分配给不同队列和应用程序。目前`Scheduler`支持诸如`CapacityScheduler`和`FairScheduler`。`CapacityScheduler`支持分层队列，以便更可预测地共享群集资源\n\n#### 1.2 ApplicationsManager\n\n`ApplicationsManager`(`ASM`)主要负责接受作业提交，协商获取第一个容器来执行应用程序的`ApplicationMaster`(negotiating the first container for executing the application specific ApplicationMaster)，并提供在故障时重新启动`ApplicationMaster`的服务。\n\n### 2. NodeManager\n\n`NodeManager`是每个节点上框架代理，主要负责启动应用所需要的容器，监视它们的资源使用情况(cpu，内存，磁盘，网络)，并将其报告给`ResourceManager`的`Scheduler`。\n\n### 3. ApplicationMaster\n\n事实上，每一个应用程序的`ApplicationMaster`是一个框架库，负责与`Scheduler`协商合适的资源容器以及与`NodeManager`一起跟踪他们的状态并监视进度。\n\n`MRV2`保持与以前稳定版本(hadoop-1.x)API的兼容性。这意味着所有的`Map-Reduce`作业仍然可以在`MRv2`上运行，只需重新编译即可。\n\n\n原文:http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html\n","source":"_posts/Hadoop/Hadoop MapReduce新一代架构MRv2.md","raw":"\n---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop MapReduce新一代架构MRv2\ndate: 2017-12-08 13:49:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n`MapReduce`在`hadoop-0.23`中经历了彻底的改变，现在我们称之为`MapReduce 2.0`(MRv2)或者`YARN`。\n\n`MRv2`的基本思想是将`JobTracker`的两个主要功能，资源管理和作业调度/监视的功能拆分为独立的守护进程。设计思想是将`MRv1`中的`JobTracker`拆分成了两个独立的服务：一个全局的资源管理器`ResourceManager`(`RM`)和每个应用程序特有的`ApplicationMaster`(`AM`)。每个应用程序要么是单个作业，要么是`DAG`作业。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%8B%E4%B8%80%E4%BB%A3MapReduce-yarn-architecture.gif?raw=true)\n\n### 1. ResourceManager\n\n`ResourceManager`(`RM`)和每个从节点以及`NodeManager`(`NM`)构成了数据计算框架。`ResourceManager`是系统中所有应用程序资源分配的最终决策者。\n\n`ResourceManager`有两个主要组件:`Scheduler`(调度器) 和 `ApplicationsManager`。\n\n#### 1.1 Scheduler\n\n`Scheduler`根据容量，队列等限制条件将资源分配给各种正在运行的应用程序。`Scheduler`是'纯调度器'，因为它负责监视或跟踪应用程序的状态。此外，它也不保证会重启由于应用程序错误或硬件故障原因导致失败的任务。`Scheduler`仅根据应用程序的资源请求来执行调度。它基于'资源容器'(Resource Container)这一抽象概念来实现的，资源容器包括如内存，cpu，磁盘，网络等。\n\n`Scheduler`是一个可插拔的组件，它负责将集群资源分配给不同队列和应用程序。目前`Scheduler`支持诸如`CapacityScheduler`和`FairScheduler`。`CapacityScheduler`支持分层队列，以便更可预测地共享群集资源\n\n#### 1.2 ApplicationsManager\n\n`ApplicationsManager`(`ASM`)主要负责接受作业提交，协商获取第一个容器来执行应用程序的`ApplicationMaster`(negotiating the first container for executing the application specific ApplicationMaster)，并提供在故障时重新启动`ApplicationMaster`的服务。\n\n### 2. NodeManager\n\n`NodeManager`是每个节点上框架代理，主要负责启动应用所需要的容器，监视它们的资源使用情况(cpu，内存，磁盘，网络)，并将其报告给`ResourceManager`的`Scheduler`。\n\n### 3. ApplicationMaster\n\n事实上，每一个应用程序的`ApplicationMaster`是一个框架库，负责与`Scheduler`协商合适的资源容器以及与`NodeManager`一起跟踪他们的状态并监视进度。\n\n`MRV2`保持与以前稳定版本(hadoop-1.x)API的兼容性。这意味着所有的`Map-Reduce`作业仍然可以在`MRv2`上运行，只需重新编译即可。\n\n\n原文:http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html\n","slug":"Hadoop/Hadoop MapReduce新一代架构MRv2","published":1,"updated":"2018-01-29T09:36:59.634Z","comments":1,"photos":[],"link":"","_id":"cje58tish002uordb7j5p8vx6","content":"<p><code>MapReduce</code>在<code>hadoop-0.23</code>中经历了彻底的改变，现在我们称之为<code>MapReduce 2.0</code>(MRv2)或者<code>YARN</code>。</p>\n<p><code>MRv2</code>的基本思想是将<code>JobTracker</code>的两个主要功能，资源管理和作业调度/监视的功能拆分为独立的守护进程。设计思想是将<code>MRv1</code>中的<code>JobTracker</code>拆分成了两个独立的服务：一个全局的资源管理器<code>ResourceManager</code>(<code>RM</code>)和每个应用程序特有的<code>ApplicationMaster</code>(<code>AM</code>)。每个应用程序要么是单个作业，要么是<code>DAG</code>作业。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%8B%E4%B8%80%E4%BB%A3MapReduce-yarn-architecture.gif?raw=true\" alt=\"\"></p>\n<h3 id=\"1-ResourceManager\"><a href=\"#1-ResourceManager\" class=\"headerlink\" title=\"1. ResourceManager\"></a>1. ResourceManager</h3><p><code>ResourceManager</code>(<code>RM</code>)和每个从节点以及<code>NodeManager</code>(<code>NM</code>)构成了数据计算框架。<code>ResourceManager</code>是系统中所有应用程序资源分配的最终决策者。</p>\n<p><code>ResourceManager</code>有两个主要组件:<code>Scheduler</code>(调度器) 和 <code>ApplicationsManager</code>。</p>\n<h4 id=\"1-1-Scheduler\"><a href=\"#1-1-Scheduler\" class=\"headerlink\" title=\"1.1 Scheduler\"></a>1.1 Scheduler</h4><p><code>Scheduler</code>根据容量，队列等限制条件将资源分配给各种正在运行的应用程序。<code>Scheduler</code>是’纯调度器’，因为它负责监视或跟踪应用程序的状态。此外，它也不保证会重启由于应用程序错误或硬件故障原因导致失败的任务。<code>Scheduler</code>仅根据应用程序的资源请求来执行调度。它基于’资源容器’(Resource Container)这一抽象概念来实现的，资源容器包括如内存，cpu，磁盘，网络等。</p>\n<p><code>Scheduler</code>是一个可插拔的组件，它负责将集群资源分配给不同队列和应用程序。目前<code>Scheduler</code>支持诸如<code>CapacityScheduler</code>和<code>FairScheduler</code>。<code>CapacityScheduler</code>支持分层队列，以便更可预测地共享群集资源</p>\n<h4 id=\"1-2-ApplicationsManager\"><a href=\"#1-2-ApplicationsManager\" class=\"headerlink\" title=\"1.2 ApplicationsManager\"></a>1.2 ApplicationsManager</h4><p><code>ApplicationsManager</code>(<code>ASM</code>)主要负责接受作业提交，协商获取第一个容器来执行应用程序的<code>ApplicationMaster</code>(negotiating the first container for executing the application specific ApplicationMaster)，并提供在故障时重新启动<code>ApplicationMaster</code>的服务。</p>\n<h3 id=\"2-NodeManager\"><a href=\"#2-NodeManager\" class=\"headerlink\" title=\"2. NodeManager\"></a>2. NodeManager</h3><p><code>NodeManager</code>是每个节点上框架代理，主要负责启动应用所需要的容器，监视它们的资源使用情况(cpu，内存，磁盘，网络)，并将其报告给<code>ResourceManager</code>的<code>Scheduler</code>。</p>\n<h3 id=\"3-ApplicationMaster\"><a href=\"#3-ApplicationMaster\" class=\"headerlink\" title=\"3. ApplicationMaster\"></a>3. ApplicationMaster</h3><p>事实上，每一个应用程序的<code>ApplicationMaster</code>是一个框架库，负责与<code>Scheduler</code>协商合适的资源容器以及与<code>NodeManager</code>一起跟踪他们的状态并监视进度。</p>\n<p><code>MRV2</code>保持与以前稳定版本(hadoop-1.x)API的兼容性。这意味着所有的<code>Map-Reduce</code>作业仍然可以在<code>MRv2</code>上运行，只需重新编译即可。</p>\n<p>原文:<a href=\"http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>MapReduce</code>在<code>hadoop-0.23</code>中经历了彻底的改变，现在我们称之为<code>MapReduce 2.0</code>(MRv2)或者<code>YARN</code>。</p>\n<p><code>MRv2</code>的基本思想是将<code>JobTracker</code>的两个主要功能，资源管理和作业调度/监视的功能拆分为独立的守护进程。设计思想是将<code>MRv1</code>中的<code>JobTracker</code>拆分成了两个独立的服务：一个全局的资源管理器<code>ResourceManager</code>(<code>RM</code>)和每个应用程序特有的<code>ApplicationMaster</code>(<code>AM</code>)。每个应用程序要么是单个作业，要么是<code>DAG</code>作业。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%E4%B8%8B%E4%B8%80%E4%BB%A3MapReduce-yarn-architecture.gif?raw=true\" alt=\"\"></p>\n<h3 id=\"1-ResourceManager\"><a href=\"#1-ResourceManager\" class=\"headerlink\" title=\"1. ResourceManager\"></a>1. ResourceManager</h3><p><code>ResourceManager</code>(<code>RM</code>)和每个从节点以及<code>NodeManager</code>(<code>NM</code>)构成了数据计算框架。<code>ResourceManager</code>是系统中所有应用程序资源分配的最终决策者。</p>\n<p><code>ResourceManager</code>有两个主要组件:<code>Scheduler</code>(调度器) 和 <code>ApplicationsManager</code>。</p>\n<h4 id=\"1-1-Scheduler\"><a href=\"#1-1-Scheduler\" class=\"headerlink\" title=\"1.1 Scheduler\"></a>1.1 Scheduler</h4><p><code>Scheduler</code>根据容量，队列等限制条件将资源分配给各种正在运行的应用程序。<code>Scheduler</code>是’纯调度器’，因为它负责监视或跟踪应用程序的状态。此外，它也不保证会重启由于应用程序错误或硬件故障原因导致失败的任务。<code>Scheduler</code>仅根据应用程序的资源请求来执行调度。它基于’资源容器’(Resource Container)这一抽象概念来实现的，资源容器包括如内存，cpu，磁盘，网络等。</p>\n<p><code>Scheduler</code>是一个可插拔的组件，它负责将集群资源分配给不同队列和应用程序。目前<code>Scheduler</code>支持诸如<code>CapacityScheduler</code>和<code>FairScheduler</code>。<code>CapacityScheduler</code>支持分层队列，以便更可预测地共享群集资源</p>\n<h4 id=\"1-2-ApplicationsManager\"><a href=\"#1-2-ApplicationsManager\" class=\"headerlink\" title=\"1.2 ApplicationsManager\"></a>1.2 ApplicationsManager</h4><p><code>ApplicationsManager</code>(<code>ASM</code>)主要负责接受作业提交，协商获取第一个容器来执行应用程序的<code>ApplicationMaster</code>(negotiating the first container for executing the application specific ApplicationMaster)，并提供在故障时重新启动<code>ApplicationMaster</code>的服务。</p>\n<h3 id=\"2-NodeManager\"><a href=\"#2-NodeManager\" class=\"headerlink\" title=\"2. NodeManager\"></a>2. NodeManager</h3><p><code>NodeManager</code>是每个节点上框架代理，主要负责启动应用所需要的容器，监视它们的资源使用情况(cpu，内存，磁盘，网络)，并将其报告给<code>ResourceManager</code>的<code>Scheduler</code>。</p>\n<h3 id=\"3-ApplicationMaster\"><a href=\"#3-ApplicationMaster\" class=\"headerlink\" title=\"3. ApplicationMaster\"></a>3. ApplicationMaster</h3><p>事实上，每一个应用程序的<code>ApplicationMaster</code>是一个框架库，负责与<code>Scheduler</code>协商合适的资源容器以及与<code>NodeManager</code>一起跟踪他们的状态并监视进度。</p>\n<p><code>MRV2</code>保持与以前稳定版本(hadoop-1.x)API的兼容性。这意味着所有的<code>Map-Reduce</code>作业仍然可以在<code>MRv2</code>上运行，只需重新编译即可。</p>\n<p>原文:<a href=\"http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/YARN.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop Partitioner使用教程","date":"2017-12-05T12:25:17.000Z","_content":"\n\n`partitioner`在处理输入数据集时就像条件表达式(condition)一样工作。分区阶段发生在`Map`阶段之后，`Reduce`阶段之前。`partitioner`的个数等于`reducer`的个数(The number of partitioners is equal to the number of reducers)。这就意味着一个`partitioner`将根据`reducer`的个数来划分数据(That means a partitioner will divide the data according to the number of reducers)。因此，从一个单独`partitioner`传递过来的数据将会交由一个单独的`reducer`处理(the data passed from a single partitioner is processed by a single Reducer)。\n\n### 1. Partitioner\n\n`partitioner`对Map中间输出结果的键值对进行分区。使用用户自定义的分区条件来对数据进行分区，它的工作方式类似于hash函数。`partitioner`的总个数与作业的`reducer`任务的个数相同。下面我们以一个例子来说明`partitioner`是如何工作的。\n\n### 2. MapReduce的Partitioner实现\n\n为了方便，假设我们有一个Employee表，数据如下。我们使用下面样例数据作为输入数据集来验证`partitioner`是如何工作的。\n\nId|Name|Age|Gender|Salary\n---|---|---|---|---\n1201|gopal|45|Male|50,000\n1202|manisha|40|Female|50,000\n1203|khalil|34|Male|30,000\n1204|prasanth|30|Male|30,000\n1205|kiran|20|Male|40,000\n1206|laxmi|25|Female|35,000\n1207|bhavya|20|Female|15,000\n1208|reshma|19|Female|15,000\n1209|kranthi|22|Male|22,000\n1210|Satish|24|Male|25,000\n1211|Krishna|25|Male|25,000\n1212|Arshad|28|Male|20,000\n1213|lavanya|18|Female|8,000\n\n我们写一个程序来处理输入数据集，对年龄进行分组(例如：小于20，21-30，大于30)，并找到每个分组中的最高工资的员工。\n\n#### 2.1 输入数据\n\n以上数据存储在`/home/xiaosi/tmp/partitionerExample/input/`目录中的`input.txt`文件中，数据存储格式如下：\n```\n1201\tgopal\t45\tMale\t50000\n1202\tmanisha\t40\tFemale\t51000\n1203\tkhaleel\t34\tMale\t30000\n1204\tprasanth\t30\tMale\t31000\n1205\tkiran\t20\tMale\t40000\n1206\tlaxmi\t25\tFemale\t35000\n1207\tbhavya\t20\tFemale\t15000\n1208\treshma\t19\tFemale\t14000\n1209\tkranthi\t22\tMale\t22000\n1210\tSatish\t24\tMale\t25000\n1211\tKrishna\t25\tMale\t26000\n1212\tArshad\t28\tMale\t20000\n1213\tlavanya\t18\tFemale\t8000\n```\n基于以上输入数据，下面是具体的算法描述。\n\n#### 2.2 Map任务\n\nMap任务以键值对作为输入，我们存储文本数据在text文件中。Map任务输入数据如下：\n\n##### 2.2.1 Input\n\nkey以`特殊key+文件名+行号`的模式表示(例如，key = @input1)，value为一行中的数据(例如，value = 1201\\tgopal\\t45\\tMale\\t50000)。\n\n##### 2.2.2 Method\n\n读取一行中数据，使用split方法以`\\t`进行分割，取出性别存储在变量中\n```java\nString[] str = value.toString().split(\"\\t\", -3);\nString gender = str[3];\n```\n以性别为key，行记录数据为value作为输出键值对，从`Map`任务传递到`Partition`任务：\n```java\ncontext.write(new Text(gender), new Text(value));\n```\n对text文件中的所有记录重复以上所有步骤。\n\n##### 2.2.3 Output\n\n得到性别与记录数据组成的键值对\n\n#### 2.3 Partition任务\n\n`Partition`任务接受来自`Map`任务的键值对作为输入。`Partition`意味着将数据分成几个片段。根据给定分区条件规则，基于年龄标准将输入键值对数据划分为三部分。\n\n#### 2.3.1 Input\n\n键值对集合中的所有数据。key为记录中性别字段值，value为该性别对应的完整记录数据。\n\n#### 2.3.2 Method\n\n从键值对数据中读取年龄字段值\n```java\nString[] str = value.toString().split(\"\\t\");\nint age = Integer.parseInt(str[2]);\n```\n\n根据如下条件校验age值：\n```java\n// age 小于等于20\nif (age <= 20) {\n   return 0;\n}\n// age 大于20 小于等于30\nelse if (age > 20 && age <= 30) {\n   return 1 % numReduceTask;\n}\n// age 大于30\nelse {\n   return 2 % numReduceTask;\n}\n```\n\n#### 2.3.3 Output\n\n键值对所有数据被分割成三个键值对集合。`Reducer`会处理每一个集合。\n\n#### 2.4 Reduce任务\n\n`partitioner`任务的数量等于`reducer`任务的数量。这里我们有三个`partitioner`任务，因此我们有三个`reducer`任务要执行。\n\n##### 2.4.1 Input\n\n`Reducer`将使用不同的键值对集合执行三次。key为记录中性别字段值，value为该性别对应的完整记录数据。\n\n##### 2.4.2 Method\n\n读取记录数据中的Salary字段值：\n```java\nString[] str = value.toString().split(\"\\t\", -3);\nint salary = Integer.parseInt(str[4]);\n```\n获取salary最大值:\n```java\nif (salary > max) {\n   max = salary;\n}\n```\n对于每个key集合（Male与Female为两个key集合）中的数据重复以上步骤。执行完这三个步骤之后，我们将会分别从女性集合中得到一个最高工资，从男性集合中得到一个最高工资。\n```java\ncontext.write(new Text(key), new IntWritable(max));\n```\n##### 2.4.3 Output\n\n最后，我们将在不同年龄段的三个集合中获得一组键值对数据。它分别包含每个年龄段的男性集合的最高工资和每个年龄段的女性集合的最高工资。\n\n执行Map，Partition和Reduce任务后，键值对数据的三个集合存储在三个不同的文件中作为输出。\n\n所有这三项任务都被视为MapReduce作业。这些作业的以下要求和规范应在配置中指定：\n- 作业名称\n- keys和values的输入输出格式\n- Map，Reduce和Partitioner任务的类\n\n```java\nConfiguration conf = getConf();\n\n//Create Job\nJob job = new Job(conf, \"topsal\");\njob.setJarByClass(PartitionerExample.class);\n\n// File Input and Output paths\nFileInputFormat.setInputPaths(job, new Path(arg[0]));\nFileOutputFormat.setOutputPath(job,new Path(arg[1]));\n\n//Set Mapper class and Output format for key-value pair.\njob.setMapperClass(MapClass.class);\njob.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(Text.class);\n\n//set partitioner statement\njob.setPartitionerClass(CaderPartitioner.class);\n\n//Set Reducer class and Input/Output format for key-value pair.\njob.setReducerClass(ReduceClass.class);\n\n//Number of Reducer tasks.\njob.setNumReduceTasks(3);\n\n//Input and Output format for data\njob.setInputFormatClass(TextInputFormat.class);\njob.setOutputFormatClass(TextOutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(Text.class);\n```\n\n### 3. Example\n\n```java\npackage com.sjf.open.test;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.CompressionCodec;\nimport org.apache.hadoop.io.compress.GzipCodec;\nimport org.apache.hadoop.mapred.JobPriority;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport com.sjf.open.utils.FileSystemUtil;\npublic class PartitionerExample extends Configured implements Tool {\n\n    public static void main(String[] args) throws Exception {\n        int status = ToolRunner.run(new PartitionerExample(), args);\n        System.exit(status);\n    }\n    private static class mapper extends Mapper<LongWritable, Text, Text, Text> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            try {\n                String[] str = value.toString().split(\"\\t\", -3);\n                String gender = str[3];\n                context.write(new Text(gender), new Text(value));\n            } catch (Exception e) {\n                System.out.println(e.getMessage());\n            }\n        }\n    }\n\n    private static class reducer extends Reducer<Text, Text, Text, IntWritable> {\n        private int max = Integer.MIN_VALUE;\n        @Override\n        protected void reduce(Text key, Iterable<Text> values, Context context)\n                throws IOException, InterruptedException {\n            for (Text value : values) {\n                String[] str = value.toString().split(\"\\t\", -3);\n                int salary = Integer.parseInt(str[4]);\n                if (salary > max) {\n                    max = salary;\n                }\n            }\n            context.write(new Text(key), new IntWritable(max));\n        }\n    }\n\n    private static class partitioner extends Partitioner<Text, Text> {\n        @Override\n        public int getPartition(Text key, Text value, int numReduceTask) {\n            System.out.println(key.toString() + \"------\" + value.toString());\n            String[] str = value.toString().split(\"\\t\");\n            int age = Integer.parseInt(str[2]);\n            if (numReduceTask == 0) {\n                return 0;\n            }\n            if (age <= 20) {\n                return 0;\n            }\n            else if (age > 20 && age <= 30) {\n                return 1 % numReduceTask;\n            }\n            else {\n                return 2 % numReduceTask;\n            }\n        }\n    }\n    @Override\n    public int run(String[] args) throws Exception {\n        if (args.length != 2) {\n            System.err.println(\"./run <input> <output>\");\n            System.exit(1);\n        }\n        String inputPath = args[0];\n        String outputPath = args[1];\n        int numReduceTasks = 3;\n        Configuration conf = this.getConf();\n        conf.set(\"mapred.job.queue.name\", \"test\");\n        conf.set(\"mapreduce.map.memory.mb\", \"1024\");\n        conf.set(\"mapreduce.reduce.memory.mb\", \"1024\");\n        conf.setBoolean(\"mapred.output.compress\", true);\n        conf.setClass(\"mapred.output.compression.codec\", GzipCodec.class, CompressionCodec.class);\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(PartitionerExample.class);\n        job.setPartitionerClass(partitioner.class);\n        job.setMapperClass(mapper.class);\n        job.setReducerClass(reducer.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(Text.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        FileSystem fileSystem = FileSystem.get(conf);\n        fileSystem.delete(new Path(outputPath), true);\n        FileSystemUtil.filterNoExistsFile(conf, job, inputPath);\n        FileOutputFormat.setOutputPath(job, new Path(outputPath));\n        job.setNumReduceTasks(numReduceTasks);\n        boolean success = job.waitForCompletion(true);\n        return success ? 0 : 1;\n    }\n}\n```\n\n### 4. 集群上执行\n\n运行结果:\n```\n17/01/03 20:22:02 INFO mapreduce.Job: Running job: job_1472052053889_7059198\n17/01/03 20:22:21 INFO mapreduce.Job: Job job_1472052053889_7059198 running in uber mode : false\n17/01/03 20:22:21 INFO mapreduce.Job:  map 0% reduce 0%\n17/01/03 20:22:37 INFO mapreduce.Job:  map 100% reduce 0%\n17/01/03 20:22:55 INFO mapreduce.Job:  map 100% reduce 100%\n17/01/03 20:22:55 INFO mapreduce.Job: Job job_1472052053889_7059198 completed successfully\n17/01/03 20:22:56 INFO mapreduce.Job: Counters: 43\n        File System Counters\n                FILE: Number of bytes read=470\n                FILE: Number of bytes written=346003\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n                HDFS: Number of bytes read=485\n                HDFS: Number of bytes written=109\n                HDFS: Number of read operations=12\n                HDFS: Number of large read operations=0\n                HDFS: Number of write operations=6\n        Job Counters\n                Launched map tasks=1\n                Launched reduce tasks=3\n                Rack-local map tasks=1\n                Total time spent by all maps in occupied slots (ms)=5559\n                Total time spent by all reduces in occupied slots (ms)=164768\n        Map-Reduce Framework\n                Map input records=13\n                Map output records=13\n                Map output bytes=426\n                Map output materialized bytes=470\n                Input split bytes=134\n                Combine input records=0\n                Combine output records=0\n                Reduce input groups=6\n                Reduce shuffle bytes=470\n                Reduce input records=13\n                Reduce output records=6\n                Spilled Records=26\n                Shuffled Maps =3\n                Failed Shuffles=0\n                Merged Map outputs=3\n                GC time elapsed (ms)=31\n                CPU time spent (ms)=2740\n                Physical memory (bytes) snapshot=1349193728\n                Virtual memory (bytes) snapshot=29673148416\n                Total committed heap usage (bytes)=6888620032\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters\n                Bytes Read=351\n        File Output Format Counters\n                Bytes Written=109\n\n```\n\n\n原文:https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm\n","source":"_posts/Hadoop/Hadoop Partitioner使用教程.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop Partitioner使用教程\ndate: 2017-12-05 20:25:17\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n\n`partitioner`在处理输入数据集时就像条件表达式(condition)一样工作。分区阶段发生在`Map`阶段之后，`Reduce`阶段之前。`partitioner`的个数等于`reducer`的个数(The number of partitioners is equal to the number of reducers)。这就意味着一个`partitioner`将根据`reducer`的个数来划分数据(That means a partitioner will divide the data according to the number of reducers)。因此，从一个单独`partitioner`传递过来的数据将会交由一个单独的`reducer`处理(the data passed from a single partitioner is processed by a single Reducer)。\n\n### 1. Partitioner\n\n`partitioner`对Map中间输出结果的键值对进行分区。使用用户自定义的分区条件来对数据进行分区，它的工作方式类似于hash函数。`partitioner`的总个数与作业的`reducer`任务的个数相同。下面我们以一个例子来说明`partitioner`是如何工作的。\n\n### 2. MapReduce的Partitioner实现\n\n为了方便，假设我们有一个Employee表，数据如下。我们使用下面样例数据作为输入数据集来验证`partitioner`是如何工作的。\n\nId|Name|Age|Gender|Salary\n---|---|---|---|---\n1201|gopal|45|Male|50,000\n1202|manisha|40|Female|50,000\n1203|khalil|34|Male|30,000\n1204|prasanth|30|Male|30,000\n1205|kiran|20|Male|40,000\n1206|laxmi|25|Female|35,000\n1207|bhavya|20|Female|15,000\n1208|reshma|19|Female|15,000\n1209|kranthi|22|Male|22,000\n1210|Satish|24|Male|25,000\n1211|Krishna|25|Male|25,000\n1212|Arshad|28|Male|20,000\n1213|lavanya|18|Female|8,000\n\n我们写一个程序来处理输入数据集，对年龄进行分组(例如：小于20，21-30，大于30)，并找到每个分组中的最高工资的员工。\n\n#### 2.1 输入数据\n\n以上数据存储在`/home/xiaosi/tmp/partitionerExample/input/`目录中的`input.txt`文件中，数据存储格式如下：\n```\n1201\tgopal\t45\tMale\t50000\n1202\tmanisha\t40\tFemale\t51000\n1203\tkhaleel\t34\tMale\t30000\n1204\tprasanth\t30\tMale\t31000\n1205\tkiran\t20\tMale\t40000\n1206\tlaxmi\t25\tFemale\t35000\n1207\tbhavya\t20\tFemale\t15000\n1208\treshma\t19\tFemale\t14000\n1209\tkranthi\t22\tMale\t22000\n1210\tSatish\t24\tMale\t25000\n1211\tKrishna\t25\tMale\t26000\n1212\tArshad\t28\tMale\t20000\n1213\tlavanya\t18\tFemale\t8000\n```\n基于以上输入数据，下面是具体的算法描述。\n\n#### 2.2 Map任务\n\nMap任务以键值对作为输入，我们存储文本数据在text文件中。Map任务输入数据如下：\n\n##### 2.2.1 Input\n\nkey以`特殊key+文件名+行号`的模式表示(例如，key = @input1)，value为一行中的数据(例如，value = 1201\\tgopal\\t45\\tMale\\t50000)。\n\n##### 2.2.2 Method\n\n读取一行中数据，使用split方法以`\\t`进行分割，取出性别存储在变量中\n```java\nString[] str = value.toString().split(\"\\t\", -3);\nString gender = str[3];\n```\n以性别为key，行记录数据为value作为输出键值对，从`Map`任务传递到`Partition`任务：\n```java\ncontext.write(new Text(gender), new Text(value));\n```\n对text文件中的所有记录重复以上所有步骤。\n\n##### 2.2.3 Output\n\n得到性别与记录数据组成的键值对\n\n#### 2.3 Partition任务\n\n`Partition`任务接受来自`Map`任务的键值对作为输入。`Partition`意味着将数据分成几个片段。根据给定分区条件规则，基于年龄标准将输入键值对数据划分为三部分。\n\n#### 2.3.1 Input\n\n键值对集合中的所有数据。key为记录中性别字段值，value为该性别对应的完整记录数据。\n\n#### 2.3.2 Method\n\n从键值对数据中读取年龄字段值\n```java\nString[] str = value.toString().split(\"\\t\");\nint age = Integer.parseInt(str[2]);\n```\n\n根据如下条件校验age值：\n```java\n// age 小于等于20\nif (age <= 20) {\n   return 0;\n}\n// age 大于20 小于等于30\nelse if (age > 20 && age <= 30) {\n   return 1 % numReduceTask;\n}\n// age 大于30\nelse {\n   return 2 % numReduceTask;\n}\n```\n\n#### 2.3.3 Output\n\n键值对所有数据被分割成三个键值对集合。`Reducer`会处理每一个集合。\n\n#### 2.4 Reduce任务\n\n`partitioner`任务的数量等于`reducer`任务的数量。这里我们有三个`partitioner`任务，因此我们有三个`reducer`任务要执行。\n\n##### 2.4.1 Input\n\n`Reducer`将使用不同的键值对集合执行三次。key为记录中性别字段值，value为该性别对应的完整记录数据。\n\n##### 2.4.2 Method\n\n读取记录数据中的Salary字段值：\n```java\nString[] str = value.toString().split(\"\\t\", -3);\nint salary = Integer.parseInt(str[4]);\n```\n获取salary最大值:\n```java\nif (salary > max) {\n   max = salary;\n}\n```\n对于每个key集合（Male与Female为两个key集合）中的数据重复以上步骤。执行完这三个步骤之后，我们将会分别从女性集合中得到一个最高工资，从男性集合中得到一个最高工资。\n```java\ncontext.write(new Text(key), new IntWritable(max));\n```\n##### 2.4.3 Output\n\n最后，我们将在不同年龄段的三个集合中获得一组键值对数据。它分别包含每个年龄段的男性集合的最高工资和每个年龄段的女性集合的最高工资。\n\n执行Map，Partition和Reduce任务后，键值对数据的三个集合存储在三个不同的文件中作为输出。\n\n所有这三项任务都被视为MapReduce作业。这些作业的以下要求和规范应在配置中指定：\n- 作业名称\n- keys和values的输入输出格式\n- Map，Reduce和Partitioner任务的类\n\n```java\nConfiguration conf = getConf();\n\n//Create Job\nJob job = new Job(conf, \"topsal\");\njob.setJarByClass(PartitionerExample.class);\n\n// File Input and Output paths\nFileInputFormat.setInputPaths(job, new Path(arg[0]));\nFileOutputFormat.setOutputPath(job,new Path(arg[1]));\n\n//Set Mapper class and Output format for key-value pair.\njob.setMapperClass(MapClass.class);\njob.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(Text.class);\n\n//set partitioner statement\njob.setPartitionerClass(CaderPartitioner.class);\n\n//Set Reducer class and Input/Output format for key-value pair.\njob.setReducerClass(ReduceClass.class);\n\n//Number of Reducer tasks.\njob.setNumReduceTasks(3);\n\n//Input and Output format for data\njob.setInputFormatClass(TextInputFormat.class);\njob.setOutputFormatClass(TextOutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(Text.class);\n```\n\n### 3. Example\n\n```java\npackage com.sjf.open.test;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.CompressionCodec;\nimport org.apache.hadoop.io.compress.GzipCodec;\nimport org.apache.hadoop.mapred.JobPriority;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport com.sjf.open.utils.FileSystemUtil;\npublic class PartitionerExample extends Configured implements Tool {\n\n    public static void main(String[] args) throws Exception {\n        int status = ToolRunner.run(new PartitionerExample(), args);\n        System.exit(status);\n    }\n    private static class mapper extends Mapper<LongWritable, Text, Text, Text> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            try {\n                String[] str = value.toString().split(\"\\t\", -3);\n                String gender = str[3];\n                context.write(new Text(gender), new Text(value));\n            } catch (Exception e) {\n                System.out.println(e.getMessage());\n            }\n        }\n    }\n\n    private static class reducer extends Reducer<Text, Text, Text, IntWritable> {\n        private int max = Integer.MIN_VALUE;\n        @Override\n        protected void reduce(Text key, Iterable<Text> values, Context context)\n                throws IOException, InterruptedException {\n            for (Text value : values) {\n                String[] str = value.toString().split(\"\\t\", -3);\n                int salary = Integer.parseInt(str[4]);\n                if (salary > max) {\n                    max = salary;\n                }\n            }\n            context.write(new Text(key), new IntWritable(max));\n        }\n    }\n\n    private static class partitioner extends Partitioner<Text, Text> {\n        @Override\n        public int getPartition(Text key, Text value, int numReduceTask) {\n            System.out.println(key.toString() + \"------\" + value.toString());\n            String[] str = value.toString().split(\"\\t\");\n            int age = Integer.parseInt(str[2]);\n            if (numReduceTask == 0) {\n                return 0;\n            }\n            if (age <= 20) {\n                return 0;\n            }\n            else if (age > 20 && age <= 30) {\n                return 1 % numReduceTask;\n            }\n            else {\n                return 2 % numReduceTask;\n            }\n        }\n    }\n    @Override\n    public int run(String[] args) throws Exception {\n        if (args.length != 2) {\n            System.err.println(\"./run <input> <output>\");\n            System.exit(1);\n        }\n        String inputPath = args[0];\n        String outputPath = args[1];\n        int numReduceTasks = 3;\n        Configuration conf = this.getConf();\n        conf.set(\"mapred.job.queue.name\", \"test\");\n        conf.set(\"mapreduce.map.memory.mb\", \"1024\");\n        conf.set(\"mapreduce.reduce.memory.mb\", \"1024\");\n        conf.setBoolean(\"mapred.output.compress\", true);\n        conf.setClass(\"mapred.output.compression.codec\", GzipCodec.class, CompressionCodec.class);\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(PartitionerExample.class);\n        job.setPartitionerClass(partitioner.class);\n        job.setMapperClass(mapper.class);\n        job.setReducerClass(reducer.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(Text.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        FileSystem fileSystem = FileSystem.get(conf);\n        fileSystem.delete(new Path(outputPath), true);\n        FileSystemUtil.filterNoExistsFile(conf, job, inputPath);\n        FileOutputFormat.setOutputPath(job, new Path(outputPath));\n        job.setNumReduceTasks(numReduceTasks);\n        boolean success = job.waitForCompletion(true);\n        return success ? 0 : 1;\n    }\n}\n```\n\n### 4. 集群上执行\n\n运行结果:\n```\n17/01/03 20:22:02 INFO mapreduce.Job: Running job: job_1472052053889_7059198\n17/01/03 20:22:21 INFO mapreduce.Job: Job job_1472052053889_7059198 running in uber mode : false\n17/01/03 20:22:21 INFO mapreduce.Job:  map 0% reduce 0%\n17/01/03 20:22:37 INFO mapreduce.Job:  map 100% reduce 0%\n17/01/03 20:22:55 INFO mapreduce.Job:  map 100% reduce 100%\n17/01/03 20:22:55 INFO mapreduce.Job: Job job_1472052053889_7059198 completed successfully\n17/01/03 20:22:56 INFO mapreduce.Job: Counters: 43\n        File System Counters\n                FILE: Number of bytes read=470\n                FILE: Number of bytes written=346003\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n                HDFS: Number of bytes read=485\n                HDFS: Number of bytes written=109\n                HDFS: Number of read operations=12\n                HDFS: Number of large read operations=0\n                HDFS: Number of write operations=6\n        Job Counters\n                Launched map tasks=1\n                Launched reduce tasks=3\n                Rack-local map tasks=1\n                Total time spent by all maps in occupied slots (ms)=5559\n                Total time spent by all reduces in occupied slots (ms)=164768\n        Map-Reduce Framework\n                Map input records=13\n                Map output records=13\n                Map output bytes=426\n                Map output materialized bytes=470\n                Input split bytes=134\n                Combine input records=0\n                Combine output records=0\n                Reduce input groups=6\n                Reduce shuffle bytes=470\n                Reduce input records=13\n                Reduce output records=6\n                Spilled Records=26\n                Shuffled Maps =3\n                Failed Shuffles=0\n                Merged Map outputs=3\n                GC time elapsed (ms)=31\n                CPU time spent (ms)=2740\n                Physical memory (bytes) snapshot=1349193728\n                Virtual memory (bytes) snapshot=29673148416\n                Total committed heap usage (bytes)=6888620032\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters\n                Bytes Read=351\n        File Output Format Counters\n                Bytes Written=109\n\n```\n\n\n原文:https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm\n","slug":"Hadoop/Hadoop Partitioner使用教程","published":1,"updated":"2018-01-29T09:36:59.634Z","comments":1,"photos":[],"link":"","_id":"cje58tisj002xordbqtlikuuk","content":"<p><code>partitioner</code>在处理输入数据集时就像条件表达式(condition)一样工作。分区阶段发生在<code>Map</code>阶段之后，<code>Reduce</code>阶段之前。<code>partitioner</code>的个数等于<code>reducer</code>的个数(The number of partitioners is equal to the number of reducers)。这就意味着一个<code>partitioner</code>将根据<code>reducer</code>的个数来划分数据(That means a partitioner will divide the data according to the number of reducers)。因此，从一个单独<code>partitioner</code>传递过来的数据将会交由一个单独的<code>reducer</code>处理(the data passed from a single partitioner is processed by a single Reducer)。</p>\n<h3 id=\"1-Partitioner\"><a href=\"#1-Partitioner\" class=\"headerlink\" title=\"1. Partitioner\"></a>1. Partitioner</h3><p><code>partitioner</code>对Map中间输出结果的键值对进行分区。使用用户自定义的分区条件来对数据进行分区，它的工作方式类似于hash函数。<code>partitioner</code>的总个数与作业的<code>reducer</code>任务的个数相同。下面我们以一个例子来说明<code>partitioner</code>是如何工作的。</p>\n<h3 id=\"2-MapReduce的Partitioner实现\"><a href=\"#2-MapReduce的Partitioner实现\" class=\"headerlink\" title=\"2. MapReduce的Partitioner实现\"></a>2. MapReduce的Partitioner实现</h3><p>为了方便，假设我们有一个Employee表，数据如下。我们使用下面样例数据作为输入数据集来验证<code>partitioner</code>是如何工作的。</p>\n<table>\n<thead>\n<tr>\n<th>Id</th>\n<th>Name</th>\n<th>Age</th>\n<th>Gender</th>\n<th>Salary</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1201</td>\n<td>gopal</td>\n<td>45</td>\n<td>Male</td>\n<td>50,000</td>\n</tr>\n<tr>\n<td>1202</td>\n<td>manisha</td>\n<td>40</td>\n<td>Female</td>\n<td>50,000</td>\n</tr>\n<tr>\n<td>1203</td>\n<td>khalil</td>\n<td>34</td>\n<td>Male</td>\n<td>30,000</td>\n</tr>\n<tr>\n<td>1204</td>\n<td>prasanth</td>\n<td>30</td>\n<td>Male</td>\n<td>30,000</td>\n</tr>\n<tr>\n<td>1205</td>\n<td>kiran</td>\n<td>20</td>\n<td>Male</td>\n<td>40,000</td>\n</tr>\n<tr>\n<td>1206</td>\n<td>laxmi</td>\n<td>25</td>\n<td>Female</td>\n<td>35,000</td>\n</tr>\n<tr>\n<td>1207</td>\n<td>bhavya</td>\n<td>20</td>\n<td>Female</td>\n<td>15,000</td>\n</tr>\n<tr>\n<td>1208</td>\n<td>reshma</td>\n<td>19</td>\n<td>Female</td>\n<td>15,000</td>\n</tr>\n<tr>\n<td>1209</td>\n<td>kranthi</td>\n<td>22</td>\n<td>Male</td>\n<td>22,000</td>\n</tr>\n<tr>\n<td>1210</td>\n<td>Satish</td>\n<td>24</td>\n<td>Male</td>\n<td>25,000</td>\n</tr>\n<tr>\n<td>1211</td>\n<td>Krishna</td>\n<td>25</td>\n<td>Male</td>\n<td>25,000</td>\n</tr>\n<tr>\n<td>1212</td>\n<td>Arshad</td>\n<td>28</td>\n<td>Male</td>\n<td>20,000</td>\n</tr>\n<tr>\n<td>1213</td>\n<td>lavanya</td>\n<td>18</td>\n<td>Female</td>\n<td>8,000</td>\n</tr>\n</tbody>\n</table>\n<p>我们写一个程序来处理输入数据集，对年龄进行分组(例如：小于20，21-30，大于30)，并找到每个分组中的最高工资的员工。</p>\n<h4 id=\"2-1-输入数据\"><a href=\"#2-1-输入数据\" class=\"headerlink\" title=\"2.1 输入数据\"></a>2.1 输入数据</h4><p>以上数据存储在<code>/home/xiaosi/tmp/partitionerExample/input/</code>目录中的<code>input.txt</code>文件中，数据存储格式如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">1201\tgopal\t45\tMale\t50000</span><br><span class=\"line\">1202\tmanisha\t40\tFemale\t51000</span><br><span class=\"line\">1203\tkhaleel\t34\tMale\t30000</span><br><span class=\"line\">1204\tprasanth\t30\tMale\t31000</span><br><span class=\"line\">1205\tkiran\t20\tMale\t40000</span><br><span class=\"line\">1206\tlaxmi\t25\tFemale\t35000</span><br><span class=\"line\">1207\tbhavya\t20\tFemale\t15000</span><br><span class=\"line\">1208\treshma\t19\tFemale\t14000</span><br><span class=\"line\">1209\tkranthi\t22\tMale\t22000</span><br><span class=\"line\">1210\tSatish\t24\tMale\t25000</span><br><span class=\"line\">1211\tKrishna\t25\tMale\t26000</span><br><span class=\"line\">1212\tArshad\t28\tMale\t20000</span><br><span class=\"line\">1213\tlavanya\t18\tFemale\t8000</span><br></pre></td></tr></table></figure></p>\n<p>基于以上输入数据，下面是具体的算法描述。</p>\n<h4 id=\"2-2-Map任务\"><a href=\"#2-2-Map任务\" class=\"headerlink\" title=\"2.2 Map任务\"></a>2.2 Map任务</h4><p>Map任务以键值对作为输入，我们存储文本数据在text文件中。Map任务输入数据如下：</p>\n<h5 id=\"2-2-1-Input\"><a href=\"#2-2-1-Input\" class=\"headerlink\" title=\"2.2.1 Input\"></a>2.2.1 Input</h5><p>key以<code>特殊key+文件名+行号</code>的模式表示(例如，key = @input1)，value为一行中的数据(例如，value = 1201\\tgopal\\t45\\tMale\\t50000)。</p>\n<h5 id=\"2-2-2-Method\"><a href=\"#2-2-2-Method\" class=\"headerlink\" title=\"2.2.2 Method\"></a>2.2.2 Method</h5><p>读取一行中数据，使用split方法以<code>\\t</code>进行分割，取出性别存储在变量中<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\">String gender = str[<span class=\"number\">3</span>];</span><br></pre></td></tr></table></figure></p>\n<p>以性别为key，行记录数据为value作为输出键值对，从<code>Map</code>任务传递到<code>Partition</code>任务：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">context.write(<span class=\"keyword\">new</span> Text(gender), <span class=\"keyword\">new</span> Text(value));</span><br></pre></td></tr></table></figure></p>\n<p>对text文件中的所有记录重复以上所有步骤。</p>\n<h5 id=\"2-2-3-Output\"><a href=\"#2-2-3-Output\" class=\"headerlink\" title=\"2.2.3 Output\"></a>2.2.3 Output</h5><p>得到性别与记录数据组成的键值对</p>\n<h4 id=\"2-3-Partition任务\"><a href=\"#2-3-Partition任务\" class=\"headerlink\" title=\"2.3 Partition任务\"></a>2.3 Partition任务</h4><p><code>Partition</code>任务接受来自<code>Map</code>任务的键值对作为输入。<code>Partition</code>意味着将数据分成几个片段。根据给定分区条件规则，基于年龄标准将输入键值对数据划分为三部分。</p>\n<h4 id=\"2-3-1-Input\"><a href=\"#2-3-1-Input\" class=\"headerlink\" title=\"2.3.1 Input\"></a>2.3.1 Input</h4><p>键值对集合中的所有数据。key为记录中性别字段值，value为该性别对应的完整记录数据。</p>\n<h4 id=\"2-3-2-Method\"><a href=\"#2-3-2-Method\" class=\"headerlink\" title=\"2.3.2 Method\"></a>2.3.2 Method</h4><p>从键值对数据中读取年龄字段值<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>);</span><br><span class=\"line\"><span class=\"keyword\">int</span> age = Integer.parseInt(str[<span class=\"number\">2</span>]);</span><br></pre></td></tr></table></figure></p>\n<p>根据如下条件校验age值：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// age 小于等于20</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (age &lt;= <span class=\"number\">20</span>) &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// age 大于20 小于等于30</span></span><br><span class=\"line\"><span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (age &gt; <span class=\"number\">20</span> &amp;&amp; age &lt;= <span class=\"number\">30</span>) &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">1</span> % numReduceTask;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// age 大于30</span></span><br><span class=\"line\"><span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">2</span> % numReduceTask;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-3-Output\"><a href=\"#2-3-3-Output\" class=\"headerlink\" title=\"2.3.3 Output\"></a>2.3.3 Output</h4><p>键值对所有数据被分割成三个键值对集合。<code>Reducer</code>会处理每一个集合。</p>\n<h4 id=\"2-4-Reduce任务\"><a href=\"#2-4-Reduce任务\" class=\"headerlink\" title=\"2.4 Reduce任务\"></a>2.4 Reduce任务</h4><p><code>partitioner</code>任务的数量等于<code>reducer</code>任务的数量。这里我们有三个<code>partitioner</code>任务，因此我们有三个<code>reducer</code>任务要执行。</p>\n<h5 id=\"2-4-1-Input\"><a href=\"#2-4-1-Input\" class=\"headerlink\" title=\"2.4.1 Input\"></a>2.4.1 Input</h5><p><code>Reducer</code>将使用不同的键值对集合执行三次。key为记录中性别字段值，value为该性别对应的完整记录数据。</p>\n<h5 id=\"2-4-2-Method\"><a href=\"#2-4-2-Method\" class=\"headerlink\" title=\"2.4.2 Method\"></a>2.4.2 Method</h5><p>读取记录数据中的Salary字段值：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\"><span class=\"keyword\">int</span> salary = Integer.parseInt(str[<span class=\"number\">4</span>]);</span><br></pre></td></tr></table></figure></p>\n<p>获取salary最大值:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (salary &gt; max) &#123;</span><br><span class=\"line\">   max = salary;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>对于每个key集合（Male与Female为两个key集合）中的数据重复以上步骤。执行完这三个步骤之后，我们将会分别从女性集合中得到一个最高工资，从男性集合中得到一个最高工资。<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">context.write(<span class=\"keyword\">new</span> Text(key), <span class=\"keyword\">new</span> IntWritable(max));</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-4-3-Output\"><a href=\"#2-4-3-Output\" class=\"headerlink\" title=\"2.4.3 Output\"></a>2.4.3 Output</h5><p>最后，我们将在不同年龄段的三个集合中获得一组键值对数据。它分别包含每个年龄段的男性集合的最高工资和每个年龄段的女性集合的最高工资。</p>\n<p>执行Map，Partition和Reduce任务后，键值对数据的三个集合存储在三个不同的文件中作为输出。</p>\n<p>所有这三项任务都被视为MapReduce作业。这些作业的以下要求和规范应在配置中指定：</p>\n<ul>\n<li>作业名称</li>\n<li>keys和values的输入输出格式</li>\n<li>Map，Reduce和Partitioner任务的类</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Configuration conf = getConf();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Create Job</span></span><br><span class=\"line\">Job job = <span class=\"keyword\">new</span> Job(conf, <span class=\"string\">\"topsal\"</span>);</span><br><span class=\"line\">job.setJarByClass(PartitionerExample.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// File Input and Output paths</span></span><br><span class=\"line\">FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(arg[<span class=\"number\">0</span>]));</span><br><span class=\"line\">FileOutputFormat.setOutputPath(job,<span class=\"keyword\">new</span> Path(arg[<span class=\"number\">1</span>]));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Set Mapper class and Output format for key-value pair.</span></span><br><span class=\"line\">job.setMapperClass(MapClass.class);</span><br><span class=\"line\">job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">job.setMapOutputValueClass(Text.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//set partitioner statement</span></span><br><span class=\"line\">job.setPartitionerClass(CaderPartitioner.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Set Reducer class and Input/Output format for key-value pair.</span></span><br><span class=\"line\">job.setReducerClass(ReduceClass.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Number of Reducer tasks.</span></span><br><span class=\"line\">job.setNumReduceTasks(<span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Input and Output format for data</span></span><br><span class=\"line\">job.setInputFormatClass(TextInputFormat.class);</span><br><span class=\"line\">job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class=\"line\">job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">job.setOutputValueClass(Text.class);</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Example\"><a href=\"#3-Example\" class=\"headerlink\" title=\"3. Example\"></a>3. Example</h3><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.sjf.open.test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configured;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.JobPriority;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.sjf.open.utils.FileSystemUtil;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PartitionerExample</span> <span class=\"keyword\">extends</span> <span class=\"title\">Configured</span> <span class=\"keyword\">implements</span> <span class=\"title\">Tool</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> status = ToolRunner.run(<span class=\"keyword\">new</span> PartitionerExample(), args);</span><br><span class=\"line\">        System.exit(status);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">mapper</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt; </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key, Text value, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\">                String gender = str[<span class=\"number\">3</span>];</span><br><span class=\"line\">                context.write(<span class=\"keyword\">new</span> Text(gender), <span class=\"keyword\">new</span> Text(value));</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">                System.out.println(e.getMessage());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reducer</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt; </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> max = Integer.MIN_VALUE;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></span><br><span class=\"line\"><span class=\"function\">                <span class=\"keyword\">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Text value : values) &#123;</span><br><span class=\"line\">                String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\">                <span class=\"keyword\">int</span> salary = Integer.parseInt(str[<span class=\"number\">4</span>]);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (salary &gt; max) &#123;</span><br><span class=\"line\">                    max = salary;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            context.write(<span class=\"keyword\">new</span> Text(key), <span class=\"keyword\">new</span> IntWritable(max));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">partitioner</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt; </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(Text key, Text value, <span class=\"keyword\">int</span> numReduceTask)</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(key.toString() + <span class=\"string\">\"------\"</span> + value.toString());</span><br><span class=\"line\">            String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>);</span><br><span class=\"line\">            <span class=\"keyword\">int</span> age = Integer.parseInt(str[<span class=\"number\">2</span>]);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (numReduceTask == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (age &lt;= <span class=\"number\">20</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (age &gt; <span class=\"number\">20</span> &amp;&amp; age &lt;= <span class=\"number\">30</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span> % numReduceTask;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">2</span> % numReduceTask;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (args.length != <span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">            System.err.println(<span class=\"string\">\"./run &lt;input&gt; &lt;output&gt;\"</span>);</span><br><span class=\"line\">            System.exit(<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        String inputPath = args[<span class=\"number\">0</span>];</span><br><span class=\"line\">        String outputPath = args[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> numReduceTasks = <span class=\"number\">3</span>;</span><br><span class=\"line\">        Configuration conf = <span class=\"keyword\">this</span>.getConf();</span><br><span class=\"line\">        conf.set(<span class=\"string\">\"mapred.job.queue.name\"</span>, <span class=\"string\">\"test\"</span>);</span><br><span class=\"line\">        conf.set(<span class=\"string\">\"mapreduce.map.memory.mb\"</span>, <span class=\"string\">\"1024\"</span>);</span><br><span class=\"line\">        conf.set(<span class=\"string\">\"mapreduce.reduce.memory.mb\"</span>, <span class=\"string\">\"1024\"</span>);</span><br><span class=\"line\">        conf.setBoolean(<span class=\"string\">\"mapred.output.compress\"</span>, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        conf.setClass(<span class=\"string\">\"mapred.output.compression.codec\"</span>, GzipCodec.class, CompressionCodec.class);</span><br><span class=\"line\">        Job job = Job.getInstance(conf);</span><br><span class=\"line\">        job.setJarByClass(PartitionerExample.class);</span><br><span class=\"line\">        job.setPartitionerClass(partitioner.class);</span><br><span class=\"line\">        job.setMapperClass(mapper.class);</span><br><span class=\"line\">        job.setReducerClass(reducer.class);</span><br><span class=\"line\">        job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setMapOutputValueClass(Text.class);</span><br><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class=\"line\">        fileSystem.delete(<span class=\"keyword\">new</span> Path(outputPath), <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        FileSystemUtil.filterNoExistsFile(conf, job, inputPath);</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(outputPath));</span><br><span class=\"line\">        job.setNumReduceTasks(numReduceTasks);</span><br><span class=\"line\">        <span class=\"keyword\">boolean</span> success = job.waitForCompletion(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> success ? <span class=\"number\">0</span> : <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-集群上执行\"><a href=\"#4-集群上执行\" class=\"headerlink\" title=\"4. 集群上执行\"></a>4. 集群上执行</h3><p>运行结果:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">17/01/03 20:22:02 INFO mapreduce.Job: Running job: job_1472052053889_7059198</span><br><span class=\"line\">17/01/03 20:22:21 INFO mapreduce.Job: Job job_1472052053889_7059198 running in uber mode : false</span><br><span class=\"line\">17/01/03 20:22:21 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class=\"line\">17/01/03 20:22:37 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class=\"line\">17/01/03 20:22:55 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class=\"line\">17/01/03 20:22:55 INFO mapreduce.Job: Job job_1472052053889_7059198 completed successfully</span><br><span class=\"line\">17/01/03 20:22:56 INFO mapreduce.Job: Counters: 43</span><br><span class=\"line\">        File System Counters</span><br><span class=\"line\">                FILE: Number of bytes read=470</span><br><span class=\"line\">                FILE: Number of bytes written=346003</span><br><span class=\"line\">                FILE: Number of read operations=0</span><br><span class=\"line\">                FILE: Number of large read operations=0</span><br><span class=\"line\">                FILE: Number of write operations=0</span><br><span class=\"line\">                HDFS: Number of bytes read=485</span><br><span class=\"line\">                HDFS: Number of bytes written=109</span><br><span class=\"line\">                HDFS: Number of read operations=12</span><br><span class=\"line\">                HDFS: Number of large read operations=0</span><br><span class=\"line\">                HDFS: Number of write operations=6</span><br><span class=\"line\">        Job Counters</span><br><span class=\"line\">                Launched map tasks=1</span><br><span class=\"line\">                Launched reduce tasks=3</span><br><span class=\"line\">                Rack-local map tasks=1</span><br><span class=\"line\">                Total time spent by all maps in occupied slots (ms)=5559</span><br><span class=\"line\">                Total time spent by all reduces in occupied slots (ms)=164768</span><br><span class=\"line\">        Map-Reduce Framework</span><br><span class=\"line\">                Map input records=13</span><br><span class=\"line\">                Map output records=13</span><br><span class=\"line\">                Map output bytes=426</span><br><span class=\"line\">                Map output materialized bytes=470</span><br><span class=\"line\">                Input split bytes=134</span><br><span class=\"line\">                Combine input records=0</span><br><span class=\"line\">                Combine output records=0</span><br><span class=\"line\">                Reduce input groups=6</span><br><span class=\"line\">                Reduce shuffle bytes=470</span><br><span class=\"line\">                Reduce input records=13</span><br><span class=\"line\">                Reduce output records=6</span><br><span class=\"line\">                Spilled Records=26</span><br><span class=\"line\">                Shuffled Maps =3</span><br><span class=\"line\">                Failed Shuffles=0</span><br><span class=\"line\">                Merged Map outputs=3</span><br><span class=\"line\">                GC time elapsed (ms)=31</span><br><span class=\"line\">                CPU time spent (ms)=2740</span><br><span class=\"line\">                Physical memory (bytes) snapshot=1349193728</span><br><span class=\"line\">                Virtual memory (bytes) snapshot=29673148416</span><br><span class=\"line\">                Total committed heap usage (bytes)=6888620032</span><br><span class=\"line\">        Shuffle Errors</span><br><span class=\"line\">                BAD_ID=0</span><br><span class=\"line\">                CONNECTION=0</span><br><span class=\"line\">                IO_ERROR=0</span><br><span class=\"line\">                WRONG_LENGTH=0</span><br><span class=\"line\">                WRONG_MAP=0</span><br><span class=\"line\">                WRONG_REDUCE=0</span><br><span class=\"line\">        File Input Format Counters</span><br><span class=\"line\">                Bytes Read=351</span><br><span class=\"line\">        File Output Format Counters</span><br><span class=\"line\">                Bytes Written=109</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm\" target=\"_blank\" rel=\"noopener\">https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>partitioner</code>在处理输入数据集时就像条件表达式(condition)一样工作。分区阶段发生在<code>Map</code>阶段之后，<code>Reduce</code>阶段之前。<code>partitioner</code>的个数等于<code>reducer</code>的个数(The number of partitioners is equal to the number of reducers)。这就意味着一个<code>partitioner</code>将根据<code>reducer</code>的个数来划分数据(That means a partitioner will divide the data according to the number of reducers)。因此，从一个单独<code>partitioner</code>传递过来的数据将会交由一个单独的<code>reducer</code>处理(the data passed from a single partitioner is processed by a single Reducer)。</p>\n<h3 id=\"1-Partitioner\"><a href=\"#1-Partitioner\" class=\"headerlink\" title=\"1. Partitioner\"></a>1. Partitioner</h3><p><code>partitioner</code>对Map中间输出结果的键值对进行分区。使用用户自定义的分区条件来对数据进行分区，它的工作方式类似于hash函数。<code>partitioner</code>的总个数与作业的<code>reducer</code>任务的个数相同。下面我们以一个例子来说明<code>partitioner</code>是如何工作的。</p>\n<h3 id=\"2-MapReduce的Partitioner实现\"><a href=\"#2-MapReduce的Partitioner实现\" class=\"headerlink\" title=\"2. MapReduce的Partitioner实现\"></a>2. MapReduce的Partitioner实现</h3><p>为了方便，假设我们有一个Employee表，数据如下。我们使用下面样例数据作为输入数据集来验证<code>partitioner</code>是如何工作的。</p>\n<table>\n<thead>\n<tr>\n<th>Id</th>\n<th>Name</th>\n<th>Age</th>\n<th>Gender</th>\n<th>Salary</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1201</td>\n<td>gopal</td>\n<td>45</td>\n<td>Male</td>\n<td>50,000</td>\n</tr>\n<tr>\n<td>1202</td>\n<td>manisha</td>\n<td>40</td>\n<td>Female</td>\n<td>50,000</td>\n</tr>\n<tr>\n<td>1203</td>\n<td>khalil</td>\n<td>34</td>\n<td>Male</td>\n<td>30,000</td>\n</tr>\n<tr>\n<td>1204</td>\n<td>prasanth</td>\n<td>30</td>\n<td>Male</td>\n<td>30,000</td>\n</tr>\n<tr>\n<td>1205</td>\n<td>kiran</td>\n<td>20</td>\n<td>Male</td>\n<td>40,000</td>\n</tr>\n<tr>\n<td>1206</td>\n<td>laxmi</td>\n<td>25</td>\n<td>Female</td>\n<td>35,000</td>\n</tr>\n<tr>\n<td>1207</td>\n<td>bhavya</td>\n<td>20</td>\n<td>Female</td>\n<td>15,000</td>\n</tr>\n<tr>\n<td>1208</td>\n<td>reshma</td>\n<td>19</td>\n<td>Female</td>\n<td>15,000</td>\n</tr>\n<tr>\n<td>1209</td>\n<td>kranthi</td>\n<td>22</td>\n<td>Male</td>\n<td>22,000</td>\n</tr>\n<tr>\n<td>1210</td>\n<td>Satish</td>\n<td>24</td>\n<td>Male</td>\n<td>25,000</td>\n</tr>\n<tr>\n<td>1211</td>\n<td>Krishna</td>\n<td>25</td>\n<td>Male</td>\n<td>25,000</td>\n</tr>\n<tr>\n<td>1212</td>\n<td>Arshad</td>\n<td>28</td>\n<td>Male</td>\n<td>20,000</td>\n</tr>\n<tr>\n<td>1213</td>\n<td>lavanya</td>\n<td>18</td>\n<td>Female</td>\n<td>8,000</td>\n</tr>\n</tbody>\n</table>\n<p>我们写一个程序来处理输入数据集，对年龄进行分组(例如：小于20，21-30，大于30)，并找到每个分组中的最高工资的员工。</p>\n<h4 id=\"2-1-输入数据\"><a href=\"#2-1-输入数据\" class=\"headerlink\" title=\"2.1 输入数据\"></a>2.1 输入数据</h4><p>以上数据存储在<code>/home/xiaosi/tmp/partitionerExample/input/</code>目录中的<code>input.txt</code>文件中，数据存储格式如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">1201\tgopal\t45\tMale\t50000</span><br><span class=\"line\">1202\tmanisha\t40\tFemale\t51000</span><br><span class=\"line\">1203\tkhaleel\t34\tMale\t30000</span><br><span class=\"line\">1204\tprasanth\t30\tMale\t31000</span><br><span class=\"line\">1205\tkiran\t20\tMale\t40000</span><br><span class=\"line\">1206\tlaxmi\t25\tFemale\t35000</span><br><span class=\"line\">1207\tbhavya\t20\tFemale\t15000</span><br><span class=\"line\">1208\treshma\t19\tFemale\t14000</span><br><span class=\"line\">1209\tkranthi\t22\tMale\t22000</span><br><span class=\"line\">1210\tSatish\t24\tMale\t25000</span><br><span class=\"line\">1211\tKrishna\t25\tMale\t26000</span><br><span class=\"line\">1212\tArshad\t28\tMale\t20000</span><br><span class=\"line\">1213\tlavanya\t18\tFemale\t8000</span><br></pre></td></tr></table></figure></p>\n<p>基于以上输入数据，下面是具体的算法描述。</p>\n<h4 id=\"2-2-Map任务\"><a href=\"#2-2-Map任务\" class=\"headerlink\" title=\"2.2 Map任务\"></a>2.2 Map任务</h4><p>Map任务以键值对作为输入，我们存储文本数据在text文件中。Map任务输入数据如下：</p>\n<h5 id=\"2-2-1-Input\"><a href=\"#2-2-1-Input\" class=\"headerlink\" title=\"2.2.1 Input\"></a>2.2.1 Input</h5><p>key以<code>特殊key+文件名+行号</code>的模式表示(例如，key = @input1)，value为一行中的数据(例如，value = 1201\\tgopal\\t45\\tMale\\t50000)。</p>\n<h5 id=\"2-2-2-Method\"><a href=\"#2-2-2-Method\" class=\"headerlink\" title=\"2.2.2 Method\"></a>2.2.2 Method</h5><p>读取一行中数据，使用split方法以<code>\\t</code>进行分割，取出性别存储在变量中<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\">String gender = str[<span class=\"number\">3</span>];</span><br></pre></td></tr></table></figure></p>\n<p>以性别为key，行记录数据为value作为输出键值对，从<code>Map</code>任务传递到<code>Partition</code>任务：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">context.write(<span class=\"keyword\">new</span> Text(gender), <span class=\"keyword\">new</span> Text(value));</span><br></pre></td></tr></table></figure></p>\n<p>对text文件中的所有记录重复以上所有步骤。</p>\n<h5 id=\"2-2-3-Output\"><a href=\"#2-2-3-Output\" class=\"headerlink\" title=\"2.2.3 Output\"></a>2.2.3 Output</h5><p>得到性别与记录数据组成的键值对</p>\n<h4 id=\"2-3-Partition任务\"><a href=\"#2-3-Partition任务\" class=\"headerlink\" title=\"2.3 Partition任务\"></a>2.3 Partition任务</h4><p><code>Partition</code>任务接受来自<code>Map</code>任务的键值对作为输入。<code>Partition</code>意味着将数据分成几个片段。根据给定分区条件规则，基于年龄标准将输入键值对数据划分为三部分。</p>\n<h4 id=\"2-3-1-Input\"><a href=\"#2-3-1-Input\" class=\"headerlink\" title=\"2.3.1 Input\"></a>2.3.1 Input</h4><p>键值对集合中的所有数据。key为记录中性别字段值，value为该性别对应的完整记录数据。</p>\n<h4 id=\"2-3-2-Method\"><a href=\"#2-3-2-Method\" class=\"headerlink\" title=\"2.3.2 Method\"></a>2.3.2 Method</h4><p>从键值对数据中读取年龄字段值<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>);</span><br><span class=\"line\"><span class=\"keyword\">int</span> age = Integer.parseInt(str[<span class=\"number\">2</span>]);</span><br></pre></td></tr></table></figure></p>\n<p>根据如下条件校验age值：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// age 小于等于20</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (age &lt;= <span class=\"number\">20</span>) &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// age 大于20 小于等于30</span></span><br><span class=\"line\"><span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (age &gt; <span class=\"number\">20</span> &amp;&amp; age &lt;= <span class=\"number\">30</span>) &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">1</span> % numReduceTask;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// age 大于30</span></span><br><span class=\"line\"><span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"number\">2</span> % numReduceTask;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-3-Output\"><a href=\"#2-3-3-Output\" class=\"headerlink\" title=\"2.3.3 Output\"></a>2.3.3 Output</h4><p>键值对所有数据被分割成三个键值对集合。<code>Reducer</code>会处理每一个集合。</p>\n<h4 id=\"2-4-Reduce任务\"><a href=\"#2-4-Reduce任务\" class=\"headerlink\" title=\"2.4 Reduce任务\"></a>2.4 Reduce任务</h4><p><code>partitioner</code>任务的数量等于<code>reducer</code>任务的数量。这里我们有三个<code>partitioner</code>任务，因此我们有三个<code>reducer</code>任务要执行。</p>\n<h5 id=\"2-4-1-Input\"><a href=\"#2-4-1-Input\" class=\"headerlink\" title=\"2.4.1 Input\"></a>2.4.1 Input</h5><p><code>Reducer</code>将使用不同的键值对集合执行三次。key为记录中性别字段值，value为该性别对应的完整记录数据。</p>\n<h5 id=\"2-4-2-Method\"><a href=\"#2-4-2-Method\" class=\"headerlink\" title=\"2.4.2 Method\"></a>2.4.2 Method</h5><p>读取记录数据中的Salary字段值：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\"><span class=\"keyword\">int</span> salary = Integer.parseInt(str[<span class=\"number\">4</span>]);</span><br></pre></td></tr></table></figure></p>\n<p>获取salary最大值:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (salary &gt; max) &#123;</span><br><span class=\"line\">   max = salary;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>对于每个key集合（Male与Female为两个key集合）中的数据重复以上步骤。执行完这三个步骤之后，我们将会分别从女性集合中得到一个最高工资，从男性集合中得到一个最高工资。<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">context.write(<span class=\"keyword\">new</span> Text(key), <span class=\"keyword\">new</span> IntWritable(max));</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-4-3-Output\"><a href=\"#2-4-3-Output\" class=\"headerlink\" title=\"2.4.3 Output\"></a>2.4.3 Output</h5><p>最后，我们将在不同年龄段的三个集合中获得一组键值对数据。它分别包含每个年龄段的男性集合的最高工资和每个年龄段的女性集合的最高工资。</p>\n<p>执行Map，Partition和Reduce任务后，键值对数据的三个集合存储在三个不同的文件中作为输出。</p>\n<p>所有这三项任务都被视为MapReduce作业。这些作业的以下要求和规范应在配置中指定：</p>\n<ul>\n<li>作业名称</li>\n<li>keys和values的输入输出格式</li>\n<li>Map，Reduce和Partitioner任务的类</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Configuration conf = getConf();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Create Job</span></span><br><span class=\"line\">Job job = <span class=\"keyword\">new</span> Job(conf, <span class=\"string\">\"topsal\"</span>);</span><br><span class=\"line\">job.setJarByClass(PartitionerExample.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// File Input and Output paths</span></span><br><span class=\"line\">FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(arg[<span class=\"number\">0</span>]));</span><br><span class=\"line\">FileOutputFormat.setOutputPath(job,<span class=\"keyword\">new</span> Path(arg[<span class=\"number\">1</span>]));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Set Mapper class and Output format for key-value pair.</span></span><br><span class=\"line\">job.setMapperClass(MapClass.class);</span><br><span class=\"line\">job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">job.setMapOutputValueClass(Text.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//set partitioner statement</span></span><br><span class=\"line\">job.setPartitionerClass(CaderPartitioner.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Set Reducer class and Input/Output format for key-value pair.</span></span><br><span class=\"line\">job.setReducerClass(ReduceClass.class);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Number of Reducer tasks.</span></span><br><span class=\"line\">job.setNumReduceTasks(<span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Input and Output format for data</span></span><br><span class=\"line\">job.setInputFormatClass(TextInputFormat.class);</span><br><span class=\"line\">job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class=\"line\">job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">job.setOutputValueClass(Text.class);</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Example\"><a href=\"#3-Example\" class=\"headerlink\" title=\"3. Example\"></a>3. Example</h3><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> com.sjf.open.test;</span><br><span class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configured;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.JobPriority;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class=\"line\"><span class=\"keyword\">import</span> com.sjf.open.utils.FileSystemUtil;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PartitionerExample</span> <span class=\"keyword\">extends</span> <span class=\"title\">Configured</span> <span class=\"keyword\">implements</span> <span class=\"title\">Tool</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> status = ToolRunner.run(<span class=\"keyword\">new</span> PartitionerExample(), args);</span><br><span class=\"line\">        System.exit(status);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">mapper</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt; </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key, Text value, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\">                String gender = str[<span class=\"number\">3</span>];</span><br><span class=\"line\">                context.write(<span class=\"keyword\">new</span> Text(gender), <span class=\"keyword\">new</span> Text(value));</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">                System.out.println(e.getMessage());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">reducer</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt; </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> max = Integer.MIN_VALUE;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></span><br><span class=\"line\"><span class=\"function\">                <span class=\"keyword\">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (Text value : values) &#123;</span><br><span class=\"line\">                String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>, -<span class=\"number\">3</span>);</span><br><span class=\"line\">                <span class=\"keyword\">int</span> salary = Integer.parseInt(str[<span class=\"number\">4</span>]);</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (salary &gt; max) &#123;</span><br><span class=\"line\">                    max = salary;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            context.write(<span class=\"keyword\">new</span> Text(key), <span class=\"keyword\">new</span> IntWritable(max));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">partitioner</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt; </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(Text key, Text value, <span class=\"keyword\">int</span> numReduceTask)</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(key.toString() + <span class=\"string\">\"------\"</span> + value.toString());</span><br><span class=\"line\">            String[] str = value.toString().split(<span class=\"string\">\"\\t\"</span>);</span><br><span class=\"line\">            <span class=\"keyword\">int</span> age = Integer.parseInt(str[<span class=\"number\">2</span>]);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (numReduceTask == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (age &lt;= <span class=\"number\">20</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (age &gt; <span class=\"number\">20</span> &amp;&amp; age &lt;= <span class=\"number\">30</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span> % numReduceTask;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">2</span> % numReduceTask;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (args.length != <span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">            System.err.println(<span class=\"string\">\"./run &lt;input&gt; &lt;output&gt;\"</span>);</span><br><span class=\"line\">            System.exit(<span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        String inputPath = args[<span class=\"number\">0</span>];</span><br><span class=\"line\">        String outputPath = args[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"keyword\">int</span> numReduceTasks = <span class=\"number\">3</span>;</span><br><span class=\"line\">        Configuration conf = <span class=\"keyword\">this</span>.getConf();</span><br><span class=\"line\">        conf.set(<span class=\"string\">\"mapred.job.queue.name\"</span>, <span class=\"string\">\"test\"</span>);</span><br><span class=\"line\">        conf.set(<span class=\"string\">\"mapreduce.map.memory.mb\"</span>, <span class=\"string\">\"1024\"</span>);</span><br><span class=\"line\">        conf.set(<span class=\"string\">\"mapreduce.reduce.memory.mb\"</span>, <span class=\"string\">\"1024\"</span>);</span><br><span class=\"line\">        conf.setBoolean(<span class=\"string\">\"mapred.output.compress\"</span>, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        conf.setClass(<span class=\"string\">\"mapred.output.compression.codec\"</span>, GzipCodec.class, CompressionCodec.class);</span><br><span class=\"line\">        Job job = Job.getInstance(conf);</span><br><span class=\"line\">        job.setJarByClass(PartitionerExample.class);</span><br><span class=\"line\">        job.setPartitionerClass(partitioner.class);</span><br><span class=\"line\">        job.setMapperClass(mapper.class);</span><br><span class=\"line\">        job.setReducerClass(reducer.class);</span><br><span class=\"line\">        job.setMapOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setMapOutputValueClass(Text.class);</span><br><span class=\"line\">        job.setOutputKeyClass(Text.class);</span><br><span class=\"line\">        job.setOutputValueClass(IntWritable.class);</span><br><span class=\"line\">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class=\"line\">        fileSystem.delete(<span class=\"keyword\">new</span> Path(outputPath), <span class=\"keyword\">true</span>);</span><br><span class=\"line\">        FileSystemUtil.filterNoExistsFile(conf, job, inputPath);</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(outputPath));</span><br><span class=\"line\">        job.setNumReduceTasks(numReduceTasks);</span><br><span class=\"line\">        <span class=\"keyword\">boolean</span> success = job.waitForCompletion(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> success ? <span class=\"number\">0</span> : <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-集群上执行\"><a href=\"#4-集群上执行\" class=\"headerlink\" title=\"4. 集群上执行\"></a>4. 集群上执行</h3><p>运行结果:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">17/01/03 20:22:02 INFO mapreduce.Job: Running job: job_1472052053889_7059198</span><br><span class=\"line\">17/01/03 20:22:21 INFO mapreduce.Job: Job job_1472052053889_7059198 running in uber mode : false</span><br><span class=\"line\">17/01/03 20:22:21 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class=\"line\">17/01/03 20:22:37 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class=\"line\">17/01/03 20:22:55 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class=\"line\">17/01/03 20:22:55 INFO mapreduce.Job: Job job_1472052053889_7059198 completed successfully</span><br><span class=\"line\">17/01/03 20:22:56 INFO mapreduce.Job: Counters: 43</span><br><span class=\"line\">        File System Counters</span><br><span class=\"line\">                FILE: Number of bytes read=470</span><br><span class=\"line\">                FILE: Number of bytes written=346003</span><br><span class=\"line\">                FILE: Number of read operations=0</span><br><span class=\"line\">                FILE: Number of large read operations=0</span><br><span class=\"line\">                FILE: Number of write operations=0</span><br><span class=\"line\">                HDFS: Number of bytes read=485</span><br><span class=\"line\">                HDFS: Number of bytes written=109</span><br><span class=\"line\">                HDFS: Number of read operations=12</span><br><span class=\"line\">                HDFS: Number of large read operations=0</span><br><span class=\"line\">                HDFS: Number of write operations=6</span><br><span class=\"line\">        Job Counters</span><br><span class=\"line\">                Launched map tasks=1</span><br><span class=\"line\">                Launched reduce tasks=3</span><br><span class=\"line\">                Rack-local map tasks=1</span><br><span class=\"line\">                Total time spent by all maps in occupied slots (ms)=5559</span><br><span class=\"line\">                Total time spent by all reduces in occupied slots (ms)=164768</span><br><span class=\"line\">        Map-Reduce Framework</span><br><span class=\"line\">                Map input records=13</span><br><span class=\"line\">                Map output records=13</span><br><span class=\"line\">                Map output bytes=426</span><br><span class=\"line\">                Map output materialized bytes=470</span><br><span class=\"line\">                Input split bytes=134</span><br><span class=\"line\">                Combine input records=0</span><br><span class=\"line\">                Combine output records=0</span><br><span class=\"line\">                Reduce input groups=6</span><br><span class=\"line\">                Reduce shuffle bytes=470</span><br><span class=\"line\">                Reduce input records=13</span><br><span class=\"line\">                Reduce output records=6</span><br><span class=\"line\">                Spilled Records=26</span><br><span class=\"line\">                Shuffled Maps =3</span><br><span class=\"line\">                Failed Shuffles=0</span><br><span class=\"line\">                Merged Map outputs=3</span><br><span class=\"line\">                GC time elapsed (ms)=31</span><br><span class=\"line\">                CPU time spent (ms)=2740</span><br><span class=\"line\">                Physical memory (bytes) snapshot=1349193728</span><br><span class=\"line\">                Virtual memory (bytes) snapshot=29673148416</span><br><span class=\"line\">                Total committed heap usage (bytes)=6888620032</span><br><span class=\"line\">        Shuffle Errors</span><br><span class=\"line\">                BAD_ID=0</span><br><span class=\"line\">                CONNECTION=0</span><br><span class=\"line\">                IO_ERROR=0</span><br><span class=\"line\">                WRONG_LENGTH=0</span><br><span class=\"line\">                WRONG_MAP=0</span><br><span class=\"line\">                WRONG_REDUCE=0</span><br><span class=\"line\">        File Input Format Counters</span><br><span class=\"line\">                Bytes Read=351</span><br><span class=\"line\">        File Output Format Counters</span><br><span class=\"line\">                Bytes Written=109</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm\" target=\"_blank\" rel=\"noopener\">https://www.tutorialspoint.com/map_reduce/map_reduce_partitioner.htm</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop Reducer总是能复用为Combiner？","date":"2017-12-06T01:39:01.000Z","_content":"\n`Combiner`函数是一个可选的中间函数，发生在`Map`阶段，`Mapper`执行完成后立即执行。使用`Combiner`有如下两个优势：\n\n- `Combiner`可以用来减少发送到`Reducer`的数据量，从而提高网络效率。\n- `Combiner`可以用于减少发送到`Reducer`的数据量，这将提高`Reduce`端的效率，因为每个`reduce`函数将处理相比于未使用`Combiner`之前更少的记录。\n\n`Combiner`与`Reducer`结构相同，因为`Combiner`和`Reducer`都对`Mapper`的输出进行处理。这给了我们一个复用`Reducer`作为`Combiner`的好机会。但问题是，复用`Reducer`作为`Combiner`总是可行的吗？\n\n### 1. Reducer作为Combiner的适用场景\n\n假设我们正在编写一个`MapReduce`程序来计算股票数据集中每个股票代码的最大收盘价。`Mapper`将数据集中每个股票记录的股票代码作为key和收盘价作为value。`Reducer`然后将循环遍历股票代码对应的所有收盘价，并从收盘价列表中计算最高收盘价。假设`Mapper 1`处理股票代码为`ABC`的3个记录，收盘价分别为50，60和111。让我们假设`Mapper 2`处理股票代码为ABC的2个记录，收盘价分别为100和31。那么`Reducer`将收到股票代码ABC五个收盘价---50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有收盘价，并将计算最高收盘价为111。\n\n我们可以在每个`Mapper`之后使用相同的`Reducer`作为`Combiner`。`Mapper 1`上的`Combiner`将处理3个收盘价格--50，60和111，并且仅输出111，因为它是3个收盘价的最大值。`Mapper 2`上的`Combiner`将处理2个收盘价格--100和31，并且仅输出100，因为它是2个收盘价的最大值。现在使用`Combiner`之后，`Reducer`仅处理股票代码ABC的2个收盘价(原先需要处理5个收盘价)，即来自`Mapper 1`的111和来自`Mapper 2`的100，并且将从这两个值中计算出最大收盘价格为111。\n\n 正如我们看到的，使用`Combiner`情况下`Reducer`输出与没有使用`Combiner`的输出结果是相同的，因此在这种情况下复用`Reducer`作为`Combiner`是没有问题。\n\n### 2. Reducer作为Combiner的不适用场景\n\n假设我们正在编写一个`MapReduce`程序来计算股票数据集中每个股票代码的平均交易量（average volume for each symbol）。`Mapper`将数据集中每个股票记录的股票代码作为key和交易量（volume）作为value。`Reducer`然后将循环遍历股票代码对应的所有交易量，并从交易量列表中计算出平均交易量（average volume from the list of volumes for that symbol）。假设`Mapper 1`处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设`Mapper 2`处理股票代码为ABC的2个记录，收盘价分别为100和31。那么`Reducer`将收到股票代码ABC五个收盘价---50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有交易量，并将计算出平均交易量为70.4。\n```\n50 + 60 + 111 + 100 + 31 /  5 = 352 / 5 = 70.4\n```\n让我们看看如果我们在每个`Mapper`之后复用`Reducer`作为`Combiner`会发生什么。`Mapper 1`上的`Combiner`将处理3个交易量--50，60和111，并计算出三个交易量的平均交易量为73.66。\n`Mapper 2`上的`Combiner`将处理2个交易量--100和31，并计算出两个交易量的平均交易量为65.5。那么在复用`Reducer`作为`Combiner`的情况下，`Reducer`仅处理股票代码ABC的2个平均交易量，来自`Mapper1`的73.66和来自`Mapper2`的65.5，并计算股票代码ABC最终的平均交易量为69.58。\n```\n73.66 + 65.5 /2  = 69.58\n```\n这与我们不复用`Reducer`作为`Combiner`得出的结果不一样，因此复用`Reducer`作为`Combiner`得出平均交易量是不正确的。\n\n所以我们可以看到`Reducer`不能总是被用于`Combiner`。所以，当你决定复用`Reducer`作为`Combiner`的时候，你需要问自己这样一个问题:使用`Combiner`与不使用`Combiner`的输出结果是否一样？\n\n### 3. 区别\n\n`Combiner`需要实现`Reducer`接口。`Combiner`只能用于特定情况。\n\n- 与`Reducer`不同，`Combiner`有一个约束，`Combiner`输入/输出键和值类型必须与`Mapper`的输出键和值类型相匹配。而`Reducer`只是输入键和值类型与`Mapper`的输出键和值类型相匹配。\n- `Combiner`只能用于满足交换律`（a.b = b.a）`和结合律`（a.(b.c)= (a.b).c）`的情况。这也意味着`Combiner`可能只能用于键和值的一个子集或者可能不能使用。\n- `Reducer`可以从多个`Mapper`获取数据。`Combiner`只能从一个`Mapper`获取其输入。\n\n\n\n原文：http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/\n","source":"_posts/Hadoop/Hadoop Reducer总是能复用为Combiner？.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop Reducer总是能复用为Combiner？\ndate: 2017-12-06 09:39:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n`Combiner`函数是一个可选的中间函数，发生在`Map`阶段，`Mapper`执行完成后立即执行。使用`Combiner`有如下两个优势：\n\n- `Combiner`可以用来减少发送到`Reducer`的数据量，从而提高网络效率。\n- `Combiner`可以用于减少发送到`Reducer`的数据量，这将提高`Reduce`端的效率，因为每个`reduce`函数将处理相比于未使用`Combiner`之前更少的记录。\n\n`Combiner`与`Reducer`结构相同，因为`Combiner`和`Reducer`都对`Mapper`的输出进行处理。这给了我们一个复用`Reducer`作为`Combiner`的好机会。但问题是，复用`Reducer`作为`Combiner`总是可行的吗？\n\n### 1. Reducer作为Combiner的适用场景\n\n假设我们正在编写一个`MapReduce`程序来计算股票数据集中每个股票代码的最大收盘价。`Mapper`将数据集中每个股票记录的股票代码作为key和收盘价作为value。`Reducer`然后将循环遍历股票代码对应的所有收盘价，并从收盘价列表中计算最高收盘价。假设`Mapper 1`处理股票代码为`ABC`的3个记录，收盘价分别为50，60和111。让我们假设`Mapper 2`处理股票代码为ABC的2个记录，收盘价分别为100和31。那么`Reducer`将收到股票代码ABC五个收盘价---50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有收盘价，并将计算最高收盘价为111。\n\n我们可以在每个`Mapper`之后使用相同的`Reducer`作为`Combiner`。`Mapper 1`上的`Combiner`将处理3个收盘价格--50，60和111，并且仅输出111，因为它是3个收盘价的最大值。`Mapper 2`上的`Combiner`将处理2个收盘价格--100和31，并且仅输出100，因为它是2个收盘价的最大值。现在使用`Combiner`之后，`Reducer`仅处理股票代码ABC的2个收盘价(原先需要处理5个收盘价)，即来自`Mapper 1`的111和来自`Mapper 2`的100，并且将从这两个值中计算出最大收盘价格为111。\n\n 正如我们看到的，使用`Combiner`情况下`Reducer`输出与没有使用`Combiner`的输出结果是相同的，因此在这种情况下复用`Reducer`作为`Combiner`是没有问题。\n\n### 2. Reducer作为Combiner的不适用场景\n\n假设我们正在编写一个`MapReduce`程序来计算股票数据集中每个股票代码的平均交易量（average volume for each symbol）。`Mapper`将数据集中每个股票记录的股票代码作为key和交易量（volume）作为value。`Reducer`然后将循环遍历股票代码对应的所有交易量，并从交易量列表中计算出平均交易量（average volume from the list of volumes for that symbol）。假设`Mapper 1`处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设`Mapper 2`处理股票代码为ABC的2个记录，收盘价分别为100和31。那么`Reducer`将收到股票代码ABC五个收盘价---50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有交易量，并将计算出平均交易量为70.4。\n```\n50 + 60 + 111 + 100 + 31 /  5 = 352 / 5 = 70.4\n```\n让我们看看如果我们在每个`Mapper`之后复用`Reducer`作为`Combiner`会发生什么。`Mapper 1`上的`Combiner`将处理3个交易量--50，60和111，并计算出三个交易量的平均交易量为73.66。\n`Mapper 2`上的`Combiner`将处理2个交易量--100和31，并计算出两个交易量的平均交易量为65.5。那么在复用`Reducer`作为`Combiner`的情况下，`Reducer`仅处理股票代码ABC的2个平均交易量，来自`Mapper1`的73.66和来自`Mapper2`的65.5，并计算股票代码ABC最终的平均交易量为69.58。\n```\n73.66 + 65.5 /2  = 69.58\n```\n这与我们不复用`Reducer`作为`Combiner`得出的结果不一样，因此复用`Reducer`作为`Combiner`得出平均交易量是不正确的。\n\n所以我们可以看到`Reducer`不能总是被用于`Combiner`。所以，当你决定复用`Reducer`作为`Combiner`的时候，你需要问自己这样一个问题:使用`Combiner`与不使用`Combiner`的输出结果是否一样？\n\n### 3. 区别\n\n`Combiner`需要实现`Reducer`接口。`Combiner`只能用于特定情况。\n\n- 与`Reducer`不同，`Combiner`有一个约束，`Combiner`输入/输出键和值类型必须与`Mapper`的输出键和值类型相匹配。而`Reducer`只是输入键和值类型与`Mapper`的输出键和值类型相匹配。\n- `Combiner`只能用于满足交换律`（a.b = b.a）`和结合律`（a.(b.c)= (a.b).c）`的情况。这也意味着`Combiner`可能只能用于键和值的一个子集或者可能不能使用。\n- `Reducer`可以从多个`Mapper`获取数据。`Combiner`只能从一个`Mapper`获取其输入。\n\n\n\n原文：http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/\n","slug":"Hadoop/Hadoop Reducer总是能复用为Combiner？","published":1,"updated":"2018-01-29T09:36:59.633Z","comments":1,"photos":[],"link":"","_id":"cje58tisn0030ordbz8fea2qg","content":"<p><code>Combiner</code>函数是一个可选的中间函数，发生在<code>Map</code>阶段，<code>Mapper</code>执行完成后立即执行。使用<code>Combiner</code>有如下两个优势：</p>\n<ul>\n<li><code>Combiner</code>可以用来减少发送到<code>Reducer</code>的数据量，从而提高网络效率。</li>\n<li><code>Combiner</code>可以用于减少发送到<code>Reducer</code>的数据量，这将提高<code>Reduce</code>端的效率，因为每个<code>reduce</code>函数将处理相比于未使用<code>Combiner</code>之前更少的记录。</li>\n</ul>\n<p><code>Combiner</code>与<code>Reducer</code>结构相同，因为<code>Combiner</code>和<code>Reducer</code>都对<code>Mapper</code>的输出进行处理。这给了我们一个复用<code>Reducer</code>作为<code>Combiner</code>的好机会。但问题是，复用<code>Reducer</code>作为<code>Combiner</code>总是可行的吗？</p>\n<h3 id=\"1-Reducer作为Combiner的适用场景\"><a href=\"#1-Reducer作为Combiner的适用场景\" class=\"headerlink\" title=\"1. Reducer作为Combiner的适用场景\"></a>1. Reducer作为Combiner的适用场景</h3><p>假设我们正在编写一个<code>MapReduce</code>程序来计算股票数据集中每个股票代码的最大收盘价。<code>Mapper</code>将数据集中每个股票记录的股票代码作为key和收盘价作为value。<code>Reducer</code>然后将循环遍历股票代码对应的所有收盘价，并从收盘价列表中计算最高收盘价。假设<code>Mapper 1</code>处理股票代码为<code>ABC</code>的3个记录，收盘价分别为50，60和111。让我们假设<code>Mapper 2</code>处理股票代码为ABC的2个记录，收盘价分别为100和31。那么<code>Reducer</code>将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有收盘价，并将计算最高收盘价为111。</p>\n<p>我们可以在每个<code>Mapper</code>之后使用相同的<code>Reducer</code>作为<code>Combiner</code>。<code>Mapper 1</code>上的<code>Combiner</code>将处理3个收盘价格–50，60和111，并且仅输出111，因为它是3个收盘价的最大值。<code>Mapper 2</code>上的<code>Combiner</code>将处理2个收盘价格–100和31，并且仅输出100，因为它是2个收盘价的最大值。现在使用<code>Combiner</code>之后，<code>Reducer</code>仅处理股票代码ABC的2个收盘价(原先需要处理5个收盘价)，即来自<code>Mapper 1</code>的111和来自<code>Mapper 2</code>的100，并且将从这两个值中计算出最大收盘价格为111。</p>\n<p> 正如我们看到的，使用<code>Combiner</code>情况下<code>Reducer</code>输出与没有使用<code>Combiner</code>的输出结果是相同的，因此在这种情况下复用<code>Reducer</code>作为<code>Combiner</code>是没有问题。</p>\n<h3 id=\"2-Reducer作为Combiner的不适用场景\"><a href=\"#2-Reducer作为Combiner的不适用场景\" class=\"headerlink\" title=\"2. Reducer作为Combiner的不适用场景\"></a>2. Reducer作为Combiner的不适用场景</h3><p>假设我们正在编写一个<code>MapReduce</code>程序来计算股票数据集中每个股票代码的平均交易量（average volume for each symbol）。<code>Mapper</code>将数据集中每个股票记录的股票代码作为key和交易量（volume）作为value。<code>Reducer</code>然后将循环遍历股票代码对应的所有交易量，并从交易量列表中计算出平均交易量（average volume from the list of volumes for that symbol）。假设<code>Mapper 1</code>处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设<code>Mapper 2</code>处理股票代码为ABC的2个记录，收盘价分别为100和31。那么<code>Reducer</code>将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有交易量，并将计算出平均交易量为70.4。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">50 + 60 + 111 + 100 + 31 /  5 = 352 / 5 = 70.4</span><br></pre></td></tr></table></figure></p>\n<p>让我们看看如果我们在每个<code>Mapper</code>之后复用<code>Reducer</code>作为<code>Combiner</code>会发生什么。<code>Mapper 1</code>上的<code>Combiner</code>将处理3个交易量–50，60和111，并计算出三个交易量的平均交易量为73.66。<br><code>Mapper 2</code>上的<code>Combiner</code>将处理2个交易量–100和31，并计算出两个交易量的平均交易量为65.5。那么在复用<code>Reducer</code>作为<code>Combiner</code>的情况下，<code>Reducer</code>仅处理股票代码ABC的2个平均交易量，来自<code>Mapper1</code>的73.66和来自<code>Mapper2</code>的65.5，并计算股票代码ABC最终的平均交易量为69.58。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">73.66 + 65.5 /2  = 69.58</span><br></pre></td></tr></table></figure></p>\n<p>这与我们不复用<code>Reducer</code>作为<code>Combiner</code>得出的结果不一样，因此复用<code>Reducer</code>作为<code>Combiner</code>得出平均交易量是不正确的。</p>\n<p>所以我们可以看到<code>Reducer</code>不能总是被用于<code>Combiner</code>。所以，当你决定复用<code>Reducer</code>作为<code>Combiner</code>的时候，你需要问自己这样一个问题:使用<code>Combiner</code>与不使用<code>Combiner</code>的输出结果是否一样？</p>\n<h3 id=\"3-区别\"><a href=\"#3-区别\" class=\"headerlink\" title=\"3. 区别\"></a>3. 区别</h3><p><code>Combiner</code>需要实现<code>Reducer</code>接口。<code>Combiner</code>只能用于特定情况。</p>\n<ul>\n<li>与<code>Reducer</code>不同，<code>Combiner</code>有一个约束，<code>Combiner</code>输入/输出键和值类型必须与<code>Mapper</code>的输出键和值类型相匹配。而<code>Reducer</code>只是输入键和值类型与<code>Mapper</code>的输出键和值类型相匹配。</li>\n<li><code>Combiner</code>只能用于满足交换律<code>（a.b = b.a）</code>和结合律<code>（a.(b.c)= (a.b).c）</code>的情况。这也意味着<code>Combiner</code>可能只能用于键和值的一个子集或者可能不能使用。</li>\n<li><code>Reducer</code>可以从多个<code>Mapper</code>获取数据。<code>Combiner</code>只能从一个<code>Mapper</code>获取其输入。</li>\n</ul>\n<p>原文：<a href=\"http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/\" target=\"_blank\" rel=\"noopener\">http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>Combiner</code>函数是一个可选的中间函数，发生在<code>Map</code>阶段，<code>Mapper</code>执行完成后立即执行。使用<code>Combiner</code>有如下两个优势：</p>\n<ul>\n<li><code>Combiner</code>可以用来减少发送到<code>Reducer</code>的数据量，从而提高网络效率。</li>\n<li><code>Combiner</code>可以用于减少发送到<code>Reducer</code>的数据量，这将提高<code>Reduce</code>端的效率，因为每个<code>reduce</code>函数将处理相比于未使用<code>Combiner</code>之前更少的记录。</li>\n</ul>\n<p><code>Combiner</code>与<code>Reducer</code>结构相同，因为<code>Combiner</code>和<code>Reducer</code>都对<code>Mapper</code>的输出进行处理。这给了我们一个复用<code>Reducer</code>作为<code>Combiner</code>的好机会。但问题是，复用<code>Reducer</code>作为<code>Combiner</code>总是可行的吗？</p>\n<h3 id=\"1-Reducer作为Combiner的适用场景\"><a href=\"#1-Reducer作为Combiner的适用场景\" class=\"headerlink\" title=\"1. Reducer作为Combiner的适用场景\"></a>1. Reducer作为Combiner的适用场景</h3><p>假设我们正在编写一个<code>MapReduce</code>程序来计算股票数据集中每个股票代码的最大收盘价。<code>Mapper</code>将数据集中每个股票记录的股票代码作为key和收盘价作为value。<code>Reducer</code>然后将循环遍历股票代码对应的所有收盘价，并从收盘价列表中计算最高收盘价。假设<code>Mapper 1</code>处理股票代码为<code>ABC</code>的3个记录，收盘价分别为50，60和111。让我们假设<code>Mapper 2</code>处理股票代码为ABC的2个记录，收盘价分别为100和31。那么<code>Reducer</code>将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有收盘价，并将计算最高收盘价为111。</p>\n<p>我们可以在每个<code>Mapper</code>之后使用相同的<code>Reducer</code>作为<code>Combiner</code>。<code>Mapper 1</code>上的<code>Combiner</code>将处理3个收盘价格–50，60和111，并且仅输出111，因为它是3个收盘价的最大值。<code>Mapper 2</code>上的<code>Combiner</code>将处理2个收盘价格–100和31，并且仅输出100，因为它是2个收盘价的最大值。现在使用<code>Combiner</code>之后，<code>Reducer</code>仅处理股票代码ABC的2个收盘价(原先需要处理5个收盘价)，即来自<code>Mapper 1</code>的111和来自<code>Mapper 2</code>的100，并且将从这两个值中计算出最大收盘价格为111。</p>\n<p> 正如我们看到的，使用<code>Combiner</code>情况下<code>Reducer</code>输出与没有使用<code>Combiner</code>的输出结果是相同的，因此在这种情况下复用<code>Reducer</code>作为<code>Combiner</code>是没有问题。</p>\n<h3 id=\"2-Reducer作为Combiner的不适用场景\"><a href=\"#2-Reducer作为Combiner的不适用场景\" class=\"headerlink\" title=\"2. Reducer作为Combiner的不适用场景\"></a>2. Reducer作为Combiner的不适用场景</h3><p>假设我们正在编写一个<code>MapReduce</code>程序来计算股票数据集中每个股票代码的平均交易量（average volume for each symbol）。<code>Mapper</code>将数据集中每个股票记录的股票代码作为key和交易量（volume）作为value。<code>Reducer</code>然后将循环遍历股票代码对应的所有交易量，并从交易量列表中计算出平均交易量（average volume from the list of volumes for that symbol）。假设<code>Mapper 1</code>处理股票代码为ABC的3个记录，收盘价分别为50，60和111。让我们假设<code>Mapper 2</code>处理股票代码为ABC的2个记录，收盘价分别为100和31。那么<code>Reducer</code>将收到股票代码ABC五个收盘价—50，60，111，100和31。Reducer的工作非常简单，它将简单地循环遍历所有交易量，并将计算出平均交易量为70.4。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">50 + 60 + 111 + 100 + 31 /  5 = 352 / 5 = 70.4</span><br></pre></td></tr></table></figure></p>\n<p>让我们看看如果我们在每个<code>Mapper</code>之后复用<code>Reducer</code>作为<code>Combiner</code>会发生什么。<code>Mapper 1</code>上的<code>Combiner</code>将处理3个交易量–50，60和111，并计算出三个交易量的平均交易量为73.66。<br><code>Mapper 2</code>上的<code>Combiner</code>将处理2个交易量–100和31，并计算出两个交易量的平均交易量为65.5。那么在复用<code>Reducer</code>作为<code>Combiner</code>的情况下，<code>Reducer</code>仅处理股票代码ABC的2个平均交易量，来自<code>Mapper1</code>的73.66和来自<code>Mapper2</code>的65.5，并计算股票代码ABC最终的平均交易量为69.58。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">73.66 + 65.5 /2  = 69.58</span><br></pre></td></tr></table></figure></p>\n<p>这与我们不复用<code>Reducer</code>作为<code>Combiner</code>得出的结果不一样，因此复用<code>Reducer</code>作为<code>Combiner</code>得出平均交易量是不正确的。</p>\n<p>所以我们可以看到<code>Reducer</code>不能总是被用于<code>Combiner</code>。所以，当你决定复用<code>Reducer</code>作为<code>Combiner</code>的时候，你需要问自己这样一个问题:使用<code>Combiner</code>与不使用<code>Combiner</code>的输出结果是否一样？</p>\n<h3 id=\"3-区别\"><a href=\"#3-区别\" class=\"headerlink\" title=\"3. 区别\"></a>3. 区别</h3><p><code>Combiner</code>需要实现<code>Reducer</code>接口。<code>Combiner</code>只能用于特定情况。</p>\n<ul>\n<li>与<code>Reducer</code>不同，<code>Combiner</code>有一个约束，<code>Combiner</code>输入/输出键和值类型必须与<code>Mapper</code>的输出键和值类型相匹配。而<code>Reducer</code>只是输入键和值类型与<code>Mapper</code>的输出键和值类型相匹配。</li>\n<li><code>Combiner</code>只能用于满足交换律<code>（a.b = b.a）</code>和结合律<code>（a.(b.c)= (a.b).c）</code>的情况。这也意味着<code>Combiner</code>可能只能用于键和值的一个子集或者可能不能使用。</li>\n<li><code>Reducer</code>可以从多个<code>Mapper</code>获取数据。<code>Combiner</code>只能从一个<code>Mapper</code>获取其输入。</li>\n</ul>\n<p>原文：<a href=\"http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/\" target=\"_blank\" rel=\"noopener\">http://hadoopinrealworld.com/can-reducer-always-be-reused-for-combiner/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop 图解HDFS工作原理","date":"2017-12-19T02:03:01.000Z","_content":"\n结合Maneesh Varshney的漫画改编，为大家分析HDFS存储机制与运行原理。\n\n### 1. HDFS角色\n\n如下图所示，`HDFS`存储相关角色与功能如下：\n- `Client`：客户端，系统使用者，调用`HDFS API`操作文件；与`NameNode`交互获取文件元数据；与`DataNode`交互进行数据读写。\n- `Namenode`：元数据节点，是系统唯一的管理者。负责元数据的管理；与`client`交互进行提供元数据查询；分配数据存储节点等。\n- `Datanode`：数据存储节点，负责数据块的存储与冗余备份；执行数据块的读写操作等。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true)\n\n### 2. HDFS写文件\n\n#### 2.1 发送写数据请求\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-2.png?raw=true)\n\n`HDFS`中的存储单元是`block`。文件通常被分成64或128M一块的数据块进行存储。与普通文件系统不同的是，在`HDFS`中，如果一个文件大小小于一个数据块的大小，它是不需要占用整个数据块的存储空间的。\n\n#### 2.2 文件切分\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-3.png?raw=true)\n\n\n#### 2.3 DataNode分配\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-4.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-5.png?raw=true)\n\n#### 2.4 数据写入\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-48.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-6.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-7.png?raw=true)\n\n#### 2.5 完成写入\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-8.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-9.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-10.png?raw=true)\n\n#### 2.6 角色定位\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-11.png?raw=true)\n\n#### 2.7 写操作分析\n\n通过写过程，我们可以了解到：\n- `HDFS`属于`Maste`r与`Slave`结构。一个集群中只有一个`NameNode`，可以有多个`DataNode`；\n- `HDFS`存储机制保存了多个副本，当写入1T文件时，我们需要3T的存储，3T的网络流量带宽；系统提供容错机制，副本丢失或宕机可自动恢复，保证系统高可用性。\n- `HDFS`默认会将文件分割成`block`。然后将`block`按键值对存储在`HDFS`上，并将键值对的映射存到内存中。如果小文件太多，会导致内存的负担很重。\n\n### 3. HDFS读文件\n\n#### 3.1 用户需求\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-12.png?raw=true)\n\n`HDFS`采用的是`一次写入多次读取`的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。\n\n#### 3.2 联系元数据节点\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-13.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-14.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-15.png?raw=true)\n\n#### 3.3 下载数据\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-16.png?raw=true)\n\n前文提到在写数据过程中，数据存储已经按照客户端与`DataNode`节点之间的距离进行了排序，距客户端越近的`DataNode`节点被放在最前面，客户端会优先从本地读取该数据块。\n\n#### 3.4 思考\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-17.png?raw=true)\n\n### 4. HDFS容错机制一\n\n#### 4.1 三类故障\n\n##### 4.1.1 节点失败\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-18.png?raw=true)\n\n##### 4.1.2 网络故障\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-19.png?raw=true)\n\n##### 4.1.3 数据损坏(脏数据)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-20.png?raw=true)\n\n#### 4.2 故障监测机制\n\n##### 4.2.1 节点失败监测机制\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-21.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-22.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-23.png?raw=true)\n\n##### 4.2.2 通信故障监测机制\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-24.png?raw=true)\n\n\n##### 4.2.3 数据错误监测机制\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-25.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-26.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-27.png?raw=true)\n\n#### 4.3 心跳信息与数据块报告\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-28.png?raw=true)\n\n`HDFS`存储理念是以最少的钱买最烂的机器并实现最安全、难度高的分布式文件系统（高容错性低成本），从上可以看出，`HDFS`认为机器故障是种常态，所以在设计时充分考虑到单个机器故障，单个磁盘故障，单个文件丢失等情况。\n\n### 5. HDFS容错机制二\n\n#### 5.1 写容错\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-29.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-30.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-31.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-32.png?raw=true)\n\n#### 5.2 读容错\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-33.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-34.png?raw=true)\n\n### 6. HDFS容错机制三\n\n#### 6.1 数据节点(DN)失效\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-35.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-36.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-37.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-38.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-39.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-40.png?raw=true)\n\n### 7. 备份规则\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-41.png?raw=true)\n\n#### 7.1 机架与数据节点\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-42.png?raw=true)\n\n\n#### 7.2 副本放置策略\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-43.png?raw=true)\n\n数据块的第一个副本优先放在写入数据块的客户端所在的节点上，但是如果这个客户端上的数据节点空间不足或者是当前负载过重，则应该从该数据节点所在的机架中选择一个合适的数据节点作为本地节点。\n\n如果客户端上没有一个数据节点的话，则从整个集群中随机选择一个合适的数据节点作为此时这个数据块的本地节点。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-44.png?raw=true)\n\n`HDFS`的存放策略是将一个副本存放在本地机架节点上，另外两个副本放在不同机架的不同节点上。\n\n这样集群可在完全失去某一机架的情况下还能存活。同时，这种策略减少了机架间的数据传输，提高了写操作的效率，因为数据块只存放在两个不同的机架上，减少了读取数据时需要的网络传输总带宽。这样在一定程度上兼顾了数据安全和网络传输的开销。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-45.png?raw=true)\n\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-46.png?raw=true)\n\n\n来源于: 京东大数据专家公众号\n","source":"_posts/Hadoop/Hadoop 图解HDFS工作原理.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop 图解HDFS工作原理\ndate: 2017-12-19 10:03:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n结合Maneesh Varshney的漫画改编，为大家分析HDFS存储机制与运行原理。\n\n### 1. HDFS角色\n\n如下图所示，`HDFS`存储相关角色与功能如下：\n- `Client`：客户端，系统使用者，调用`HDFS API`操作文件；与`NameNode`交互获取文件元数据；与`DataNode`交互进行数据读写。\n- `Namenode`：元数据节点，是系统唯一的管理者。负责元数据的管理；与`client`交互进行提供元数据查询；分配数据存储节点等。\n- `Datanode`：数据存储节点，负责数据块的存储与冗余备份；执行数据块的读写操作等。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true)\n\n### 2. HDFS写文件\n\n#### 2.1 发送写数据请求\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-2.png?raw=true)\n\n`HDFS`中的存储单元是`block`。文件通常被分成64或128M一块的数据块进行存储。与普通文件系统不同的是，在`HDFS`中，如果一个文件大小小于一个数据块的大小，它是不需要占用整个数据块的存储空间的。\n\n#### 2.2 文件切分\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-3.png?raw=true)\n\n\n#### 2.3 DataNode分配\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-4.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-5.png?raw=true)\n\n#### 2.4 数据写入\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-48.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-6.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-7.png?raw=true)\n\n#### 2.5 完成写入\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-8.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-9.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-10.png?raw=true)\n\n#### 2.6 角色定位\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-11.png?raw=true)\n\n#### 2.7 写操作分析\n\n通过写过程，我们可以了解到：\n- `HDFS`属于`Maste`r与`Slave`结构。一个集群中只有一个`NameNode`，可以有多个`DataNode`；\n- `HDFS`存储机制保存了多个副本，当写入1T文件时，我们需要3T的存储，3T的网络流量带宽；系统提供容错机制，副本丢失或宕机可自动恢复，保证系统高可用性。\n- `HDFS`默认会将文件分割成`block`。然后将`block`按键值对存储在`HDFS`上，并将键值对的映射存到内存中。如果小文件太多，会导致内存的负担很重。\n\n### 3. HDFS读文件\n\n#### 3.1 用户需求\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-12.png?raw=true)\n\n`HDFS`采用的是`一次写入多次读取`的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。\n\n#### 3.2 联系元数据节点\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-13.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-14.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-15.png?raw=true)\n\n#### 3.3 下载数据\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-16.png?raw=true)\n\n前文提到在写数据过程中，数据存储已经按照客户端与`DataNode`节点之间的距离进行了排序，距客户端越近的`DataNode`节点被放在最前面，客户端会优先从本地读取该数据块。\n\n#### 3.4 思考\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-17.png?raw=true)\n\n### 4. HDFS容错机制一\n\n#### 4.1 三类故障\n\n##### 4.1.1 节点失败\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-18.png?raw=true)\n\n##### 4.1.2 网络故障\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-19.png?raw=true)\n\n##### 4.1.3 数据损坏(脏数据)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-20.png?raw=true)\n\n#### 4.2 故障监测机制\n\n##### 4.2.1 节点失败监测机制\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-21.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-22.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-23.png?raw=true)\n\n##### 4.2.2 通信故障监测机制\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-24.png?raw=true)\n\n\n##### 4.2.3 数据错误监测机制\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-25.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-26.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-27.png?raw=true)\n\n#### 4.3 心跳信息与数据块报告\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-28.png?raw=true)\n\n`HDFS`存储理念是以最少的钱买最烂的机器并实现最安全、难度高的分布式文件系统（高容错性低成本），从上可以看出，`HDFS`认为机器故障是种常态，所以在设计时充分考虑到单个机器故障，单个磁盘故障，单个文件丢失等情况。\n\n### 5. HDFS容错机制二\n\n#### 5.1 写容错\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-29.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-30.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-31.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-32.png?raw=true)\n\n#### 5.2 读容错\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-33.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-34.png?raw=true)\n\n### 6. HDFS容错机制三\n\n#### 6.1 数据节点(DN)失效\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-35.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-36.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-37.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-38.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-39.png?raw=true)\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-40.png?raw=true)\n\n### 7. 备份规则\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-41.png?raw=true)\n\n#### 7.1 机架与数据节点\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-42.png?raw=true)\n\n\n#### 7.2 副本放置策略\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-43.png?raw=true)\n\n数据块的第一个副本优先放在写入数据块的客户端所在的节点上，但是如果这个客户端上的数据节点空间不足或者是当前负载过重，则应该从该数据节点所在的机架中选择一个合适的数据节点作为本地节点。\n\n如果客户端上没有一个数据节点的话，则从整个集群中随机选择一个合适的数据节点作为此时这个数据块的本地节点。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-44.png?raw=true)\n\n`HDFS`的存放策略是将一个副本存放在本地机架节点上，另外两个副本放在不同机架的不同节点上。\n\n这样集群可在完全失去某一机架的情况下还能存活。同时，这种策略减少了机架间的数据传输，提高了写操作的效率，因为数据块只存放在两个不同的机架上，减少了读取数据时需要的网络传输总带宽。这样在一定程度上兼顾了数据安全和网络传输的开销。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-45.png?raw=true)\n\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-46.png?raw=true)\n\n\n来源于: 京东大数据专家公众号\n","slug":"Hadoop/Hadoop 图解HDFS工作原理","published":1,"updated":"2018-01-29T09:36:59.632Z","comments":1,"photos":[],"link":"","_id":"cje58tisp0033ordbfbgq50t5","content":"<p>结合Maneesh Varshney的漫画改编，为大家分析HDFS存储机制与运行原理。</p>\n<h3 id=\"1-HDFS角色\"><a href=\"#1-HDFS角色\" class=\"headerlink\" title=\"1. HDFS角色\"></a>1. HDFS角色</h3><p>如下图所示，<code>HDFS</code>存储相关角色与功能如下：</p>\n<ul>\n<li><code>Client</code>：客户端，系统使用者，调用<code>HDFS API</code>操作文件；与<code>NameNode</code>交互获取文件元数据；与<code>DataNode</code>交互进行数据读写。</li>\n<li><code>Namenode</code>：元数据节点，是系统唯一的管理者。负责元数据的管理；与<code>client</code>交互进行提供元数据查询；分配数据存储节点等。</li>\n<li><code>Datanode</code>：数据存储节点，负责数据块的存储与冗余备份；执行数据块的读写操作等。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-HDFS写文件\"><a href=\"#2-HDFS写文件\" class=\"headerlink\" title=\"2. HDFS写文件\"></a>2. HDFS写文件</h3><h4 id=\"2-1-发送写数据请求\"><a href=\"#2-1-发送写数据请求\" class=\"headerlink\" title=\"2.1 发送写数据请求\"></a>2.1 发送写数据请求</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-2.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>中的存储单元是<code>block</code>。文件通常被分成64或128M一块的数据块进行存储。与普通文件系统不同的是，在<code>HDFS</code>中，如果一个文件大小小于一个数据块的大小，它是不需要占用整个数据块的存储空间的。</p>\n<h4 id=\"2-2-文件切分\"><a href=\"#2-2-文件切分\" class=\"headerlink\" title=\"2.2 文件切分\"></a>2.2 文件切分</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-DataNode分配\"><a href=\"#2-3-DataNode分配\" class=\"headerlink\" title=\"2.3 DataNode分配\"></a>2.3 DataNode分配</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-4.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-5.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-4-数据写入\"><a href=\"#2-4-数据写入\" class=\"headerlink\" title=\"2.4 数据写入\"></a>2.4 数据写入</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-48.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-6.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-7.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-5-完成写入\"><a href=\"#2-5-完成写入\" class=\"headerlink\" title=\"2.5 完成写入\"></a>2.5 完成写入</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-8.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-9.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-10.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-6-角色定位\"><a href=\"#2-6-角色定位\" class=\"headerlink\" title=\"2.6 角色定位\"></a>2.6 角色定位</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-11.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-7-写操作分析\"><a href=\"#2-7-写操作分析\" class=\"headerlink\" title=\"2.7 写操作分析\"></a>2.7 写操作分析</h4><p>通过写过程，我们可以了解到：</p>\n<ul>\n<li><code>HDFS</code>属于<code>Maste</code>r与<code>Slave</code>结构。一个集群中只有一个<code>NameNode</code>，可以有多个<code>DataNode</code>；</li>\n<li><code>HDFS</code>存储机制保存了多个副本，当写入1T文件时，我们需要3T的存储，3T的网络流量带宽；系统提供容错机制，副本丢失或宕机可自动恢复，保证系统高可用性。</li>\n<li><code>HDFS</code>默认会将文件分割成<code>block</code>。然后将<code>block</code>按键值对存储在<code>HDFS</code>上，并将键值对的映射存到内存中。如果小文件太多，会导致内存的负担很重。</li>\n</ul>\n<h3 id=\"3-HDFS读文件\"><a href=\"#3-HDFS读文件\" class=\"headerlink\" title=\"3. HDFS读文件\"></a>3. HDFS读文件</h3><h4 id=\"3-1-用户需求\"><a href=\"#3-1-用户需求\" class=\"headerlink\" title=\"3.1 用户需求\"></a>3.1 用户需求</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-12.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>采用的是<code>一次写入多次读取</code>的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。</p>\n<h4 id=\"3-2-联系元数据节点\"><a href=\"#3-2-联系元数据节点\" class=\"headerlink\" title=\"3.2 联系元数据节点\"></a>3.2 联系元数据节点</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-13.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-14.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-15.png?raw=true\" alt=\"\"></p>\n<h4 id=\"3-3-下载数据\"><a href=\"#3-3-下载数据\" class=\"headerlink\" title=\"3.3 下载数据\"></a>3.3 下载数据</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-16.png?raw=true\" alt=\"\"></p>\n<p>前文提到在写数据过程中，数据存储已经按照客户端与<code>DataNode</code>节点之间的距离进行了排序，距客户端越近的<code>DataNode</code>节点被放在最前面，客户端会优先从本地读取该数据块。</p>\n<h4 id=\"3-4-思考\"><a href=\"#3-4-思考\" class=\"headerlink\" title=\"3.4 思考\"></a>3.4 思考</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-17.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-HDFS容错机制一\"><a href=\"#4-HDFS容错机制一\" class=\"headerlink\" title=\"4. HDFS容错机制一\"></a>4. HDFS容错机制一</h3><h4 id=\"4-1-三类故障\"><a href=\"#4-1-三类故障\" class=\"headerlink\" title=\"4.1 三类故障\"></a>4.1 三类故障</h4><h5 id=\"4-1-1-节点失败\"><a href=\"#4-1-1-节点失败\" class=\"headerlink\" title=\"4.1.1 节点失败\"></a>4.1.1 节点失败</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-18.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-1-2-网络故障\"><a href=\"#4-1-2-网络故障\" class=\"headerlink\" title=\"4.1.2 网络故障\"></a>4.1.2 网络故障</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-19.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-1-3-数据损坏-脏数据\"><a href=\"#4-1-3-数据损坏-脏数据\" class=\"headerlink\" title=\"4.1.3 数据损坏(脏数据)\"></a>4.1.3 数据损坏(脏数据)</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-20.png?raw=true\" alt=\"\"></p>\n<h4 id=\"4-2-故障监测机制\"><a href=\"#4-2-故障监测机制\" class=\"headerlink\" title=\"4.2 故障监测机制\"></a>4.2 故障监测机制</h4><h5 id=\"4-2-1-节点失败监测机制\"><a href=\"#4-2-1-节点失败监测机制\" class=\"headerlink\" title=\"4.2.1 节点失败监测机制\"></a>4.2.1 节点失败监测机制</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-21.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-22.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-23.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-2-2-通信故障监测机制\"><a href=\"#4-2-2-通信故障监测机制\" class=\"headerlink\" title=\"4.2.2 通信故障监测机制\"></a>4.2.2 通信故障监测机制</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-24.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-2-3-数据错误监测机制\"><a href=\"#4-2-3-数据错误监测机制\" class=\"headerlink\" title=\"4.2.3 数据错误监测机制\"></a>4.2.3 数据错误监测机制</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-25.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-26.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-27.png?raw=true\" alt=\"\"></p>\n<h4 id=\"4-3-心跳信息与数据块报告\"><a href=\"#4-3-心跳信息与数据块报告\" class=\"headerlink\" title=\"4.3 心跳信息与数据块报告\"></a>4.3 心跳信息与数据块报告</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-28.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>存储理念是以最少的钱买最烂的机器并实现最安全、难度高的分布式文件系统（高容错性低成本），从上可以看出，<code>HDFS</code>认为机器故障是种常态，所以在设计时充分考虑到单个机器故障，单个磁盘故障，单个文件丢失等情况。</p>\n<h3 id=\"5-HDFS容错机制二\"><a href=\"#5-HDFS容错机制二\" class=\"headerlink\" title=\"5. HDFS容错机制二\"></a>5. HDFS容错机制二</h3><h4 id=\"5-1-写容错\"><a href=\"#5-1-写容错\" class=\"headerlink\" title=\"5.1 写容错\"></a>5.1 写容错</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-29.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-30.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-31.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-32.png?raw=true\" alt=\"\"></p>\n<h4 id=\"5-2-读容错\"><a href=\"#5-2-读容错\" class=\"headerlink\" title=\"5.2 读容错\"></a>5.2 读容错</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-33.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-34.png?raw=true\" alt=\"\"></p>\n<h3 id=\"6-HDFS容错机制三\"><a href=\"#6-HDFS容错机制三\" class=\"headerlink\" title=\"6. HDFS容错机制三\"></a>6. HDFS容错机制三</h3><h4 id=\"6-1-数据节点-DN-失效\"><a href=\"#6-1-数据节点-DN-失效\" class=\"headerlink\" title=\"6.1 数据节点(DN)失效\"></a>6.1 数据节点(DN)失效</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-35.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-36.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-37.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-38.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-39.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-40.png?raw=true\" alt=\"\"></p>\n<h3 id=\"7-备份规则\"><a href=\"#7-备份规则\" class=\"headerlink\" title=\"7. 备份规则\"></a>7. 备份规则</h3><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-41.png?raw=true\" alt=\"\"></p>\n<h4 id=\"7-1-机架与数据节点\"><a href=\"#7-1-机架与数据节点\" class=\"headerlink\" title=\"7.1 机架与数据节点\"></a>7.1 机架与数据节点</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-42.png?raw=true\" alt=\"\"></p>\n<h4 id=\"7-2-副本放置策略\"><a href=\"#7-2-副本放置策略\" class=\"headerlink\" title=\"7.2 副本放置策略\"></a>7.2 副本放置策略</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-43.png?raw=true\" alt=\"\"></p>\n<p>数据块的第一个副本优先放在写入数据块的客户端所在的节点上，但是如果这个客户端上的数据节点空间不足或者是当前负载过重，则应该从该数据节点所在的机架中选择一个合适的数据节点作为本地节点。</p>\n<p>如果客户端上没有一个数据节点的话，则从整个集群中随机选择一个合适的数据节点作为此时这个数据块的本地节点。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-44.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>的存放策略是将一个副本存放在本地机架节点上，另外两个副本放在不同机架的不同节点上。</p>\n<p>这样集群可在完全失去某一机架的情况下还能存活。同时，这种策略减少了机架间的数据传输，提高了写操作的效率，因为数据块只存放在两个不同的机架上，减少了读取数据时需要的网络传输总带宽。这样在一定程度上兼顾了数据安全和网络传输的开销。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-45.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-46.png?raw=true\" alt=\"\"></p>\n<p>来源于: 京东大数据专家公众号</p>\n","site":{"data":{}},"excerpt":"","more":"<p>结合Maneesh Varshney的漫画改编，为大家分析HDFS存储机制与运行原理。</p>\n<h3 id=\"1-HDFS角色\"><a href=\"#1-HDFS角色\" class=\"headerlink\" title=\"1. HDFS角色\"></a>1. HDFS角色</h3><p>如下图所示，<code>HDFS</code>存储相关角色与功能如下：</p>\n<ul>\n<li><code>Client</code>：客户端，系统使用者，调用<code>HDFS API</code>操作文件；与<code>NameNode</code>交互获取文件元数据；与<code>DataNode</code>交互进行数据读写。</li>\n<li><code>Namenode</code>：元数据节点，是系统唯一的管理者。负责元数据的管理；与<code>client</code>交互进行提供元数据查询；分配数据存储节点等。</li>\n<li><code>Datanode</code>：数据存储节点，负责数据块的存储与冗余备份；执行数据块的读写操作等。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-HDFS写文件\"><a href=\"#2-HDFS写文件\" class=\"headerlink\" title=\"2. HDFS写文件\"></a>2. HDFS写文件</h3><h4 id=\"2-1-发送写数据请求\"><a href=\"#2-1-发送写数据请求\" class=\"headerlink\" title=\"2.1 发送写数据请求\"></a>2.1 发送写数据请求</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-2.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>中的存储单元是<code>block</code>。文件通常被分成64或128M一块的数据块进行存储。与普通文件系统不同的是，在<code>HDFS</code>中，如果一个文件大小小于一个数据块的大小，它是不需要占用整个数据块的存储空间的。</p>\n<h4 id=\"2-2-文件切分\"><a href=\"#2-2-文件切分\" class=\"headerlink\" title=\"2.2 文件切分\"></a>2.2 文件切分</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-DataNode分配\"><a href=\"#2-3-DataNode分配\" class=\"headerlink\" title=\"2.3 DataNode分配\"></a>2.3 DataNode分配</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-4.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-5.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-4-数据写入\"><a href=\"#2-4-数据写入\" class=\"headerlink\" title=\"2.4 数据写入\"></a>2.4 数据写入</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-48.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-6.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-7.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-5-完成写入\"><a href=\"#2-5-完成写入\" class=\"headerlink\" title=\"2.5 完成写入\"></a>2.5 完成写入</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-8.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-9.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-10.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-6-角色定位\"><a href=\"#2-6-角色定位\" class=\"headerlink\" title=\"2.6 角色定位\"></a>2.6 角色定位</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-11.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-7-写操作分析\"><a href=\"#2-7-写操作分析\" class=\"headerlink\" title=\"2.7 写操作分析\"></a>2.7 写操作分析</h4><p>通过写过程，我们可以了解到：</p>\n<ul>\n<li><code>HDFS</code>属于<code>Maste</code>r与<code>Slave</code>结构。一个集群中只有一个<code>NameNode</code>，可以有多个<code>DataNode</code>；</li>\n<li><code>HDFS</code>存储机制保存了多个副本，当写入1T文件时，我们需要3T的存储，3T的网络流量带宽；系统提供容错机制，副本丢失或宕机可自动恢复，保证系统高可用性。</li>\n<li><code>HDFS</code>默认会将文件分割成<code>block</code>。然后将<code>block</code>按键值对存储在<code>HDFS</code>上，并将键值对的映射存到内存中。如果小文件太多，会导致内存的负担很重。</li>\n</ul>\n<h3 id=\"3-HDFS读文件\"><a href=\"#3-HDFS读文件\" class=\"headerlink\" title=\"3. HDFS读文件\"></a>3. HDFS读文件</h3><h4 id=\"3-1-用户需求\"><a href=\"#3-1-用户需求\" class=\"headerlink\" title=\"3.1 用户需求\"></a>3.1 用户需求</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-12.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>采用的是<code>一次写入多次读取</code>的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。</p>\n<h4 id=\"3-2-联系元数据节点\"><a href=\"#3-2-联系元数据节点\" class=\"headerlink\" title=\"3.2 联系元数据节点\"></a>3.2 联系元数据节点</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-13.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-14.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-15.png?raw=true\" alt=\"\"></p>\n<h4 id=\"3-3-下载数据\"><a href=\"#3-3-下载数据\" class=\"headerlink\" title=\"3.3 下载数据\"></a>3.3 下载数据</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-16.png?raw=true\" alt=\"\"></p>\n<p>前文提到在写数据过程中，数据存储已经按照客户端与<code>DataNode</code>节点之间的距离进行了排序，距客户端越近的<code>DataNode</code>节点被放在最前面，客户端会优先从本地读取该数据块。</p>\n<h4 id=\"3-4-思考\"><a href=\"#3-4-思考\" class=\"headerlink\" title=\"3.4 思考\"></a>3.4 思考</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-17.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-HDFS容错机制一\"><a href=\"#4-HDFS容错机制一\" class=\"headerlink\" title=\"4. HDFS容错机制一\"></a>4. HDFS容错机制一</h3><h4 id=\"4-1-三类故障\"><a href=\"#4-1-三类故障\" class=\"headerlink\" title=\"4.1 三类故障\"></a>4.1 三类故障</h4><h5 id=\"4-1-1-节点失败\"><a href=\"#4-1-1-节点失败\" class=\"headerlink\" title=\"4.1.1 节点失败\"></a>4.1.1 节点失败</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-18.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-1-2-网络故障\"><a href=\"#4-1-2-网络故障\" class=\"headerlink\" title=\"4.1.2 网络故障\"></a>4.1.2 网络故障</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-19.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-1-3-数据损坏-脏数据\"><a href=\"#4-1-3-数据损坏-脏数据\" class=\"headerlink\" title=\"4.1.3 数据损坏(脏数据)\"></a>4.1.3 数据损坏(脏数据)</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-20.png?raw=true\" alt=\"\"></p>\n<h4 id=\"4-2-故障监测机制\"><a href=\"#4-2-故障监测机制\" class=\"headerlink\" title=\"4.2 故障监测机制\"></a>4.2 故障监测机制</h4><h5 id=\"4-2-1-节点失败监测机制\"><a href=\"#4-2-1-节点失败监测机制\" class=\"headerlink\" title=\"4.2.1 节点失败监测机制\"></a>4.2.1 节点失败监测机制</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-21.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-22.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-23.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-2-2-通信故障监测机制\"><a href=\"#4-2-2-通信故障监测机制\" class=\"headerlink\" title=\"4.2.2 通信故障监测机制\"></a>4.2.2 通信故障监测机制</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-24.png?raw=true\" alt=\"\"></p>\n<h5 id=\"4-2-3-数据错误监测机制\"><a href=\"#4-2-3-数据错误监测机制\" class=\"headerlink\" title=\"4.2.3 数据错误监测机制\"></a>4.2.3 数据错误监测机制</h5><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-25.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-26.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-27.png?raw=true\" alt=\"\"></p>\n<h4 id=\"4-3-心跳信息与数据块报告\"><a href=\"#4-3-心跳信息与数据块报告\" class=\"headerlink\" title=\"4.3 心跳信息与数据块报告\"></a>4.3 心跳信息与数据块报告</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-28.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>存储理念是以最少的钱买最烂的机器并实现最安全、难度高的分布式文件系统（高容错性低成本），从上可以看出，<code>HDFS</code>认为机器故障是种常态，所以在设计时充分考虑到单个机器故障，单个磁盘故障，单个文件丢失等情况。</p>\n<h3 id=\"5-HDFS容错机制二\"><a href=\"#5-HDFS容错机制二\" class=\"headerlink\" title=\"5. HDFS容错机制二\"></a>5. HDFS容错机制二</h3><h4 id=\"5-1-写容错\"><a href=\"#5-1-写容错\" class=\"headerlink\" title=\"5.1 写容错\"></a>5.1 写容错</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-29.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-30.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-31.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-32.png?raw=true\" alt=\"\"></p>\n<h4 id=\"5-2-读容错\"><a href=\"#5-2-读容错\" class=\"headerlink\" title=\"5.2 读容错\"></a>5.2 读容错</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-33.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-34.png?raw=true\" alt=\"\"></p>\n<h3 id=\"6-HDFS容错机制三\"><a href=\"#6-HDFS容错机制三\" class=\"headerlink\" title=\"6. HDFS容错机制三\"></a>6. HDFS容错机制三</h3><h4 id=\"6-1-数据节点-DN-失效\"><a href=\"#6-1-数据节点-DN-失效\" class=\"headerlink\" title=\"6.1 数据节点(DN)失效\"></a>6.1 数据节点(DN)失效</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-35.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-36.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-37.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-38.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-39.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-40.png?raw=true\" alt=\"\"></p>\n<h3 id=\"7-备份规则\"><a href=\"#7-备份规则\" class=\"headerlink\" title=\"7. 备份规则\"></a>7. 备份规则</h3><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-41.png?raw=true\" alt=\"\"></p>\n<h4 id=\"7-1-机架与数据节点\"><a href=\"#7-1-机架与数据节点\" class=\"headerlink\" title=\"7.1 机架与数据节点\"></a>7.1 机架与数据节点</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-42.png?raw=true\" alt=\"\"></p>\n<h4 id=\"7-2-副本放置策略\"><a href=\"#7-2-副本放置策略\" class=\"headerlink\" title=\"7.2 副本放置策略\"></a>7.2 副本放置策略</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-43.png?raw=true\" alt=\"\"></p>\n<p>数据块的第一个副本优先放在写入数据块的客户端所在的节点上，但是如果这个客户端上的数据节点空间不足或者是当前负载过重，则应该从该数据节点所在的机架中选择一个合适的数据节点作为本地节点。</p>\n<p>如果客户端上没有一个数据节点的话，则从整个集群中随机选择一个合适的数据节点作为此时这个数据块的本地节点。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-44.png?raw=true\" alt=\"\"></p>\n<p><code>HDFS</code>的存放策略是将一个副本存放在本地机架节点上，另外两个副本放在不同机架的不同节点上。</p>\n<p>这样集群可在完全失去某一机架的情况下还能存活。同时，这种策略减少了机架间的数据传输，提高了写操作的效率，因为数据块只存放在两个不同的机架上，减少了读取数据时需要的网络传输总带宽。这样在一定程度上兼顾了数据安全和网络传输的开销。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-45.png?raw=true\" alt=\"\"></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20%E5%9B%BE%E8%A7%A3HDFS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-46.png?raw=true\" alt=\"\"></p>\n<p>来源于: 京东大数据专家公众号</p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop1.x Secondary NameNode的作用","date":"2017-12-20T12:26:01.000Z","_content":"\n`Secondary NameNode`是`Hadoop`中命名不当的其中一个组件。不当命名很容易造成歧义，通过`Secondary NameNode`这个名字，我们很容易理解为是一个备份`NameNode`，但实际上它不是。很多`Hadoop`的初学者对`Secondary NameNode`究竟做了什么以及为什么存在于`HDFS`中感到困惑。因此，在这篇博文中，我试图解释`HDFS`中`Secondary NameNode`的作用。\n\n通过它的名字，你可能会认为它和`NameNode`有关，它确实是`NameNode`相关。所以在我们深入研究`Secondary NameNode`之前，让我们看看`NameNode`究竟做了什么。\n\n### 1. NameNode\n\n`NameNode`保存HDFS的元数据，如名称空间信息，块信息等。使用时，所有这些信息都存储在主存储器中。 但是这些信息也存储在磁盘中用于持久性存储。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-1.png?raw=true)\n\n上图显示了`NameNode`如何将信息存储在磁盘中。上图中两个不同的文件是：\n- `fsimage` - 它是`NameNode`启动时文件系统元数据的快照\n- 编辑日志 - 它是在`NameNode`启动之后对文件系统进行更改的序列\n\n只有在重新启动`NameNode`时，编辑日志才会合并到`fsimage`以获取文件系统元数据的最新快照。但是在线上集群上，`NameNode`重启并不是很常见，这就意味对于`NameNode`长时间运行的集群来说编辑日志可能会变得非常大(可能会无限增长)。在这种情况下我们会遇到以下问题：\n- 编辑日志变得非常大，对于管理来说是一个挑战\n- `NameNode`重启需要很长时间，因为很多更改需要合并(译者注:需要恢复编辑日志中的各项操作，导致`NameNode`重启会比较慢)\n- 在崩溃的情况下，我们将丢失大量的元数据，因为`fsimage`是比较旧的(译者注:生成最新`fsimage`之后的各项操作都保存在编辑日志中，而不是`fsimage`，还未合并)\n\n所以为了解决这个问题，我们需要一个机制来帮助我们减少编辑日志的大小，并且得到一个最新的`fsimage`，这样`NameNode`上的负载就会降低一些。这与`Windows`恢复点非常相似，它可以让我们获得操作系统的快照，以便在出现问题时退回到上一个恢复点。\n\n现在我们理解了`NameNode`的功能，以及保持最新元数据的挑战。所以这一切都与`Seconadary NameNode`有关？\n\n### 2. Seconadary NameNode\n\n通过`Secondary NameNode`实现编辑日志与`fsimage`的合来解决上述问题。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-2.png?raw=true)\n\n上图显示了`Secondary NameNode`的工作原理：\n\n- 它定期从`NameNode`获取编辑日志，并与`fsimage`合并成新的`fsimage`\n- 一旦生成新的`fsimage`，就会复制回`NameNode`\n- `NameNode`下次重新启动时将使用这个`fsimage`进行重新启动，从而减少启动时间\n\n`Secondary NameNode`的整个目的就是在`HDFS`中提供一个检查点。它只是`NameNode`的一个帮助节点。这也是它在社区内被认为是检查点节点的原因。\n\n所以我们现在明白所有的`Secondary NameNode`都会在文件系统中设置一个检查点，这将有助于`NameNode`更好地运行。它不可以替换`NameNode`或也不是`NameNode`的备份。所以从现在开始习惯把它叫做检查点节点。\n\n\n原文:http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/\n","source":"_posts/Hadoop/Hadoop1.x Secondary NameNode的作用.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop1.x Secondary NameNode的作用\ndate: 2017-12-20 20:26:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n`Secondary NameNode`是`Hadoop`中命名不当的其中一个组件。不当命名很容易造成歧义，通过`Secondary NameNode`这个名字，我们很容易理解为是一个备份`NameNode`，但实际上它不是。很多`Hadoop`的初学者对`Secondary NameNode`究竟做了什么以及为什么存在于`HDFS`中感到困惑。因此，在这篇博文中，我试图解释`HDFS`中`Secondary NameNode`的作用。\n\n通过它的名字，你可能会认为它和`NameNode`有关，它确实是`NameNode`相关。所以在我们深入研究`Secondary NameNode`之前，让我们看看`NameNode`究竟做了什么。\n\n### 1. NameNode\n\n`NameNode`保存HDFS的元数据，如名称空间信息，块信息等。使用时，所有这些信息都存储在主存储器中。 但是这些信息也存储在磁盘中用于持久性存储。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-1.png?raw=true)\n\n上图显示了`NameNode`如何将信息存储在磁盘中。上图中两个不同的文件是：\n- `fsimage` - 它是`NameNode`启动时文件系统元数据的快照\n- 编辑日志 - 它是在`NameNode`启动之后对文件系统进行更改的序列\n\n只有在重新启动`NameNode`时，编辑日志才会合并到`fsimage`以获取文件系统元数据的最新快照。但是在线上集群上，`NameNode`重启并不是很常见，这就意味对于`NameNode`长时间运行的集群来说编辑日志可能会变得非常大(可能会无限增长)。在这种情况下我们会遇到以下问题：\n- 编辑日志变得非常大，对于管理来说是一个挑战\n- `NameNode`重启需要很长时间，因为很多更改需要合并(译者注:需要恢复编辑日志中的各项操作，导致`NameNode`重启会比较慢)\n- 在崩溃的情况下，我们将丢失大量的元数据，因为`fsimage`是比较旧的(译者注:生成最新`fsimage`之后的各项操作都保存在编辑日志中，而不是`fsimage`，还未合并)\n\n所以为了解决这个问题，我们需要一个机制来帮助我们减少编辑日志的大小，并且得到一个最新的`fsimage`，这样`NameNode`上的负载就会降低一些。这与`Windows`恢复点非常相似，它可以让我们获得操作系统的快照，以便在出现问题时退回到上一个恢复点。\n\n现在我们理解了`NameNode`的功能，以及保持最新元数据的挑战。所以这一切都与`Seconadary NameNode`有关？\n\n### 2. Seconadary NameNode\n\n通过`Secondary NameNode`实现编辑日志与`fsimage`的合来解决上述问题。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-2.png?raw=true)\n\n上图显示了`Secondary NameNode`的工作原理：\n\n- 它定期从`NameNode`获取编辑日志，并与`fsimage`合并成新的`fsimage`\n- 一旦生成新的`fsimage`，就会复制回`NameNode`\n- `NameNode`下次重新启动时将使用这个`fsimage`进行重新启动，从而减少启动时间\n\n`Secondary NameNode`的整个目的就是在`HDFS`中提供一个检查点。它只是`NameNode`的一个帮助节点。这也是它在社区内被认为是检查点节点的原因。\n\n所以我们现在明白所有的`Secondary NameNode`都会在文件系统中设置一个检查点，这将有助于`NameNode`更好地运行。它不可以替换`NameNode`或也不是`NameNode`的备份。所以从现在开始习惯把它叫做检查点节点。\n\n\n原文:http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/\n","slug":"Hadoop/Hadoop1.x Secondary NameNode的作用","published":1,"updated":"2018-01-29T09:36:59.631Z","comments":1,"photos":[],"link":"","_id":"cje58tisr0036ordb4eqke0ex","content":"<p><code>Secondary NameNode</code>是<code>Hadoop</code>中命名不当的其中一个组件。不当命名很容易造成歧义，通过<code>Secondary NameNode</code>这个名字，我们很容易理解为是一个备份<code>NameNode</code>，但实际上它不是。很多<code>Hadoop</code>的初学者对<code>Secondary NameNode</code>究竟做了什么以及为什么存在于<code>HDFS</code>中感到困惑。因此，在这篇博文中，我试图解释<code>HDFS</code>中<code>Secondary NameNode</code>的作用。</p>\n<p>通过它的名字，你可能会认为它和<code>NameNode</code>有关，它确实是<code>NameNode</code>相关。所以在我们深入研究<code>Secondary NameNode</code>之前，让我们看看<code>NameNode</code>究竟做了什么。</p>\n<h3 id=\"1-NameNode\"><a href=\"#1-NameNode\" class=\"headerlink\" title=\"1. NameNode\"></a>1. NameNode</h3><p><code>NameNode</code>保存HDFS的元数据，如名称空间信息，块信息等。使用时，所有这些信息都存储在主存储器中。 但是这些信息也存储在磁盘中用于持久性存储。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-1.png?raw=true\" alt=\"\"></p>\n<p>上图显示了<code>NameNode</code>如何将信息存储在磁盘中。上图中两个不同的文件是：</p>\n<ul>\n<li><code>fsimage</code> - 它是<code>NameNode</code>启动时文件系统元数据的快照</li>\n<li>编辑日志 - 它是在<code>NameNode</code>启动之后对文件系统进行更改的序列</li>\n</ul>\n<p>只有在重新启动<code>NameNode</code>时，编辑日志才会合并到<code>fsimage</code>以获取文件系统元数据的最新快照。但是在线上集群上，<code>NameNode</code>重启并不是很常见，这就意味对于<code>NameNode</code>长时间运行的集群来说编辑日志可能会变得非常大(可能会无限增长)。在这种情况下我们会遇到以下问题：</p>\n<ul>\n<li>编辑日志变得非常大，对于管理来说是一个挑战</li>\n<li><code>NameNode</code>重启需要很长时间，因为很多更改需要合并(译者注:需要恢复编辑日志中的各项操作，导致<code>NameNode</code>重启会比较慢)</li>\n<li>在崩溃的情况下，我们将丢失大量的元数据，因为<code>fsimage</code>是比较旧的(译者注:生成最新<code>fsimage</code>之后的各项操作都保存在编辑日志中，而不是<code>fsimage</code>，还未合并)</li>\n</ul>\n<p>所以为了解决这个问题，我们需要一个机制来帮助我们减少编辑日志的大小，并且得到一个最新的<code>fsimage</code>，这样<code>NameNode</code>上的负载就会降低一些。这与<code>Windows</code>恢复点非常相似，它可以让我们获得操作系统的快照，以便在出现问题时退回到上一个恢复点。</p>\n<p>现在我们理解了<code>NameNode</code>的功能，以及保持最新元数据的挑战。所以这一切都与<code>Seconadary NameNode</code>有关？</p>\n<h3 id=\"2-Seconadary-NameNode\"><a href=\"#2-Seconadary-NameNode\" class=\"headerlink\" title=\"2. Seconadary NameNode\"></a>2. Seconadary NameNode</h3><p>通过<code>Secondary NameNode</code>实现编辑日志与<code>fsimage</code>的合来解决上述问题。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-2.png?raw=true\" alt=\"\"></p>\n<p>上图显示了<code>Secondary NameNode</code>的工作原理：</p>\n<ul>\n<li>它定期从<code>NameNode</code>获取编辑日志，并与<code>fsimage</code>合并成新的<code>fsimage</code></li>\n<li>一旦生成新的<code>fsimage</code>，就会复制回<code>NameNode</code></li>\n<li><code>NameNode</code>下次重新启动时将使用这个<code>fsimage</code>进行重新启动，从而减少启动时间</li>\n</ul>\n<p><code>Secondary NameNode</code>的整个目的就是在<code>HDFS</code>中提供一个检查点。它只是<code>NameNode</code>的一个帮助节点。这也是它在社区内被认为是检查点节点的原因。</p>\n<p>所以我们现在明白所有的<code>Secondary NameNode</code>都会在文件系统中设置一个检查点，这将有助于<code>NameNode</code>更好地运行。它不可以替换<code>NameNode</code>或也不是<code>NameNode</code>的备份。所以从现在开始习惯把它叫做检查点节点。</p>\n<p>原文:<a href=\"http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/\" target=\"_blank\" rel=\"noopener\">http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>Secondary NameNode</code>是<code>Hadoop</code>中命名不当的其中一个组件。不当命名很容易造成歧义，通过<code>Secondary NameNode</code>这个名字，我们很容易理解为是一个备份<code>NameNode</code>，但实际上它不是。很多<code>Hadoop</code>的初学者对<code>Secondary NameNode</code>究竟做了什么以及为什么存在于<code>HDFS</code>中感到困惑。因此，在这篇博文中，我试图解释<code>HDFS</code>中<code>Secondary NameNode</code>的作用。</p>\n<p>通过它的名字，你可能会认为它和<code>NameNode</code>有关，它确实是<code>NameNode</code>相关。所以在我们深入研究<code>Secondary NameNode</code>之前，让我们看看<code>NameNode</code>究竟做了什么。</p>\n<h3 id=\"1-NameNode\"><a href=\"#1-NameNode\" class=\"headerlink\" title=\"1. NameNode\"></a>1. NameNode</h3><p><code>NameNode</code>保存HDFS的元数据，如名称空间信息，块信息等。使用时，所有这些信息都存储在主存储器中。 但是这些信息也存储在磁盘中用于持久性存储。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-1.png?raw=true\" alt=\"\"></p>\n<p>上图显示了<code>NameNode</code>如何将信息存储在磁盘中。上图中两个不同的文件是：</p>\n<ul>\n<li><code>fsimage</code> - 它是<code>NameNode</code>启动时文件系统元数据的快照</li>\n<li>编辑日志 - 它是在<code>NameNode</code>启动之后对文件系统进行更改的序列</li>\n</ul>\n<p>只有在重新启动<code>NameNode</code>时，编辑日志才会合并到<code>fsimage</code>以获取文件系统元数据的最新快照。但是在线上集群上，<code>NameNode</code>重启并不是很常见，这就意味对于<code>NameNode</code>长时间运行的集群来说编辑日志可能会变得非常大(可能会无限增长)。在这种情况下我们会遇到以下问题：</p>\n<ul>\n<li>编辑日志变得非常大，对于管理来说是一个挑战</li>\n<li><code>NameNode</code>重启需要很长时间，因为很多更改需要合并(译者注:需要恢复编辑日志中的各项操作，导致<code>NameNode</code>重启会比较慢)</li>\n<li>在崩溃的情况下，我们将丢失大量的元数据，因为<code>fsimage</code>是比较旧的(译者注:生成最新<code>fsimage</code>之后的各项操作都保存在编辑日志中，而不是<code>fsimage</code>，还未合并)</li>\n</ul>\n<p>所以为了解决这个问题，我们需要一个机制来帮助我们减少编辑日志的大小，并且得到一个最新的<code>fsimage</code>，这样<code>NameNode</code>上的负载就会降低一些。这与<code>Windows</code>恢复点非常相似，它可以让我们获得操作系统的快照，以便在出现问题时退回到上一个恢复点。</p>\n<p>现在我们理解了<code>NameNode</code>的功能，以及保持最新元数据的挑战。所以这一切都与<code>Seconadary NameNode</code>有关？</p>\n<h3 id=\"2-Seconadary-NameNode\"><a href=\"#2-Seconadary-NameNode\" class=\"headerlink\" title=\"2. Seconadary NameNode\"></a>2. Seconadary NameNode</h3><p>通过<code>Secondary NameNode</code>实现编辑日志与<code>fsimage</code>的合来解决上述问题。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop%20Secondary%20NameNode%E7%9A%84%E4%BD%9C%E7%94%A8-2.png?raw=true\" alt=\"\"></p>\n<p>上图显示了<code>Secondary NameNode</code>的工作原理：</p>\n<ul>\n<li>它定期从<code>NameNode</code>获取编辑日志，并与<code>fsimage</code>合并成新的<code>fsimage</code></li>\n<li>一旦生成新的<code>fsimage</code>，就会复制回<code>NameNode</code></li>\n<li><code>NameNode</code>下次重新启动时将使用这个<code>fsimage</code>进行重新启动，从而减少启动时间</li>\n</ul>\n<p><code>Secondary NameNode</code>的整个目的就是在<code>HDFS</code>中提供一个检查点。它只是<code>NameNode</code>的一个帮助节点。这也是它在社区内被认为是检查点节点的原因。</p>\n<p>所以我们现在明白所有的<code>Secondary NameNode</code>都会在文件系统中设置一个检查点，这将有助于<code>NameNode</code>更好地运行。它不可以替换<code>NameNode</code>或也不是<code>NameNode</code>的备份。所以从现在开始习惯把它叫做检查点节点。</p>\n<p>原文:<a href=\"http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/\" target=\"_blank\" rel=\"noopener\">http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop Trash回收站使用指南","date":"2017-12-07T02:07:01.000Z","_content":"\n我们在删除一个文件时，遇到如下问题，提示我们不能删除文件放回回收站:\n```\nsudo -uxiaosi hadoop fs -rm -r tmp/data_group/test/employee/employee_salary.txt\n17/12/06 16:34:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.\n17/12/06 16:34:48 WARN fs.TrashPolicyDefault: Can't create trash directory: hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee\nrm: Failed to move to trash: hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt. Consider using -skipTrash option\n```\n\n去回收站对应目录下观察一下，得出的结论是：无法创建目录`employee`，因为`employee`文件已经存在，自然导致`employee_salary.txt`文件不能放回收回站:\n```\n-rw-r--r--   3 xiaosi xiaosi  352 2017-12-06 16:18 hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee\n```\n跟如下是同样的道理:\n```\nxiaosi@yoona:~$ ll employee\n-rw-rw-r-- 1 xiaosi xiaosi 0 12月  6 16:56 employee\nxiaosi@yoona:~$\nxiaosi@yoona:~$\nxiaosi@yoona:~$ mkdir employee\nmkdir: 无法创建目录\"employee\": 文件已存在\n```\n借此机会，详细研究了一下`HDFS`的`Trash`回收站机制。\n\n### 1. 配置\n\n`HDFS`的回收站就像`Windows`操作系统中的回收站一样。它的目的是防止你无意中删除某些东西。你可以通过设置如下属性来启用此功能(默认是不开启的)：\n```\n<property>  \n    <name>fs.trash.interval</name>  \n    <value>1440</value>  \n    <description>Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.</description>  \n</property>  \n\n<property>  \n    <name>fs.trash.checkpoint.interval</name>  \n    <value>0</value>  \n    <description>Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.</description>  \n</property>\n```\n\n属性|说明\n---|---\nfs.trash.interval|分钟数，当超过这个分钟数后检查点会被删除。如果为零，回收站功能将被禁用。\nfs.trash.checkpoint.interval|检查点创建的时间间隔(单位为分钟)。其值应该小于或等于`fs.trash.interval`。如果为零，则将该值设置为`fs.trash.interval`的值。\n\n### 2. Trash\n\n启用回收站功能后，使用`rm`命令从`HDFS`中删除某些内容时，文件或目录不会立即被清除，它们将被移动到回收站`Current`目录中(`/user/${username}/.Trash/current`)。如果检查点已经启用，会定期使用时间戳重命名`Current`目录。`.Trash`中的文件在用户可配置的时间延迟后被永久删除。回收站中的文件和目录可以简单地通过将它们移动到`.Trash`目录之外的位置来恢复:\n```\nsudo -uxiaosi hadoop fs -rm tmp/data_group/test/employee/employee_salary.txt\n17/12/06 17:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt' to trash at: hdfs://cluster/user/xiaosi/.Trash/Current\n```\n说明:\n- `Deletion interval`表示检查点删除时间间隔(单位为分钟)。这里是`fs.trash.interval`的值。`NameNode`运行一个线程来定期从文件系统中删除过期的检查点。\n- `Emptier interval`表示在运行线程来管理检查点之前，`NameNode`需要等待多长时间(以分钟为单位)，即检查点创建时间间隔。`NameNode`删除超过`fs.trash.interval`的检查点，并为`/user/${username}/.Trash/Current`创建一个新的检查点。该频率由`fs.trash.checkpoint.interval`的值确定，且不得大于`Deletion interval`。这确保了在`emptier`窗口内回收站中有一个或多个检查点。\n\n例如，可以设置如下:\n```\nfs.trash.interval = 360 (deletion interval = 6 hours)\nfs.trash.checkpoint.interval = 60 (emptier interval = 1 hour)\n```\n这导致`NameNode`为`Current`目录下的垃圾文件每小时创建一个新的检查点，并删除已经存在超过6个小时的检查点。\n\n在回收站生命周期结束后，`NameNode`从`HDFS`命名空间中删除该文件。删除文件会导致与文件关联的块被释放。请注意，用户删除文件的时间与HDFS中相应增加可用空间的时间之间可能存在明显的时间延迟，即用户删除文件，HDFS可用空间不会立马增加，中间有一定的延迟。\n\n### 3. 检查点\n\n检查点仅仅是用户回收站下的一个目录，用于存储在创建检查点之前删除的所有文件或目录。如果你想查看回收站目录，可以在`/user/${username}/.Trash/{timestamp_of_checkpoint_creation}`处看到:\n```\nhadoop fs -ls hdfs://cluster/user/xiaosi/.Trash/\nFound 3 items\ndrwx------   - xiaosi xiaosi          0 2017-12-05 08:00 hdfs://cluster/user/xiaosi/.Trash/171205200038\ndrwx------   - xiaosi xiaosi          0 2017-12-06 01:00 hdfs://cluster/user/xiaosi/.Trash/171206080038\ndrwx------   - xiaosi xiaosi          0 2017-12-06 08:00 hdfs://cluster/user/xiaosi/.Trash/Current\n```\n最近删除的文件被移动到回收站`Current`目录，并且在可配置的时间间隔内，`HDFS`会为在`Current`回收站目录下的文件创建检查点`/user/${username}/.Trash/<日期>`，并在过期时删除旧的检查点。\n\n\n### 4. 清空回收站\n\n首先想到的是只要删除整个回收站目录，将会清空回收站。诚然，这是一个选择。但是我们有更好的选择。`HDFS`提供了一个命令行工具来完成这个工作：\n```\nhadoop fs -expunge\n```\n该命令使`NameNode`永久删除回收站中比阈值更早的文件，而不是等待下一个`emptier`窗口。它立即从文件系统中删除过期的检查点。\n\n### 5. 注意点\n\n回收站功能默认是禁用的。对于生产环境，建议启用回收站功能以避免意外的删除操作。启用回收站提供了从用户操作删除或用户意外删除中恢复数据的机会。但是为`fs.trash.interval`和`fs.trash.checkpoint.interval`设置合适的值也是非常重要的，以使垃圾回收以你期望的方式运作。例如，如果你需要经常从`HDFS`上传和删除文件，则可能需要将`fs.trash.interval`设置为较小的值，否则检查点将占用太多空间。\n\n当启用垃圾回收并删除一些文件时，`HDFS`容量不会增加，因为文件并未真正删除。`HDFS`不会回收空间，除非文件从回收站中删除，只有在检查点过期后才会发生。\n\n回收站功能默认只适用于使用`Hadoop shell`删除的文件和目录。使用其他接口(例如`WebHDFS`或`Java API`)以编程的方式删除的文件或目录不会移动到回收站，即使已启用回收站，除非程序已经实现了对回收站功能的调用。\n\n有时你可能想要在删除文件时临时禁用回收站，也就是删除的文件或目录不用放在回收站而直接删除，在这种情况下，可以使用`-skipTrash`选项运行`rm`命令。例如：\n```\nsudo -uxiaosi hadoop fs -rm -skipTrash tmp/data_group/test/employee/employee_salary.txt\n```\n这会绕过垃圾回收站并立即从文件系统中删除文件。\n\n\n\n资料:\n\nhttps://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/\n\nhttps://my.oschina.net/cloudcoder/blog/179381\n\nhttp://debugo.com/hdfs-trash/\n\nhttp://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes\n","source":"_posts/Hadoop/Hadoop Trash回收站使用指南.md","raw":"\n---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop Trash回收站使用指南\ndate: 2017-12-07 10:07:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n我们在删除一个文件时，遇到如下问题，提示我们不能删除文件放回回收站:\n```\nsudo -uxiaosi hadoop fs -rm -r tmp/data_group/test/employee/employee_salary.txt\n17/12/06 16:34:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.\n17/12/06 16:34:48 WARN fs.TrashPolicyDefault: Can't create trash directory: hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee\nrm: Failed to move to trash: hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt. Consider using -skipTrash option\n```\n\n去回收站对应目录下观察一下，得出的结论是：无法创建目录`employee`，因为`employee`文件已经存在，自然导致`employee_salary.txt`文件不能放回收回站:\n```\n-rw-r--r--   3 xiaosi xiaosi  352 2017-12-06 16:18 hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee\n```\n跟如下是同样的道理:\n```\nxiaosi@yoona:~$ ll employee\n-rw-rw-r-- 1 xiaosi xiaosi 0 12月  6 16:56 employee\nxiaosi@yoona:~$\nxiaosi@yoona:~$\nxiaosi@yoona:~$ mkdir employee\nmkdir: 无法创建目录\"employee\": 文件已存在\n```\n借此机会，详细研究了一下`HDFS`的`Trash`回收站机制。\n\n### 1. 配置\n\n`HDFS`的回收站就像`Windows`操作系统中的回收站一样。它的目的是防止你无意中删除某些东西。你可以通过设置如下属性来启用此功能(默认是不开启的)：\n```\n<property>  \n    <name>fs.trash.interval</name>  \n    <value>1440</value>  \n    <description>Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.</description>  \n</property>  \n\n<property>  \n    <name>fs.trash.checkpoint.interval</name>  \n    <value>0</value>  \n    <description>Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.</description>  \n</property>\n```\n\n属性|说明\n---|---\nfs.trash.interval|分钟数，当超过这个分钟数后检查点会被删除。如果为零，回收站功能将被禁用。\nfs.trash.checkpoint.interval|检查点创建的时间间隔(单位为分钟)。其值应该小于或等于`fs.trash.interval`。如果为零，则将该值设置为`fs.trash.interval`的值。\n\n### 2. Trash\n\n启用回收站功能后，使用`rm`命令从`HDFS`中删除某些内容时，文件或目录不会立即被清除，它们将被移动到回收站`Current`目录中(`/user/${username}/.Trash/current`)。如果检查点已经启用，会定期使用时间戳重命名`Current`目录。`.Trash`中的文件在用户可配置的时间延迟后被永久删除。回收站中的文件和目录可以简单地通过将它们移动到`.Trash`目录之外的位置来恢复:\n```\nsudo -uxiaosi hadoop fs -rm tmp/data_group/test/employee/employee_salary.txt\n17/12/06 17:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt' to trash at: hdfs://cluster/user/xiaosi/.Trash/Current\n```\n说明:\n- `Deletion interval`表示检查点删除时间间隔(单位为分钟)。这里是`fs.trash.interval`的值。`NameNode`运行一个线程来定期从文件系统中删除过期的检查点。\n- `Emptier interval`表示在运行线程来管理检查点之前，`NameNode`需要等待多长时间(以分钟为单位)，即检查点创建时间间隔。`NameNode`删除超过`fs.trash.interval`的检查点，并为`/user/${username}/.Trash/Current`创建一个新的检查点。该频率由`fs.trash.checkpoint.interval`的值确定，且不得大于`Deletion interval`。这确保了在`emptier`窗口内回收站中有一个或多个检查点。\n\n例如，可以设置如下:\n```\nfs.trash.interval = 360 (deletion interval = 6 hours)\nfs.trash.checkpoint.interval = 60 (emptier interval = 1 hour)\n```\n这导致`NameNode`为`Current`目录下的垃圾文件每小时创建一个新的检查点，并删除已经存在超过6个小时的检查点。\n\n在回收站生命周期结束后，`NameNode`从`HDFS`命名空间中删除该文件。删除文件会导致与文件关联的块被释放。请注意，用户删除文件的时间与HDFS中相应增加可用空间的时间之间可能存在明显的时间延迟，即用户删除文件，HDFS可用空间不会立马增加，中间有一定的延迟。\n\n### 3. 检查点\n\n检查点仅仅是用户回收站下的一个目录，用于存储在创建检查点之前删除的所有文件或目录。如果你想查看回收站目录，可以在`/user/${username}/.Trash/{timestamp_of_checkpoint_creation}`处看到:\n```\nhadoop fs -ls hdfs://cluster/user/xiaosi/.Trash/\nFound 3 items\ndrwx------   - xiaosi xiaosi          0 2017-12-05 08:00 hdfs://cluster/user/xiaosi/.Trash/171205200038\ndrwx------   - xiaosi xiaosi          0 2017-12-06 01:00 hdfs://cluster/user/xiaosi/.Trash/171206080038\ndrwx------   - xiaosi xiaosi          0 2017-12-06 08:00 hdfs://cluster/user/xiaosi/.Trash/Current\n```\n最近删除的文件被移动到回收站`Current`目录，并且在可配置的时间间隔内，`HDFS`会为在`Current`回收站目录下的文件创建检查点`/user/${username}/.Trash/<日期>`，并在过期时删除旧的检查点。\n\n\n### 4. 清空回收站\n\n首先想到的是只要删除整个回收站目录，将会清空回收站。诚然，这是一个选择。但是我们有更好的选择。`HDFS`提供了一个命令行工具来完成这个工作：\n```\nhadoop fs -expunge\n```\n该命令使`NameNode`永久删除回收站中比阈值更早的文件，而不是等待下一个`emptier`窗口。它立即从文件系统中删除过期的检查点。\n\n### 5. 注意点\n\n回收站功能默认是禁用的。对于生产环境，建议启用回收站功能以避免意外的删除操作。启用回收站提供了从用户操作删除或用户意外删除中恢复数据的机会。但是为`fs.trash.interval`和`fs.trash.checkpoint.interval`设置合适的值也是非常重要的，以使垃圾回收以你期望的方式运作。例如，如果你需要经常从`HDFS`上传和删除文件，则可能需要将`fs.trash.interval`设置为较小的值，否则检查点将占用太多空间。\n\n当启用垃圾回收并删除一些文件时，`HDFS`容量不会增加，因为文件并未真正删除。`HDFS`不会回收空间，除非文件从回收站中删除，只有在检查点过期后才会发生。\n\n回收站功能默认只适用于使用`Hadoop shell`删除的文件和目录。使用其他接口(例如`WebHDFS`或`Java API`)以编程的方式删除的文件或目录不会移动到回收站，即使已启用回收站，除非程序已经实现了对回收站功能的调用。\n\n有时你可能想要在删除文件时临时禁用回收站，也就是删除的文件或目录不用放在回收站而直接删除，在这种情况下，可以使用`-skipTrash`选项运行`rm`命令。例如：\n```\nsudo -uxiaosi hadoop fs -rm -skipTrash tmp/data_group/test/employee/employee_salary.txt\n```\n这会绕过垃圾回收站并立即从文件系统中删除文件。\n\n\n\n资料:\n\nhttps://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/\n\nhttps://my.oschina.net/cloudcoder/blog/179381\n\nhttp://debugo.com/hdfs-trash/\n\nhttp://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes\n","slug":"Hadoop/Hadoop Trash回收站使用指南","published":1,"updated":"2018-01-29T09:36:59.632Z","comments":1,"photos":[],"link":"","_id":"cje58tisv0039ordbguxkoksj","content":"<p>我们在删除一个文件时，遇到如下问题，提示我们不能删除文件放回回收站:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -rm -r tmp/data_group/test/employee/employee_salary.txt</span><br><span class=\"line\">17/12/06 16:34:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.</span><br><span class=\"line\">17/12/06 16:34:48 WARN fs.TrashPolicyDefault: Can&apos;t create trash directory: hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee</span><br><span class=\"line\">rm: Failed to move to trash: hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt. Consider using -skipTrash option</span><br></pre></td></tr></table></figure></p>\n<p>去回收站对应目录下观察一下，得出的结论是：无法创建目录<code>employee</code>，因为<code>employee</code>文件已经存在，自然导致<code>employee_salary.txt</code>文件不能放回收回站:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">-rw-r--r--   3 xiaosi xiaosi  352 2017-12-06 16:18 hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee</span><br></pre></td></tr></table></figure></p>\n<p>跟如下是同样的道理:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ ll employee</span><br><span class=\"line\">-rw-rw-r-- 1 xiaosi xiaosi 0 12月  6 16:56 employee</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ mkdir employee</span><br><span class=\"line\">mkdir: 无法创建目录&quot;employee&quot;: 文件已存在</span><br></pre></td></tr></table></figure></p>\n<p>借此机会，详细研究了一下<code>HDFS</code>的<code>Trash</code>回收站机制。</p>\n<h3 id=\"1-配置\"><a href=\"#1-配置\" class=\"headerlink\" title=\"1. 配置\"></a>1. 配置</h3><p><code>HDFS</code>的回收站就像<code>Windows</code>操作系统中的回收站一样。它的目的是防止你无意中删除某些东西。你可以通过设置如下属性来启用此功能(默认是不开启的)：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;  </span><br><span class=\"line\">    &lt;name&gt;fs.trash.interval&lt;/name&gt;  </span><br><span class=\"line\">    &lt;value&gt;1440&lt;/value&gt;  </span><br><span class=\"line\">    &lt;description&gt;Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.&lt;/description&gt;  </span><br><span class=\"line\">&lt;/property&gt;  </span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;  </span><br><span class=\"line\">    &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;  </span><br><span class=\"line\">    &lt;value&gt;0&lt;/value&gt;  </span><br><span class=\"line\">    &lt;description&gt;Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.&lt;/description&gt;  </span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>fs.trash.interval</td>\n<td>分钟数，当超过这个分钟数后检查点会被删除。如果为零，回收站功能将被禁用。</td>\n</tr>\n<tr>\n<td>fs.trash.checkpoint.interval</td>\n<td>检查点创建的时间间隔(单位为分钟)。其值应该小于或等于<code>fs.trash.interval</code>。如果为零，则将该值设置为<code>fs.trash.interval</code>的值。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-Trash\"><a href=\"#2-Trash\" class=\"headerlink\" title=\"2. Trash\"></a>2. Trash</h3><p>启用回收站功能后，使用<code>rm</code>命令从<code>HDFS</code>中删除某些内容时，文件或目录不会立即被清除，它们将被移动到回收站<code>Current</code>目录中(<code>/user/${username}/.Trash/current</code>)。如果检查点已经启用，会定期使用时间戳重命名<code>Current</code>目录。<code>.Trash</code>中的文件在用户可配置的时间延迟后被永久删除。回收站中的文件和目录可以简单地通过将它们移动到<code>.Trash</code>目录之外的位置来恢复:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -rm tmp/data_group/test/employee/employee_salary.txt</span><br><span class=\"line\">17/12/06 17:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.</span><br><span class=\"line\">Moved: &apos;hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt&apos; to trash at: hdfs://cluster/user/xiaosi/.Trash/Current</span><br></pre></td></tr></table></figure></p>\n<p>说明:</p>\n<ul>\n<li><code>Deletion interval</code>表示检查点删除时间间隔(单位为分钟)。这里是<code>fs.trash.interval</code>的值。<code>NameNode</code>运行一个线程来定期从文件系统中删除过期的检查点。</li>\n<li><code>Emptier interval</code>表示在运行线程来管理检查点之前，<code>NameNode</code>需要等待多长时间(以分钟为单位)，即检查点创建时间间隔。<code>NameNode</code>删除超过<code>fs.trash.interval</code>的检查点，并为<code>/user/${username}/.Trash/Current</code>创建一个新的检查点。该频率由<code>fs.trash.checkpoint.interval</code>的值确定，且不得大于<code>Deletion interval</code>。这确保了在<code>emptier</code>窗口内回收站中有一个或多个检查点。</li>\n</ul>\n<p>例如，可以设置如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">fs.trash.interval = 360 (deletion interval = 6 hours)</span><br><span class=\"line\">fs.trash.checkpoint.interval = 60 (emptier interval = 1 hour)</span><br></pre></td></tr></table></figure></p>\n<p>这导致<code>NameNode</code>为<code>Current</code>目录下的垃圾文件每小时创建一个新的检查点，并删除已经存在超过6个小时的检查点。</p>\n<p>在回收站生命周期结束后，<code>NameNode</code>从<code>HDFS</code>命名空间中删除该文件。删除文件会导致与文件关联的块被释放。请注意，用户删除文件的时间与HDFS中相应增加可用空间的时间之间可能存在明显的时间延迟，即用户删除文件，HDFS可用空间不会立马增加，中间有一定的延迟。</p>\n<h3 id=\"3-检查点\"><a href=\"#3-检查点\" class=\"headerlink\" title=\"3. 检查点\"></a>3. 检查点</h3><p>检查点仅仅是用户回收站下的一个目录，用于存储在创建检查点之前删除的所有文件或目录。如果你想查看回收站目录，可以在<code>/user/${username}/.Trash/{timestamp_of_checkpoint_creation}</code>处看到:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hadoop fs -ls hdfs://cluster/user/xiaosi/.Trash/</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwx------   - xiaosi xiaosi          0 2017-12-05 08:00 hdfs://cluster/user/xiaosi/.Trash/171205200038</span><br><span class=\"line\">drwx------   - xiaosi xiaosi          0 2017-12-06 01:00 hdfs://cluster/user/xiaosi/.Trash/171206080038</span><br><span class=\"line\">drwx------   - xiaosi xiaosi          0 2017-12-06 08:00 hdfs://cluster/user/xiaosi/.Trash/Current</span><br></pre></td></tr></table></figure></p>\n<p>最近删除的文件被移动到回收站<code>Current</code>目录，并且在可配置的时间间隔内，<code>HDFS</code>会为在<code>Current</code>回收站目录下的文件创建检查点<code>/user/${username}/.Trash/&lt;日期&gt;</code>，并在过期时删除旧的检查点。</p>\n<h3 id=\"4-清空回收站\"><a href=\"#4-清空回收站\" class=\"headerlink\" title=\"4. 清空回收站\"></a>4. 清空回收站</h3><p>首先想到的是只要删除整个回收站目录，将会清空回收站。诚然，这是一个选择。但是我们有更好的选择。<code>HDFS</code>提供了一个命令行工具来完成这个工作：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hadoop fs -expunge</span><br></pre></td></tr></table></figure></p>\n<p>该命令使<code>NameNode</code>永久删除回收站中比阈值更早的文件，而不是等待下一个<code>emptier</code>窗口。它立即从文件系统中删除过期的检查点。</p>\n<h3 id=\"5-注意点\"><a href=\"#5-注意点\" class=\"headerlink\" title=\"5. 注意点\"></a>5. 注意点</h3><p>回收站功能默认是禁用的。对于生产环境，建议启用回收站功能以避免意外的删除操作。启用回收站提供了从用户操作删除或用户意外删除中恢复数据的机会。但是为<code>fs.trash.interval</code>和<code>fs.trash.checkpoint.interval</code>设置合适的值也是非常重要的，以使垃圾回收以你期望的方式运作。例如，如果你需要经常从<code>HDFS</code>上传和删除文件，则可能需要将<code>fs.trash.interval</code>设置为较小的值，否则检查点将占用太多空间。</p>\n<p>当启用垃圾回收并删除一些文件时，<code>HDFS</code>容量不会增加，因为文件并未真正删除。<code>HDFS</code>不会回收空间，除非文件从回收站中删除，只有在检查点过期后才会发生。</p>\n<p>回收站功能默认只适用于使用<code>Hadoop shell</code>删除的文件和目录。使用其他接口(例如<code>WebHDFS</code>或<code>Java API</code>)以编程的方式删除的文件或目录不会移动到回收站，即使已启用回收站，除非程序已经实现了对回收站功能的调用。</p>\n<p>有时你可能想要在删除文件时临时禁用回收站，也就是删除的文件或目录不用放在回收站而直接删除，在这种情况下，可以使用<code>-skipTrash</code>选项运行<code>rm</code>命令。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -rm -skipTrash tmp/data_group/test/employee/employee_salary.txt</span><br></pre></td></tr></table></figure></p>\n<p>这会绕过垃圾回收站并立即从文件系统中删除文件。</p>\n<p>资料:</p>\n<p><a href=\"https://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/\" target=\"_blank\" rel=\"noopener\">https://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/</a></p>\n<p><a href=\"https://my.oschina.net/cloudcoder/blog/179381\" target=\"_blank\" rel=\"noopener\">https://my.oschina.net/cloudcoder/blog/179381</a></p>\n<p><a href=\"http://debugo.com/hdfs-trash/\" target=\"_blank\" rel=\"noopener\">http://debugo.com/hdfs-trash/</a></p>\n<p><a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>我们在删除一个文件时，遇到如下问题，提示我们不能删除文件放回回收站:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -rm -r tmp/data_group/test/employee/employee_salary.txt</span><br><span class=\"line\">17/12/06 16:34:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.</span><br><span class=\"line\">17/12/06 16:34:48 WARN fs.TrashPolicyDefault: Can&apos;t create trash directory: hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee</span><br><span class=\"line\">rm: Failed to move to trash: hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt. Consider using -skipTrash option</span><br></pre></td></tr></table></figure></p>\n<p>去回收站对应目录下观察一下，得出的结论是：无法创建目录<code>employee</code>，因为<code>employee</code>文件已经存在，自然导致<code>employee_salary.txt</code>文件不能放回收回站:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">-rw-r--r--   3 xiaosi xiaosi  352 2017-12-06 16:18 hdfs://cluster/user/xiaosi/.Trash/Current/user/xiaosi/tmp/data_group/test/employee</span><br></pre></td></tr></table></figure></p>\n<p>跟如下是同样的道理:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ ll employee</span><br><span class=\"line\">-rw-rw-r-- 1 xiaosi xiaosi 0 12月  6 16:56 employee</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ mkdir employee</span><br><span class=\"line\">mkdir: 无法创建目录&quot;employee&quot;: 文件已存在</span><br></pre></td></tr></table></figure></p>\n<p>借此机会，详细研究了一下<code>HDFS</code>的<code>Trash</code>回收站机制。</p>\n<h3 id=\"1-配置\"><a href=\"#1-配置\" class=\"headerlink\" title=\"1. 配置\"></a>1. 配置</h3><p><code>HDFS</code>的回收站就像<code>Windows</code>操作系统中的回收站一样。它的目的是防止你无意中删除某些东西。你可以通过设置如下属性来启用此功能(默认是不开启的)：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;  </span><br><span class=\"line\">    &lt;name&gt;fs.trash.interval&lt;/name&gt;  </span><br><span class=\"line\">    &lt;value&gt;1440&lt;/value&gt;  </span><br><span class=\"line\">    &lt;description&gt;Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.&lt;/description&gt;  </span><br><span class=\"line\">&lt;/property&gt;  </span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;  </span><br><span class=\"line\">    &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;  </span><br><span class=\"line\">    &lt;value&gt;0&lt;/value&gt;  </span><br><span class=\"line\">    &lt;description&gt;Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval.&lt;/description&gt;  </span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>fs.trash.interval</td>\n<td>分钟数，当超过这个分钟数后检查点会被删除。如果为零，回收站功能将被禁用。</td>\n</tr>\n<tr>\n<td>fs.trash.checkpoint.interval</td>\n<td>检查点创建的时间间隔(单位为分钟)。其值应该小于或等于<code>fs.trash.interval</code>。如果为零，则将该值设置为<code>fs.trash.interval</code>的值。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-Trash\"><a href=\"#2-Trash\" class=\"headerlink\" title=\"2. Trash\"></a>2. Trash</h3><p>启用回收站功能后，使用<code>rm</code>命令从<code>HDFS</code>中删除某些内容时，文件或目录不会立即被清除，它们将被移动到回收站<code>Current</code>目录中(<code>/user/${username}/.Trash/current</code>)。如果检查点已经启用，会定期使用时间戳重命名<code>Current</code>目录。<code>.Trash</code>中的文件在用户可配置的时间延迟后被永久删除。回收站中的文件和目录可以简单地通过将它们移动到<code>.Trash</code>目录之外的位置来恢复:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -rm tmp/data_group/test/employee/employee_salary.txt</span><br><span class=\"line\">17/12/06 17:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 72000000 minutes, Emptier interval = 0 minutes.</span><br><span class=\"line\">Moved: &apos;hdfs://cluster/user/xiaosi/tmp/data_group/test/employee/employee_salary.txt&apos; to trash at: hdfs://cluster/user/xiaosi/.Trash/Current</span><br></pre></td></tr></table></figure></p>\n<p>说明:</p>\n<ul>\n<li><code>Deletion interval</code>表示检查点删除时间间隔(单位为分钟)。这里是<code>fs.trash.interval</code>的值。<code>NameNode</code>运行一个线程来定期从文件系统中删除过期的检查点。</li>\n<li><code>Emptier interval</code>表示在运行线程来管理检查点之前，<code>NameNode</code>需要等待多长时间(以分钟为单位)，即检查点创建时间间隔。<code>NameNode</code>删除超过<code>fs.trash.interval</code>的检查点，并为<code>/user/${username}/.Trash/Current</code>创建一个新的检查点。该频率由<code>fs.trash.checkpoint.interval</code>的值确定，且不得大于<code>Deletion interval</code>。这确保了在<code>emptier</code>窗口内回收站中有一个或多个检查点。</li>\n</ul>\n<p>例如，可以设置如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">fs.trash.interval = 360 (deletion interval = 6 hours)</span><br><span class=\"line\">fs.trash.checkpoint.interval = 60 (emptier interval = 1 hour)</span><br></pre></td></tr></table></figure></p>\n<p>这导致<code>NameNode</code>为<code>Current</code>目录下的垃圾文件每小时创建一个新的检查点，并删除已经存在超过6个小时的检查点。</p>\n<p>在回收站生命周期结束后，<code>NameNode</code>从<code>HDFS</code>命名空间中删除该文件。删除文件会导致与文件关联的块被释放。请注意，用户删除文件的时间与HDFS中相应增加可用空间的时间之间可能存在明显的时间延迟，即用户删除文件，HDFS可用空间不会立马增加，中间有一定的延迟。</p>\n<h3 id=\"3-检查点\"><a href=\"#3-检查点\" class=\"headerlink\" title=\"3. 检查点\"></a>3. 检查点</h3><p>检查点仅仅是用户回收站下的一个目录，用于存储在创建检查点之前删除的所有文件或目录。如果你想查看回收站目录，可以在<code>/user/${username}/.Trash/{timestamp_of_checkpoint_creation}</code>处看到:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hadoop fs -ls hdfs://cluster/user/xiaosi/.Trash/</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwx------   - xiaosi xiaosi          0 2017-12-05 08:00 hdfs://cluster/user/xiaosi/.Trash/171205200038</span><br><span class=\"line\">drwx------   - xiaosi xiaosi          0 2017-12-06 01:00 hdfs://cluster/user/xiaosi/.Trash/171206080038</span><br><span class=\"line\">drwx------   - xiaosi xiaosi          0 2017-12-06 08:00 hdfs://cluster/user/xiaosi/.Trash/Current</span><br></pre></td></tr></table></figure></p>\n<p>最近删除的文件被移动到回收站<code>Current</code>目录，并且在可配置的时间间隔内，<code>HDFS</code>会为在<code>Current</code>回收站目录下的文件创建检查点<code>/user/${username}/.Trash/&lt;日期&gt;</code>，并在过期时删除旧的检查点。</p>\n<h3 id=\"4-清空回收站\"><a href=\"#4-清空回收站\" class=\"headerlink\" title=\"4. 清空回收站\"></a>4. 清空回收站</h3><p>首先想到的是只要删除整个回收站目录，将会清空回收站。诚然，这是一个选择。但是我们有更好的选择。<code>HDFS</code>提供了一个命令行工具来完成这个工作：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hadoop fs -expunge</span><br></pre></td></tr></table></figure></p>\n<p>该命令使<code>NameNode</code>永久删除回收站中比阈值更早的文件，而不是等待下一个<code>emptier</code>窗口。它立即从文件系统中删除过期的检查点。</p>\n<h3 id=\"5-注意点\"><a href=\"#5-注意点\" class=\"headerlink\" title=\"5. 注意点\"></a>5. 注意点</h3><p>回收站功能默认是禁用的。对于生产环境，建议启用回收站功能以避免意外的删除操作。启用回收站提供了从用户操作删除或用户意外删除中恢复数据的机会。但是为<code>fs.trash.interval</code>和<code>fs.trash.checkpoint.interval</code>设置合适的值也是非常重要的，以使垃圾回收以你期望的方式运作。例如，如果你需要经常从<code>HDFS</code>上传和删除文件，则可能需要将<code>fs.trash.interval</code>设置为较小的值，否则检查点将占用太多空间。</p>\n<p>当启用垃圾回收并删除一些文件时，<code>HDFS</code>容量不会增加，因为文件并未真正删除。<code>HDFS</code>不会回收空间，除非文件从回收站中删除，只有在检查点过期后才会发生。</p>\n<p>回收站功能默认只适用于使用<code>Hadoop shell</code>删除的文件和目录。使用其他接口(例如<code>WebHDFS</code>或<code>Java API</code>)以编程的方式删除的文件或目录不会移动到回收站，即使已启用回收站，除非程序已经实现了对回收站功能的调用。</p>\n<p>有时你可能想要在删除文件时临时禁用回收站，也就是删除的文件或目录不用放在回收站而直接删除，在这种情况下，可以使用<code>-skipTrash</code>选项运行<code>rm</code>命令。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -rm -skipTrash tmp/data_group/test/employee/employee_salary.txt</span><br></pre></td></tr></table></figure></p>\n<p>这会绕过垃圾回收站并立即从文件系统中删除文件。</p>\n<p>资料:</p>\n<p><a href=\"https://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/\" target=\"_blank\" rel=\"noopener\">https://developer.ibm.com/hadoop/2015/10/22/hdfs-trash/</a></p>\n<p><a href=\"https://my.oschina.net/cloudcoder/blog/179381\" target=\"_blank\" rel=\"noopener\">https://my.oschina.net/cloudcoder/blog/179381</a></p>\n<p><a href=\"http://debugo.com/hdfs-trash/\" target=\"_blank\" rel=\"noopener\">http://debugo.com/hdfs-trash/</a></p>\n<p><a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#File_Deletes_and_Undeletes</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop2.x HDFS架构","date":"2017-12-20T12:29:01.000Z","_content":"\n### 1. 概述\n\nHadoop分布式文件系统(`HDFS`)是一个分布式文件系统，设计初衷是可以在商用硬件上运行。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的也有显著的差异。`HDFS`具有高容错能力，可以部署在低成本的硬件上。`HDFS`提供对应用程序数据的高吞吐量访问，适用于具有大数据集的应用程序。`HDFS`放宽了一些POSIX要求，以便对文件系统数据进行流式访问。`HDFS`最初是作为`Apache Nutch`网络搜索引擎项目的基础架构构建的。`HDFS`是`Apache Hadoop Core`项目的一部分。项目URL为: http://hadoop.apache.org/\n\n### 2. 设想与目标\n\n#### 2.1 硬件故障\n\n 硬件故障很常见不要感到意外。`HDFS`实例可能由成百上千台服务器机器组成，每台机器存储部分文件系统的数据。事实上，有大量的组件，并且每个组件具有不一定的故障概率，这意味着可能`HDFS`的某些组件总是不起作用的。因此，故障检测和快速自动恢复是`HDFS`的核心架构。\n\n#### 2.2 流式数据访问\n\n运行在`HDFS`上的应用程序需要流式访问其数据集。`HDFS`不是运行在通用文件系统上通用应用程序。`HDFS`设计是为了更多的批量处理，而不是与用户进行交互。重点是数据访问的高吞吐量，而不是数据访问的低延迟。\n\n#### 2.3 大数据集\n\n运行在`HDFS`上的应用程序具有较大的数据集。`HDFS`中的文件大小一般为几GB或几TB。因此，`HDFS`需要支持大文件。它需要提供高数据聚合带宽并可以在单个集群中扩展到的数百个节点。它需要在一个实例中支持数千万个文件。\n\n#### 2.4 简单一致性模型\n\n`HDFS`数据访问模式为一次写入多次读取。文件一旦创建、写入和关闭后，除了追加和截断外，文件不能更改。可以支持将内容追加到文件末尾，但不能在随意位置更新文件内容。该假设简化了数据一致性问题，并实现了数据访问的高吞吐量。`MapReduce`应用程序或Web爬虫程序应用程序与此模型完美匹配。\n\n#### 2.5 '移动计算比移动数据便宜'\n\n如果应用程序能够在其操作的数据附近执行，那么应用程序所请求的计算效率会更高一些。当数据集很大时，这一点更能体现。这样可以最大限度地减少网络拥塞并提高系统的整体吞吐量。我们假设将计算迁移到更靠近数据的位置比将数据转移到应用程序运行的位置更好。`HDFS`为应用程序提供接口，使其更靠近数据所在的位置。\n\n#### 2.6 跨越异构硬件和软件平台的可移植性\n\n`HDFS`被设计为可以从一个平台轻松地移植到另一个平台。这有助于`HDFS`作为大型应用程序的首选平台。\n\n### 3. NameNode and DataNodes\n\n`HDFS`是一个主/从结构。一个`HDFS`集群包含一个`NameNode`，管理文件系统命名空间以及管理客户端对文件访问的主服务。除此之外，还有一些`DataNode`，通常集群中的每个节点都有一个`DataNode`，用于管理它们所运行节点相关的存储。`HDFS`公开文件系统命名空间，并允许用户数据存储在文件中。在内部，一个文件被分成一个或多个数据块，这些数据块被存储在一组`DataNode`中。`NameNode`执行文件系统命名空间操作，例如打开，关闭和重命名文件和目录等。它也决定数据块到`DataNode`的映射。`DataNode`负责为文件系统客户端的读写请求提供服务。`DataNode`还根据来自`NameNode`的指令执行数据块的创建，删除和复制。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-1.png?raw=true)\n\n`NameNode`和`DataNode`是设计用于在商业机器上运行的软件。这些机器通常运行GNU/Linux操作系统(OS)。`HDFS`是使用`Java`语言构建的; 任何支持`Java`的机器都可以运行`NameNode`或`DataNode`。使用高可移植性的`Java`语言意味着`HDFS`可以部署在各种机器上。一个典型的部署是有一台专用机器来运行`NameNode`。集群中的其他机器运行`DataNode`实例。该体系结构并不排除在同一台计算机上运行多个`DataNode`，但在实际部署中很少出现这种情况。\n\n集群中`NameNode`的存在大大简化了系统的体系结构。`NameNode`是所有`HDFS`元数据的决策者和存储仓库。系统的这种设计方式可以允许用户数据不会经过`NameNode`，直接与`DataNode`进行连接。\n\n### 4. 文件系统命名空间\n\n`HDFS`支持传统的分层文件组织方式。用户或应用程序可以创建目录以及在这些目录内存储文件。文件系统命名空间层次结构与大多数其他文件系统类似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。`HDFS`支持用户配额和访问权限。`HDFS`不支持硬链接或软链接。但是，`HDFS`体系结构并不排除实现这些功能。\n\n`NameNode`维护文件系统的命名空间。对文件系统命名空间或其属性的任何更改都会在`NameNode`中记录。应用程序可以指定`HDFS`应该维护的文件的副本数量。文件的副本数称为该文件的复制因子。这个信息由`NameNode`存储。\n\n### 5. 数据复制\n\n`HDFS`旨在大型集群多台机器上可靠地存储非常大的文件。将每个文件存储为一系列的数据块。文件的数据块被复制多份以实现容错。数据块大小和副本因子是可以通过配置文件进行配置。\n\n一个文件的数据块除最后一个块以外的所有其他块的大小都相同，在添加对可变长度块和`hsync`的支持后，用户可以不用填充最后一个块到配置大小而启动一个新块。\n\n应用程序可以指定文件的副本数量。复制因子可以在文件创建时指定，也可以在以后更改。`HDFS`中的文件是一次性编写的(追加和截断除外)，并且严格限定在任何时候都只能有一个编写者。\n\n`NameNode`做出关于块复制的所有决定。它周期性的从集群中的每个`DataNode`接收`Heartbeat`和`Blockreport`。收到`Heartbeat`意味着`DataNode`运行正常。`Blockreport`包含`DataNode`上所有块的列表。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-2.png?raw=true)\n\n#### 5.1 副本安置\n\n副本的放置对`HDFS`的可靠性和性能至关重要。优化副本放置能将`HDFS`与大多数其他分布式文件系统区分开来。这是一个需要大量调整和体验的功能。机架感知副本放置策略的目的是提高数据可靠性，可用性和网络带宽利用率。副本放置策略的目前实现是朝这个方向迈进的第一步。实施这一策略的短期目标是在生产环境上进行验证，更多地了解其行为，并为测试和研究更复杂的策略奠定基础。\n\n大型`HDFS`实例运行在通常分布在多个机架上的一组计算机上。不同机架中的两个节点之间的通信必须经过交换机。在大多数情况下，同一机架中的机器之间的网络带宽大于不同机架中的机器之间的网络带宽。\n\n`NameNode`通过`Hadoop`机架感知中概述的过程确定每个`DataNode`所属的机架Id。一个简单但不是最佳的策略是将副本放在不同的机架上。这可以防止整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在集群中均匀分配副本，以便轻松平衡组件故障的负载(This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure)。但是，此策略会增加写入成本，因为写入需要将数据块传输到多个机架。\n\n正常情况下，当复制因子为3时，`HDFS`的放置策略是将一个副本放在本地机架的同一个节点上，另一个放在本地机架的不同节点上，最后放在另一个机架的不同节点上。这个政策降低了机架间写入流量，这通常会提高写入性能。机架故障的几率远远小于节点故障的几率;此策略不会影响数据可靠性和可用性的保证。但是，它降低了读取数据时使用的总体网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本在同一个节点上，三分之二的副本在同一个机架上，另外三分之一在其它机架上均匀分布。此策略可提高写入性能，而不会影响数据可靠性或读取性能。\n\n这里描述的就是当前默认副本放置策略。\n\n#### 5.2 副本选择\n\n为了尽量减少全局带宽消耗和读取延迟，`HDFS`会尝试将读取请求发送到离读取者最近的副本上(HDFS tries to satisfy a read request from a replica that is closest to the reader.)。 如果在与读取者节点相同的机架上存在副本，则该副本优选满足读取请求。如果`HDFS`进群跨越多个数据中心，则保存在本地数据中心的副本优先于任何远程副本。\n\n#### 5.3 安全模式\n\n在启动时，`NameNode`进入一个称为`Safemode`(安全模式)的特殊状态。当`NameNode`处于安全模式状态时，不会发生数据块的复制。`NameNode`接收来自`DataNode`的`Heartbeat`和`Blockreport`消息。`Blockreport`包含`DataNode`托管的数据块列表。每个块都有指定的最小数量的副本。当该数据块的最小副本数与`NameNode`签入时，将认为该块被安全地复制。在安全复制数据块的可配置百分比检入`NameNode`（再加上30秒）之后，`NameNode`退出安全模式状态。然后确定仍然少于指定副本数量的数据块列表（如果有的话）。`NameNode`然后将这些块复制到其他`DataNode`。\n\n### 6. 文件系统元数据持久化\n\n`HDFS`命名空间存储在`NameNode`中。`NameNode`使用称之为`EditLog`编辑日志的事务日志来持久化存储在文件系统元数据上发生的每一个变化。例如，在`HDFS`中创建一个新文件会导致`NameNode`向`EditLog`编辑日志中插入一条记录。同样，更改文件的复制因子也会导致将新记录插入到`EditLog`编辑日志中。`NameNode`使用其本地主机OS文件系统中的文件来存储`EditLog`编辑日志。整个文件系统命名空间，包括数据块到文件的映射以及文件系统属性，都存储在一个名为`FsImage`的文件中。`FsImage`作为文件存储在`NameNode`的本地文件系统中。\n\n`NameNode`将整个文件系统命名空间和文件`Blockmap`的快照(image)保存在内存中。这个关键的元数据被设计得很紧凑，这样一个具有4GB内存的`NameNode`足以支持大量的文件和目录。当`NameNode`启动时，它会从磁盘中读取`FsImage`和`EditLog`编辑日志，将`EditLog`编辑日志中的所有事务应用到内存中的`FsImage`(applies all the transactions from the EditLog to the in-memory representation of the FsImage)，并将这个新版本刷新到磁盘上生成一个新`FsImage`。它可以截断旧的`EditLog`编辑日志，因为它的事务已经被应用到持久化的`FsImage`上。这个过程被称为检查点。在目前的实现中，只有在`NameNode`启动时才会出现检查点。在未来版本中正在进行工作的`NameNode`也会支持周期性的检查点。\n\n`DataNode`将`HDFS`数据存储在本地文件系统的文件中。`DataNode`不了解`HDFS`文件(The DataNode has no knowledge about HDFS files)。它将每个`HDFS`数据块存储在本地文件系统中的单个文件中。`DataNode`不会在同一目录中创建所有文件。相反，它使用启发式来确定每个目录的最佳文件数量并适当地创建子目录。由于本地文件系统可能无法有效地支持单个目录中的大量文件，因此在同一目录中创建所有本地文件并不是最佳选择。当`DataNode`启动时，它会扫描其本地文件系统，生成一个包含所有`HDFS`数据块(与每个本地文件相对应)的列表，并将此报告发送给`NameNode`：这是`Blockreport`。\n\n### 7. 通信协议\n\n所有的`HDFS`通信协议都是基于`TCP/IP`协议的。客户端建立到`NameNode`机器上的可配置TCP端口的连接。它使用`ClientProtocol`与`NameNode`交谈。`DataNode`使用`DataNode`协议与`NameNode`进行通信。远程过程调用(RPC)抽象包装客户端协议和数据节点协议。根据设计，`NameNode`永远不会启动任何RPC。而是只响应由`DataNode`或客户端发出的RPC请求。\n\n### 8. 稳定性\n\n`HDFS`的主要目标是即使在出现故障时也能可靠地存储数据。三种常见的故障类型是`NameNode`故障，`DataNode`故障和网络分裂(network partitions)。\n\n#### 8.1 数据磁盘故障，心跳和重新复制\n\n每个`DataNode`定期向`NameNode`发送一个`Heartbeat`消息。网络分裂可能导致一组`DataNode`与`NameNode`失去联系。`NameNode`通过丢失`Heartbeat`消息来检测这种情况。`NameNode`将最近没有`Heartbeats`的`DataNode`标记为死亡，并且不会将任何新的IO请求转发给它们。任何注册在标记为死亡的`DataNode`中的数据不再可用。`DataNode`死亡可能导致某些块的复制因子降到其指定值以下。`NameNode`不断跟踪哪些块需要复制，并在需要时启动复制。重新复制可能由于许多原因而产生：`DataNode`可能变得不可用，副本可能被破坏，`DataNode`上的硬盘可能出现故障，或者文件的复制因子可能需要增加。\n\n为了避免由于`DataNode`的状态震荡而导致的复制风暴，标记`DataNode`死亡的超时时间设置的比较保守(The time-out to mark DataNodes dead is conservatively long)(默认超过10分钟)。用户可以设置较短的时间间隔以将`DataNode`标记为陈旧，并避免陈旧节点在读取或按配置写入时性能出现负载(Users can set shorter interval to mark DataNodes as stale and avoid stale nodes on reading and/or writing by configuration for performance sensitive workloads)。\n\n#### 8.2 集群重新平衡\n\n`HDFS`体系结构与数据重新平衡方案兼容。如果某个`DataNode`上的可用空间低于某个阈值，那么会自动将数据从一个`DataNode`移动到另一个`DataNode`。对于特定文件突然高需求(sudden high demand)的情况下，可能会动态创建额外的副本并重新平衡集群中的其他数据。这些类型的数据重新平衡方案尚未实现。\n\n#### 8.3 数据完整性\n\n从`DataNode`上获取的数据块可能会损坏。发生损坏可能是由存储设备故障，网络故障或软件错误引起。`HDFS`客户端实现了对`HDFS`上文件内容进行校验和检查。当客户端创建一个`HDFS`文件时，它会计算每个文件的对应数据块的校验和，并将这些校验和存储在同一个`HDFS`命名空间中的单独隐藏文件中。当客户端检索文件内容时，它会验证从每个`DataNode`收到的数据是否与存储在相关校验和文件中的校验和相匹配。如果不匹配，那么客户端可以选择从另一个具有该数据块副本的`DataNode`中检索该数据块。\n\n#### 8.4 元数据磁盘故障\n\n`FsImage`和`EditLog`编辑日志是`HDFS`中的中心数据结构。这些文件的损坏可能会导致`HDFS`实例无法正常运行。为此，`NameNode`可以配置为支持维护`FsImage`和`EditLog`编辑日志的多个副本。任何对`FsImage`或`EditLog`编辑日志的更新都会引起每个`FsImages`和`EditLogs`编辑日志同步更新。同步更新`FsImage`和`EditLog`编辑日志的多个副本可能会降低`NameNode`支持的每秒的命名空间事务的速度(degrade the rate of namespace transactions per second)。但是，这种降低是可以接受的，因为尽管`HDFS`应用程序实质上是非常密集的数据，但是它们也不是元数据密集型的。当`NameNode`重新启动时，它会选择最新的一致的`FsImage`和`EditLog`编辑日志来使用。\n\n另一个增强防御故障的方法是使用多个`NameNode`以启用高可用性，或者使用[`NFS`上的共享存储](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html)或使用[分布式编辑日志](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)(称为`Journal`)。后者是推荐的方法。\n\n#### 8.5 快照\n\n[快照](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html)支持在特定时刻存储数据副本。快照功能的一种用法是将损坏的`HDFS`实例回滚到先前已知的良好时间点。\n\n### 9. 数据组织\n\n#### 9.1 数据块\n\n`HDFS`为支持大文件而设计的。与`HDFS`兼容的应用程序是处理大型数据集的应用程序。这些应用程序只写入数据一次，但是读取一次或多次，并读取速度要求满足流式处理速度。`HDFS`支持在文件上一次写入多次读取语义。`HDFS`使用的一般块大小为128 MB。因此，一个`HDFS`文件被分成多个128MB的块，如果可能的话，每个块将保存在不同的`DataNode`上。\n\n#### 9.2 分阶段\n\n客户端创建文件的请求不会立即到达`NameNode`。事实上，最初`HDFS`客户端将文件数据缓存到本地缓冲区。应用程序写入重定向到本地缓冲区。当本地文件累积超过一个块大小的数据时，客户端才会联系`NameNode`。`NameNode`将文件名插入到文件系统层次结构中，并为其分配一个数据块。`NameNode`将`DataNode`和目标数据块的标识和返回给客户请求。然后，客户端将本地缓冲区中的数据块保存到指定的`DataNode`上。当文件关闭时，本地缓冲区中剩余的未保存数据也被传输到`DataNode`。客户端然后告诉`NameNode`该文件已关闭。此时，`NameNode`将文件创建操作提交到持久化存储中。如果`NameNode`在文件关闭之前崩溃，那么文件会丢失。\n\n在仔细考虑在`HDFS`上运行的目标应用程序之后，采用了上述方法。这些应用程序需要流式写入文件。如果客户端直接写入远程文件目录而没有在客户端进行任何缓冲，那么网络速度和网络拥塞会大大影响吞吐量。这种方法并非没有先例。较早的分布式文件系统，例如`AFS`，已经使用客户端缓存来提高性能。`POSIX`的要求已经放宽，以实现更高的数据传输性能。\n\n### 9.3 副本流水线\n\n当客户端将数据写入`HDFS`文件时，首先将数据写入本地缓冲区，如上一节所述。假设`HDFS`文件复制因子为3。当本地缓冲区累积了一个块的用户数据时，客户端从`NameNode`中检索`DataNode`列表。该列表包含保存数据的数据块副本的`DataNode`。客户端然后将数据块刷新到第一个`DataNode`。第一个`DataNode`开始接收一小部分数据，将这一小部分数据写入其本地存储库，然后传输到列表中的第二个`DataNode`。第二个`DataNode`依次接收数据块的每一部分数据，将其写入存储库，然后再将刷新到第三个`DataNode`。最后，第三个`DataNode`将数据写入其本地存储库。因此，`DataNode`可以以流水线的方式从前一个`DataNode`接收数据，同时将数据转发到流水线中的下一个`DataNode`。因此，数据从一个`DataNode`流到下一个。\n\n### 10. 访问\n\n应用程序可以以多种不同的方式访问`HDFS`。`HDFS`为应用程序提供了一个`FileSystem Java API`。`Java API`和`REST API`的C语言包装器也可以使用。另外还有一个HTTP浏览器(HTTP browser)，也可以用来浏览`HDFS`实例的文件。通过使用`NFS`网关，可以将`HDFS`作为客户端本地文件系统的一部分。\n\n#### 10.1 FS Shell\n\n`HDFS`将用户数据以文件和目录的形式进行组织。它提供了一个名为`FS shell`的命令行接口，让用户可以与`HDFS`中的数据进行交互。这个命令集的语法类似于用户已经熟悉的其他`shell`(例如`bash`，`csh`)。 以下是一些示例操作/命令对：\n\n操作|命令\n---|---\n创建`/foodir`目录|`bin/hadoop dfs -mkdir /foodir`\n删除目录`/foodir`|`bin/hadoop fs -rm -R /foodir`\n查看`/foodir/myfile.txt`中内容|`bin/hadoop dfs -cat /foodir/myfile.txt`\n\n`FS shell`针对需要脚本语言与存储数据进行交互的应用程序。\n\n#### 10.2 DFSAdmin\n\n`DFSAdmin`命令集用于管理`HDFS`集群。这些是仅能由`HDFS`管理员使用的命令。以下是一些示例操作/命令对：\n\n操作|命令\n---|---\n使集群处于安全模式|`bin/hdfs dfsadmin -safemode enter`\n生成`DataNode`列表|`bin/hdfs dfsadmin -report`\n重新投放或停用`DataNode(s)`|`bin/hdfs dfsadmin -refreshNodes`\n\n#### 10.3 浏览器接口\n\n一个典型的`HDFS`安装会配置一个`Web`服务器，通过一个可配置的`TCP`端口公开`HDFS`命名空间。这允许用户使用Web浏览器浏览`HDFS`命名空间并查看其文件的内容。\n\n\n备注:\n```\nHadoop版本: 2.7.3\n```\n\n原文:http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\n","source":"_posts/Hadoop/Hadoop2.x HDFS架构.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop2.x HDFS架构\ndate: 2017-12-20 20:29:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. 概述\n\nHadoop分布式文件系统(`HDFS`)是一个分布式文件系统，设计初衷是可以在商用硬件上运行。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的也有显著的差异。`HDFS`具有高容错能力，可以部署在低成本的硬件上。`HDFS`提供对应用程序数据的高吞吐量访问，适用于具有大数据集的应用程序。`HDFS`放宽了一些POSIX要求，以便对文件系统数据进行流式访问。`HDFS`最初是作为`Apache Nutch`网络搜索引擎项目的基础架构构建的。`HDFS`是`Apache Hadoop Core`项目的一部分。项目URL为: http://hadoop.apache.org/\n\n### 2. 设想与目标\n\n#### 2.1 硬件故障\n\n 硬件故障很常见不要感到意外。`HDFS`实例可能由成百上千台服务器机器组成，每台机器存储部分文件系统的数据。事实上，有大量的组件，并且每个组件具有不一定的故障概率，这意味着可能`HDFS`的某些组件总是不起作用的。因此，故障检测和快速自动恢复是`HDFS`的核心架构。\n\n#### 2.2 流式数据访问\n\n运行在`HDFS`上的应用程序需要流式访问其数据集。`HDFS`不是运行在通用文件系统上通用应用程序。`HDFS`设计是为了更多的批量处理，而不是与用户进行交互。重点是数据访问的高吞吐量，而不是数据访问的低延迟。\n\n#### 2.3 大数据集\n\n运行在`HDFS`上的应用程序具有较大的数据集。`HDFS`中的文件大小一般为几GB或几TB。因此，`HDFS`需要支持大文件。它需要提供高数据聚合带宽并可以在单个集群中扩展到的数百个节点。它需要在一个实例中支持数千万个文件。\n\n#### 2.4 简单一致性模型\n\n`HDFS`数据访问模式为一次写入多次读取。文件一旦创建、写入和关闭后，除了追加和截断外，文件不能更改。可以支持将内容追加到文件末尾，但不能在随意位置更新文件内容。该假设简化了数据一致性问题，并实现了数据访问的高吞吐量。`MapReduce`应用程序或Web爬虫程序应用程序与此模型完美匹配。\n\n#### 2.5 '移动计算比移动数据便宜'\n\n如果应用程序能够在其操作的数据附近执行，那么应用程序所请求的计算效率会更高一些。当数据集很大时，这一点更能体现。这样可以最大限度地减少网络拥塞并提高系统的整体吞吐量。我们假设将计算迁移到更靠近数据的位置比将数据转移到应用程序运行的位置更好。`HDFS`为应用程序提供接口，使其更靠近数据所在的位置。\n\n#### 2.6 跨越异构硬件和软件平台的可移植性\n\n`HDFS`被设计为可以从一个平台轻松地移植到另一个平台。这有助于`HDFS`作为大型应用程序的首选平台。\n\n### 3. NameNode and DataNodes\n\n`HDFS`是一个主/从结构。一个`HDFS`集群包含一个`NameNode`，管理文件系统命名空间以及管理客户端对文件访问的主服务。除此之外，还有一些`DataNode`，通常集群中的每个节点都有一个`DataNode`，用于管理它们所运行节点相关的存储。`HDFS`公开文件系统命名空间，并允许用户数据存储在文件中。在内部，一个文件被分成一个或多个数据块，这些数据块被存储在一组`DataNode`中。`NameNode`执行文件系统命名空间操作，例如打开，关闭和重命名文件和目录等。它也决定数据块到`DataNode`的映射。`DataNode`负责为文件系统客户端的读写请求提供服务。`DataNode`还根据来自`NameNode`的指令执行数据块的创建，删除和复制。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-1.png?raw=true)\n\n`NameNode`和`DataNode`是设计用于在商业机器上运行的软件。这些机器通常运行GNU/Linux操作系统(OS)。`HDFS`是使用`Java`语言构建的; 任何支持`Java`的机器都可以运行`NameNode`或`DataNode`。使用高可移植性的`Java`语言意味着`HDFS`可以部署在各种机器上。一个典型的部署是有一台专用机器来运行`NameNode`。集群中的其他机器运行`DataNode`实例。该体系结构并不排除在同一台计算机上运行多个`DataNode`，但在实际部署中很少出现这种情况。\n\n集群中`NameNode`的存在大大简化了系统的体系结构。`NameNode`是所有`HDFS`元数据的决策者和存储仓库。系统的这种设计方式可以允许用户数据不会经过`NameNode`，直接与`DataNode`进行连接。\n\n### 4. 文件系统命名空间\n\n`HDFS`支持传统的分层文件组织方式。用户或应用程序可以创建目录以及在这些目录内存储文件。文件系统命名空间层次结构与大多数其他文件系统类似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。`HDFS`支持用户配额和访问权限。`HDFS`不支持硬链接或软链接。但是，`HDFS`体系结构并不排除实现这些功能。\n\n`NameNode`维护文件系统的命名空间。对文件系统命名空间或其属性的任何更改都会在`NameNode`中记录。应用程序可以指定`HDFS`应该维护的文件的副本数量。文件的副本数称为该文件的复制因子。这个信息由`NameNode`存储。\n\n### 5. 数据复制\n\n`HDFS`旨在大型集群多台机器上可靠地存储非常大的文件。将每个文件存储为一系列的数据块。文件的数据块被复制多份以实现容错。数据块大小和副本因子是可以通过配置文件进行配置。\n\n一个文件的数据块除最后一个块以外的所有其他块的大小都相同，在添加对可变长度块和`hsync`的支持后，用户可以不用填充最后一个块到配置大小而启动一个新块。\n\n应用程序可以指定文件的副本数量。复制因子可以在文件创建时指定，也可以在以后更改。`HDFS`中的文件是一次性编写的(追加和截断除外)，并且严格限定在任何时候都只能有一个编写者。\n\n`NameNode`做出关于块复制的所有决定。它周期性的从集群中的每个`DataNode`接收`Heartbeat`和`Blockreport`。收到`Heartbeat`意味着`DataNode`运行正常。`Blockreport`包含`DataNode`上所有块的列表。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-2.png?raw=true)\n\n#### 5.1 副本安置\n\n副本的放置对`HDFS`的可靠性和性能至关重要。优化副本放置能将`HDFS`与大多数其他分布式文件系统区分开来。这是一个需要大量调整和体验的功能。机架感知副本放置策略的目的是提高数据可靠性，可用性和网络带宽利用率。副本放置策略的目前实现是朝这个方向迈进的第一步。实施这一策略的短期目标是在生产环境上进行验证，更多地了解其行为，并为测试和研究更复杂的策略奠定基础。\n\n大型`HDFS`实例运行在通常分布在多个机架上的一组计算机上。不同机架中的两个节点之间的通信必须经过交换机。在大多数情况下，同一机架中的机器之间的网络带宽大于不同机架中的机器之间的网络带宽。\n\n`NameNode`通过`Hadoop`机架感知中概述的过程确定每个`DataNode`所属的机架Id。一个简单但不是最佳的策略是将副本放在不同的机架上。这可以防止整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在集群中均匀分配副本，以便轻松平衡组件故障的负载(This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure)。但是，此策略会增加写入成本，因为写入需要将数据块传输到多个机架。\n\n正常情况下，当复制因子为3时，`HDFS`的放置策略是将一个副本放在本地机架的同一个节点上，另一个放在本地机架的不同节点上，最后放在另一个机架的不同节点上。这个政策降低了机架间写入流量，这通常会提高写入性能。机架故障的几率远远小于节点故障的几率;此策略不会影响数据可靠性和可用性的保证。但是，它降低了读取数据时使用的总体网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本在同一个节点上，三分之二的副本在同一个机架上，另外三分之一在其它机架上均匀分布。此策略可提高写入性能，而不会影响数据可靠性或读取性能。\n\n这里描述的就是当前默认副本放置策略。\n\n#### 5.2 副本选择\n\n为了尽量减少全局带宽消耗和读取延迟，`HDFS`会尝试将读取请求发送到离读取者最近的副本上(HDFS tries to satisfy a read request from a replica that is closest to the reader.)。 如果在与读取者节点相同的机架上存在副本，则该副本优选满足读取请求。如果`HDFS`进群跨越多个数据中心，则保存在本地数据中心的副本优先于任何远程副本。\n\n#### 5.3 安全模式\n\n在启动时，`NameNode`进入一个称为`Safemode`(安全模式)的特殊状态。当`NameNode`处于安全模式状态时，不会发生数据块的复制。`NameNode`接收来自`DataNode`的`Heartbeat`和`Blockreport`消息。`Blockreport`包含`DataNode`托管的数据块列表。每个块都有指定的最小数量的副本。当该数据块的最小副本数与`NameNode`签入时，将认为该块被安全地复制。在安全复制数据块的可配置百分比检入`NameNode`（再加上30秒）之后，`NameNode`退出安全模式状态。然后确定仍然少于指定副本数量的数据块列表（如果有的话）。`NameNode`然后将这些块复制到其他`DataNode`。\n\n### 6. 文件系统元数据持久化\n\n`HDFS`命名空间存储在`NameNode`中。`NameNode`使用称之为`EditLog`编辑日志的事务日志来持久化存储在文件系统元数据上发生的每一个变化。例如，在`HDFS`中创建一个新文件会导致`NameNode`向`EditLog`编辑日志中插入一条记录。同样，更改文件的复制因子也会导致将新记录插入到`EditLog`编辑日志中。`NameNode`使用其本地主机OS文件系统中的文件来存储`EditLog`编辑日志。整个文件系统命名空间，包括数据块到文件的映射以及文件系统属性，都存储在一个名为`FsImage`的文件中。`FsImage`作为文件存储在`NameNode`的本地文件系统中。\n\n`NameNode`将整个文件系统命名空间和文件`Blockmap`的快照(image)保存在内存中。这个关键的元数据被设计得很紧凑，这样一个具有4GB内存的`NameNode`足以支持大量的文件和目录。当`NameNode`启动时，它会从磁盘中读取`FsImage`和`EditLog`编辑日志，将`EditLog`编辑日志中的所有事务应用到内存中的`FsImage`(applies all the transactions from the EditLog to the in-memory representation of the FsImage)，并将这个新版本刷新到磁盘上生成一个新`FsImage`。它可以截断旧的`EditLog`编辑日志，因为它的事务已经被应用到持久化的`FsImage`上。这个过程被称为检查点。在目前的实现中，只有在`NameNode`启动时才会出现检查点。在未来版本中正在进行工作的`NameNode`也会支持周期性的检查点。\n\n`DataNode`将`HDFS`数据存储在本地文件系统的文件中。`DataNode`不了解`HDFS`文件(The DataNode has no knowledge about HDFS files)。它将每个`HDFS`数据块存储在本地文件系统中的单个文件中。`DataNode`不会在同一目录中创建所有文件。相反，它使用启发式来确定每个目录的最佳文件数量并适当地创建子目录。由于本地文件系统可能无法有效地支持单个目录中的大量文件，因此在同一目录中创建所有本地文件并不是最佳选择。当`DataNode`启动时，它会扫描其本地文件系统，生成一个包含所有`HDFS`数据块(与每个本地文件相对应)的列表，并将此报告发送给`NameNode`：这是`Blockreport`。\n\n### 7. 通信协议\n\n所有的`HDFS`通信协议都是基于`TCP/IP`协议的。客户端建立到`NameNode`机器上的可配置TCP端口的连接。它使用`ClientProtocol`与`NameNode`交谈。`DataNode`使用`DataNode`协议与`NameNode`进行通信。远程过程调用(RPC)抽象包装客户端协议和数据节点协议。根据设计，`NameNode`永远不会启动任何RPC。而是只响应由`DataNode`或客户端发出的RPC请求。\n\n### 8. 稳定性\n\n`HDFS`的主要目标是即使在出现故障时也能可靠地存储数据。三种常见的故障类型是`NameNode`故障，`DataNode`故障和网络分裂(network partitions)。\n\n#### 8.1 数据磁盘故障，心跳和重新复制\n\n每个`DataNode`定期向`NameNode`发送一个`Heartbeat`消息。网络分裂可能导致一组`DataNode`与`NameNode`失去联系。`NameNode`通过丢失`Heartbeat`消息来检测这种情况。`NameNode`将最近没有`Heartbeats`的`DataNode`标记为死亡，并且不会将任何新的IO请求转发给它们。任何注册在标记为死亡的`DataNode`中的数据不再可用。`DataNode`死亡可能导致某些块的复制因子降到其指定值以下。`NameNode`不断跟踪哪些块需要复制，并在需要时启动复制。重新复制可能由于许多原因而产生：`DataNode`可能变得不可用，副本可能被破坏，`DataNode`上的硬盘可能出现故障，或者文件的复制因子可能需要增加。\n\n为了避免由于`DataNode`的状态震荡而导致的复制风暴，标记`DataNode`死亡的超时时间设置的比较保守(The time-out to mark DataNodes dead is conservatively long)(默认超过10分钟)。用户可以设置较短的时间间隔以将`DataNode`标记为陈旧，并避免陈旧节点在读取或按配置写入时性能出现负载(Users can set shorter interval to mark DataNodes as stale and avoid stale nodes on reading and/or writing by configuration for performance sensitive workloads)。\n\n#### 8.2 集群重新平衡\n\n`HDFS`体系结构与数据重新平衡方案兼容。如果某个`DataNode`上的可用空间低于某个阈值，那么会自动将数据从一个`DataNode`移动到另一个`DataNode`。对于特定文件突然高需求(sudden high demand)的情况下，可能会动态创建额外的副本并重新平衡集群中的其他数据。这些类型的数据重新平衡方案尚未实现。\n\n#### 8.3 数据完整性\n\n从`DataNode`上获取的数据块可能会损坏。发生损坏可能是由存储设备故障，网络故障或软件错误引起。`HDFS`客户端实现了对`HDFS`上文件内容进行校验和检查。当客户端创建一个`HDFS`文件时，它会计算每个文件的对应数据块的校验和，并将这些校验和存储在同一个`HDFS`命名空间中的单独隐藏文件中。当客户端检索文件内容时，它会验证从每个`DataNode`收到的数据是否与存储在相关校验和文件中的校验和相匹配。如果不匹配，那么客户端可以选择从另一个具有该数据块副本的`DataNode`中检索该数据块。\n\n#### 8.4 元数据磁盘故障\n\n`FsImage`和`EditLog`编辑日志是`HDFS`中的中心数据结构。这些文件的损坏可能会导致`HDFS`实例无法正常运行。为此，`NameNode`可以配置为支持维护`FsImage`和`EditLog`编辑日志的多个副本。任何对`FsImage`或`EditLog`编辑日志的更新都会引起每个`FsImages`和`EditLogs`编辑日志同步更新。同步更新`FsImage`和`EditLog`编辑日志的多个副本可能会降低`NameNode`支持的每秒的命名空间事务的速度(degrade the rate of namespace transactions per second)。但是，这种降低是可以接受的，因为尽管`HDFS`应用程序实质上是非常密集的数据，但是它们也不是元数据密集型的。当`NameNode`重新启动时，它会选择最新的一致的`FsImage`和`EditLog`编辑日志来使用。\n\n另一个增强防御故障的方法是使用多个`NameNode`以启用高可用性，或者使用[`NFS`上的共享存储](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html)或使用[分布式编辑日志](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)(称为`Journal`)。后者是推荐的方法。\n\n#### 8.5 快照\n\n[快照](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html)支持在特定时刻存储数据副本。快照功能的一种用法是将损坏的`HDFS`实例回滚到先前已知的良好时间点。\n\n### 9. 数据组织\n\n#### 9.1 数据块\n\n`HDFS`为支持大文件而设计的。与`HDFS`兼容的应用程序是处理大型数据集的应用程序。这些应用程序只写入数据一次，但是读取一次或多次，并读取速度要求满足流式处理速度。`HDFS`支持在文件上一次写入多次读取语义。`HDFS`使用的一般块大小为128 MB。因此，一个`HDFS`文件被分成多个128MB的块，如果可能的话，每个块将保存在不同的`DataNode`上。\n\n#### 9.2 分阶段\n\n客户端创建文件的请求不会立即到达`NameNode`。事实上，最初`HDFS`客户端将文件数据缓存到本地缓冲区。应用程序写入重定向到本地缓冲区。当本地文件累积超过一个块大小的数据时，客户端才会联系`NameNode`。`NameNode`将文件名插入到文件系统层次结构中，并为其分配一个数据块。`NameNode`将`DataNode`和目标数据块的标识和返回给客户请求。然后，客户端将本地缓冲区中的数据块保存到指定的`DataNode`上。当文件关闭时，本地缓冲区中剩余的未保存数据也被传输到`DataNode`。客户端然后告诉`NameNode`该文件已关闭。此时，`NameNode`将文件创建操作提交到持久化存储中。如果`NameNode`在文件关闭之前崩溃，那么文件会丢失。\n\n在仔细考虑在`HDFS`上运行的目标应用程序之后，采用了上述方法。这些应用程序需要流式写入文件。如果客户端直接写入远程文件目录而没有在客户端进行任何缓冲，那么网络速度和网络拥塞会大大影响吞吐量。这种方法并非没有先例。较早的分布式文件系统，例如`AFS`，已经使用客户端缓存来提高性能。`POSIX`的要求已经放宽，以实现更高的数据传输性能。\n\n### 9.3 副本流水线\n\n当客户端将数据写入`HDFS`文件时，首先将数据写入本地缓冲区，如上一节所述。假设`HDFS`文件复制因子为3。当本地缓冲区累积了一个块的用户数据时，客户端从`NameNode`中检索`DataNode`列表。该列表包含保存数据的数据块副本的`DataNode`。客户端然后将数据块刷新到第一个`DataNode`。第一个`DataNode`开始接收一小部分数据，将这一小部分数据写入其本地存储库，然后传输到列表中的第二个`DataNode`。第二个`DataNode`依次接收数据块的每一部分数据，将其写入存储库，然后再将刷新到第三个`DataNode`。最后，第三个`DataNode`将数据写入其本地存储库。因此，`DataNode`可以以流水线的方式从前一个`DataNode`接收数据，同时将数据转发到流水线中的下一个`DataNode`。因此，数据从一个`DataNode`流到下一个。\n\n### 10. 访问\n\n应用程序可以以多种不同的方式访问`HDFS`。`HDFS`为应用程序提供了一个`FileSystem Java API`。`Java API`和`REST API`的C语言包装器也可以使用。另外还有一个HTTP浏览器(HTTP browser)，也可以用来浏览`HDFS`实例的文件。通过使用`NFS`网关，可以将`HDFS`作为客户端本地文件系统的一部分。\n\n#### 10.1 FS Shell\n\n`HDFS`将用户数据以文件和目录的形式进行组织。它提供了一个名为`FS shell`的命令行接口，让用户可以与`HDFS`中的数据进行交互。这个命令集的语法类似于用户已经熟悉的其他`shell`(例如`bash`，`csh`)。 以下是一些示例操作/命令对：\n\n操作|命令\n---|---\n创建`/foodir`目录|`bin/hadoop dfs -mkdir /foodir`\n删除目录`/foodir`|`bin/hadoop fs -rm -R /foodir`\n查看`/foodir/myfile.txt`中内容|`bin/hadoop dfs -cat /foodir/myfile.txt`\n\n`FS shell`针对需要脚本语言与存储数据进行交互的应用程序。\n\n#### 10.2 DFSAdmin\n\n`DFSAdmin`命令集用于管理`HDFS`集群。这些是仅能由`HDFS`管理员使用的命令。以下是一些示例操作/命令对：\n\n操作|命令\n---|---\n使集群处于安全模式|`bin/hdfs dfsadmin -safemode enter`\n生成`DataNode`列表|`bin/hdfs dfsadmin -report`\n重新投放或停用`DataNode(s)`|`bin/hdfs dfsadmin -refreshNodes`\n\n#### 10.3 浏览器接口\n\n一个典型的`HDFS`安装会配置一个`Web`服务器，通过一个可配置的`TCP`端口公开`HDFS`命名空间。这允许用户使用Web浏览器浏览`HDFS`命名空间并查看其文件的内容。\n\n\n备注:\n```\nHadoop版本: 2.7.3\n```\n\n原文:http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\n","slug":"Hadoop/Hadoop2.x HDFS架构","published":1,"updated":"2018-01-29T09:36:59.630Z","comments":1,"photos":[],"link":"","_id":"cje58tisz003cordblp7vaf4p","content":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>Hadoop分布式文件系统(<code>HDFS</code>)是一个分布式文件系统，设计初衷是可以在商用硬件上运行。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的也有显著的差异。<code>HDFS</code>具有高容错能力，可以部署在低成本的硬件上。<code>HDFS</code>提供对应用程序数据的高吞吐量访问，适用于具有大数据集的应用程序。<code>HDFS</code>放宽了一些POSIX要求，以便对文件系统数据进行流式访问。<code>HDFS</code>最初是作为<code>Apache Nutch</code>网络搜索引擎项目的基础架构构建的。<code>HDFS</code>是<code>Apache Hadoop Core</code>项目的一部分。项目URL为: <a href=\"http://hadoop.apache.org/\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/</a></p>\n<h3 id=\"2-设想与目标\"><a href=\"#2-设想与目标\" class=\"headerlink\" title=\"2. 设想与目标\"></a>2. 设想与目标</h3><h4 id=\"2-1-硬件故障\"><a href=\"#2-1-硬件故障\" class=\"headerlink\" title=\"2.1 硬件故障\"></a>2.1 硬件故障</h4><p> 硬件故障很常见不要感到意外。<code>HDFS</code>实例可能由成百上千台服务器机器组成，每台机器存储部分文件系统的数据。事实上，有大量的组件，并且每个组件具有不一定的故障概率，这意味着可能<code>HDFS</code>的某些组件总是不起作用的。因此，故障检测和快速自动恢复是<code>HDFS</code>的核心架构。</p>\n<h4 id=\"2-2-流式数据访问\"><a href=\"#2-2-流式数据访问\" class=\"headerlink\" title=\"2.2 流式数据访问\"></a>2.2 流式数据访问</h4><p>运行在<code>HDFS</code>上的应用程序需要流式访问其数据集。<code>HDFS</code>不是运行在通用文件系统上通用应用程序。<code>HDFS</code>设计是为了更多的批量处理，而不是与用户进行交互。重点是数据访问的高吞吐量，而不是数据访问的低延迟。</p>\n<h4 id=\"2-3-大数据集\"><a href=\"#2-3-大数据集\" class=\"headerlink\" title=\"2.3 大数据集\"></a>2.3 大数据集</h4><p>运行在<code>HDFS</code>上的应用程序具有较大的数据集。<code>HDFS</code>中的文件大小一般为几GB或几TB。因此，<code>HDFS</code>需要支持大文件。它需要提供高数据聚合带宽并可以在单个集群中扩展到的数百个节点。它需要在一个实例中支持数千万个文件。</p>\n<h4 id=\"2-4-简单一致性模型\"><a href=\"#2-4-简单一致性模型\" class=\"headerlink\" title=\"2.4 简单一致性模型\"></a>2.4 简单一致性模型</h4><p><code>HDFS</code>数据访问模式为一次写入多次读取。文件一旦创建、写入和关闭后，除了追加和截断外，文件不能更改。可以支持将内容追加到文件末尾，但不能在随意位置更新文件内容。该假设简化了数据一致性问题，并实现了数据访问的高吞吐量。<code>MapReduce</code>应用程序或Web爬虫程序应用程序与此模型完美匹配。</p>\n<h4 id=\"2-5-‘移动计算比移动数据便宜’\"><a href=\"#2-5-‘移动计算比移动数据便宜’\" class=\"headerlink\" title=\"2.5 ‘移动计算比移动数据便宜’\"></a>2.5 ‘移动计算比移动数据便宜’</h4><p>如果应用程序能够在其操作的数据附近执行，那么应用程序所请求的计算效率会更高一些。当数据集很大时，这一点更能体现。这样可以最大限度地减少网络拥塞并提高系统的整体吞吐量。我们假设将计算迁移到更靠近数据的位置比将数据转移到应用程序运行的位置更好。<code>HDFS</code>为应用程序提供接口，使其更靠近数据所在的位置。</p>\n<h4 id=\"2-6-跨越异构硬件和软件平台的可移植性\"><a href=\"#2-6-跨越异构硬件和软件平台的可移植性\" class=\"headerlink\" title=\"2.6 跨越异构硬件和软件平台的可移植性\"></a>2.6 跨越异构硬件和软件平台的可移植性</h4><p><code>HDFS</code>被设计为可以从一个平台轻松地移植到另一个平台。这有助于<code>HDFS</code>作为大型应用程序的首选平台。</p>\n<h3 id=\"3-NameNode-and-DataNodes\"><a href=\"#3-NameNode-and-DataNodes\" class=\"headerlink\" title=\"3. NameNode and DataNodes\"></a>3. NameNode and DataNodes</h3><p><code>HDFS</code>是一个主/从结构。一个<code>HDFS</code>集群包含一个<code>NameNode</code>，管理文件系统命名空间以及管理客户端对文件访问的主服务。除此之外，还有一些<code>DataNode</code>，通常集群中的每个节点都有一个<code>DataNode</code>，用于管理它们所运行节点相关的存储。<code>HDFS</code>公开文件系统命名空间，并允许用户数据存储在文件中。在内部，一个文件被分成一个或多个数据块，这些数据块被存储在一组<code>DataNode</code>中。<code>NameNode</code>执行文件系统命名空间操作，例如打开，关闭和重命名文件和目录等。它也决定数据块到<code>DataNode</code>的映射。<code>DataNode</code>负责为文件系统客户端的读写请求提供服务。<code>DataNode</code>还根据来自<code>NameNode</code>的指令执行数据块的创建，删除和复制。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-1.png?raw=true\" alt=\"\"></p>\n<p><code>NameNode</code>和<code>DataNode</code>是设计用于在商业机器上运行的软件。这些机器通常运行GNU/Linux操作系统(OS)。<code>HDFS</code>是使用<code>Java</code>语言构建的; 任何支持<code>Java</code>的机器都可以运行<code>NameNode</code>或<code>DataNode</code>。使用高可移植性的<code>Java</code>语言意味着<code>HDFS</code>可以部署在各种机器上。一个典型的部署是有一台专用机器来运行<code>NameNode</code>。集群中的其他机器运行<code>DataNode</code>实例。该体系结构并不排除在同一台计算机上运行多个<code>DataNode</code>，但在实际部署中很少出现这种情况。</p>\n<p>集群中<code>NameNode</code>的存在大大简化了系统的体系结构。<code>NameNode</code>是所有<code>HDFS</code>元数据的决策者和存储仓库。系统的这种设计方式可以允许用户数据不会经过<code>NameNode</code>，直接与<code>DataNode</code>进行连接。</p>\n<h3 id=\"4-文件系统命名空间\"><a href=\"#4-文件系统命名空间\" class=\"headerlink\" title=\"4. 文件系统命名空间\"></a>4. 文件系统命名空间</h3><p><code>HDFS</code>支持传统的分层文件组织方式。用户或应用程序可以创建目录以及在这些目录内存储文件。文件系统命名空间层次结构与大多数其他文件系统类似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。<code>HDFS</code>支持用户配额和访问权限。<code>HDFS</code>不支持硬链接或软链接。但是，<code>HDFS</code>体系结构并不排除实现这些功能。</p>\n<p><code>NameNode</code>维护文件系统的命名空间。对文件系统命名空间或其属性的任何更改都会在<code>NameNode</code>中记录。应用程序可以指定<code>HDFS</code>应该维护的文件的副本数量。文件的副本数称为该文件的复制因子。这个信息由<code>NameNode</code>存储。</p>\n<h3 id=\"5-数据复制\"><a href=\"#5-数据复制\" class=\"headerlink\" title=\"5. 数据复制\"></a>5. 数据复制</h3><p><code>HDFS</code>旨在大型集群多台机器上可靠地存储非常大的文件。将每个文件存储为一系列的数据块。文件的数据块被复制多份以实现容错。数据块大小和副本因子是可以通过配置文件进行配置。</p>\n<p>一个文件的数据块除最后一个块以外的所有其他块的大小都相同，在添加对可变长度块和<code>hsync</code>的支持后，用户可以不用填充最后一个块到配置大小而启动一个新块。</p>\n<p>应用程序可以指定文件的副本数量。复制因子可以在文件创建时指定，也可以在以后更改。<code>HDFS</code>中的文件是一次性编写的(追加和截断除外)，并且严格限定在任何时候都只能有一个编写者。</p>\n<p><code>NameNode</code>做出关于块复制的所有决定。它周期性的从集群中的每个<code>DataNode</code>接收<code>Heartbeat</code>和<code>Blockreport</code>。收到<code>Heartbeat</code>意味着<code>DataNode</code>运行正常。<code>Blockreport</code>包含<code>DataNode</code>上所有块的列表。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-2.png?raw=true\" alt=\"\"></p>\n<h4 id=\"5-1-副本安置\"><a href=\"#5-1-副本安置\" class=\"headerlink\" title=\"5.1 副本安置\"></a>5.1 副本安置</h4><p>副本的放置对<code>HDFS</code>的可靠性和性能至关重要。优化副本放置能将<code>HDFS</code>与大多数其他分布式文件系统区分开来。这是一个需要大量调整和体验的功能。机架感知副本放置策略的目的是提高数据可靠性，可用性和网络带宽利用率。副本放置策略的目前实现是朝这个方向迈进的第一步。实施这一策略的短期目标是在生产环境上进行验证，更多地了解其行为，并为测试和研究更复杂的策略奠定基础。</p>\n<p>大型<code>HDFS</code>实例运行在通常分布在多个机架上的一组计算机上。不同机架中的两个节点之间的通信必须经过交换机。在大多数情况下，同一机架中的机器之间的网络带宽大于不同机架中的机器之间的网络带宽。</p>\n<p><code>NameNode</code>通过<code>Hadoop</code>机架感知中概述的过程确定每个<code>DataNode</code>所属的机架Id。一个简单但不是最佳的策略是将副本放在不同的机架上。这可以防止整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在集群中均匀分配副本，以便轻松平衡组件故障的负载(This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure)。但是，此策略会增加写入成本，因为写入需要将数据块传输到多个机架。</p>\n<p>正常情况下，当复制因子为3时，<code>HDFS</code>的放置策略是将一个副本放在本地机架的同一个节点上，另一个放在本地机架的不同节点上，最后放在另一个机架的不同节点上。这个政策降低了机架间写入流量，这通常会提高写入性能。机架故障的几率远远小于节点故障的几率;此策略不会影响数据可靠性和可用性的保证。但是，它降低了读取数据时使用的总体网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本在同一个节点上，三分之二的副本在同一个机架上，另外三分之一在其它机架上均匀分布。此策略可提高写入性能，而不会影响数据可靠性或读取性能。</p>\n<p>这里描述的就是当前默认副本放置策略。</p>\n<h4 id=\"5-2-副本选择\"><a href=\"#5-2-副本选择\" class=\"headerlink\" title=\"5.2 副本选择\"></a>5.2 副本选择</h4><p>为了尽量减少全局带宽消耗和读取延迟，<code>HDFS</code>会尝试将读取请求发送到离读取者最近的副本上(HDFS tries to satisfy a read request from a replica that is closest to the reader.)。 如果在与读取者节点相同的机架上存在副本，则该副本优选满足读取请求。如果<code>HDFS</code>进群跨越多个数据中心，则保存在本地数据中心的副本优先于任何远程副本。</p>\n<h4 id=\"5-3-安全模式\"><a href=\"#5-3-安全模式\" class=\"headerlink\" title=\"5.3 安全模式\"></a>5.3 安全模式</h4><p>在启动时，<code>NameNode</code>进入一个称为<code>Safemode</code>(安全模式)的特殊状态。当<code>NameNode</code>处于安全模式状态时，不会发生数据块的复制。<code>NameNode</code>接收来自<code>DataNode</code>的<code>Heartbeat</code>和<code>Blockreport</code>消息。<code>Blockreport</code>包含<code>DataNode</code>托管的数据块列表。每个块都有指定的最小数量的副本。当该数据块的最小副本数与<code>NameNode</code>签入时，将认为该块被安全地复制。在安全复制数据块的可配置百分比检入<code>NameNode</code>（再加上30秒）之后，<code>NameNode</code>退出安全模式状态。然后确定仍然少于指定副本数量的数据块列表（如果有的话）。<code>NameNode</code>然后将这些块复制到其他<code>DataNode</code>。</p>\n<h3 id=\"6-文件系统元数据持久化\"><a href=\"#6-文件系统元数据持久化\" class=\"headerlink\" title=\"6. 文件系统元数据持久化\"></a>6. 文件系统元数据持久化</h3><p><code>HDFS</code>命名空间存储在<code>NameNode</code>中。<code>NameNode</code>使用称之为<code>EditLog</code>编辑日志的事务日志来持久化存储在文件系统元数据上发生的每一个变化。例如，在<code>HDFS</code>中创建一个新文件会导致<code>NameNode</code>向<code>EditLog</code>编辑日志中插入一条记录。同样，更改文件的复制因子也会导致将新记录插入到<code>EditLog</code>编辑日志中。<code>NameNode</code>使用其本地主机OS文件系统中的文件来存储<code>EditLog</code>编辑日志。整个文件系统命名空间，包括数据块到文件的映射以及文件系统属性，都存储在一个名为<code>FsImage</code>的文件中。<code>FsImage</code>作为文件存储在<code>NameNode</code>的本地文件系统中。</p>\n<p><code>NameNode</code>将整个文件系统命名空间和文件<code>Blockmap</code>的快照(image)保存在内存中。这个关键的元数据被设计得很紧凑，这样一个具有4GB内存的<code>NameNode</code>足以支持大量的文件和目录。当<code>NameNode</code>启动时，它会从磁盘中读取<code>FsImage</code>和<code>EditLog</code>编辑日志，将<code>EditLog</code>编辑日志中的所有事务应用到内存中的<code>FsImage</code>(applies all the transactions from the EditLog to the in-memory representation of the FsImage)，并将这个新版本刷新到磁盘上生成一个新<code>FsImage</code>。它可以截断旧的<code>EditLog</code>编辑日志，因为它的事务已经被应用到持久化的<code>FsImage</code>上。这个过程被称为检查点。在目前的实现中，只有在<code>NameNode</code>启动时才会出现检查点。在未来版本中正在进行工作的<code>NameNode</code>也会支持周期性的检查点。</p>\n<p><code>DataNode</code>将<code>HDFS</code>数据存储在本地文件系统的文件中。<code>DataNode</code>不了解<code>HDFS</code>文件(The DataNode has no knowledge about HDFS files)。它将每个<code>HDFS</code>数据块存储在本地文件系统中的单个文件中。<code>DataNode</code>不会在同一目录中创建所有文件。相反，它使用启发式来确定每个目录的最佳文件数量并适当地创建子目录。由于本地文件系统可能无法有效地支持单个目录中的大量文件，因此在同一目录中创建所有本地文件并不是最佳选择。当<code>DataNode</code>启动时，它会扫描其本地文件系统，生成一个包含所有<code>HDFS</code>数据块(与每个本地文件相对应)的列表，并将此报告发送给<code>NameNode</code>：这是<code>Blockreport</code>。</p>\n<h3 id=\"7-通信协议\"><a href=\"#7-通信协议\" class=\"headerlink\" title=\"7. 通信协议\"></a>7. 通信协议</h3><p>所有的<code>HDFS</code>通信协议都是基于<code>TCP/IP</code>协议的。客户端建立到<code>NameNode</code>机器上的可配置TCP端口的连接。它使用<code>ClientProtocol</code>与<code>NameNode</code>交谈。<code>DataNode</code>使用<code>DataNode</code>协议与<code>NameNode</code>进行通信。远程过程调用(RPC)抽象包装客户端协议和数据节点协议。根据设计，<code>NameNode</code>永远不会启动任何RPC。而是只响应由<code>DataNode</code>或客户端发出的RPC请求。</p>\n<h3 id=\"8-稳定性\"><a href=\"#8-稳定性\" class=\"headerlink\" title=\"8. 稳定性\"></a>8. 稳定性</h3><p><code>HDFS</code>的主要目标是即使在出现故障时也能可靠地存储数据。三种常见的故障类型是<code>NameNode</code>故障，<code>DataNode</code>故障和网络分裂(network partitions)。</p>\n<h4 id=\"8-1-数据磁盘故障，心跳和重新复制\"><a href=\"#8-1-数据磁盘故障，心跳和重新复制\" class=\"headerlink\" title=\"8.1 数据磁盘故障，心跳和重新复制\"></a>8.1 数据磁盘故障，心跳和重新复制</h4><p>每个<code>DataNode</code>定期向<code>NameNode</code>发送一个<code>Heartbeat</code>消息。网络分裂可能导致一组<code>DataNode</code>与<code>NameNode</code>失去联系。<code>NameNode</code>通过丢失<code>Heartbeat</code>消息来检测这种情况。<code>NameNode</code>将最近没有<code>Heartbeats</code>的<code>DataNode</code>标记为死亡，并且不会将任何新的IO请求转发给它们。任何注册在标记为死亡的<code>DataNode</code>中的数据不再可用。<code>DataNode</code>死亡可能导致某些块的复制因子降到其指定值以下。<code>NameNode</code>不断跟踪哪些块需要复制，并在需要时启动复制。重新复制可能由于许多原因而产生：<code>DataNode</code>可能变得不可用，副本可能被破坏，<code>DataNode</code>上的硬盘可能出现故障，或者文件的复制因子可能需要增加。</p>\n<p>为了避免由于<code>DataNode</code>的状态震荡而导致的复制风暴，标记<code>DataNode</code>死亡的超时时间设置的比较保守(The time-out to mark DataNodes dead is conservatively long)(默认超过10分钟)。用户可以设置较短的时间间隔以将<code>DataNode</code>标记为陈旧，并避免陈旧节点在读取或按配置写入时性能出现负载(Users can set shorter interval to mark DataNodes as stale and avoid stale nodes on reading and/or writing by configuration for performance sensitive workloads)。</p>\n<h4 id=\"8-2-集群重新平衡\"><a href=\"#8-2-集群重新平衡\" class=\"headerlink\" title=\"8.2 集群重新平衡\"></a>8.2 集群重新平衡</h4><p><code>HDFS</code>体系结构与数据重新平衡方案兼容。如果某个<code>DataNode</code>上的可用空间低于某个阈值，那么会自动将数据从一个<code>DataNode</code>移动到另一个<code>DataNode</code>。对于特定文件突然高需求(sudden high demand)的情况下，可能会动态创建额外的副本并重新平衡集群中的其他数据。这些类型的数据重新平衡方案尚未实现。</p>\n<h4 id=\"8-3-数据完整性\"><a href=\"#8-3-数据完整性\" class=\"headerlink\" title=\"8.3 数据完整性\"></a>8.3 数据完整性</h4><p>从<code>DataNode</code>上获取的数据块可能会损坏。发生损坏可能是由存储设备故障，网络故障或软件错误引起。<code>HDFS</code>客户端实现了对<code>HDFS</code>上文件内容进行校验和检查。当客户端创建一个<code>HDFS</code>文件时，它会计算每个文件的对应数据块的校验和，并将这些校验和存储在同一个<code>HDFS</code>命名空间中的单独隐藏文件中。当客户端检索文件内容时，它会验证从每个<code>DataNode</code>收到的数据是否与存储在相关校验和文件中的校验和相匹配。如果不匹配，那么客户端可以选择从另一个具有该数据块副本的<code>DataNode</code>中检索该数据块。</p>\n<h4 id=\"8-4-元数据磁盘故障\"><a href=\"#8-4-元数据磁盘故障\" class=\"headerlink\" title=\"8.4 元数据磁盘故障\"></a>8.4 元数据磁盘故障</h4><p><code>FsImage</code>和<code>EditLog</code>编辑日志是<code>HDFS</code>中的中心数据结构。这些文件的损坏可能会导致<code>HDFS</code>实例无法正常运行。为此，<code>NameNode</code>可以配置为支持维护<code>FsImage</code>和<code>EditLog</code>编辑日志的多个副本。任何对<code>FsImage</code>或<code>EditLog</code>编辑日志的更新都会引起每个<code>FsImages</code>和<code>EditLogs</code>编辑日志同步更新。同步更新<code>FsImage</code>和<code>EditLog</code>编辑日志的多个副本可能会降低<code>NameNode</code>支持的每秒的命名空间事务的速度(degrade the rate of namespace transactions per second)。但是，这种降低是可以接受的，因为尽管<code>HDFS</code>应用程序实质上是非常密集的数据，但是它们也不是元数据密集型的。当<code>NameNode</code>重新启动时，它会选择最新的一致的<code>FsImage</code>和<code>EditLog</code>编辑日志来使用。</p>\n<p>另一个增强防御故障的方法是使用多个<code>NameNode</code>以启用高可用性，或者使用<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html\" target=\"_blank\" rel=\"noopener\"><code>NFS</code>上的共享存储</a>或使用<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html\" target=\"_blank\" rel=\"noopener\">分布式编辑日志</a>(称为<code>Journal</code>)。后者是推荐的方法。</p>\n<h4 id=\"8-5-快照\"><a href=\"#8-5-快照\" class=\"headerlink\" title=\"8.5 快照\"></a>8.5 快照</h4><p><a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html\" target=\"_blank\" rel=\"noopener\">快照</a>支持在特定时刻存储数据副本。快照功能的一种用法是将损坏的<code>HDFS</code>实例回滚到先前已知的良好时间点。</p>\n<h3 id=\"9-数据组织\"><a href=\"#9-数据组织\" class=\"headerlink\" title=\"9. 数据组织\"></a>9. 数据组织</h3><h4 id=\"9-1-数据块\"><a href=\"#9-1-数据块\" class=\"headerlink\" title=\"9.1 数据块\"></a>9.1 数据块</h4><p><code>HDFS</code>为支持大文件而设计的。与<code>HDFS</code>兼容的应用程序是处理大型数据集的应用程序。这些应用程序只写入数据一次，但是读取一次或多次，并读取速度要求满足流式处理速度。<code>HDFS</code>支持在文件上一次写入多次读取语义。<code>HDFS</code>使用的一般块大小为128 MB。因此，一个<code>HDFS</code>文件被分成多个128MB的块，如果可能的话，每个块将保存在不同的<code>DataNode</code>上。</p>\n<h4 id=\"9-2-分阶段\"><a href=\"#9-2-分阶段\" class=\"headerlink\" title=\"9.2 分阶段\"></a>9.2 分阶段</h4><p>客户端创建文件的请求不会立即到达<code>NameNode</code>。事实上，最初<code>HDFS</code>客户端将文件数据缓存到本地缓冲区。应用程序写入重定向到本地缓冲区。当本地文件累积超过一个块大小的数据时，客户端才会联系<code>NameNode</code>。<code>NameNode</code>将文件名插入到文件系统层次结构中，并为其分配一个数据块。<code>NameNode</code>将<code>DataNode</code>和目标数据块的标识和返回给客户请求。然后，客户端将本地缓冲区中的数据块保存到指定的<code>DataNode</code>上。当文件关闭时，本地缓冲区中剩余的未保存数据也被传输到<code>DataNode</code>。客户端然后告诉<code>NameNode</code>该文件已关闭。此时，<code>NameNode</code>将文件创建操作提交到持久化存储中。如果<code>NameNode</code>在文件关闭之前崩溃，那么文件会丢失。</p>\n<p>在仔细考虑在<code>HDFS</code>上运行的目标应用程序之后，采用了上述方法。这些应用程序需要流式写入文件。如果客户端直接写入远程文件目录而没有在客户端进行任何缓冲，那么网络速度和网络拥塞会大大影响吞吐量。这种方法并非没有先例。较早的分布式文件系统，例如<code>AFS</code>，已经使用客户端缓存来提高性能。<code>POSIX</code>的要求已经放宽，以实现更高的数据传输性能。</p>\n<h3 id=\"9-3-副本流水线\"><a href=\"#9-3-副本流水线\" class=\"headerlink\" title=\"9.3 副本流水线\"></a>9.3 副本流水线</h3><p>当客户端将数据写入<code>HDFS</code>文件时，首先将数据写入本地缓冲区，如上一节所述。假设<code>HDFS</code>文件复制因子为3。当本地缓冲区累积了一个块的用户数据时，客户端从<code>NameNode</code>中检索<code>DataNode</code>列表。该列表包含保存数据的数据块副本的<code>DataNode</code>。客户端然后将数据块刷新到第一个<code>DataNode</code>。第一个<code>DataNode</code>开始接收一小部分数据，将这一小部分数据写入其本地存储库，然后传输到列表中的第二个<code>DataNode</code>。第二个<code>DataNode</code>依次接收数据块的每一部分数据，将其写入存储库，然后再将刷新到第三个<code>DataNode</code>。最后，第三个<code>DataNode</code>将数据写入其本地存储库。因此，<code>DataNode</code>可以以流水线的方式从前一个<code>DataNode</code>接收数据，同时将数据转发到流水线中的下一个<code>DataNode</code>。因此，数据从一个<code>DataNode</code>流到下一个。</p>\n<h3 id=\"10-访问\"><a href=\"#10-访问\" class=\"headerlink\" title=\"10. 访问\"></a>10. 访问</h3><p>应用程序可以以多种不同的方式访问<code>HDFS</code>。<code>HDFS</code>为应用程序提供了一个<code>FileSystem Java API</code>。<code>Java API</code>和<code>REST API</code>的C语言包装器也可以使用。另外还有一个HTTP浏览器(HTTP browser)，也可以用来浏览<code>HDFS</code>实例的文件。通过使用<code>NFS</code>网关，可以将<code>HDFS</code>作为客户端本地文件系统的一部分。</p>\n<h4 id=\"10-1-FS-Shell\"><a href=\"#10-1-FS-Shell\" class=\"headerlink\" title=\"10.1 FS Shell\"></a>10.1 FS Shell</h4><p><code>HDFS</code>将用户数据以文件和目录的形式进行组织。它提供了一个名为<code>FS shell</code>的命令行接口，让用户可以与<code>HDFS</code>中的数据进行交互。这个命令集的语法类似于用户已经熟悉的其他<code>shell</code>(例如<code>bash</code>，<code>csh</code>)。 以下是一些示例操作/命令对：</p>\n<table>\n<thead>\n<tr>\n<th>操作</th>\n<th>命令</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>创建<code>/foodir</code>目录</td>\n<td><code>bin/hadoop dfs -mkdir /foodir</code></td>\n</tr>\n<tr>\n<td>删除目录<code>/foodir</code></td>\n<td><code>bin/hadoop fs -rm -R /foodir</code></td>\n</tr>\n<tr>\n<td>查看<code>/foodir/myfile.txt</code>中内容</td>\n<td><code>bin/hadoop dfs -cat /foodir/myfile.txt</code></td>\n</tr>\n</tbody>\n</table>\n<p><code>FS shell</code>针对需要脚本语言与存储数据进行交互的应用程序。</p>\n<h4 id=\"10-2-DFSAdmin\"><a href=\"#10-2-DFSAdmin\" class=\"headerlink\" title=\"10.2 DFSAdmin\"></a>10.2 DFSAdmin</h4><p><code>DFSAdmin</code>命令集用于管理<code>HDFS</code>集群。这些是仅能由<code>HDFS</code>管理员使用的命令。以下是一些示例操作/命令对：</p>\n<table>\n<thead>\n<tr>\n<th>操作</th>\n<th>命令</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>使集群处于安全模式</td>\n<td><code>bin/hdfs dfsadmin -safemode enter</code></td>\n</tr>\n<tr>\n<td>生成<code>DataNode</code>列表</td>\n<td><code>bin/hdfs dfsadmin -report</code></td>\n</tr>\n<tr>\n<td>重新投放或停用<code>DataNode(s)</code></td>\n<td><code>bin/hdfs dfsadmin -refreshNodes</code></td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"10-3-浏览器接口\"><a href=\"#10-3-浏览器接口\" class=\"headerlink\" title=\"10.3 浏览器接口\"></a>10.3 浏览器接口</h4><p>一个典型的<code>HDFS</code>安装会配置一个<code>Web</code>服务器，通过一个可配置的<code>TCP</code>端口公开<code>HDFS</code>命名空间。这允许用户使用Web浏览器浏览<code>HDFS</code>命名空间并查看其文件的内容。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Hadoop版本: 2.7.3</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h3><p>Hadoop分布式文件系统(<code>HDFS</code>)是一个分布式文件系统，设计初衷是可以在商用硬件上运行。它与现有的分布式文件系统有许多相似之处。但是，与其他分布式文件系统的也有显著的差异。<code>HDFS</code>具有高容错能力，可以部署在低成本的硬件上。<code>HDFS</code>提供对应用程序数据的高吞吐量访问，适用于具有大数据集的应用程序。<code>HDFS</code>放宽了一些POSIX要求，以便对文件系统数据进行流式访问。<code>HDFS</code>最初是作为<code>Apache Nutch</code>网络搜索引擎项目的基础架构构建的。<code>HDFS</code>是<code>Apache Hadoop Core</code>项目的一部分。项目URL为: <a href=\"http://hadoop.apache.org/\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/</a></p>\n<h3 id=\"2-设想与目标\"><a href=\"#2-设想与目标\" class=\"headerlink\" title=\"2. 设想与目标\"></a>2. 设想与目标</h3><h4 id=\"2-1-硬件故障\"><a href=\"#2-1-硬件故障\" class=\"headerlink\" title=\"2.1 硬件故障\"></a>2.1 硬件故障</h4><p> 硬件故障很常见不要感到意外。<code>HDFS</code>实例可能由成百上千台服务器机器组成，每台机器存储部分文件系统的数据。事实上，有大量的组件，并且每个组件具有不一定的故障概率，这意味着可能<code>HDFS</code>的某些组件总是不起作用的。因此，故障检测和快速自动恢复是<code>HDFS</code>的核心架构。</p>\n<h4 id=\"2-2-流式数据访问\"><a href=\"#2-2-流式数据访问\" class=\"headerlink\" title=\"2.2 流式数据访问\"></a>2.2 流式数据访问</h4><p>运行在<code>HDFS</code>上的应用程序需要流式访问其数据集。<code>HDFS</code>不是运行在通用文件系统上通用应用程序。<code>HDFS</code>设计是为了更多的批量处理，而不是与用户进行交互。重点是数据访问的高吞吐量，而不是数据访问的低延迟。</p>\n<h4 id=\"2-3-大数据集\"><a href=\"#2-3-大数据集\" class=\"headerlink\" title=\"2.3 大数据集\"></a>2.3 大数据集</h4><p>运行在<code>HDFS</code>上的应用程序具有较大的数据集。<code>HDFS</code>中的文件大小一般为几GB或几TB。因此，<code>HDFS</code>需要支持大文件。它需要提供高数据聚合带宽并可以在单个集群中扩展到的数百个节点。它需要在一个实例中支持数千万个文件。</p>\n<h4 id=\"2-4-简单一致性模型\"><a href=\"#2-4-简单一致性模型\" class=\"headerlink\" title=\"2.4 简单一致性模型\"></a>2.4 简单一致性模型</h4><p><code>HDFS</code>数据访问模式为一次写入多次读取。文件一旦创建、写入和关闭后，除了追加和截断外，文件不能更改。可以支持将内容追加到文件末尾，但不能在随意位置更新文件内容。该假设简化了数据一致性问题，并实现了数据访问的高吞吐量。<code>MapReduce</code>应用程序或Web爬虫程序应用程序与此模型完美匹配。</p>\n<h4 id=\"2-5-‘移动计算比移动数据便宜’\"><a href=\"#2-5-‘移动计算比移动数据便宜’\" class=\"headerlink\" title=\"2.5 ‘移动计算比移动数据便宜’\"></a>2.5 ‘移动计算比移动数据便宜’</h4><p>如果应用程序能够在其操作的数据附近执行，那么应用程序所请求的计算效率会更高一些。当数据集很大时，这一点更能体现。这样可以最大限度地减少网络拥塞并提高系统的整体吞吐量。我们假设将计算迁移到更靠近数据的位置比将数据转移到应用程序运行的位置更好。<code>HDFS</code>为应用程序提供接口，使其更靠近数据所在的位置。</p>\n<h4 id=\"2-6-跨越异构硬件和软件平台的可移植性\"><a href=\"#2-6-跨越异构硬件和软件平台的可移植性\" class=\"headerlink\" title=\"2.6 跨越异构硬件和软件平台的可移植性\"></a>2.6 跨越异构硬件和软件平台的可移植性</h4><p><code>HDFS</code>被设计为可以从一个平台轻松地移植到另一个平台。这有助于<code>HDFS</code>作为大型应用程序的首选平台。</p>\n<h3 id=\"3-NameNode-and-DataNodes\"><a href=\"#3-NameNode-and-DataNodes\" class=\"headerlink\" title=\"3. NameNode and DataNodes\"></a>3. NameNode and DataNodes</h3><p><code>HDFS</code>是一个主/从结构。一个<code>HDFS</code>集群包含一个<code>NameNode</code>，管理文件系统命名空间以及管理客户端对文件访问的主服务。除此之外，还有一些<code>DataNode</code>，通常集群中的每个节点都有一个<code>DataNode</code>，用于管理它们所运行节点相关的存储。<code>HDFS</code>公开文件系统命名空间，并允许用户数据存储在文件中。在内部，一个文件被分成一个或多个数据块，这些数据块被存储在一组<code>DataNode</code>中。<code>NameNode</code>执行文件系统命名空间操作，例如打开，关闭和重命名文件和目录等。它也决定数据块到<code>DataNode</code>的映射。<code>DataNode</code>负责为文件系统客户端的读写请求提供服务。<code>DataNode</code>还根据来自<code>NameNode</code>的指令执行数据块的创建，删除和复制。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-1.png?raw=true\" alt=\"\"></p>\n<p><code>NameNode</code>和<code>DataNode</code>是设计用于在商业机器上运行的软件。这些机器通常运行GNU/Linux操作系统(OS)。<code>HDFS</code>是使用<code>Java</code>语言构建的; 任何支持<code>Java</code>的机器都可以运行<code>NameNode</code>或<code>DataNode</code>。使用高可移植性的<code>Java</code>语言意味着<code>HDFS</code>可以部署在各种机器上。一个典型的部署是有一台专用机器来运行<code>NameNode</code>。集群中的其他机器运行<code>DataNode</code>实例。该体系结构并不排除在同一台计算机上运行多个<code>DataNode</code>，但在实际部署中很少出现这种情况。</p>\n<p>集群中<code>NameNode</code>的存在大大简化了系统的体系结构。<code>NameNode</code>是所有<code>HDFS</code>元数据的决策者和存储仓库。系统的这种设计方式可以允许用户数据不会经过<code>NameNode</code>，直接与<code>DataNode</code>进行连接。</p>\n<h3 id=\"4-文件系统命名空间\"><a href=\"#4-文件系统命名空间\" class=\"headerlink\" title=\"4. 文件系统命名空间\"></a>4. 文件系统命名空间</h3><p><code>HDFS</code>支持传统的分层文件组织方式。用户或应用程序可以创建目录以及在这些目录内存储文件。文件系统命名空间层次结构与大多数其他文件系统类似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。<code>HDFS</code>支持用户配额和访问权限。<code>HDFS</code>不支持硬链接或软链接。但是，<code>HDFS</code>体系结构并不排除实现这些功能。</p>\n<p><code>NameNode</code>维护文件系统的命名空间。对文件系统命名空间或其属性的任何更改都会在<code>NameNode</code>中记录。应用程序可以指定<code>HDFS</code>应该维护的文件的副本数量。文件的副本数称为该文件的复制因子。这个信息由<code>NameNode</code>存储。</p>\n<h3 id=\"5-数据复制\"><a href=\"#5-数据复制\" class=\"headerlink\" title=\"5. 数据复制\"></a>5. 数据复制</h3><p><code>HDFS</code>旨在大型集群多台机器上可靠地存储非常大的文件。将每个文件存储为一系列的数据块。文件的数据块被复制多份以实现容错。数据块大小和副本因子是可以通过配置文件进行配置。</p>\n<p>一个文件的数据块除最后一个块以外的所有其他块的大小都相同，在添加对可变长度块和<code>hsync</code>的支持后，用户可以不用填充最后一个块到配置大小而启动一个新块。</p>\n<p>应用程序可以指定文件的副本数量。复制因子可以在文件创建时指定，也可以在以后更改。<code>HDFS</code>中的文件是一次性编写的(追加和截断除外)，并且严格限定在任何时候都只能有一个编写者。</p>\n<p><code>NameNode</code>做出关于块复制的所有决定。它周期性的从集群中的每个<code>DataNode</code>接收<code>Heartbeat</code>和<code>Blockreport</code>。收到<code>Heartbeat</code>意味着<code>DataNode</code>运行正常。<code>Blockreport</code>包含<code>DataNode</code>上所有块的列表。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/Hadoop2.x%20HDFS%E6%9E%B6%E6%9E%84-2.png?raw=true\" alt=\"\"></p>\n<h4 id=\"5-1-副本安置\"><a href=\"#5-1-副本安置\" class=\"headerlink\" title=\"5.1 副本安置\"></a>5.1 副本安置</h4><p>副本的放置对<code>HDFS</code>的可靠性和性能至关重要。优化副本放置能将<code>HDFS</code>与大多数其他分布式文件系统区分开来。这是一个需要大量调整和体验的功能。机架感知副本放置策略的目的是提高数据可靠性，可用性和网络带宽利用率。副本放置策略的目前实现是朝这个方向迈进的第一步。实施这一策略的短期目标是在生产环境上进行验证，更多地了解其行为，并为测试和研究更复杂的策略奠定基础。</p>\n<p>大型<code>HDFS</code>实例运行在通常分布在多个机架上的一组计算机上。不同机架中的两个节点之间的通信必须经过交换机。在大多数情况下，同一机架中的机器之间的网络带宽大于不同机架中的机器之间的网络带宽。</p>\n<p><code>NameNode</code>通过<code>Hadoop</code>机架感知中概述的过程确定每个<code>DataNode</code>所属的机架Id。一个简单但不是最佳的策略是将副本放在不同的机架上。这可以防止整个机架出现故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在集群中均匀分配副本，以便轻松平衡组件故障的负载(This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure)。但是，此策略会增加写入成本，因为写入需要将数据块传输到多个机架。</p>\n<p>正常情况下，当复制因子为3时，<code>HDFS</code>的放置策略是将一个副本放在本地机架的同一个节点上，另一个放在本地机架的不同节点上，最后放在另一个机架的不同节点上。这个政策降低了机架间写入流量，这通常会提高写入性能。机架故障的几率远远小于节点故障的几率;此策略不会影响数据可靠性和可用性的保证。但是，它降低了读取数据时使用的总体网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本在同一个节点上，三分之二的副本在同一个机架上，另外三分之一在其它机架上均匀分布。此策略可提高写入性能，而不会影响数据可靠性或读取性能。</p>\n<p>这里描述的就是当前默认副本放置策略。</p>\n<h4 id=\"5-2-副本选择\"><a href=\"#5-2-副本选择\" class=\"headerlink\" title=\"5.2 副本选择\"></a>5.2 副本选择</h4><p>为了尽量减少全局带宽消耗和读取延迟，<code>HDFS</code>会尝试将读取请求发送到离读取者最近的副本上(HDFS tries to satisfy a read request from a replica that is closest to the reader.)。 如果在与读取者节点相同的机架上存在副本，则该副本优选满足读取请求。如果<code>HDFS</code>进群跨越多个数据中心，则保存在本地数据中心的副本优先于任何远程副本。</p>\n<h4 id=\"5-3-安全模式\"><a href=\"#5-3-安全模式\" class=\"headerlink\" title=\"5.3 安全模式\"></a>5.3 安全模式</h4><p>在启动时，<code>NameNode</code>进入一个称为<code>Safemode</code>(安全模式)的特殊状态。当<code>NameNode</code>处于安全模式状态时，不会发生数据块的复制。<code>NameNode</code>接收来自<code>DataNode</code>的<code>Heartbeat</code>和<code>Blockreport</code>消息。<code>Blockreport</code>包含<code>DataNode</code>托管的数据块列表。每个块都有指定的最小数量的副本。当该数据块的最小副本数与<code>NameNode</code>签入时，将认为该块被安全地复制。在安全复制数据块的可配置百分比检入<code>NameNode</code>（再加上30秒）之后，<code>NameNode</code>退出安全模式状态。然后确定仍然少于指定副本数量的数据块列表（如果有的话）。<code>NameNode</code>然后将这些块复制到其他<code>DataNode</code>。</p>\n<h3 id=\"6-文件系统元数据持久化\"><a href=\"#6-文件系统元数据持久化\" class=\"headerlink\" title=\"6. 文件系统元数据持久化\"></a>6. 文件系统元数据持久化</h3><p><code>HDFS</code>命名空间存储在<code>NameNode</code>中。<code>NameNode</code>使用称之为<code>EditLog</code>编辑日志的事务日志来持久化存储在文件系统元数据上发生的每一个变化。例如，在<code>HDFS</code>中创建一个新文件会导致<code>NameNode</code>向<code>EditLog</code>编辑日志中插入一条记录。同样，更改文件的复制因子也会导致将新记录插入到<code>EditLog</code>编辑日志中。<code>NameNode</code>使用其本地主机OS文件系统中的文件来存储<code>EditLog</code>编辑日志。整个文件系统命名空间，包括数据块到文件的映射以及文件系统属性，都存储在一个名为<code>FsImage</code>的文件中。<code>FsImage</code>作为文件存储在<code>NameNode</code>的本地文件系统中。</p>\n<p><code>NameNode</code>将整个文件系统命名空间和文件<code>Blockmap</code>的快照(image)保存在内存中。这个关键的元数据被设计得很紧凑，这样一个具有4GB内存的<code>NameNode</code>足以支持大量的文件和目录。当<code>NameNode</code>启动时，它会从磁盘中读取<code>FsImage</code>和<code>EditLog</code>编辑日志，将<code>EditLog</code>编辑日志中的所有事务应用到内存中的<code>FsImage</code>(applies all the transactions from the EditLog to the in-memory representation of the FsImage)，并将这个新版本刷新到磁盘上生成一个新<code>FsImage</code>。它可以截断旧的<code>EditLog</code>编辑日志，因为它的事务已经被应用到持久化的<code>FsImage</code>上。这个过程被称为检查点。在目前的实现中，只有在<code>NameNode</code>启动时才会出现检查点。在未来版本中正在进行工作的<code>NameNode</code>也会支持周期性的检查点。</p>\n<p><code>DataNode</code>将<code>HDFS</code>数据存储在本地文件系统的文件中。<code>DataNode</code>不了解<code>HDFS</code>文件(The DataNode has no knowledge about HDFS files)。它将每个<code>HDFS</code>数据块存储在本地文件系统中的单个文件中。<code>DataNode</code>不会在同一目录中创建所有文件。相反，它使用启发式来确定每个目录的最佳文件数量并适当地创建子目录。由于本地文件系统可能无法有效地支持单个目录中的大量文件，因此在同一目录中创建所有本地文件并不是最佳选择。当<code>DataNode</code>启动时，它会扫描其本地文件系统，生成一个包含所有<code>HDFS</code>数据块(与每个本地文件相对应)的列表，并将此报告发送给<code>NameNode</code>：这是<code>Blockreport</code>。</p>\n<h3 id=\"7-通信协议\"><a href=\"#7-通信协议\" class=\"headerlink\" title=\"7. 通信协议\"></a>7. 通信协议</h3><p>所有的<code>HDFS</code>通信协议都是基于<code>TCP/IP</code>协议的。客户端建立到<code>NameNode</code>机器上的可配置TCP端口的连接。它使用<code>ClientProtocol</code>与<code>NameNode</code>交谈。<code>DataNode</code>使用<code>DataNode</code>协议与<code>NameNode</code>进行通信。远程过程调用(RPC)抽象包装客户端协议和数据节点协议。根据设计，<code>NameNode</code>永远不会启动任何RPC。而是只响应由<code>DataNode</code>或客户端发出的RPC请求。</p>\n<h3 id=\"8-稳定性\"><a href=\"#8-稳定性\" class=\"headerlink\" title=\"8. 稳定性\"></a>8. 稳定性</h3><p><code>HDFS</code>的主要目标是即使在出现故障时也能可靠地存储数据。三种常见的故障类型是<code>NameNode</code>故障，<code>DataNode</code>故障和网络分裂(network partitions)。</p>\n<h4 id=\"8-1-数据磁盘故障，心跳和重新复制\"><a href=\"#8-1-数据磁盘故障，心跳和重新复制\" class=\"headerlink\" title=\"8.1 数据磁盘故障，心跳和重新复制\"></a>8.1 数据磁盘故障，心跳和重新复制</h4><p>每个<code>DataNode</code>定期向<code>NameNode</code>发送一个<code>Heartbeat</code>消息。网络分裂可能导致一组<code>DataNode</code>与<code>NameNode</code>失去联系。<code>NameNode</code>通过丢失<code>Heartbeat</code>消息来检测这种情况。<code>NameNode</code>将最近没有<code>Heartbeats</code>的<code>DataNode</code>标记为死亡，并且不会将任何新的IO请求转发给它们。任何注册在标记为死亡的<code>DataNode</code>中的数据不再可用。<code>DataNode</code>死亡可能导致某些块的复制因子降到其指定值以下。<code>NameNode</code>不断跟踪哪些块需要复制，并在需要时启动复制。重新复制可能由于许多原因而产生：<code>DataNode</code>可能变得不可用，副本可能被破坏，<code>DataNode</code>上的硬盘可能出现故障，或者文件的复制因子可能需要增加。</p>\n<p>为了避免由于<code>DataNode</code>的状态震荡而导致的复制风暴，标记<code>DataNode</code>死亡的超时时间设置的比较保守(The time-out to mark DataNodes dead is conservatively long)(默认超过10分钟)。用户可以设置较短的时间间隔以将<code>DataNode</code>标记为陈旧，并避免陈旧节点在读取或按配置写入时性能出现负载(Users can set shorter interval to mark DataNodes as stale and avoid stale nodes on reading and/or writing by configuration for performance sensitive workloads)。</p>\n<h4 id=\"8-2-集群重新平衡\"><a href=\"#8-2-集群重新平衡\" class=\"headerlink\" title=\"8.2 集群重新平衡\"></a>8.2 集群重新平衡</h4><p><code>HDFS</code>体系结构与数据重新平衡方案兼容。如果某个<code>DataNode</code>上的可用空间低于某个阈值，那么会自动将数据从一个<code>DataNode</code>移动到另一个<code>DataNode</code>。对于特定文件突然高需求(sudden high demand)的情况下，可能会动态创建额外的副本并重新平衡集群中的其他数据。这些类型的数据重新平衡方案尚未实现。</p>\n<h4 id=\"8-3-数据完整性\"><a href=\"#8-3-数据完整性\" class=\"headerlink\" title=\"8.3 数据完整性\"></a>8.3 数据完整性</h4><p>从<code>DataNode</code>上获取的数据块可能会损坏。发生损坏可能是由存储设备故障，网络故障或软件错误引起。<code>HDFS</code>客户端实现了对<code>HDFS</code>上文件内容进行校验和检查。当客户端创建一个<code>HDFS</code>文件时，它会计算每个文件的对应数据块的校验和，并将这些校验和存储在同一个<code>HDFS</code>命名空间中的单独隐藏文件中。当客户端检索文件内容时，它会验证从每个<code>DataNode</code>收到的数据是否与存储在相关校验和文件中的校验和相匹配。如果不匹配，那么客户端可以选择从另一个具有该数据块副本的<code>DataNode</code>中检索该数据块。</p>\n<h4 id=\"8-4-元数据磁盘故障\"><a href=\"#8-4-元数据磁盘故障\" class=\"headerlink\" title=\"8.4 元数据磁盘故障\"></a>8.4 元数据磁盘故障</h4><p><code>FsImage</code>和<code>EditLog</code>编辑日志是<code>HDFS</code>中的中心数据结构。这些文件的损坏可能会导致<code>HDFS</code>实例无法正常运行。为此，<code>NameNode</code>可以配置为支持维护<code>FsImage</code>和<code>EditLog</code>编辑日志的多个副本。任何对<code>FsImage</code>或<code>EditLog</code>编辑日志的更新都会引起每个<code>FsImages</code>和<code>EditLogs</code>编辑日志同步更新。同步更新<code>FsImage</code>和<code>EditLog</code>编辑日志的多个副本可能会降低<code>NameNode</code>支持的每秒的命名空间事务的速度(degrade the rate of namespace transactions per second)。但是，这种降低是可以接受的，因为尽管<code>HDFS</code>应用程序实质上是非常密集的数据，但是它们也不是元数据密集型的。当<code>NameNode</code>重新启动时，它会选择最新的一致的<code>FsImage</code>和<code>EditLog</code>编辑日志来使用。</p>\n<p>另一个增强防御故障的方法是使用多个<code>NameNode</code>以启用高可用性，或者使用<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html\" target=\"_blank\" rel=\"noopener\"><code>NFS</code>上的共享存储</a>或使用<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html\" target=\"_blank\" rel=\"noopener\">分布式编辑日志</a>(称为<code>Journal</code>)。后者是推荐的方法。</p>\n<h4 id=\"8-5-快照\"><a href=\"#8-5-快照\" class=\"headerlink\" title=\"8.5 快照\"></a>8.5 快照</h4><p><a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html\" target=\"_blank\" rel=\"noopener\">快照</a>支持在特定时刻存储数据副本。快照功能的一种用法是将损坏的<code>HDFS</code>实例回滚到先前已知的良好时间点。</p>\n<h3 id=\"9-数据组织\"><a href=\"#9-数据组织\" class=\"headerlink\" title=\"9. 数据组织\"></a>9. 数据组织</h3><h4 id=\"9-1-数据块\"><a href=\"#9-1-数据块\" class=\"headerlink\" title=\"9.1 数据块\"></a>9.1 数据块</h4><p><code>HDFS</code>为支持大文件而设计的。与<code>HDFS</code>兼容的应用程序是处理大型数据集的应用程序。这些应用程序只写入数据一次，但是读取一次或多次，并读取速度要求满足流式处理速度。<code>HDFS</code>支持在文件上一次写入多次读取语义。<code>HDFS</code>使用的一般块大小为128 MB。因此，一个<code>HDFS</code>文件被分成多个128MB的块，如果可能的话，每个块将保存在不同的<code>DataNode</code>上。</p>\n<h4 id=\"9-2-分阶段\"><a href=\"#9-2-分阶段\" class=\"headerlink\" title=\"9.2 分阶段\"></a>9.2 分阶段</h4><p>客户端创建文件的请求不会立即到达<code>NameNode</code>。事实上，最初<code>HDFS</code>客户端将文件数据缓存到本地缓冲区。应用程序写入重定向到本地缓冲区。当本地文件累积超过一个块大小的数据时，客户端才会联系<code>NameNode</code>。<code>NameNode</code>将文件名插入到文件系统层次结构中，并为其分配一个数据块。<code>NameNode</code>将<code>DataNode</code>和目标数据块的标识和返回给客户请求。然后，客户端将本地缓冲区中的数据块保存到指定的<code>DataNode</code>上。当文件关闭时，本地缓冲区中剩余的未保存数据也被传输到<code>DataNode</code>。客户端然后告诉<code>NameNode</code>该文件已关闭。此时，<code>NameNode</code>将文件创建操作提交到持久化存储中。如果<code>NameNode</code>在文件关闭之前崩溃，那么文件会丢失。</p>\n<p>在仔细考虑在<code>HDFS</code>上运行的目标应用程序之后，采用了上述方法。这些应用程序需要流式写入文件。如果客户端直接写入远程文件目录而没有在客户端进行任何缓冲，那么网络速度和网络拥塞会大大影响吞吐量。这种方法并非没有先例。较早的分布式文件系统，例如<code>AFS</code>，已经使用客户端缓存来提高性能。<code>POSIX</code>的要求已经放宽，以实现更高的数据传输性能。</p>\n<h3 id=\"9-3-副本流水线\"><a href=\"#9-3-副本流水线\" class=\"headerlink\" title=\"9.3 副本流水线\"></a>9.3 副本流水线</h3><p>当客户端将数据写入<code>HDFS</code>文件时，首先将数据写入本地缓冲区，如上一节所述。假设<code>HDFS</code>文件复制因子为3。当本地缓冲区累积了一个块的用户数据时，客户端从<code>NameNode</code>中检索<code>DataNode</code>列表。该列表包含保存数据的数据块副本的<code>DataNode</code>。客户端然后将数据块刷新到第一个<code>DataNode</code>。第一个<code>DataNode</code>开始接收一小部分数据，将这一小部分数据写入其本地存储库，然后传输到列表中的第二个<code>DataNode</code>。第二个<code>DataNode</code>依次接收数据块的每一部分数据，将其写入存储库，然后再将刷新到第三个<code>DataNode</code>。最后，第三个<code>DataNode</code>将数据写入其本地存储库。因此，<code>DataNode</code>可以以流水线的方式从前一个<code>DataNode</code>接收数据，同时将数据转发到流水线中的下一个<code>DataNode</code>。因此，数据从一个<code>DataNode</code>流到下一个。</p>\n<h3 id=\"10-访问\"><a href=\"#10-访问\" class=\"headerlink\" title=\"10. 访问\"></a>10. 访问</h3><p>应用程序可以以多种不同的方式访问<code>HDFS</code>。<code>HDFS</code>为应用程序提供了一个<code>FileSystem Java API</code>。<code>Java API</code>和<code>REST API</code>的C语言包装器也可以使用。另外还有一个HTTP浏览器(HTTP browser)，也可以用来浏览<code>HDFS</code>实例的文件。通过使用<code>NFS</code>网关，可以将<code>HDFS</code>作为客户端本地文件系统的一部分。</p>\n<h4 id=\"10-1-FS-Shell\"><a href=\"#10-1-FS-Shell\" class=\"headerlink\" title=\"10.1 FS Shell\"></a>10.1 FS Shell</h4><p><code>HDFS</code>将用户数据以文件和目录的形式进行组织。它提供了一个名为<code>FS shell</code>的命令行接口，让用户可以与<code>HDFS</code>中的数据进行交互。这个命令集的语法类似于用户已经熟悉的其他<code>shell</code>(例如<code>bash</code>，<code>csh</code>)。 以下是一些示例操作/命令对：</p>\n<table>\n<thead>\n<tr>\n<th>操作</th>\n<th>命令</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>创建<code>/foodir</code>目录</td>\n<td><code>bin/hadoop dfs -mkdir /foodir</code></td>\n</tr>\n<tr>\n<td>删除目录<code>/foodir</code></td>\n<td><code>bin/hadoop fs -rm -R /foodir</code></td>\n</tr>\n<tr>\n<td>查看<code>/foodir/myfile.txt</code>中内容</td>\n<td><code>bin/hadoop dfs -cat /foodir/myfile.txt</code></td>\n</tr>\n</tbody>\n</table>\n<p><code>FS shell</code>针对需要脚本语言与存储数据进行交互的应用程序。</p>\n<h4 id=\"10-2-DFSAdmin\"><a href=\"#10-2-DFSAdmin\" class=\"headerlink\" title=\"10.2 DFSAdmin\"></a>10.2 DFSAdmin</h4><p><code>DFSAdmin</code>命令集用于管理<code>HDFS</code>集群。这些是仅能由<code>HDFS</code>管理员使用的命令。以下是一些示例操作/命令对：</p>\n<table>\n<thead>\n<tr>\n<th>操作</th>\n<th>命令</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>使集群处于安全模式</td>\n<td><code>bin/hdfs dfsadmin -safemode enter</code></td>\n</tr>\n<tr>\n<td>生成<code>DataNode</code>列表</td>\n<td><code>bin/hdfs dfsadmin -report</code></td>\n</tr>\n<tr>\n<td>重新投放或停用<code>DataNode(s)</code></td>\n<td><code>bin/hdfs dfsadmin -refreshNodes</code></td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"10-3-浏览器接口\"><a href=\"#10-3-浏览器接口\" class=\"headerlink\" title=\"10.3 浏览器接口\"></a>10.3 浏览器接口</h4><p>一个典型的<code>HDFS</code>安装会配置一个<code>Web</code>服务器，通过一个可配置的<code>TCP</code>端口公开<code>HDFS</code>命名空间。这允许用户使用Web浏览器浏览<code>HDFS</code>命名空间并查看其文件的内容。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Hadoop版本: 2.7.3</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop SSH免密码登录","date":"2016-12-29T03:13:01.000Z","_content":"\n### 1. 创建ssh-key\n\n这里我们采用rsa方式，使用如下命令：\n```\nxiaosi@xiaosi:~$ ssh-keygen -t rsa -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\nCreated directory '/home/xiaosi/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/xiaosi/.ssh/id_rsa.\nYour public key has been saved in /home/xiaosi/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:n/sFaAT94A/xxxxxxxxxxxxxxxxxxxxxxx xiaosi@xiaosi\nThe key's randomart image is:\n+---[xxxxx]----+\n|        o= .. .. |\n|        o.= ..  .|\n|         *.* o  .|\n|        +.4.=E+..|\n|       .SBo=. h+ |\n|        ogo..oo. |\n|          or +j..|\n|          ...+o=.|\n|          ... o=+|\n+----[xxxxx]-----+\n```\n备注：\n```\n这里会提示输入pass phrase，不需要输入任何字符，回车即可。\n```\n\n### 2. 生成authorized_keys文件\n```\nxiaosi@xiaosi:~$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n```\n\n记得要把`authorized_keys`文件放到`.ssh`目录下，与`rsa`等文件放在一起，否则免登录失败，debug如下（ssh -vvv localhost进行调试，查找错误原因）：\n```\nxiaosi@xiaosi:~$ ssh -vvv localhost\nOpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips  1 Mar 2016\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: /etc/ssh/ssh_config line 19: Applying options for *\ndebug2: resolving \"localhost\" port 22\ndebug2: ssh_connect_direct: needpriv 0\ndebug1: Connecting to localhost [127.0.0.1] port 22.\ndebug1: Connection established.\ndebug1: identity file /home/xiaosi/.ssh/id_rsa type 1\n\n...\n\ndebug2: we sent a publickey packet, wait for reply\ndebug3: receive packet: type 51\ndebug1: Authentications that can continue: publickey,password\ndebug1: Trying private key: /home/xiaosi/.ssh/id_dsa\ndebug3: no such identity: /home/xiaosi/.ssh/id_dsa: No such file or directory\ndebug1: Trying private key: /home/xiaosi/.ssh/id_ecdsa\ndebug3: no such identity: /home/xiaosi/.ssh/id_ecdsa: No such file or directory\ndebug1: Trying private key: /home/xiaosi/.ssh/id_ed25519\ndebug3: no such identity: /home/xiaosi/.ssh/id_ed25519: No such file or directory\ndebug2: we did not send a packet, disable method\ndebug3: authmethod_lookup password\ndebug3: remaining preferred: ,password\ndebug3: authmethod_is_enabled password\ndebug1: Next authentication method: password\nxiaosi@localhost's password:\n```\n\n### 3. 验证\n```\nxiaosi@xiaosi:~$ ssh localhost\nThe authenticity of host 'localhost (127.0.0.1)' can't be established.\nECDSA key fingerprint is SHA256:378enl3ckhdpObP8fnsHr1EXz4d1q2Jde+jUplkub/Y.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\nsign_and_send_pubkey: signing failed: agent refused operation\nxiaosi@localhost's password:\n```\n\n### 4. authorized_keys权限\n\n我们可以看到还是让我输入密码，很大可能是`authorized_keys`文件权限的问题，我们给该文件赋予一定权限：\n```\nxiaosi@xiaosi:~$ chmod 600 ~/.ssh/authorized_keys\n```\n再次验证：\n```\nxiaosi@xiaosi:~$ ssh localhost\nWelcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-24-generic x86_64)\n * Documentation:  https://help.ubuntu.com/\n0 个可升级软件包。\n0 个安全更新。\nLast login: Thu Jun 16 08:05:50 2016 from 127.0.0.1\n```\n到此表示OK了。\n\n\n\n备注：\n```\n第一次需要输入密码，以后再次登陆就不需要输入密码了。\n```\n\n\n有更明白的小伙伴可以指导一下。。。。。。\n","source":"_posts/Hadoop/[Hadoop]Hadoop SSH免密码登录.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop SSH免密码登录\ndate: 2016-12-29 11:13:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. 创建ssh-key\n\n这里我们采用rsa方式，使用如下命令：\n```\nxiaosi@xiaosi:~$ ssh-keygen -t rsa -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\nCreated directory '/home/xiaosi/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/xiaosi/.ssh/id_rsa.\nYour public key has been saved in /home/xiaosi/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:n/sFaAT94A/xxxxxxxxxxxxxxxxxxxxxxx xiaosi@xiaosi\nThe key's randomart image is:\n+---[xxxxx]----+\n|        o= .. .. |\n|        o.= ..  .|\n|         *.* o  .|\n|        +.4.=E+..|\n|       .SBo=. h+ |\n|        ogo..oo. |\n|          or +j..|\n|          ...+o=.|\n|          ... o=+|\n+----[xxxxx]-----+\n```\n备注：\n```\n这里会提示输入pass phrase，不需要输入任何字符，回车即可。\n```\n\n### 2. 生成authorized_keys文件\n```\nxiaosi@xiaosi:~$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n```\n\n记得要把`authorized_keys`文件放到`.ssh`目录下，与`rsa`等文件放在一起，否则免登录失败，debug如下（ssh -vvv localhost进行调试，查找错误原因）：\n```\nxiaosi@xiaosi:~$ ssh -vvv localhost\nOpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips  1 Mar 2016\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: /etc/ssh/ssh_config line 19: Applying options for *\ndebug2: resolving \"localhost\" port 22\ndebug2: ssh_connect_direct: needpriv 0\ndebug1: Connecting to localhost [127.0.0.1] port 22.\ndebug1: Connection established.\ndebug1: identity file /home/xiaosi/.ssh/id_rsa type 1\n\n...\n\ndebug2: we sent a publickey packet, wait for reply\ndebug3: receive packet: type 51\ndebug1: Authentications that can continue: publickey,password\ndebug1: Trying private key: /home/xiaosi/.ssh/id_dsa\ndebug3: no such identity: /home/xiaosi/.ssh/id_dsa: No such file or directory\ndebug1: Trying private key: /home/xiaosi/.ssh/id_ecdsa\ndebug3: no such identity: /home/xiaosi/.ssh/id_ecdsa: No such file or directory\ndebug1: Trying private key: /home/xiaosi/.ssh/id_ed25519\ndebug3: no such identity: /home/xiaosi/.ssh/id_ed25519: No such file or directory\ndebug2: we did not send a packet, disable method\ndebug3: authmethod_lookup password\ndebug3: remaining preferred: ,password\ndebug3: authmethod_is_enabled password\ndebug1: Next authentication method: password\nxiaosi@localhost's password:\n```\n\n### 3. 验证\n```\nxiaosi@xiaosi:~$ ssh localhost\nThe authenticity of host 'localhost (127.0.0.1)' can't be established.\nECDSA key fingerprint is SHA256:378enl3ckhdpObP8fnsHr1EXz4d1q2Jde+jUplkub/Y.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\nsign_and_send_pubkey: signing failed: agent refused operation\nxiaosi@localhost's password:\n```\n\n### 4. authorized_keys权限\n\n我们可以看到还是让我输入密码，很大可能是`authorized_keys`文件权限的问题，我们给该文件赋予一定权限：\n```\nxiaosi@xiaosi:~$ chmod 600 ~/.ssh/authorized_keys\n```\n再次验证：\n```\nxiaosi@xiaosi:~$ ssh localhost\nWelcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-24-generic x86_64)\n * Documentation:  https://help.ubuntu.com/\n0 个可升级软件包。\n0 个安全更新。\nLast login: Thu Jun 16 08:05:50 2016 from 127.0.0.1\n```\n到此表示OK了。\n\n\n\n备注：\n```\n第一次需要输入密码，以后再次登陆就不需要输入密码了。\n```\n\n\n有更明白的小伙伴可以指导一下。。。。。。\n","slug":"Hadoop/[Hadoop]Hadoop SSH免密码登录","published":1,"updated":"2018-01-29T09:36:59.630Z","comments":1,"photos":[],"link":"","_id":"cje58tit2003fordb4muv5js7","content":"<h3 id=\"1-创建ssh-key\"><a href=\"#1-创建ssh-key\" class=\"headerlink\" title=\"1. 创建ssh-key\"></a>1. 创建ssh-key</h3><p>这里我们采用rsa方式，使用如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh-keygen -t rsa -f ~/.ssh/id_rsa</span><br><span class=\"line\">Generating public/private rsa key pair.</span><br><span class=\"line\">Created directory &apos;/home/xiaosi/.ssh&apos;.</span><br><span class=\"line\">Enter passphrase (empty for no passphrase):</span><br><span class=\"line\">Enter same passphrase again:</span><br><span class=\"line\">Your identification has been saved in /home/xiaosi/.ssh/id_rsa.</span><br><span class=\"line\">Your public key has been saved in /home/xiaosi/.ssh/id_rsa.pub.</span><br><span class=\"line\">The key fingerprint is:</span><br><span class=\"line\">SHA256:n/sFaAT94A/xxxxxxxxxxxxxxxxxxxxxxx xiaosi@xiaosi</span><br><span class=\"line\">The key&apos;s randomart image is:</span><br><span class=\"line\">+---[xxxxx]----+</span><br><span class=\"line\">|        o= .. .. |</span><br><span class=\"line\">|        o.= ..  .|</span><br><span class=\"line\">|         *.* o  .|</span><br><span class=\"line\">|        +.4.=E+..|</span><br><span class=\"line\">|       .SBo=. h+ |</span><br><span class=\"line\">|        ogo..oo. |</span><br><span class=\"line\">|          or +j..|</span><br><span class=\"line\">|          ...+o=.|</span><br><span class=\"line\">|          ... o=+|</span><br><span class=\"line\">+----[xxxxx]-----+</span><br></pre></td></tr></table></figure></p>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">这里会提示输入pass phrase，不需要输入任何字符，回车即可。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-生成authorized-keys文件\"><a href=\"#2-生成authorized-keys文件\" class=\"headerlink\" title=\"2. 生成authorized_keys文件\"></a>2. 生成authorized_keys文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>\n<p>记得要把<code>authorized_keys</code>文件放到<code>.ssh</code>目录下，与<code>rsa</code>等文件放在一起，否则免登录失败，debug如下（ssh -vvv localhost进行调试，查找错误原因）：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh -vvv localhost</span><br><span class=\"line\">OpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips  1 Mar 2016</span><br><span class=\"line\">debug1: Reading configuration data /etc/ssh/ssh_config</span><br><span class=\"line\">debug1: /etc/ssh/ssh_config line 19: Applying options for *</span><br><span class=\"line\">debug2: resolving &quot;localhost&quot; port 22</span><br><span class=\"line\">debug2: ssh_connect_direct: needpriv 0</span><br><span class=\"line\">debug1: Connecting to localhost [127.0.0.1] port 22.</span><br><span class=\"line\">debug1: Connection established.</span><br><span class=\"line\">debug1: identity file /home/xiaosi/.ssh/id_rsa type 1</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">debug2: we sent a publickey packet, wait for reply</span><br><span class=\"line\">debug3: receive packet: type 51</span><br><span class=\"line\">debug1: Authentications that can continue: publickey,password</span><br><span class=\"line\">debug1: Trying private key: /home/xiaosi/.ssh/id_dsa</span><br><span class=\"line\">debug3: no such identity: /home/xiaosi/.ssh/id_dsa: No such file or directory</span><br><span class=\"line\">debug1: Trying private key: /home/xiaosi/.ssh/id_ecdsa</span><br><span class=\"line\">debug3: no such identity: /home/xiaosi/.ssh/id_ecdsa: No such file or directory</span><br><span class=\"line\">debug1: Trying private key: /home/xiaosi/.ssh/id_ed25519</span><br><span class=\"line\">debug3: no such identity: /home/xiaosi/.ssh/id_ed25519: No such file or directory</span><br><span class=\"line\">debug2: we did not send a packet, disable method</span><br><span class=\"line\">debug3: authmethod_lookup password</span><br><span class=\"line\">debug3: remaining preferred: ,password</span><br><span class=\"line\">debug3: authmethod_is_enabled password</span><br><span class=\"line\">debug1: Next authentication method: password</span><br><span class=\"line\">xiaosi@localhost&apos;s password:</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-验证\"><a href=\"#3-验证\" class=\"headerlink\" title=\"3. 验证\"></a>3. 验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh localhost</span><br><span class=\"line\">The authenticity of host &apos;localhost (127.0.0.1)&apos; can&apos;t be established.</span><br><span class=\"line\">ECDSA key fingerprint is SHA256:378enl3ckhdpObP8fnsHr1EXz4d1q2Jde+jUplkub/Y.</span><br><span class=\"line\">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class=\"line\">Warning: Permanently added &apos;localhost&apos; (ECDSA) to the list of known hosts.</span><br><span class=\"line\">sign_and_send_pubkey: signing failed: agent refused operation</span><br><span class=\"line\">xiaosi@localhost&apos;s password:</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-authorized-keys权限\"><a href=\"#4-authorized-keys权限\" class=\"headerlink\" title=\"4. authorized_keys权限\"></a>4. authorized_keys权限</h3><p>我们可以看到还是让我输入密码，很大可能是<code>authorized_keys</code>文件权限的问题，我们给该文件赋予一定权限：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>\n<p>再次验证：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh localhost</span><br><span class=\"line\">Welcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-24-generic x86_64)</span><br><span class=\"line\"> * Documentation:  https://help.ubuntu.com/</span><br><span class=\"line\">0 个可升级软件包。</span><br><span class=\"line\">0 个安全更新。</span><br><span class=\"line\">Last login: Thu Jun 16 08:05:50 2016 from 127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>到此表示OK了。</p>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">第一次需要输入密码，以后再次登陆就不需要输入密码了。</span><br></pre></td></tr></table></figure></p>\n<p>有更明白的小伙伴可以指导一下。。。。。。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-创建ssh-key\"><a href=\"#1-创建ssh-key\" class=\"headerlink\" title=\"1. 创建ssh-key\"></a>1. 创建ssh-key</h3><p>这里我们采用rsa方式，使用如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh-keygen -t rsa -f ~/.ssh/id_rsa</span><br><span class=\"line\">Generating public/private rsa key pair.</span><br><span class=\"line\">Created directory &apos;/home/xiaosi/.ssh&apos;.</span><br><span class=\"line\">Enter passphrase (empty for no passphrase):</span><br><span class=\"line\">Enter same passphrase again:</span><br><span class=\"line\">Your identification has been saved in /home/xiaosi/.ssh/id_rsa.</span><br><span class=\"line\">Your public key has been saved in /home/xiaosi/.ssh/id_rsa.pub.</span><br><span class=\"line\">The key fingerprint is:</span><br><span class=\"line\">SHA256:n/sFaAT94A/xxxxxxxxxxxxxxxxxxxxxxx xiaosi@xiaosi</span><br><span class=\"line\">The key&apos;s randomart image is:</span><br><span class=\"line\">+---[xxxxx]----+</span><br><span class=\"line\">|        o= .. .. |</span><br><span class=\"line\">|        o.= ..  .|</span><br><span class=\"line\">|         *.* o  .|</span><br><span class=\"line\">|        +.4.=E+..|</span><br><span class=\"line\">|       .SBo=. h+ |</span><br><span class=\"line\">|        ogo..oo. |</span><br><span class=\"line\">|          or +j..|</span><br><span class=\"line\">|          ...+o=.|</span><br><span class=\"line\">|          ... o=+|</span><br><span class=\"line\">+----[xxxxx]-----+</span><br></pre></td></tr></table></figure></p>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">这里会提示输入pass phrase，不需要输入任何字符，回车即可。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-生成authorized-keys文件\"><a href=\"#2-生成authorized-keys文件\" class=\"headerlink\" title=\"2. 生成authorized_keys文件\"></a>2. 生成authorized_keys文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>\n<p>记得要把<code>authorized_keys</code>文件放到<code>.ssh</code>目录下，与<code>rsa</code>等文件放在一起，否则免登录失败，debug如下（ssh -vvv localhost进行调试，查找错误原因）：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh -vvv localhost</span><br><span class=\"line\">OpenSSH_7.2p2 Ubuntu-4ubuntu1, OpenSSL 1.0.2g-fips  1 Mar 2016</span><br><span class=\"line\">debug1: Reading configuration data /etc/ssh/ssh_config</span><br><span class=\"line\">debug1: /etc/ssh/ssh_config line 19: Applying options for *</span><br><span class=\"line\">debug2: resolving &quot;localhost&quot; port 22</span><br><span class=\"line\">debug2: ssh_connect_direct: needpriv 0</span><br><span class=\"line\">debug1: Connecting to localhost [127.0.0.1] port 22.</span><br><span class=\"line\">debug1: Connection established.</span><br><span class=\"line\">debug1: identity file /home/xiaosi/.ssh/id_rsa type 1</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">debug2: we sent a publickey packet, wait for reply</span><br><span class=\"line\">debug3: receive packet: type 51</span><br><span class=\"line\">debug1: Authentications that can continue: publickey,password</span><br><span class=\"line\">debug1: Trying private key: /home/xiaosi/.ssh/id_dsa</span><br><span class=\"line\">debug3: no such identity: /home/xiaosi/.ssh/id_dsa: No such file or directory</span><br><span class=\"line\">debug1: Trying private key: /home/xiaosi/.ssh/id_ecdsa</span><br><span class=\"line\">debug3: no such identity: /home/xiaosi/.ssh/id_ecdsa: No such file or directory</span><br><span class=\"line\">debug1: Trying private key: /home/xiaosi/.ssh/id_ed25519</span><br><span class=\"line\">debug3: no such identity: /home/xiaosi/.ssh/id_ed25519: No such file or directory</span><br><span class=\"line\">debug2: we did not send a packet, disable method</span><br><span class=\"line\">debug3: authmethod_lookup password</span><br><span class=\"line\">debug3: remaining preferred: ,password</span><br><span class=\"line\">debug3: authmethod_is_enabled password</span><br><span class=\"line\">debug1: Next authentication method: password</span><br><span class=\"line\">xiaosi@localhost&apos;s password:</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-验证\"><a href=\"#3-验证\" class=\"headerlink\" title=\"3. 验证\"></a>3. 验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh localhost</span><br><span class=\"line\">The authenticity of host &apos;localhost (127.0.0.1)&apos; can&apos;t be established.</span><br><span class=\"line\">ECDSA key fingerprint is SHA256:378enl3ckhdpObP8fnsHr1EXz4d1q2Jde+jUplkub/Y.</span><br><span class=\"line\">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class=\"line\">Warning: Permanently added &apos;localhost&apos; (ECDSA) to the list of known hosts.</span><br><span class=\"line\">sign_and_send_pubkey: signing failed: agent refused operation</span><br><span class=\"line\">xiaosi@localhost&apos;s password:</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-authorized-keys权限\"><a href=\"#4-authorized-keys权限\" class=\"headerlink\" title=\"4. authorized_keys权限\"></a>4. authorized_keys权限</h3><p>我们可以看到还是让我输入密码，很大可能是<code>authorized_keys</code>文件权限的问题，我们给该文件赋予一定权限：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>\n<p>再次验证：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@xiaosi:~$ ssh localhost</span><br><span class=\"line\">Welcome to Ubuntu 16.04 LTS (GNU/Linux 4.4.0-24-generic x86_64)</span><br><span class=\"line\"> * Documentation:  https://help.ubuntu.com/</span><br><span class=\"line\">0 个可升级软件包。</span><br><span class=\"line\">0 个安全更新。</span><br><span class=\"line\">Last login: Thu Jun 16 08:05:50 2016 from 127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>到此表示OK了。</p>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">第一次需要输入密码，以后再次登陆就不需要输入密码了。</span><br></pre></td></tr></table></figure></p>\n<p>有更明白的小伙伴可以指导一下。。。。。。</p>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop 安装与启动","date":"2016-12-29T03:01:01.000Z","_content":"\n### 1. SSH\n\n参考博文：[Hadoop]SSH免密码登录以及失败解决方案（http://blog.csdn.net/sunnyyoona/article/details/51689041#t1）\n\n### 2. 下载\n\n(1) 直接从官网上下载 http://hadoop.apache.org/releases.html\n\n(2) 使用命令行下载：\n```\nxiaosi@yoona:~$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n--2016-06-16 08:40:07--  http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n正在解析主机 mirrors.hust.edu.cn (mirrors.hust.edu.cn)... 202.114.18.160\n正在连接 mirrors.hust.edu.cn (mirrors.hust.edu.cn)|202.114.18.160|:80... 已连接。\n已发出 HTTP 请求，正在等待回应... 200 OK\n长度： 196015975 (187M) [application/octet-stream]\n正在保存至: “hadoop-2.6.4.tar.gz”\n```\n\n### 3. 解压缩Hadoop包\n\n解压位于根目录`/`文件夹下的`hadoop-2.7.3.tar.gz`到`~/opt`文件夹下\n```\nxiaosi@yoona:~$ tar -zxvf hadoop-2.7.3.tar.gz -C opt/\n```\n\n### 4. 配置\n\n配置文件都位于安装目录下的`/etc/hadoop`文件夹下：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3/etc/hadoop$ ls\ncapacity-scheduler.xml  hadoop-env.sh              httpfs-log4j.properties  log4j.properties            mapred-site.xml.template\nconfiguration.xsl       hadoop-metrics2.properties  httpfs-signature.secret  log4j.properties          slaves\ncontainer-executor.cfg  hadoop-metrics.properties   httpfs-site.xml          mapred-env.cmd              ssl-client.xml.example\ncore-site.xml           hadoop-policy.xml           kms-acls.xml             mapred-env.sh               ssl-server.xml.example\ncore-site.xml          hdfs-site.xml               kms-env.sh               mapred-queues.xml.template  yarn-env.cmd\nhadoop-env.cmd          hdfs-site.xml              kms-log4j.properties     mapred-site.xml             yarn-env.sh\nhadoop-env.sh           httpfs-env.sh               kms-site.xml             mapred-site.xml            yarn-site.xml\n```\nHadoop的各个组件均可利用`XML`文件进行配置。`core-site.xml`文件用于配置`Common`组件的属性，`hdfs-site.xml`文件用于配置HDFS属性，而`mapred-site.xml`文件则用于配置`MapReduce`属性。\n\n备注：\n```\nHadoop早期版本采用一个配置文件hadoop-site.xml来配置Common，HDFS和MapReduce组件。从0.20.0版本开始该文件以分为三，各对应一个组件。\n```\n\n#### 4.1 配置core-site.xml\n\n`core-site.xml` 配置如下：\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n   <property>\n      <name>hadoop.tmp.dir</name>\n      <value>/home/${user.name}/tmp/hadoop</value>\n      <description>Abase for other temporary directories.</description>\n   </property>\n   <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://localhost:9000</value>\n   </property>\n\n    <property>\n       <name>hadoop.proxyuser.xiaosi.hosts</name>\n       <value>*</value>\n       <description>The superuser can connect only from host1 and host2 to impersonate a user</description>\n    </property>\n    <property>\n       <name>hadoop.proxyuser.xiaosi.groups</name>\n       <value>*</value>\n       <description>Allow the superuser oozie to impersonate any members of the group group1 and group2</description>\n    </property>\n</configuration>\n```\n\n#### 4.2 配置hdfs-site.xml\n\n`hdfs-site.xml`配置如下：\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n   <property>\n      <name>dfs.replication</name>\n      <value>1</value>\n   </property>\n   <property>\n      <name>dfs.namenode.name.dir</name>\n      <value>file:/home/xiaosi/tmp/hadoop/dfs/name</value>\n   </property>\n   <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>file:/home/xiaosi/tmp/hadoop/dfs/data</value>\n   </property>\n</configuration>\n```\n\n#### 4.3 配置 mapred-site.xml\n\n`mapred-site.xml`配置如下：\n```xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n   <property>\n      <name>mapred.job.tracker</name>\n      <value>localhost:9001</value>\n   </property>\n</configuration>\n```\n\n运行`Hadoop`的时候可能会找不到`jdk`，需要我们修改`hadoop.env.sh`脚本文件，唯一需要修改的环境变量就是`JAVE_HOME`，其他选项都是可选的：\n```\nexport JAVA_HOME=/home/xiaosi/opt/jdk-1.8.0\n```\n\n### 5. 运行\n\n#### 5.1 初始化HDFS系统\n\n在配置完成后，运行`hadoop`前，要初始化`HDFS`系统，在`bin/`目录下执行如下命令：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ ./bin/hdfs namenode -format\n```\n#### 5.2 启动\n\n开启`NameNode`和`DataNode`守护进程：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ ./sbin/start-dfs.sh\nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-namenode-yoona.out\nlocalhost: starting datanode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-datanode-yoona.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-secondarynamenode-yoona.out\n```\n通过`jps`命令查看`namenode`和`datanode`是否已经启动起来：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ jps\n13400 SecondaryNameNode\n13035 NameNode\n13197 DataNode\n13535 Jps\n```\n从启动日志我们可以知道，日志信息存储在`hadoop-2.7.3/logs/`目录下，如果启动过程中有任何问题，可以通过查看日志来确认问题原因。\n\n### 6. Yarn模式安装\n\n#### 6.1 配置\n\n修改`yarn-site.xml`，添加如下配置：\n```xml\n<configuration>\n   <property>\n      <name>yarn.nodemanager.aux-services</name>\n      <value>mapreduce.shuffle</value>\n   </property>\n   <property>\n      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n      <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n   </property>\n</configuration>\n```\n修改`mapred-site.xml`，做如下修改：\n```xml\n<property>\n   <name>mapreduce.framework.name</name>\n   <value>yarn</value>\n</property>\n```\n\n本地模式下是`value`值是`local`，`yarn`模式下`value`值是`yarn`。\n\n#### 6.2 启动yarn\n\n启动`yarn`：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/start-yarn.sh\nstarting yarn daemons\nstarting resourcemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-resourcemanager-yoona.out\nlocalhost: starting nodemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-nodemanager-yoona.out\n```\n关闭yarn:\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/stop-yarn.sh\nstopping yarn daemons\nstopping resourcemanager\nlocalhost: stopping nodemanager\n```\n#### 6.3 检查是否运行成功\n\n打开浏览器，输入：http://localhost:8088/cluster\n","source":"_posts/Hadoop/[Hadoop]Hadoop 安装与启动.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop 安装与启动\ndate: 2016-12-29 11:01:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. SSH\n\n参考博文：[Hadoop]SSH免密码登录以及失败解决方案（http://blog.csdn.net/sunnyyoona/article/details/51689041#t1）\n\n### 2. 下载\n\n(1) 直接从官网上下载 http://hadoop.apache.org/releases.html\n\n(2) 使用命令行下载：\n```\nxiaosi@yoona:~$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n--2016-06-16 08:40:07--  http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n正在解析主机 mirrors.hust.edu.cn (mirrors.hust.edu.cn)... 202.114.18.160\n正在连接 mirrors.hust.edu.cn (mirrors.hust.edu.cn)|202.114.18.160|:80... 已连接。\n已发出 HTTP 请求，正在等待回应... 200 OK\n长度： 196015975 (187M) [application/octet-stream]\n正在保存至: “hadoop-2.6.4.tar.gz”\n```\n\n### 3. 解压缩Hadoop包\n\n解压位于根目录`/`文件夹下的`hadoop-2.7.3.tar.gz`到`~/opt`文件夹下\n```\nxiaosi@yoona:~$ tar -zxvf hadoop-2.7.3.tar.gz -C opt/\n```\n\n### 4. 配置\n\n配置文件都位于安装目录下的`/etc/hadoop`文件夹下：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3/etc/hadoop$ ls\ncapacity-scheduler.xml  hadoop-env.sh              httpfs-log4j.properties  log4j.properties            mapred-site.xml.template\nconfiguration.xsl       hadoop-metrics2.properties  httpfs-signature.secret  log4j.properties          slaves\ncontainer-executor.cfg  hadoop-metrics.properties   httpfs-site.xml          mapred-env.cmd              ssl-client.xml.example\ncore-site.xml           hadoop-policy.xml           kms-acls.xml             mapred-env.sh               ssl-server.xml.example\ncore-site.xml          hdfs-site.xml               kms-env.sh               mapred-queues.xml.template  yarn-env.cmd\nhadoop-env.cmd          hdfs-site.xml              kms-log4j.properties     mapred-site.xml             yarn-env.sh\nhadoop-env.sh           httpfs-env.sh               kms-site.xml             mapred-site.xml            yarn-site.xml\n```\nHadoop的各个组件均可利用`XML`文件进行配置。`core-site.xml`文件用于配置`Common`组件的属性，`hdfs-site.xml`文件用于配置HDFS属性，而`mapred-site.xml`文件则用于配置`MapReduce`属性。\n\n备注：\n```\nHadoop早期版本采用一个配置文件hadoop-site.xml来配置Common，HDFS和MapReduce组件。从0.20.0版本开始该文件以分为三，各对应一个组件。\n```\n\n#### 4.1 配置core-site.xml\n\n`core-site.xml` 配置如下：\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n   <property>\n      <name>hadoop.tmp.dir</name>\n      <value>/home/${user.name}/tmp/hadoop</value>\n      <description>Abase for other temporary directories.</description>\n   </property>\n   <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://localhost:9000</value>\n   </property>\n\n    <property>\n       <name>hadoop.proxyuser.xiaosi.hosts</name>\n       <value>*</value>\n       <description>The superuser can connect only from host1 and host2 to impersonate a user</description>\n    </property>\n    <property>\n       <name>hadoop.proxyuser.xiaosi.groups</name>\n       <value>*</value>\n       <description>Allow the superuser oozie to impersonate any members of the group group1 and group2</description>\n    </property>\n</configuration>\n```\n\n#### 4.2 配置hdfs-site.xml\n\n`hdfs-site.xml`配置如下：\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n   <property>\n      <name>dfs.replication</name>\n      <value>1</value>\n   </property>\n   <property>\n      <name>dfs.namenode.name.dir</name>\n      <value>file:/home/xiaosi/tmp/hadoop/dfs/name</value>\n   </property>\n   <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>file:/home/xiaosi/tmp/hadoop/dfs/data</value>\n   </property>\n</configuration>\n```\n\n#### 4.3 配置 mapred-site.xml\n\n`mapred-site.xml`配置如下：\n```xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n   <property>\n      <name>mapred.job.tracker</name>\n      <value>localhost:9001</value>\n   </property>\n</configuration>\n```\n\n运行`Hadoop`的时候可能会找不到`jdk`，需要我们修改`hadoop.env.sh`脚本文件，唯一需要修改的环境变量就是`JAVE_HOME`，其他选项都是可选的：\n```\nexport JAVA_HOME=/home/xiaosi/opt/jdk-1.8.0\n```\n\n### 5. 运行\n\n#### 5.1 初始化HDFS系统\n\n在配置完成后，运行`hadoop`前，要初始化`HDFS`系统，在`bin/`目录下执行如下命令：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ ./bin/hdfs namenode -format\n```\n#### 5.2 启动\n\n开启`NameNode`和`DataNode`守护进程：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ ./sbin/start-dfs.sh\nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-namenode-yoona.out\nlocalhost: starting datanode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-datanode-yoona.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-secondarynamenode-yoona.out\n```\n通过`jps`命令查看`namenode`和`datanode`是否已经启动起来：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ jps\n13400 SecondaryNameNode\n13035 NameNode\n13197 DataNode\n13535 Jps\n```\n从启动日志我们可以知道，日志信息存储在`hadoop-2.7.3/logs/`目录下，如果启动过程中有任何问题，可以通过查看日志来确认问题原因。\n\n### 6. Yarn模式安装\n\n#### 6.1 配置\n\n修改`yarn-site.xml`，添加如下配置：\n```xml\n<configuration>\n   <property>\n      <name>yarn.nodemanager.aux-services</name>\n      <value>mapreduce.shuffle</value>\n   </property>\n   <property>\n      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n      <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n   </property>\n</configuration>\n```\n修改`mapred-site.xml`，做如下修改：\n```xml\n<property>\n   <name>mapreduce.framework.name</name>\n   <value>yarn</value>\n</property>\n```\n\n本地模式下是`value`值是`local`，`yarn`模式下`value`值是`yarn`。\n\n#### 6.2 启动yarn\n\n启动`yarn`：\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/start-yarn.sh\nstarting yarn daemons\nstarting resourcemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-resourcemanager-yoona.out\nlocalhost: starting nodemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-nodemanager-yoona.out\n```\n关闭yarn:\n```\nxiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/stop-yarn.sh\nstopping yarn daemons\nstopping resourcemanager\nlocalhost: stopping nodemanager\n```\n#### 6.3 检查是否运行成功\n\n打开浏览器，输入：http://localhost:8088/cluster\n","slug":"Hadoop/[Hadoop]Hadoop 安装与启动","published":1,"updated":"2018-01-29T09:36:59.629Z","comments":1,"photos":[],"link":"","_id":"cje58tit4003iordb18fe4hvr","content":"<h3 id=\"1-SSH\"><a href=\"#1-SSH\" class=\"headerlink\" title=\"1. SSH\"></a>1. SSH</h3><p>参考博文：[Hadoop]SSH免密码登录以及失败解决方案（<a href=\"http://blog.csdn.net/sunnyyoona/article/details/51689041#t1）\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/sunnyyoona/article/details/51689041#t1）</a></p>\n<h3 id=\"2-下载\"><a href=\"#2-下载\" class=\"headerlink\" title=\"2. 下载\"></a>2. 下载</h3><p>(1) 直接从官网上下载 <a href=\"http://hadoop.apache.org/releases.html\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/releases.html</a></p>\n<p>(2) 使用命令行下载：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</span><br><span class=\"line\">--2016-06-16 08:40:07--  http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</span><br><span class=\"line\">正在解析主机 mirrors.hust.edu.cn (mirrors.hust.edu.cn)... 202.114.18.160</span><br><span class=\"line\">正在连接 mirrors.hust.edu.cn (mirrors.hust.edu.cn)|202.114.18.160|:80... 已连接。</span><br><span class=\"line\">已发出 HTTP 请求，正在等待回应... 200 OK</span><br><span class=\"line\">长度： 196015975 (187M) [application/octet-stream]</span><br><span class=\"line\">正在保存至: “hadoop-2.6.4.tar.gz”</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-解压缩Hadoop包\"><a href=\"#3-解压缩Hadoop包\" class=\"headerlink\" title=\"3. 解压缩Hadoop包\"></a>3. 解压缩Hadoop包</h3><p>解压位于根目录<code>/</code>文件夹下的<code>hadoop-2.7.3.tar.gz</code>到<code>~/opt</code>文件夹下<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ tar -zxvf hadoop-2.7.3.tar.gz -C opt/</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-配置\"><a href=\"#4-配置\" class=\"headerlink\" title=\"4. 配置\"></a>4. 配置</h3><p>配置文件都位于安装目录下的<code>/etc/hadoop</code>文件夹下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3/etc/hadoop$ ls</span><br><span class=\"line\">capacity-scheduler.xml  hadoop-env.sh              httpfs-log4j.properties  log4j.properties            mapred-site.xml.template</span><br><span class=\"line\">configuration.xsl       hadoop-metrics2.properties  httpfs-signature.secret  log4j.properties          slaves</span><br><span class=\"line\">container-executor.cfg  hadoop-metrics.properties   httpfs-site.xml          mapred-env.cmd              ssl-client.xml.example</span><br><span class=\"line\">core-site.xml           hadoop-policy.xml           kms-acls.xml             mapred-env.sh               ssl-server.xml.example</span><br><span class=\"line\">core-site.xml          hdfs-site.xml               kms-env.sh               mapred-queues.xml.template  yarn-env.cmd</span><br><span class=\"line\">hadoop-env.cmd          hdfs-site.xml              kms-log4j.properties     mapred-site.xml             yarn-env.sh</span><br><span class=\"line\">hadoop-env.sh           httpfs-env.sh               kms-site.xml             mapred-site.xml            yarn-site.xml</span><br></pre></td></tr></table></figure></p>\n<p>Hadoop的各个组件均可利用<code>XML</code>文件进行配置。<code>core-site.xml</code>文件用于配置<code>Common</code>组件的属性，<code>hdfs-site.xml</code>文件用于配置HDFS属性，而<code>mapred-site.xml</code>文件则用于配置<code>MapReduce</code>属性。</p>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Hadoop早期版本采用一个配置文件hadoop-site.xml来配置Common，HDFS和MapReduce组件。从0.20.0版本开始该文件以分为三，各对应一个组件。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-1-配置core-site-xml\"><a href=\"#4-1-配置core-site-xml\" class=\"headerlink\" title=\"4.1 配置core-site.xml\"></a>4.1 配置core-site.xml</h4><p><code>core-site.xml</code> 配置如下：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">  Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/$&#123;user.name&#125;/tmp/hadoop<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Abase for other temporary directories.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>fs.defaultFS<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hdfs://localhost:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.xiaosi.hosts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The superuser can connect only from host1 and host2 to impersonate a user<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.xiaosi.groups<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Allow the superuser oozie to impersonate any members of the group group1 and group2<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-2-配置hdfs-site-xml\"><a href=\"#4-2-配置hdfs-site-xml\" class=\"headerlink\" title=\"4.2 配置hdfs-site.xml\"></a>4.2 配置hdfs-site.xml</h4><p><code>hdfs-site.xml</code>配置如下：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">  Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.replication<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.name.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>file:/home/xiaosi/tmp/hadoop/dfs/name<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.data.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>file:/home/xiaosi/tmp/hadoop/dfs/data<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-3-配置-mapred-site-xml\"><a href=\"#4-3-配置-mapred-site-xml\" class=\"headerlink\" title=\"4.3 配置 mapred-site.xml\"></a>4.3 配置 mapred-site.xml</h4><p><code>mapred-site.xml</code>配置如下：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=\"1.0\"?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">  Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.job.tracker<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>localhost:9001<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>运行<code>Hadoop</code>的时候可能会找不到<code>jdk</code>，需要我们修改<code>hadoop.env.sh</code>脚本文件，唯一需要修改的环境变量就是<code>JAVE_HOME</code>，其他选项都是可选的：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/home/xiaosi/opt/jdk-1.8.0</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-运行\"><a href=\"#5-运行\" class=\"headerlink\" title=\"5. 运行\"></a>5. 运行</h3><h4 id=\"5-1-初始化HDFS系统\"><a href=\"#5-1-初始化HDFS系统\" class=\"headerlink\" title=\"5.1 初始化HDFS系统\"></a>5.1 初始化HDFS系统</h4><p>在配置完成后，运行<code>hadoop</code>前，要初始化<code>HDFS</code>系统，在<code>bin/</code>目录下执行如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ ./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"5-2-启动\"><a href=\"#5-2-启动\" class=\"headerlink\" title=\"5.2 启动\"></a>5.2 启动</h4><p>开启<code>NameNode</code>和<code>DataNode</code>守护进程：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ ./sbin/start-dfs.sh</span><br><span class=\"line\">Starting namenodes on [localhost]</span><br><span class=\"line\">localhost: starting namenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-namenode-yoona.out</span><br><span class=\"line\">localhost: starting datanode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-datanode-yoona.out</span><br><span class=\"line\">Starting secondary namenodes [0.0.0.0]</span><br><span class=\"line\">0.0.0.0: starting secondarynamenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-secondarynamenode-yoona.out</span><br></pre></td></tr></table></figure></p>\n<p>通过<code>jps</code>命令查看<code>namenode</code>和<code>datanode</code>是否已经启动起来：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ jps</span><br><span class=\"line\">13400 SecondaryNameNode</span><br><span class=\"line\">13035 NameNode</span><br><span class=\"line\">13197 DataNode</span><br><span class=\"line\">13535 Jps</span><br></pre></td></tr></table></figure></p>\n<p>从启动日志我们可以知道，日志信息存储在<code>hadoop-2.7.3/logs/</code>目录下，如果启动过程中有任何问题，可以通过查看日志来确认问题原因。</p>\n<h3 id=\"6-Yarn模式安装\"><a href=\"#6-Yarn模式安装\" class=\"headerlink\" title=\"6. Yarn模式安装\"></a>6. Yarn模式安装</h3><h4 id=\"6-1-配置\"><a href=\"#6-1-配置\" class=\"headerlink\" title=\"6.1 配置\"></a>6.1 配置</h4><p>修改<code>yarn-site.xml</code>，添加如下配置：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce.shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>修改<code>mapred-site.xml</code>，做如下修改：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>本地模式下是<code>value</code>值是<code>local</code>，<code>yarn</code>模式下<code>value</code>值是<code>yarn</code>。</p>\n<h4 id=\"6-2-启动yarn\"><a href=\"#6-2-启动yarn\" class=\"headerlink\" title=\"6.2 启动yarn\"></a>6.2 启动yarn</h4><p>启动<code>yarn</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/start-yarn.sh</span><br><span class=\"line\">starting yarn daemons</span><br><span class=\"line\">starting resourcemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-resourcemanager-yoona.out</span><br><span class=\"line\">localhost: starting nodemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-nodemanager-yoona.out</span><br></pre></td></tr></table></figure></p>\n<p>关闭yarn:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/stop-yarn.sh</span><br><span class=\"line\">stopping yarn daemons</span><br><span class=\"line\">stopping resourcemanager</span><br><span class=\"line\">localhost: stopping nodemanager</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"6-3-检查是否运行成功\"><a href=\"#6-3-检查是否运行成功\" class=\"headerlink\" title=\"6.3 检查是否运行成功\"></a>6.3 检查是否运行成功</h4><p>打开浏览器，输入：<a href=\"http://localhost:8088/cluster\" target=\"_blank\" rel=\"noopener\">http://localhost:8088/cluster</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-SSH\"><a href=\"#1-SSH\" class=\"headerlink\" title=\"1. SSH\"></a>1. SSH</h3><p>参考博文：[Hadoop]SSH免密码登录以及失败解决方案（<a href=\"http://blog.csdn.net/sunnyyoona/article/details/51689041#t1）\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/sunnyyoona/article/details/51689041#t1）</a></p>\n<h3 id=\"2-下载\"><a href=\"#2-下载\" class=\"headerlink\" title=\"2. 下载\"></a>2. 下载</h3><p>(1) 直接从官网上下载 <a href=\"http://hadoop.apache.org/releases.html\" target=\"_blank\" rel=\"noopener\">http://hadoop.apache.org/releases.html</a></p>\n<p>(2) 使用命令行下载：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</span><br><span class=\"line\">--2016-06-16 08:40:07--  http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</span><br><span class=\"line\">正在解析主机 mirrors.hust.edu.cn (mirrors.hust.edu.cn)... 202.114.18.160</span><br><span class=\"line\">正在连接 mirrors.hust.edu.cn (mirrors.hust.edu.cn)|202.114.18.160|:80... 已连接。</span><br><span class=\"line\">已发出 HTTP 请求，正在等待回应... 200 OK</span><br><span class=\"line\">长度： 196015975 (187M) [application/octet-stream]</span><br><span class=\"line\">正在保存至: “hadoop-2.6.4.tar.gz”</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-解压缩Hadoop包\"><a href=\"#3-解压缩Hadoop包\" class=\"headerlink\" title=\"3. 解压缩Hadoop包\"></a>3. 解压缩Hadoop包</h3><p>解压位于根目录<code>/</code>文件夹下的<code>hadoop-2.7.3.tar.gz</code>到<code>~/opt</code>文件夹下<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ tar -zxvf hadoop-2.7.3.tar.gz -C opt/</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-配置\"><a href=\"#4-配置\" class=\"headerlink\" title=\"4. 配置\"></a>4. 配置</h3><p>配置文件都位于安装目录下的<code>/etc/hadoop</code>文件夹下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3/etc/hadoop$ ls</span><br><span class=\"line\">capacity-scheduler.xml  hadoop-env.sh              httpfs-log4j.properties  log4j.properties            mapred-site.xml.template</span><br><span class=\"line\">configuration.xsl       hadoop-metrics2.properties  httpfs-signature.secret  log4j.properties          slaves</span><br><span class=\"line\">container-executor.cfg  hadoop-metrics.properties   httpfs-site.xml          mapred-env.cmd              ssl-client.xml.example</span><br><span class=\"line\">core-site.xml           hadoop-policy.xml           kms-acls.xml             mapred-env.sh               ssl-server.xml.example</span><br><span class=\"line\">core-site.xml          hdfs-site.xml               kms-env.sh               mapred-queues.xml.template  yarn-env.cmd</span><br><span class=\"line\">hadoop-env.cmd          hdfs-site.xml              kms-log4j.properties     mapred-site.xml             yarn-env.sh</span><br><span class=\"line\">hadoop-env.sh           httpfs-env.sh               kms-site.xml             mapred-site.xml            yarn-site.xml</span><br></pre></td></tr></table></figure></p>\n<p>Hadoop的各个组件均可利用<code>XML</code>文件进行配置。<code>core-site.xml</code>文件用于配置<code>Common</code>组件的属性，<code>hdfs-site.xml</code>文件用于配置HDFS属性，而<code>mapred-site.xml</code>文件则用于配置<code>MapReduce</code>属性。</p>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Hadoop早期版本采用一个配置文件hadoop-site.xml来配置Common，HDFS和MapReduce组件。从0.20.0版本开始该文件以分为三，各对应一个组件。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-1-配置core-site-xml\"><a href=\"#4-1-配置core-site-xml\" class=\"headerlink\" title=\"4.1 配置core-site.xml\"></a>4.1 配置core-site.xml</h4><p><code>core-site.xml</code> 配置如下：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">  Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/$&#123;user.name&#125;/tmp/hadoop<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Abase for other temporary directories.<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>fs.defaultFS<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hdfs://localhost:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.xiaosi.hosts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>The superuser can connect only from host1 and host2 to impersonate a user<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.xiaosi.groups<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Allow the superuser oozie to impersonate any members of the group group1 and group2<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-2-配置hdfs-site-xml\"><a href=\"#4-2-配置hdfs-site-xml\" class=\"headerlink\" title=\"4.2 配置hdfs-site.xml\"></a>4.2 配置hdfs-site.xml</h4><p><code>hdfs-site.xml</code>配置如下：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">  Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.replication<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.name.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>file:/home/xiaosi/tmp/hadoop/dfs/name<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.data.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>file:/home/xiaosi/tmp/hadoop/dfs/data<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-3-配置-mapred-site-xml\"><a href=\"#4-3-配置-mapred-site-xml\" class=\"headerlink\" title=\"4.3 配置 mapred-site.xml\"></a>4.3 配置 mapred-site.xml</h4><p><code>mapred-site.xml</code>配置如下：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=\"1.0\"?&gt;</span><br><span class=\"line\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">  Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.job.tracker<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>localhost:9001<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>运行<code>Hadoop</code>的时候可能会找不到<code>jdk</code>，需要我们修改<code>hadoop.env.sh</code>脚本文件，唯一需要修改的环境变量就是<code>JAVE_HOME</code>，其他选项都是可选的：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/home/xiaosi/opt/jdk-1.8.0</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-运行\"><a href=\"#5-运行\" class=\"headerlink\" title=\"5. 运行\"></a>5. 运行</h3><h4 id=\"5-1-初始化HDFS系统\"><a href=\"#5-1-初始化HDFS系统\" class=\"headerlink\" title=\"5.1 初始化HDFS系统\"></a>5.1 初始化HDFS系统</h4><p>在配置完成后，运行<code>hadoop</code>前，要初始化<code>HDFS</code>系统，在<code>bin/</code>目录下执行如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ ./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"5-2-启动\"><a href=\"#5-2-启动\" class=\"headerlink\" title=\"5.2 启动\"></a>5.2 启动</h4><p>开启<code>NameNode</code>和<code>DataNode</code>守护进程：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ ./sbin/start-dfs.sh</span><br><span class=\"line\">Starting namenodes on [localhost]</span><br><span class=\"line\">localhost: starting namenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-namenode-yoona.out</span><br><span class=\"line\">localhost: starting datanode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-datanode-yoona.out</span><br><span class=\"line\">Starting secondary namenodes [0.0.0.0]</span><br><span class=\"line\">0.0.0.0: starting secondarynamenode, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/hadoop-xiaosi-secondarynamenode-yoona.out</span><br></pre></td></tr></table></figure></p>\n<p>通过<code>jps</code>命令查看<code>namenode</code>和<code>datanode</code>是否已经启动起来：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ jps</span><br><span class=\"line\">13400 SecondaryNameNode</span><br><span class=\"line\">13035 NameNode</span><br><span class=\"line\">13197 DataNode</span><br><span class=\"line\">13535 Jps</span><br></pre></td></tr></table></figure></p>\n<p>从启动日志我们可以知道，日志信息存储在<code>hadoop-2.7.3/logs/</code>目录下，如果启动过程中有任何问题，可以通过查看日志来确认问题原因。</p>\n<h3 id=\"6-Yarn模式安装\"><a href=\"#6-Yarn模式安装\" class=\"headerlink\" title=\"6. Yarn模式安装\"></a>6. Yarn模式安装</h3><h4 id=\"6-1-配置\"><a href=\"#6-1-配置\" class=\"headerlink\" title=\"6.1 配置\"></a>6.1 配置</h4><p>修改<code>yarn-site.xml</code>，添加如下配置：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce.shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>修改<code>mapred-site.xml</code>，做如下修改：<br><figure class=\"highlight xml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>本地模式下是<code>value</code>值是<code>local</code>，<code>yarn</code>模式下<code>value</code>值是<code>yarn</code>。</p>\n<h4 id=\"6-2-启动yarn\"><a href=\"#6-2-启动yarn\" class=\"headerlink\" title=\"6.2 启动yarn\"></a>6.2 启动yarn</h4><p>启动<code>yarn</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/start-yarn.sh</span><br><span class=\"line\">starting yarn daemons</span><br><span class=\"line\">starting resourcemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-resourcemanager-yoona.out</span><br><span class=\"line\">localhost: starting nodemanager, logging to /home/xiaosi/opt/hadoop-2.7.3/logs/yarn-xiaosi-nodemanager-yoona.out</span><br></pre></td></tr></table></figure></p>\n<p>关闭yarn:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/opt/hadoop-2.7.3$ sbin/stop-yarn.sh</span><br><span class=\"line\">stopping yarn daemons</span><br><span class=\"line\">stopping resourcemanager</span><br><span class=\"line\">localhost: stopping nodemanager</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"6-3-检查是否运行成功\"><a href=\"#6-3-检查是否运行成功\" class=\"headerlink\" title=\"6.3 检查是否运行成功\"></a>6.3 检查是否运行成功</h4><p>打开浏览器，输入：<a href=\"http://localhost:8088/cluster\" target=\"_blank\" rel=\"noopener\">http://localhost:8088/cluster</a></p>\n"},{"title":"Hexo+Github搭建博客更换皮肤","date":"2017-12-02T10:17:23.000Z","archives":"Hexo","_content":"\n### 1. 更换皮肤\n\n主题地址: https://github.com/haojen/hexo-theme-Anisina\n\n\n\n### 2. 添加Disqus评论系统\n\n`Anisina`主题支持`Disqus`和`多说`评论，要想使用这两者，需要对其进行使用配置\n\n(1) 首先，你需要注册其中任何一个评论系统的帐号，在这里我们使用`Disqus`评论系统:\n```\nhttps://disqus.com\n```\n(2) 在Disqus设置页面中点 Add Disqus to your site 添加你的网站地址, 和设置Choose your unique Disqus URL, 这一栏填写的就是hexo中所使用的short_name\n\n\n\n### 3. Tags\n\n```\nxiaosi@yoona:~/qunar/study/hexo-blog$ hexo new page \"Tags\"\nINFO  Created: /media/xiaosi/司吉峰/study/hexo-blog/source/Tags/index.md\n```\n","source":"_posts/Hexo/Hexo+Github搭建博客更换皮肤.md","raw":"---\ntitle: Hexo+Github搭建博客更换皮肤\ndate: 2017-12-02 18:17:23\ntags:\n- Hexo\n\narchives: Hexo\n---\n\n### 1. 更换皮肤\n\n主题地址: https://github.com/haojen/hexo-theme-Anisina\n\n\n\n### 2. 添加Disqus评论系统\n\n`Anisina`主题支持`Disqus`和`多说`评论，要想使用这两者，需要对其进行使用配置\n\n(1) 首先，你需要注册其中任何一个评论系统的帐号，在这里我们使用`Disqus`评论系统:\n```\nhttps://disqus.com\n```\n(2) 在Disqus设置页面中点 Add Disqus to your site 添加你的网站地址, 和设置Choose your unique Disqus URL, 这一栏填写的就是hexo中所使用的short_name\n\n\n\n### 3. Tags\n\n```\nxiaosi@yoona:~/qunar/study/hexo-blog$ hexo new page \"Tags\"\nINFO  Created: /media/xiaosi/司吉峰/study/hexo-blog/source/Tags/index.md\n```\n","slug":"Hexo/Hexo+Github搭建博客更换皮肤","published":1,"updated":"2018-01-29T09:36:59.628Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje58tit6003lordbfxl6u1f9","content":"<h3 id=\"1-更换皮肤\"><a href=\"#1-更换皮肤\" class=\"headerlink\" title=\"1. 更换皮肤\"></a>1. 更换皮肤</h3><p>主题地址: <a href=\"https://github.com/haojen/hexo-theme-Anisina\" target=\"_blank\" rel=\"noopener\">https://github.com/haojen/hexo-theme-Anisina</a></p>\n<h3 id=\"2-添加Disqus评论系统\"><a href=\"#2-添加Disqus评论系统\" class=\"headerlink\" title=\"2. 添加Disqus评论系统\"></a>2. 添加Disqus评论系统</h3><p><code>Anisina</code>主题支持<code>Disqus</code>和<code>多说</code>评论，要想使用这两者，需要对其进行使用配置</p>\n<p>(1) 首先，你需要注册其中任何一个评论系统的帐号，在这里我们使用<code>Disqus</code>评论系统:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">https://disqus.com</span><br></pre></td></tr></table></figure></p>\n<p>(2) 在Disqus设置页面中点 Add Disqus to your site 添加你的网站地址, 和设置Choose your unique Disqus URL, 这一栏填写的就是hexo中所使用的short_name</p>\n<h3 id=\"3-Tags\"><a href=\"#3-Tags\" class=\"headerlink\" title=\"3. Tags\"></a>3. Tags</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/qunar/study/hexo-blog$ hexo new page &quot;Tags&quot;</span><br><span class=\"line\">INFO  Created: /media/xiaosi/司吉峰/study/hexo-blog/source/Tags/index.md</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-更换皮肤\"><a href=\"#1-更换皮肤\" class=\"headerlink\" title=\"1. 更换皮肤\"></a>1. 更换皮肤</h3><p>主题地址: <a href=\"https://github.com/haojen/hexo-theme-Anisina\" target=\"_blank\" rel=\"noopener\">https://github.com/haojen/hexo-theme-Anisina</a></p>\n<h3 id=\"2-添加Disqus评论系统\"><a href=\"#2-添加Disqus评论系统\" class=\"headerlink\" title=\"2. 添加Disqus评论系统\"></a>2. 添加Disqus评论系统</h3><p><code>Anisina</code>主题支持<code>Disqus</code>和<code>多说</code>评论，要想使用这两者，需要对其进行使用配置</p>\n<p>(1) 首先，你需要注册其中任何一个评论系统的帐号，在这里我们使用<code>Disqus</code>评论系统:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">https://disqus.com</span><br></pre></td></tr></table></figure></p>\n<p>(2) 在Disqus设置页面中点 Add Disqus to your site 添加你的网站地址, 和设置Choose your unique Disqus URL, 这一栏填写的就是hexo中所使用的short_name</p>\n<h3 id=\"3-Tags\"><a href=\"#3-Tags\" class=\"headerlink\" title=\"3. Tags\"></a>3. Tags</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/qunar/study/hexo-blog$ hexo new page &quot;Tags&quot;</span><br><span class=\"line\">INFO  Created: /media/xiaosi/司吉峰/study/hexo-blog/source/Tags/index.md</span><br></pre></td></tr></table></figure>\n"},{"layout":"post","author":"sjf0115","title":"Hadoop 脱离JVM？ Hadoop生态圈的挣扎与演化","date":"2018-02-02T01:50:01.000Z","_content":"\n新世纪以来，互联网及个人终端的普及，传统行业的信息化及物联网的发展等产业变化产生了大量的数据，远远超出了单台机器能够处理的范围，分布式存储与处理成为唯一的选项。从2005年开始，Hadoop从最初Nutch项目的一部分，逐步发展成为目前最流行的大数据处理平台。Hadoop生态圈的各个项目，围绕着大数据的存储，计算，分析，展示，安全等各个方面，构建了一个完整的大数据生态系统，并有Cloudera，HortonWorks，MapR等数十家公司基于开源的Hadoop平台构建自己的商业模式，可以认为是最近十年来最成功的开源社区。\n\nHadoop 的成功固然是由于其顺应了新世纪以来互联网技术的发展趋势，同时其基于JVM的平台开发也为Hadoop的快速发展起到了促进作用。Hadoop生态圈的项目大都基于Java，Scala，Clojure等JVM语言开发，这些语言良好的语法规范，丰富的第三方类库以及完善的工具支持，为Hadoop这样的超大型项目提供了基础支撑。同时，作为在程序员中普及率最高的语言之一，它也降低了更多程序员使用，或是参与开发Hadoop项目的门槛。同时，基于Scala开发的Spark，甚至因为项目的火热反过来极大的促进了Scala语言的推广。但是随着Hadoop平台的逐步发展，Hadoop生态圈的项目之间的竞争加剧，越来越多的Hadoop项目注意到了这些JVM语言的一些不足之处，希望通过更有效率的处理方式，提升分布式系统的执行效率与健壮性。本文主要以Spark和Flink项目为例，介绍Hadoop社区观察到的一些因为JVM语言的不足导致的问题，以及相应的解决方案与未来可能的发展方向。\n\n--more--\n\n注：\n```\n本文假设读者对Java和Hadoop系统有基本了解。\n```\n\n### 1. 背景\n\n目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，分布式计算引擎项目之间的竞争也相当激烈。MapReduce作为Hadoop平台的第一个分布式计算引擎，具有非常良好的可扩展性，Yahoo曾成功的搭建了上万台节点的MapReduce系统。但是MapReduce只支持Map和Reduce编程范式，使得复杂数据计算逻辑需要分割为多个Hadoop Job，而每个Hadoop Job都需要从HDFS读取数据，并将Job执行结果写回HDFS，所以会产生大量额外的IO开销，目前MapReduce正在逐渐被其他三个分布式计算引擎替代。Tez,Spark和Flink都支持图结构的分布式计算流，可在同一Job内支持任意复杂逻辑的计算流。Tez的抽象层次较低，用户不易直接使用，Spark与Flink都提供了抽象的分布式数据集以及可在数据集上使用的操作符，用户可以像操作Scala数据集合类似的方式在Spark/FLink中的操作分布式数据集，非常的容易上手，同时，Spark与Flink都在分布式计算引擎之上，提供了针对SQL，流处理，机器学习和图计算等特定数据处理领域的库。\n\n随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了JVM本身，希望通过解决JVM本身带来的一些问题，提高分布式系统的性能或是健壮性，从而增强自身的竞争力。\n\nJVM本身作为一个各种类型应用执行的平台，其对Java对象的管理也是基于通用的处理策略，其垃圾回收器通过估算Java对象的生命周期对Java对象进行有效率的管理。针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的JVM参数更有效率的管理Java对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身以及JVM的各参数有深入的了解，极大的提高了分布式计算平台的调优门槛（例如这篇文章中对Spark的调优 [Tuning Java Garbage Collection for Spark Applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)）。然而类似Spark或是Flink的分布式计算框架，框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的Java对象生命周期，从而为更有效率的管理Java对象提供了可能。\n\n### 2. JVM存在的问题\n\n#### 2.1. Java对象开销\n\n相对于c/c++等更加接近底层的语言，Java对象的存储密度相对偏低，例如【1】，“abcd”这样简单的字符串在UTF-8编码中需要4个字节存储，但Java采用UTF-16编码存储字符串，需要8个字节存储“abcd”，同时Java对象还对象header等其他额外信息，一个4字节字符串对象，在Java中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率的内存存储，则意味着CPU数据访问吞吐量更高，以及更少的磁盘落地可能。\n\n#### 2.2. 对象存储结构引发的cache miss\n\n为了缓解CPU处理速度与内存访问速度的差距【2】，现代CPU数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以cache line为单位加载数据，所以当CPU访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果CPU要访问的数据不在当前缓存所有的cache line中，则需要从内存中加载对应的数据，这被称为一次cache miss。当cache miss非常高的时候，CPU大部分的时间都在等待数据加载，而不是真正的处理数据。Java对象并不是连续的存储在内存上，同时很多的Java数据结构的数据聚集性也不好，在Spark的性能调优中，经常能够观测到大量的cache miss。Java社区有个项目叫做Project Valhalla，可能会部分的解决这个问题，有兴趣的可以看看这儿 [OpenJDK: Valhalla](http://openjdk.java.net/projects/valhalla/)。\n\n#### 2.3. 大数据的垃圾回收\n\nJava的垃圾回收机制，一直让Java开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面，垃圾回收也是Java应用的一颗不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大的影响了Java应用的性能和可用性。在当前的数据中心中，大容量的内存得到了广泛的应用，甚至出现了单台机器配置TB内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换，清洗，处理等步骤。在这个过程中，会产生海量的Java对象，JVM的垃圾回收执行效率对性能有很大影响。通过JVM参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及JVM的各参数有深入的了解，而且有时候这也远远不够。\n\n#### 2.4. OOM问题\n\nOutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会出现OutOfMemoryError错误，JVM崩溃，分布式框架的健壮性和性能都会受到影响。通过JVM管理内存，同时试图解决OOM问题的应用，通常都需要检查Java对象的大小，并在某些存储Java对象特别多的数据结构中设置阈值进行控制。但是JVM并没有提供官方的检查Java对象大小的工具，第三方的工具类库可能无法准确通用的确定Java对象的大小【6】。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外的业务逻辑无关的代码。\n\n\n### 3. 解决方案\n\n为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术：\n\n(1) 定制的序列化工具。显式内存管理的前提步骤就是序列化，将Java对象序列化成二进制数据存储在内存上（on heap或是off-heap）。通用的序列化框架，如Java默认的java.io.Serializable将Java对象以及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于Spark或是Flink这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如Hadoop的org.apache.hadoop.io.Writable，需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。\n\n(2) 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有的内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收的压力。\n\n(3) 缓存友好的数据结构和算法。只将操作相关的数据连续存储，可以最大化的利用L1/L2/L3缓存，减少Cache miss的概率，提升CPU计算的吞吐量。以排序为例，由于排序的主要操作是对Key进行对比，如果将所有排序数据的Key与Value分开，对Key连续存储，则访问Key时的Cache命中率会大大提高。\n\n#### 3.1 定制的序列化工具\n\n分布式计算框架可以使用定制序列化工具的前提是要处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。Spark与Flink数据集都支持任意Java或是Scala类型，通过自动生成定制序列化工具，Spark与Flink既保证了API接口对用户的友好度（不用像Hadoop那样数据类型需要继承实现org.apache.hadoop.io.Writable接口），同时也达到了和Hadoop类似的序列化效率。\n\n##### 3.1.1 Spark的序列化框架\n\nSpark 支持通用的计算框架，如 Java Serialization和 Kryo。其缺点之前也略有论述，总结如下：\n- 占用较多内存。Kryo相对于Java Serialization更高，它支持一种类型到Integer的映射机制，序列化时用Integer代替类型信息，但还不及定制的序列化工具效率。\n- 反序列化时，必须反序列化整个Java对象。\n- 无法直接操作序列化后的二进制数据。\n\n[Project Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html) 提供了一种更好的解决方式，针对于DataFrame API（Spark针对结构化数据的类SQL分析API，参考 [Spark DataFrame Blog](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)），由于其数据集是有固定Schema的Tuple（可大概类比为数据库中的行），序列化是针对每个Tuple存储其类型信息以及其成员的类型信息是非常浪费内存的，对于Spark来说，Tuple类型信息是全局可知的，所以其定制的序列化工具只存储Tuple的数据，如下图所示\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-1.jpg?raw=true)\n\n对于固定大小的成员，如int，long等，其按照偏移量直接内联存储。对于变长的成员，如String，其存储一个指针，指向真正的数据存储位置，并在数据存储开始处存储其长度。通过这种存储方式，保证了在反序列化时，当只需访问某一个成员时，只需根据偏移量反序列化这个成员，并不需要反序列化整个Tuple。\n\nProject Tungsten 的定制序列化工具应用在 Sort，HashTable，Shuffle等很多对Spark性能影响最大的地方。比如在Shuffle阶段，定制序列化工具不仅提升了序列化的性能，而且减少了网络传输的数据量，根据DataBricks的Blog介绍，相对于Kryo，Shuffle800万复杂Tuple数据时，其性能至少提高2倍以上。此外，Project Tungsten也计划通过Code generation技术，自动生成序列化代码，将定制序列化工具推广到Spark Core层，从而使得更多的Spark应用受惠于此优化。\n\n##### 3.1.2 Flink的序列化框架\n\nFlink在系统设计之初，就借鉴了很多传统 RDBMS 的设计，其中之一就是对数据集的类型信息进行分析，对于特定 Schema 的数据集的处理过程，进行类似RDBMS执行计划优化的优化。同时，数据集的类型信息也可以用来设计定制的序列化工具。和Spark类似，Flink支持任意的Java或是Scala类型，Flink通过Java Reflection框架分析基于Java的Flink程序UDF(User Define Function)的返回类型的类型信息，通过Scala Compiler分析基于Scala的Flink程序UDF的返回类型的类型信息。类型信息由TypeInformation类表示，这个类有诸多具体实现类，例如（更多详情参考Flink官方博客 [Apache Flink: Juggling with Bits and Bytes](http://link.zhihu.com/?target=http%3A//flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html)）：\n- BasicTypeInfo: 任意Java基本类型（装包或未装包）和String类型。\n- BasicArrayTypeInfo: 任意Java基本类型数组（装包或未装包）和String数组。\n- WritableTypeInfo: 任意Hadoop’s Writable接口的实现类.\n- TupleTypeInfo: 任意的Flink tuple类型(支持Tuple1 to Tuple25). Flink tuples是固定长度固定类型的Java Tuple实现。\n- CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples).\n- PojoTypeInfo: 任意的POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是public修饰符定义，要么有getter/setter方法。\n- GenericTypeInfo: 任意无法匹配之前几种类型的类。）\n\n前6种类型数据集几乎覆盖了绝大部分的Flink程序，针对前6种类型数据集，Flink皆可以自动生成对应的TypeSerializer定制序列化工具，非常有效率的对数据集进行序列化和反序列化。对于第7中类型，Flink使用Kryo进行序列化和反序列化。此外，对于可被用作Key的类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据直接进行compare，hash等之类的操作。对于Tuple，CaseClass，Pojo等组合类型，Flink自动生成的TypeSerializer，TypeComparator同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的TypeSerializer，TypeComparator，如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-2.jpg?raw=true)\n\n此外，如有需要，用户可通过集成TypeInformation接口，定制实现自己的序列化工具。\n\n##### 3.2 显式的内存管理\n\n垃圾回收的JVM内存管理回避不了的问题，JDK8的G1算法改善了JVM垃圾回收的效率和可用范围，但对于大数据处理的实际环境中，还是远远不够。这也和现在分布式框架的发展趋势有冲突，越来越多的分布式计算框架希望尽可能多的将待处理的数据集放在内存中，而对于JVM垃圾回收来说，内存中Java对象越少，存活时间越短，其效率越高。通过JVM进行内存管理的话，OutOfMemoryError也是一个很难解决的问题。同时，在JVM内存管理中，Java对象有潜在的碎片化存储问题（Java对象所有信息可能不是在内存中连续存储），也有可能在所有Java对象大小没有超过JVM分配内存时，出现OutOfMemoryError问题。\n\n##### 3.2.1 Flink的内存管理\n\nFlink将内存分为三个部分，每个部分都有不同的用途：\n- Network buffers: 一些以32KB Byte数组为单位的buffer，主要被网络模块用于数据的网络传输。\n- Memory Manager pool: 大量以32KB Byte数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。\n- Remaining (Free) Heap: 主要留给UDF中用户自己创建的Java对象，由JVM管理。\n\nNetwork buffers在Flink中主要基于Netty的网络传输，无需多讲。Remaining Heap用于UDF中用户自己创建的Java对象，在UDF中，用户通常是流式的处理数据，并不需要很多内存，同时Flink也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool（以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。\n\n在Flink中，内存池由多个MemorySegment组成，每个MemorySegment代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment提供了根据偏移量访问数据的各种方法，如get/put int，long，float，double等，MemorySegment之间数据拷贝等方法，和java.nio.ByteBuffer类似。对于Flink的数据结构，通常包括多个向内存池申请的MemeorySegment，所有要存入的对象，通过TypeSerializer序列化之后，将二进制数据存储在MemorySegment中，在取出时，通过TypeSerializer反序列化。数据结构通过MemorySegment提供的set/get方法访问具体的二进制数据。\n\nFlink这种看起来比较复杂的内存管理方式带来的好处主要有：\n- 二进制的数据存储大大提高了数据存储密度，节省了存储空间。\n- 所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。而对于大部分的分布式计算框架来说，这部分由于要缓存大量数据，是最有可能导致OOM的地方。\n- 内存池虽然占据了大部分内存，但其中的MemorySegment容量较大(默认32KB)，所以内存池中的Java对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了JVM垃圾回收的压力。\n- Remaining Heap的内存虽然由JVM管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的Minor GC就会全部回收掉，一般不会触发Full GC。\n\nFlink当前的内存管理在最底层是基于byte[]，所以数据最终还是on-heap，最近Flink增加了off-heap的内存管理支持，将会在下一个release中正式出现。Flink off-heap的内存管理相对于on-heap的优点主要在于（更多细节，请参考 [Apache Flink: Off-heap Memory in Apache Flink and the curious JIT compiler](http://flink.apache.org/news/2015/09/16/off-heap-memory.html)）：\n- 启动分配了大内存(例如100G)的JVM很耗费时间，垃圾回收也很慢。如果采用off-heap，剩下的Network buffer和Remaining heap都会很小，垃圾回收也不用考虑MemorySegment中的Java对象了。\n- 更有效率的IO操作。在off-heap下，将MemorySegment写到磁盘或是网络，可以支持zeor-copy技术，而on-heap的话，则至少需要一次内存拷贝。\n- off-heap可用于错误恢复，比如JVM崩溃，在on-heap时，数据也随之丢失，但在off-heap下，off-heap的数据可能还在。此外，off-heap上的数据还可以和其他程序共享。\n\n##### 3.2.2 Spark的内存管理\n\nSpark的off-heap内存管理与Flink off-heap模式比较相似，也是通过Java UnSafe API直接访问off-heap内存，通过定制的序列化工具将序列化后的二进制数据存储与off-heap上，Spark的数据结构和算法直接访问和操作在off-heap上的二进制数据。Project Tungsten是一个正在进行中的项目，想了解具体进展可以访问：[SPARK-7075 Project Tungsten (Spark 1.5 Phase 1)](https://issues.apache.org/jira/browse/SPARK-7075)， [SPARK-9697 Project Tungsten (Spark 1.6)](https://issues.apache.org/jira/browse/SPARK-9697)。\n\n#### 3.3 缓存友好的计算\n\n磁盘IO和网络IO之前一直被认为是Hadoop系统的瓶颈，但是随着Spark，Flink等新一代的分布式计算框架的发展，越来越多的趋势使得CPU/Memory逐渐成为瓶颈，这些趋势包括：\n- 更先进的IO硬件逐渐普及。10GB网络和SSD硬盘等已经被越来越多的数据中心使用。\n- 更高效的存储格式。Parquet，ORC等列式存储被越来越多的Hadoop项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。\n- 更高效的执行计划。例如Spark DataFrame的执行计划优化器的Fliter-Push-Down优化会将过滤条件尽可能的提前，甚至提前到Parquet的数据访问层，使得在很多实际的工作负载中，并不需要很多的磁盘IO。\n\n由于CPU处理速度和内存访问速度的差距，提升CPU的处理效率的关键在于最大化的利用L1/L2/L3/Memory，减少任何不必要的Cache miss。定制的序列化工具给Spark和Flink提供了可能，通过定制的序列化工具，Spark和Flink访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时，尽量连续存储，减少内存碎片化对Cache命中率的影响，甚至更进一步，Spark与Flink可以将需要操作的部分数据（如排序时的Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能的提升Cache命中的概率。\n\n##### 3.3.1 Flink中的数据结构\n\n以Flink中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink通过特殊设计的排序算法，获得了非常好了性能，其排序算法的实现如下：\n- 将待排序的数据经过序列化后存储在两个不同的MemorySegment集中。数据全部的序列化值存放于其中一个MemorySegment集中。数据序列化后的Key和指向第一个MemorySegment集中其值的指针存放于第二个MemorySegment集中。\n- 对第二个MemorySegment集中的Key进行排序，如需交换Key位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。\n- 排序完成后，访问数据时，按照第二个MemorySegment集中Key的顺序访问，并通过Pinter值找到数据在第一个MemorySegment集中的位置，通过TypeSerializer反序列化成Java对象返回。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-3.jpg?raw=true)\n\n这样实现的好处有：\n- 通过Key和Full data分离存储的方式，尽量将被操作的数据最小化，提高Cache命中的概率，从而提高CPU的吞吐量。\n- 移动数据时，只需移动Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。\n- TypeComparator直接基于二进制数据进行操作，节省了反序列化的时间。\n\n##### 3.3.2 Spark的数据结构\n\nSpark中基于off-heap的排序与Flink几乎一模一样，在这里就不多做介绍了，感兴趣的话，请参考：[Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)。\n\n### 4. 总结\n\n本文主要介绍了Hadoop生态圈的一些项目遇到的一些因为JVM内存管理导致的问题，以及社区是如何应对的。基本上，以内存为中心的分布式计算框架，大都开始了部分脱离JVM，走上了自己管理内存的路线，Project Tungsten甚至更进一步，提出了通过LLVM，将部分逻辑编译成本地代码，从而更加深入的挖掘SIMD等CPU潜力。此外，除了Spark，Flink这样的分布式计算框架，HBase（HBASE-11425），HDFS（HDFS-7844）等项目也在部分性能相关的模块通过自己管理内存来规避JVM的一些缺陷，同时提升性能。\n\n\n参考：\n\n1. [project tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html, http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen)\n\n2. [The \"Memory Wall\": Modern Microprocessors](http://www.lighterra.com/papers/modernmicroprocessors/)\n\n3. flink memory management: Apache Flink: [Juggling with Bits and Bytes](http://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html)\n\n4. [java GC：Tuning Java Garbage Collection for Spark Applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)\n\n5. [Project Valhalla: OpenJDK: Valhalla](http://openjdk.java.net/projects/valhalla/)\n\n6. [java object size: dweiss/java-sizeof · GitHub](https://github.com/dweiss/java-sizeof)\n\n7. [Big Data Performance Engineering](http://www.bigsynapse.com/addressing-big-data-performance)\n\n原文: https://zhuanlan.zhihu.com/hadoop/20228397\n","source":"_posts/Hadoop/[Hadoop]脱离JVM？ Hadoop生态圈的挣扎与演化.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hadoop 脱离JVM？ Hadoop生态圈的挣扎与演化\ndate: 2018-02-02 09:50:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\npermalink: hadoop-ecosystem-break-away-jvm\n---\n\n新世纪以来，互联网及个人终端的普及，传统行业的信息化及物联网的发展等产业变化产生了大量的数据，远远超出了单台机器能够处理的范围，分布式存储与处理成为唯一的选项。从2005年开始，Hadoop从最初Nutch项目的一部分，逐步发展成为目前最流行的大数据处理平台。Hadoop生态圈的各个项目，围绕着大数据的存储，计算，分析，展示，安全等各个方面，构建了一个完整的大数据生态系统，并有Cloudera，HortonWorks，MapR等数十家公司基于开源的Hadoop平台构建自己的商业模式，可以认为是最近十年来最成功的开源社区。\n\nHadoop 的成功固然是由于其顺应了新世纪以来互联网技术的发展趋势，同时其基于JVM的平台开发也为Hadoop的快速发展起到了促进作用。Hadoop生态圈的项目大都基于Java，Scala，Clojure等JVM语言开发，这些语言良好的语法规范，丰富的第三方类库以及完善的工具支持，为Hadoop这样的超大型项目提供了基础支撑。同时，作为在程序员中普及率最高的语言之一，它也降低了更多程序员使用，或是参与开发Hadoop项目的门槛。同时，基于Scala开发的Spark，甚至因为项目的火热反过来极大的促进了Scala语言的推广。但是随着Hadoop平台的逐步发展，Hadoop生态圈的项目之间的竞争加剧，越来越多的Hadoop项目注意到了这些JVM语言的一些不足之处，希望通过更有效率的处理方式，提升分布式系统的执行效率与健壮性。本文主要以Spark和Flink项目为例，介绍Hadoop社区观察到的一些因为JVM语言的不足导致的问题，以及相应的解决方案与未来可能的发展方向。\n\n--more--\n\n注：\n```\n本文假设读者对Java和Hadoop系统有基本了解。\n```\n\n### 1. 背景\n\n目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，分布式计算引擎项目之间的竞争也相当激烈。MapReduce作为Hadoop平台的第一个分布式计算引擎，具有非常良好的可扩展性，Yahoo曾成功的搭建了上万台节点的MapReduce系统。但是MapReduce只支持Map和Reduce编程范式，使得复杂数据计算逻辑需要分割为多个Hadoop Job，而每个Hadoop Job都需要从HDFS读取数据，并将Job执行结果写回HDFS，所以会产生大量额外的IO开销，目前MapReduce正在逐渐被其他三个分布式计算引擎替代。Tez,Spark和Flink都支持图结构的分布式计算流，可在同一Job内支持任意复杂逻辑的计算流。Tez的抽象层次较低，用户不易直接使用，Spark与Flink都提供了抽象的分布式数据集以及可在数据集上使用的操作符，用户可以像操作Scala数据集合类似的方式在Spark/FLink中的操作分布式数据集，非常的容易上手，同时，Spark与Flink都在分布式计算引擎之上，提供了针对SQL，流处理，机器学习和图计算等特定数据处理领域的库。\n\n随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了JVM本身，希望通过解决JVM本身带来的一些问题，提高分布式系统的性能或是健壮性，从而增强自身的竞争力。\n\nJVM本身作为一个各种类型应用执行的平台，其对Java对象的管理也是基于通用的处理策略，其垃圾回收器通过估算Java对象的生命周期对Java对象进行有效率的管理。针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的JVM参数更有效率的管理Java对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身以及JVM的各参数有深入的了解，极大的提高了分布式计算平台的调优门槛（例如这篇文章中对Spark的调优 [Tuning Java Garbage Collection for Spark Applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)）。然而类似Spark或是Flink的分布式计算框架，框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的Java对象生命周期，从而为更有效率的管理Java对象提供了可能。\n\n### 2. JVM存在的问题\n\n#### 2.1. Java对象开销\n\n相对于c/c++等更加接近底层的语言，Java对象的存储密度相对偏低，例如【1】，“abcd”这样简单的字符串在UTF-8编码中需要4个字节存储，但Java采用UTF-16编码存储字符串，需要8个字节存储“abcd”，同时Java对象还对象header等其他额外信息，一个4字节字符串对象，在Java中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率的内存存储，则意味着CPU数据访问吞吐量更高，以及更少的磁盘落地可能。\n\n#### 2.2. 对象存储结构引发的cache miss\n\n为了缓解CPU处理速度与内存访问速度的差距【2】，现代CPU数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以cache line为单位加载数据，所以当CPU访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果CPU要访问的数据不在当前缓存所有的cache line中，则需要从内存中加载对应的数据，这被称为一次cache miss。当cache miss非常高的时候，CPU大部分的时间都在等待数据加载，而不是真正的处理数据。Java对象并不是连续的存储在内存上，同时很多的Java数据结构的数据聚集性也不好，在Spark的性能调优中，经常能够观测到大量的cache miss。Java社区有个项目叫做Project Valhalla，可能会部分的解决这个问题，有兴趣的可以看看这儿 [OpenJDK: Valhalla](http://openjdk.java.net/projects/valhalla/)。\n\n#### 2.3. 大数据的垃圾回收\n\nJava的垃圾回收机制，一直让Java开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面，垃圾回收也是Java应用的一颗不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大的影响了Java应用的性能和可用性。在当前的数据中心中，大容量的内存得到了广泛的应用，甚至出现了单台机器配置TB内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换，清洗，处理等步骤。在这个过程中，会产生海量的Java对象，JVM的垃圾回收执行效率对性能有很大影响。通过JVM参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及JVM的各参数有深入的了解，而且有时候这也远远不够。\n\n#### 2.4. OOM问题\n\nOutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会出现OutOfMemoryError错误，JVM崩溃，分布式框架的健壮性和性能都会受到影响。通过JVM管理内存，同时试图解决OOM问题的应用，通常都需要检查Java对象的大小，并在某些存储Java对象特别多的数据结构中设置阈值进行控制。但是JVM并没有提供官方的检查Java对象大小的工具，第三方的工具类库可能无法准确通用的确定Java对象的大小【6】。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外的业务逻辑无关的代码。\n\n\n### 3. 解决方案\n\n为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术：\n\n(1) 定制的序列化工具。显式内存管理的前提步骤就是序列化，将Java对象序列化成二进制数据存储在内存上（on heap或是off-heap）。通用的序列化框架，如Java默认的java.io.Serializable将Java对象以及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于Spark或是Flink这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如Hadoop的org.apache.hadoop.io.Writable，需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。\n\n(2) 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有的内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收的压力。\n\n(3) 缓存友好的数据结构和算法。只将操作相关的数据连续存储，可以最大化的利用L1/L2/L3缓存，减少Cache miss的概率，提升CPU计算的吞吐量。以排序为例，由于排序的主要操作是对Key进行对比，如果将所有排序数据的Key与Value分开，对Key连续存储，则访问Key时的Cache命中率会大大提高。\n\n#### 3.1 定制的序列化工具\n\n分布式计算框架可以使用定制序列化工具的前提是要处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。Spark与Flink数据集都支持任意Java或是Scala类型，通过自动生成定制序列化工具，Spark与Flink既保证了API接口对用户的友好度（不用像Hadoop那样数据类型需要继承实现org.apache.hadoop.io.Writable接口），同时也达到了和Hadoop类似的序列化效率。\n\n##### 3.1.1 Spark的序列化框架\n\nSpark 支持通用的计算框架，如 Java Serialization和 Kryo。其缺点之前也略有论述，总结如下：\n- 占用较多内存。Kryo相对于Java Serialization更高，它支持一种类型到Integer的映射机制，序列化时用Integer代替类型信息，但还不及定制的序列化工具效率。\n- 反序列化时，必须反序列化整个Java对象。\n- 无法直接操作序列化后的二进制数据。\n\n[Project Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html) 提供了一种更好的解决方式，针对于DataFrame API（Spark针对结构化数据的类SQL分析API，参考 [Spark DataFrame Blog](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)），由于其数据集是有固定Schema的Tuple（可大概类比为数据库中的行），序列化是针对每个Tuple存储其类型信息以及其成员的类型信息是非常浪费内存的，对于Spark来说，Tuple类型信息是全局可知的，所以其定制的序列化工具只存储Tuple的数据，如下图所示\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-1.jpg?raw=true)\n\n对于固定大小的成员，如int，long等，其按照偏移量直接内联存储。对于变长的成员，如String，其存储一个指针，指向真正的数据存储位置，并在数据存储开始处存储其长度。通过这种存储方式，保证了在反序列化时，当只需访问某一个成员时，只需根据偏移量反序列化这个成员，并不需要反序列化整个Tuple。\n\nProject Tungsten 的定制序列化工具应用在 Sort，HashTable，Shuffle等很多对Spark性能影响最大的地方。比如在Shuffle阶段，定制序列化工具不仅提升了序列化的性能，而且减少了网络传输的数据量，根据DataBricks的Blog介绍，相对于Kryo，Shuffle800万复杂Tuple数据时，其性能至少提高2倍以上。此外，Project Tungsten也计划通过Code generation技术，自动生成序列化代码，将定制序列化工具推广到Spark Core层，从而使得更多的Spark应用受惠于此优化。\n\n##### 3.1.2 Flink的序列化框架\n\nFlink在系统设计之初，就借鉴了很多传统 RDBMS 的设计，其中之一就是对数据集的类型信息进行分析，对于特定 Schema 的数据集的处理过程，进行类似RDBMS执行计划优化的优化。同时，数据集的类型信息也可以用来设计定制的序列化工具。和Spark类似，Flink支持任意的Java或是Scala类型，Flink通过Java Reflection框架分析基于Java的Flink程序UDF(User Define Function)的返回类型的类型信息，通过Scala Compiler分析基于Scala的Flink程序UDF的返回类型的类型信息。类型信息由TypeInformation类表示，这个类有诸多具体实现类，例如（更多详情参考Flink官方博客 [Apache Flink: Juggling with Bits and Bytes](http://link.zhihu.com/?target=http%3A//flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html)）：\n- BasicTypeInfo: 任意Java基本类型（装包或未装包）和String类型。\n- BasicArrayTypeInfo: 任意Java基本类型数组（装包或未装包）和String数组。\n- WritableTypeInfo: 任意Hadoop’s Writable接口的实现类.\n- TupleTypeInfo: 任意的Flink tuple类型(支持Tuple1 to Tuple25). Flink tuples是固定长度固定类型的Java Tuple实现。\n- CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples).\n- PojoTypeInfo: 任意的POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是public修饰符定义，要么有getter/setter方法。\n- GenericTypeInfo: 任意无法匹配之前几种类型的类。）\n\n前6种类型数据集几乎覆盖了绝大部分的Flink程序，针对前6种类型数据集，Flink皆可以自动生成对应的TypeSerializer定制序列化工具，非常有效率的对数据集进行序列化和反序列化。对于第7中类型，Flink使用Kryo进行序列化和反序列化。此外，对于可被用作Key的类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据直接进行compare，hash等之类的操作。对于Tuple，CaseClass，Pojo等组合类型，Flink自动生成的TypeSerializer，TypeComparator同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的TypeSerializer，TypeComparator，如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-2.jpg?raw=true)\n\n此外，如有需要，用户可通过集成TypeInformation接口，定制实现自己的序列化工具。\n\n##### 3.2 显式的内存管理\n\n垃圾回收的JVM内存管理回避不了的问题，JDK8的G1算法改善了JVM垃圾回收的效率和可用范围，但对于大数据处理的实际环境中，还是远远不够。这也和现在分布式框架的发展趋势有冲突，越来越多的分布式计算框架希望尽可能多的将待处理的数据集放在内存中，而对于JVM垃圾回收来说，内存中Java对象越少，存活时间越短，其效率越高。通过JVM进行内存管理的话，OutOfMemoryError也是一个很难解决的问题。同时，在JVM内存管理中，Java对象有潜在的碎片化存储问题（Java对象所有信息可能不是在内存中连续存储），也有可能在所有Java对象大小没有超过JVM分配内存时，出现OutOfMemoryError问题。\n\n##### 3.2.1 Flink的内存管理\n\nFlink将内存分为三个部分，每个部分都有不同的用途：\n- Network buffers: 一些以32KB Byte数组为单位的buffer，主要被网络模块用于数据的网络传输。\n- Memory Manager pool: 大量以32KB Byte数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。\n- Remaining (Free) Heap: 主要留给UDF中用户自己创建的Java对象，由JVM管理。\n\nNetwork buffers在Flink中主要基于Netty的网络传输，无需多讲。Remaining Heap用于UDF中用户自己创建的Java对象，在UDF中，用户通常是流式的处理数据，并不需要很多内存，同时Flink也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool（以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。\n\n在Flink中，内存池由多个MemorySegment组成，每个MemorySegment代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment提供了根据偏移量访问数据的各种方法，如get/put int，long，float，double等，MemorySegment之间数据拷贝等方法，和java.nio.ByteBuffer类似。对于Flink的数据结构，通常包括多个向内存池申请的MemeorySegment，所有要存入的对象，通过TypeSerializer序列化之后，将二进制数据存储在MemorySegment中，在取出时，通过TypeSerializer反序列化。数据结构通过MemorySegment提供的set/get方法访问具体的二进制数据。\n\nFlink这种看起来比较复杂的内存管理方式带来的好处主要有：\n- 二进制的数据存储大大提高了数据存储密度，节省了存储空间。\n- 所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。而对于大部分的分布式计算框架来说，这部分由于要缓存大量数据，是最有可能导致OOM的地方。\n- 内存池虽然占据了大部分内存，但其中的MemorySegment容量较大(默认32KB)，所以内存池中的Java对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了JVM垃圾回收的压力。\n- Remaining Heap的内存虽然由JVM管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的Minor GC就会全部回收掉，一般不会触发Full GC。\n\nFlink当前的内存管理在最底层是基于byte[]，所以数据最终还是on-heap，最近Flink增加了off-heap的内存管理支持，将会在下一个release中正式出现。Flink off-heap的内存管理相对于on-heap的优点主要在于（更多细节，请参考 [Apache Flink: Off-heap Memory in Apache Flink and the curious JIT compiler](http://flink.apache.org/news/2015/09/16/off-heap-memory.html)）：\n- 启动分配了大内存(例如100G)的JVM很耗费时间，垃圾回收也很慢。如果采用off-heap，剩下的Network buffer和Remaining heap都会很小，垃圾回收也不用考虑MemorySegment中的Java对象了。\n- 更有效率的IO操作。在off-heap下，将MemorySegment写到磁盘或是网络，可以支持zeor-copy技术，而on-heap的话，则至少需要一次内存拷贝。\n- off-heap可用于错误恢复，比如JVM崩溃，在on-heap时，数据也随之丢失，但在off-heap下，off-heap的数据可能还在。此外，off-heap上的数据还可以和其他程序共享。\n\n##### 3.2.2 Spark的内存管理\n\nSpark的off-heap内存管理与Flink off-heap模式比较相似，也是通过Java UnSafe API直接访问off-heap内存，通过定制的序列化工具将序列化后的二进制数据存储与off-heap上，Spark的数据结构和算法直接访问和操作在off-heap上的二进制数据。Project Tungsten是一个正在进行中的项目，想了解具体进展可以访问：[SPARK-7075 Project Tungsten (Spark 1.5 Phase 1)](https://issues.apache.org/jira/browse/SPARK-7075)， [SPARK-9697 Project Tungsten (Spark 1.6)](https://issues.apache.org/jira/browse/SPARK-9697)。\n\n#### 3.3 缓存友好的计算\n\n磁盘IO和网络IO之前一直被认为是Hadoop系统的瓶颈，但是随着Spark，Flink等新一代的分布式计算框架的发展，越来越多的趋势使得CPU/Memory逐渐成为瓶颈，这些趋势包括：\n- 更先进的IO硬件逐渐普及。10GB网络和SSD硬盘等已经被越来越多的数据中心使用。\n- 更高效的存储格式。Parquet，ORC等列式存储被越来越多的Hadoop项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。\n- 更高效的执行计划。例如Spark DataFrame的执行计划优化器的Fliter-Push-Down优化会将过滤条件尽可能的提前，甚至提前到Parquet的数据访问层，使得在很多实际的工作负载中，并不需要很多的磁盘IO。\n\n由于CPU处理速度和内存访问速度的差距，提升CPU的处理效率的关键在于最大化的利用L1/L2/L3/Memory，减少任何不必要的Cache miss。定制的序列化工具给Spark和Flink提供了可能，通过定制的序列化工具，Spark和Flink访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时，尽量连续存储，减少内存碎片化对Cache命中率的影响，甚至更进一步，Spark与Flink可以将需要操作的部分数据（如排序时的Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能的提升Cache命中的概率。\n\n##### 3.3.1 Flink中的数据结构\n\n以Flink中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink通过特殊设计的排序算法，获得了非常好了性能，其排序算法的实现如下：\n- 将待排序的数据经过序列化后存储在两个不同的MemorySegment集中。数据全部的序列化值存放于其中一个MemorySegment集中。数据序列化后的Key和指向第一个MemorySegment集中其值的指针存放于第二个MemorySegment集中。\n- 对第二个MemorySegment集中的Key进行排序，如需交换Key位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。\n- 排序完成后，访问数据时，按照第二个MemorySegment集中Key的顺序访问，并通过Pinter值找到数据在第一个MemorySegment集中的位置，通过TypeSerializer反序列化成Java对象返回。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-3.jpg?raw=true)\n\n这样实现的好处有：\n- 通过Key和Full data分离存储的方式，尽量将被操作的数据最小化，提高Cache命中的概率，从而提高CPU的吞吐量。\n- 移动数据时，只需移动Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。\n- TypeComparator直接基于二进制数据进行操作，节省了反序列化的时间。\n\n##### 3.3.2 Spark的数据结构\n\nSpark中基于off-heap的排序与Flink几乎一模一样，在这里就不多做介绍了，感兴趣的话，请参考：[Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)。\n\n### 4. 总结\n\n本文主要介绍了Hadoop生态圈的一些项目遇到的一些因为JVM内存管理导致的问题，以及社区是如何应对的。基本上，以内存为中心的分布式计算框架，大都开始了部分脱离JVM，走上了自己管理内存的路线，Project Tungsten甚至更进一步，提出了通过LLVM，将部分逻辑编译成本地代码，从而更加深入的挖掘SIMD等CPU潜力。此外，除了Spark，Flink这样的分布式计算框架，HBase（HBASE-11425），HDFS（HDFS-7844）等项目也在部分性能相关的模块通过自己管理内存来规避JVM的一些缺陷，同时提升性能。\n\n\n参考：\n\n1. [project tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html, http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen)\n\n2. [The \"Memory Wall\": Modern Microprocessors](http://www.lighterra.com/papers/modernmicroprocessors/)\n\n3. flink memory management: Apache Flink: [Juggling with Bits and Bytes](http://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html)\n\n4. [java GC：Tuning Java Garbage Collection for Spark Applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)\n\n5. [Project Valhalla: OpenJDK: Valhalla](http://openjdk.java.net/projects/valhalla/)\n\n6. [java object size: dweiss/java-sizeof · GitHub](https://github.com/dweiss/java-sizeof)\n\n7. [Big Data Performance Engineering](http://www.bigsynapse.com/addressing-big-data-performance)\n\n原文: https://zhuanlan.zhihu.com/hadoop/20228397\n","slug":"hadoop-ecosystem-break-away-jvm","published":1,"updated":"2018-02-05T03:47:51.139Z","comments":1,"photos":[],"link":"","_id":"cje58tita003oordb3gzguiv6","content":"<p>新世纪以来，互联网及个人终端的普及，传统行业的信息化及物联网的发展等产业变化产生了大量的数据，远远超出了单台机器能够处理的范围，分布式存储与处理成为唯一的选项。从2005年开始，Hadoop从最初Nutch项目的一部分，逐步发展成为目前最流行的大数据处理平台。Hadoop生态圈的各个项目，围绕着大数据的存储，计算，分析，展示，安全等各个方面，构建了一个完整的大数据生态系统，并有Cloudera，HortonWorks，MapR等数十家公司基于开源的Hadoop平台构建自己的商业模式，可以认为是最近十年来最成功的开源社区。</p>\n<p>Hadoop 的成功固然是由于其顺应了新世纪以来互联网技术的发展趋势，同时其基于JVM的平台开发也为Hadoop的快速发展起到了促进作用。Hadoop生态圈的项目大都基于Java，Scala，Clojure等JVM语言开发，这些语言良好的语法规范，丰富的第三方类库以及完善的工具支持，为Hadoop这样的超大型项目提供了基础支撑。同时，作为在程序员中普及率最高的语言之一，它也降低了更多程序员使用，或是参与开发Hadoop项目的门槛。同时，基于Scala开发的Spark，甚至因为项目的火热反过来极大的促进了Scala语言的推广。但是随着Hadoop平台的逐步发展，Hadoop生态圈的项目之间的竞争加剧，越来越多的Hadoop项目注意到了这些JVM语言的一些不足之处，希望通过更有效率的处理方式，提升分布式系统的执行效率与健壮性。本文主要以Spark和Flink项目为例，介绍Hadoop社区观察到的一些因为JVM语言的不足导致的问题，以及相应的解决方案与未来可能的发展方向。</p>\n<p>–more–</p>\n<p>注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">本文假设读者对Java和Hadoop系统有基本了解。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-背景\"><a href=\"#1-背景\" class=\"headerlink\" title=\"1. 背景\"></a>1. 背景</h3><p>目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，分布式计算引擎项目之间的竞争也相当激烈。MapReduce作为Hadoop平台的第一个分布式计算引擎，具有非常良好的可扩展性，Yahoo曾成功的搭建了上万台节点的MapReduce系统。但是MapReduce只支持Map和Reduce编程范式，使得复杂数据计算逻辑需要分割为多个Hadoop Job，而每个Hadoop Job都需要从HDFS读取数据，并将Job执行结果写回HDFS，所以会产生大量额外的IO开销，目前MapReduce正在逐渐被其他三个分布式计算引擎替代。Tez,Spark和Flink都支持图结构的分布式计算流，可在同一Job内支持任意复杂逻辑的计算流。Tez的抽象层次较低，用户不易直接使用，Spark与Flink都提供了抽象的分布式数据集以及可在数据集上使用的操作符，用户可以像操作Scala数据集合类似的方式在Spark/FLink中的操作分布式数据集，非常的容易上手，同时，Spark与Flink都在分布式计算引擎之上，提供了针对SQL，流处理，机器学习和图计算等特定数据处理领域的库。</p>\n<p>随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了JVM本身，希望通过解决JVM本身带来的一些问题，提高分布式系统的性能或是健壮性，从而增强自身的竞争力。</p>\n<p>JVM本身作为一个各种类型应用执行的平台，其对Java对象的管理也是基于通用的处理策略，其垃圾回收器通过估算Java对象的生命周期对Java对象进行有效率的管理。针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的JVM参数更有效率的管理Java对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身以及JVM的各参数有深入的了解，极大的提高了分布式计算平台的调优门槛（例如这篇文章中对Spark的调优 <a href=\"https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html\" target=\"_blank\" rel=\"noopener\">Tuning Java Garbage Collection for Spark Applications</a>）。然而类似Spark或是Flink的分布式计算框架，框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的Java对象生命周期，从而为更有效率的管理Java对象提供了可能。</p>\n<h3 id=\"2-JVM存在的问题\"><a href=\"#2-JVM存在的问题\" class=\"headerlink\" title=\"2. JVM存在的问题\"></a>2. JVM存在的问题</h3><h4 id=\"2-1-Java对象开销\"><a href=\"#2-1-Java对象开销\" class=\"headerlink\" title=\"2.1. Java对象开销\"></a>2.1. Java对象开销</h4><p>相对于c/c++等更加接近底层的语言，Java对象的存储密度相对偏低，例如【1】，“abcd”这样简单的字符串在UTF-8编码中需要4个字节存储，但Java采用UTF-16编码存储字符串，需要8个字节存储“abcd”，同时Java对象还对象header等其他额外信息，一个4字节字符串对象，在Java中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率的内存存储，则意味着CPU数据访问吞吐量更高，以及更少的磁盘落地可能。</p>\n<h4 id=\"2-2-对象存储结构引发的cache-miss\"><a href=\"#2-2-对象存储结构引发的cache-miss\" class=\"headerlink\" title=\"2.2. 对象存储结构引发的cache miss\"></a>2.2. 对象存储结构引发的cache miss</h4><p>为了缓解CPU处理速度与内存访问速度的差距【2】，现代CPU数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以cache line为单位加载数据，所以当CPU访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果CPU要访问的数据不在当前缓存所有的cache line中，则需要从内存中加载对应的数据，这被称为一次cache miss。当cache miss非常高的时候，CPU大部分的时间都在等待数据加载，而不是真正的处理数据。Java对象并不是连续的存储在内存上，同时很多的Java数据结构的数据聚集性也不好，在Spark的性能调优中，经常能够观测到大量的cache miss。Java社区有个项目叫做Project Valhalla，可能会部分的解决这个问题，有兴趣的可以看看这儿 <a href=\"http://openjdk.java.net/projects/valhalla/\" target=\"_blank\" rel=\"noopener\">OpenJDK: Valhalla</a>。</p>\n<h4 id=\"2-3-大数据的垃圾回收\"><a href=\"#2-3-大数据的垃圾回收\" class=\"headerlink\" title=\"2.3. 大数据的垃圾回收\"></a>2.3. 大数据的垃圾回收</h4><p>Java的垃圾回收机制，一直让Java开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面，垃圾回收也是Java应用的一颗不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大的影响了Java应用的性能和可用性。在当前的数据中心中，大容量的内存得到了广泛的应用，甚至出现了单台机器配置TB内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换，清洗，处理等步骤。在这个过程中，会产生海量的Java对象，JVM的垃圾回收执行效率对性能有很大影响。通过JVM参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及JVM的各参数有深入的了解，而且有时候这也远远不够。</p>\n<h4 id=\"2-4-OOM问题\"><a href=\"#2-4-OOM问题\" class=\"headerlink\" title=\"2.4. OOM问题\"></a>2.4. OOM问题</h4><p>OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会出现OutOfMemoryError错误，JVM崩溃，分布式框架的健壮性和性能都会受到影响。通过JVM管理内存，同时试图解决OOM问题的应用，通常都需要检查Java对象的大小，并在某些存储Java对象特别多的数据结构中设置阈值进行控制。但是JVM并没有提供官方的检查Java对象大小的工具，第三方的工具类库可能无法准确通用的确定Java对象的大小【6】。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外的业务逻辑无关的代码。</p>\n<h3 id=\"3-解决方案\"><a href=\"#3-解决方案\" class=\"headerlink\" title=\"3. 解决方案\"></a>3. 解决方案</h3><p>为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术：</p>\n<p>(1) 定制的序列化工具。显式内存管理的前提步骤就是序列化，将Java对象序列化成二进制数据存储在内存上（on heap或是off-heap）。通用的序列化框架，如Java默认的java.io.Serializable将Java对象以及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于Spark或是Flink这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如Hadoop的org.apache.hadoop.io.Writable，需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。</p>\n<p>(2) 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有的内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收的压力。</p>\n<p>(3) 缓存友好的数据结构和算法。只将操作相关的数据连续存储，可以最大化的利用L1/L2/L3缓存，减少Cache miss的概率，提升CPU计算的吞吐量。以排序为例，由于排序的主要操作是对Key进行对比，如果将所有排序数据的Key与Value分开，对Key连续存储，则访问Key时的Cache命中率会大大提高。</p>\n<h4 id=\"3-1-定制的序列化工具\"><a href=\"#3-1-定制的序列化工具\" class=\"headerlink\" title=\"3.1 定制的序列化工具\"></a>3.1 定制的序列化工具</h4><p>分布式计算框架可以使用定制序列化工具的前提是要处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。Spark与Flink数据集都支持任意Java或是Scala类型，通过自动生成定制序列化工具，Spark与Flink既保证了API接口对用户的友好度（不用像Hadoop那样数据类型需要继承实现org.apache.hadoop.io.Writable接口），同时也达到了和Hadoop类似的序列化效率。</p>\n<h5 id=\"3-1-1-Spark的序列化框架\"><a href=\"#3-1-1-Spark的序列化框架\" class=\"headerlink\" title=\"3.1.1 Spark的序列化框架\"></a>3.1.1 Spark的序列化框架</h5><p>Spark 支持通用的计算框架，如 Java Serialization和 Kryo。其缺点之前也略有论述，总结如下：</p>\n<ul>\n<li>占用较多内存。Kryo相对于Java Serialization更高，它支持一种类型到Integer的映射机制，序列化时用Integer代替类型信息，但还不及定制的序列化工具效率。</li>\n<li>反序列化时，必须反序列化整个Java对象。</li>\n<li>无法直接操作序列化后的二进制数据。</li>\n</ul>\n<p><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten</a> 提供了一种更好的解决方式，针对于DataFrame API（Spark针对结构化数据的类SQL分析API，参考 <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\" rel=\"noopener\">Spark DataFrame Blog</a>），由于其数据集是有固定Schema的Tuple（可大概类比为数据库中的行），序列化是针对每个Tuple存储其类型信息以及其成员的类型信息是非常浪费内存的，对于Spark来说，Tuple类型信息是全局可知的，所以其定制的序列化工具只存储Tuple的数据，如下图所示</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-1.jpg?raw=true\" alt=\"\"></p>\n<p>对于固定大小的成员，如int，long等，其按照偏移量直接内联存储。对于变长的成员，如String，其存储一个指针，指向真正的数据存储位置，并在数据存储开始处存储其长度。通过这种存储方式，保证了在反序列化时，当只需访问某一个成员时，只需根据偏移量反序列化这个成员，并不需要反序列化整个Tuple。</p>\n<p>Project Tungsten 的定制序列化工具应用在 Sort，HashTable，Shuffle等很多对Spark性能影响最大的地方。比如在Shuffle阶段，定制序列化工具不仅提升了序列化的性能，而且减少了网络传输的数据量，根据DataBricks的Blog介绍，相对于Kryo，Shuffle800万复杂Tuple数据时，其性能至少提高2倍以上。此外，Project Tungsten也计划通过Code generation技术，自动生成序列化代码，将定制序列化工具推广到Spark Core层，从而使得更多的Spark应用受惠于此优化。</p>\n<h5 id=\"3-1-2-Flink的序列化框架\"><a href=\"#3-1-2-Flink的序列化框架\" class=\"headerlink\" title=\"3.1.2 Flink的序列化框架\"></a>3.1.2 Flink的序列化框架</h5><p>Flink在系统设计之初，就借鉴了很多传统 RDBMS 的设计，其中之一就是对数据集的类型信息进行分析，对于特定 Schema 的数据集的处理过程，进行类似RDBMS执行计划优化的优化。同时，数据集的类型信息也可以用来设计定制的序列化工具。和Spark类似，Flink支持任意的Java或是Scala类型，Flink通过Java Reflection框架分析基于Java的Flink程序UDF(User Define Function)的返回类型的类型信息，通过Scala Compiler分析基于Scala的Flink程序UDF的返回类型的类型信息。类型信息由TypeInformation类表示，这个类有诸多具体实现类，例如（更多详情参考Flink官方博客 <a href=\"http://link.zhihu.com/?target=http%3A//flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html\" target=\"_blank\" rel=\"noopener\">Apache Flink: Juggling with Bits and Bytes</a>）：</p>\n<ul>\n<li>BasicTypeInfo: 任意Java基本类型（装包或未装包）和String类型。</li>\n<li>BasicArrayTypeInfo: 任意Java基本类型数组（装包或未装包）和String数组。</li>\n<li>WritableTypeInfo: 任意Hadoop’s Writable接口的实现类.</li>\n<li>TupleTypeInfo: 任意的Flink tuple类型(支持Tuple1 to Tuple25). Flink tuples是固定长度固定类型的Java Tuple实现。</li>\n<li>CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples).</li>\n<li>PojoTypeInfo: 任意的POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是public修饰符定义，要么有getter/setter方法。</li>\n<li>GenericTypeInfo: 任意无法匹配之前几种类型的类。）</li>\n</ul>\n<p>前6种类型数据集几乎覆盖了绝大部分的Flink程序，针对前6种类型数据集，Flink皆可以自动生成对应的TypeSerializer定制序列化工具，非常有效率的对数据集进行序列化和反序列化。对于第7中类型，Flink使用Kryo进行序列化和反序列化。此外，对于可被用作Key的类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据直接进行compare，hash等之类的操作。对于Tuple，CaseClass，Pojo等组合类型，Flink自动生成的TypeSerializer，TypeComparator同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的TypeSerializer，TypeComparator，如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-2.jpg?raw=true\" alt=\"\"></p>\n<p>此外，如有需要，用户可通过集成TypeInformation接口，定制实现自己的序列化工具。</p>\n<h5 id=\"3-2-显式的内存管理\"><a href=\"#3-2-显式的内存管理\" class=\"headerlink\" title=\"3.2 显式的内存管理\"></a>3.2 显式的内存管理</h5><p>垃圾回收的JVM内存管理回避不了的问题，JDK8的G1算法改善了JVM垃圾回收的效率和可用范围，但对于大数据处理的实际环境中，还是远远不够。这也和现在分布式框架的发展趋势有冲突，越来越多的分布式计算框架希望尽可能多的将待处理的数据集放在内存中，而对于JVM垃圾回收来说，内存中Java对象越少，存活时间越短，其效率越高。通过JVM进行内存管理的话，OutOfMemoryError也是一个很难解决的问题。同时，在JVM内存管理中，Java对象有潜在的碎片化存储问题（Java对象所有信息可能不是在内存中连续存储），也有可能在所有Java对象大小没有超过JVM分配内存时，出现OutOfMemoryError问题。</p>\n<h5 id=\"3-2-1-Flink的内存管理\"><a href=\"#3-2-1-Flink的内存管理\" class=\"headerlink\" title=\"3.2.1 Flink的内存管理\"></a>3.2.1 Flink的内存管理</h5><p>Flink将内存分为三个部分，每个部分都有不同的用途：</p>\n<ul>\n<li>Network buffers: 一些以32KB Byte数组为单位的buffer，主要被网络模块用于数据的网络传输。</li>\n<li>Memory Manager pool: 大量以32KB Byte数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。</li>\n<li>Remaining (Free) Heap: 主要留给UDF中用户自己创建的Java对象，由JVM管理。</li>\n</ul>\n<p>Network buffers在Flink中主要基于Netty的网络传输，无需多讲。Remaining Heap用于UDF中用户自己创建的Java对象，在UDF中，用户通常是流式的处理数据，并不需要很多内存，同时Flink也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool（以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。</p>\n<p>在Flink中，内存池由多个MemorySegment组成，每个MemorySegment代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment提供了根据偏移量访问数据的各种方法，如get/put int，long，float，double等，MemorySegment之间数据拷贝等方法，和java.nio.ByteBuffer类似。对于Flink的数据结构，通常包括多个向内存池申请的MemeorySegment，所有要存入的对象，通过TypeSerializer序列化之后，将二进制数据存储在MemorySegment中，在取出时，通过TypeSerializer反序列化。数据结构通过MemorySegment提供的set/get方法访问具体的二进制数据。</p>\n<p>Flink这种看起来比较复杂的内存管理方式带来的好处主要有：</p>\n<ul>\n<li>二进制的数据存储大大提高了数据存储密度，节省了存储空间。</li>\n<li>所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。而对于大部分的分布式计算框架来说，这部分由于要缓存大量数据，是最有可能导致OOM的地方。</li>\n<li>内存池虽然占据了大部分内存，但其中的MemorySegment容量较大(默认32KB)，所以内存池中的Java对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了JVM垃圾回收的压力。</li>\n<li>Remaining Heap的内存虽然由JVM管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的Minor GC就会全部回收掉，一般不会触发Full GC。</li>\n</ul>\n<p>Flink当前的内存管理在最底层是基于byte[]，所以数据最终还是on-heap，最近Flink增加了off-heap的内存管理支持，将会在下一个release中正式出现。Flink off-heap的内存管理相对于on-heap的优点主要在于（更多细节，请参考 <a href=\"http://flink.apache.org/news/2015/09/16/off-heap-memory.html\" target=\"_blank\" rel=\"noopener\">Apache Flink: Off-heap Memory in Apache Flink and the curious JIT compiler</a>）：</p>\n<ul>\n<li>启动分配了大内存(例如100G)的JVM很耗费时间，垃圾回收也很慢。如果采用off-heap，剩下的Network buffer和Remaining heap都会很小，垃圾回收也不用考虑MemorySegment中的Java对象了。</li>\n<li>更有效率的IO操作。在off-heap下，将MemorySegment写到磁盘或是网络，可以支持zeor-copy技术，而on-heap的话，则至少需要一次内存拷贝。</li>\n<li>off-heap可用于错误恢复，比如JVM崩溃，在on-heap时，数据也随之丢失，但在off-heap下，off-heap的数据可能还在。此外，off-heap上的数据还可以和其他程序共享。</li>\n</ul>\n<h5 id=\"3-2-2-Spark的内存管理\"><a href=\"#3-2-2-Spark的内存管理\" class=\"headerlink\" title=\"3.2.2 Spark的内存管理\"></a>3.2.2 Spark的内存管理</h5><p>Spark的off-heap内存管理与Flink off-heap模式比较相似，也是通过Java UnSafe API直接访问off-heap内存，通过定制的序列化工具将序列化后的二进制数据存储与off-heap上，Spark的数据结构和算法直接访问和操作在off-heap上的二进制数据。Project Tungsten是一个正在进行中的项目，想了解具体进展可以访问：<a href=\"https://issues.apache.org/jira/browse/SPARK-7075\" target=\"_blank\" rel=\"noopener\">SPARK-7075 Project Tungsten (Spark 1.5 Phase 1)</a>， <a href=\"https://issues.apache.org/jira/browse/SPARK-9697\" target=\"_blank\" rel=\"noopener\">SPARK-9697 Project Tungsten (Spark 1.6)</a>。</p>\n<h4 id=\"3-3-缓存友好的计算\"><a href=\"#3-3-缓存友好的计算\" class=\"headerlink\" title=\"3.3 缓存友好的计算\"></a>3.3 缓存友好的计算</h4><p>磁盘IO和网络IO之前一直被认为是Hadoop系统的瓶颈，但是随着Spark，Flink等新一代的分布式计算框架的发展，越来越多的趋势使得CPU/Memory逐渐成为瓶颈，这些趋势包括：</p>\n<ul>\n<li>更先进的IO硬件逐渐普及。10GB网络和SSD硬盘等已经被越来越多的数据中心使用。</li>\n<li>更高效的存储格式。Parquet，ORC等列式存储被越来越多的Hadoop项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。</li>\n<li>更高效的执行计划。例如Spark DataFrame的执行计划优化器的Fliter-Push-Down优化会将过滤条件尽可能的提前，甚至提前到Parquet的数据访问层，使得在很多实际的工作负载中，并不需要很多的磁盘IO。</li>\n</ul>\n<p>由于CPU处理速度和内存访问速度的差距，提升CPU的处理效率的关键在于最大化的利用L1/L2/L3/Memory，减少任何不必要的Cache miss。定制的序列化工具给Spark和Flink提供了可能，通过定制的序列化工具，Spark和Flink访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时，尽量连续存储，减少内存碎片化对Cache命中率的影响，甚至更进一步，Spark与Flink可以将需要操作的部分数据（如排序时的Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能的提升Cache命中的概率。</p>\n<h5 id=\"3-3-1-Flink中的数据结构\"><a href=\"#3-3-1-Flink中的数据结构\" class=\"headerlink\" title=\"3.3.1 Flink中的数据结构\"></a>3.3.1 Flink中的数据结构</h5><p>以Flink中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink通过特殊设计的排序算法，获得了非常好了性能，其排序算法的实现如下：</p>\n<ul>\n<li>将待排序的数据经过序列化后存储在两个不同的MemorySegment集中。数据全部的序列化值存放于其中一个MemorySegment集中。数据序列化后的Key和指向第一个MemorySegment集中其值的指针存放于第二个MemorySegment集中。</li>\n<li>对第二个MemorySegment集中的Key进行排序，如需交换Key位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。</li>\n<li>排序完成后，访问数据时，按照第二个MemorySegment集中Key的顺序访问，并通过Pinter值找到数据在第一个MemorySegment集中的位置，通过TypeSerializer反序列化成Java对象返回。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-3.jpg?raw=true\" alt=\"\"></p>\n<p>这样实现的好处有：</p>\n<ul>\n<li>通过Key和Full data分离存储的方式，尽量将被操作的数据最小化，提高Cache命中的概率，从而提高CPU的吞吐量。</li>\n<li>移动数据时，只需移动Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。</li>\n<li>TypeComparator直接基于二进制数据进行操作，节省了反序列化的时间。</li>\n</ul>\n<h5 id=\"3-3-2-Spark的数据结构\"><a href=\"#3-3-2-Spark的数据结构\" class=\"headerlink\" title=\"3.3.2 Spark的数据结构\"></a>3.3.2 Spark的数据结构</h5><p>Spark中基于off-heap的排序与Flink几乎一模一样，在这里就不多做介绍了，感兴趣的话，请参考：<a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a>。</p>\n<h3 id=\"4-总结\"><a href=\"#4-总结\" class=\"headerlink\" title=\"4. 总结\"></a>4. 总结</h3><p>本文主要介绍了Hadoop生态圈的一些项目遇到的一些因为JVM内存管理导致的问题，以及社区是如何应对的。基本上，以内存为中心的分布式计算框架，大都开始了部分脱离JVM，走上了自己管理内存的路线，Project Tungsten甚至更进一步，提出了通过LLVM，将部分逻辑编译成本地代码，从而更加深入的挖掘SIMD等CPU潜力。此外，除了Spark，Flink这样的分布式计算框架，HBase（HBASE-11425），HDFS（HDFS-7844）等项目也在部分性能相关的模块通过自己管理内存来规避JVM的一些缺陷，同时提升性能。</p>\n<p>参考：</p>\n<ol>\n<li><p><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html, http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen\" target=\"_blank\" rel=\"noopener\">project tungsten</a></p>\n</li>\n<li><p><a href=\"http://www.lighterra.com/papers/modernmicroprocessors/\" target=\"_blank\" rel=\"noopener\">The “Memory Wall”: Modern Microprocessors</a></p>\n</li>\n<li><p>flink memory management: Apache Flink: <a href=\"http://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html\" target=\"_blank\" rel=\"noopener\">Juggling with Bits and Bytes</a></p>\n</li>\n<li><p><a href=\"https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html\" target=\"_blank\" rel=\"noopener\">java GC：Tuning Java Garbage Collection for Spark Applications</a></p>\n</li>\n<li><p><a href=\"http://openjdk.java.net/projects/valhalla/\" target=\"_blank\" rel=\"noopener\">Project Valhalla: OpenJDK: Valhalla</a></p>\n</li>\n<li><p><a href=\"https://github.com/dweiss/java-sizeof\" target=\"_blank\" rel=\"noopener\">java object size: dweiss/java-sizeof · GitHub</a></p>\n</li>\n<li><p><a href=\"http://www.bigsynapse.com/addressing-big-data-performance\" target=\"_blank\" rel=\"noopener\">Big Data Performance Engineering</a></p>\n</li>\n</ol>\n<p>原文: <a href=\"https://zhuanlan.zhihu.com/hadoop/20228397\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/hadoop/20228397</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>新世纪以来，互联网及个人终端的普及，传统行业的信息化及物联网的发展等产业变化产生了大量的数据，远远超出了单台机器能够处理的范围，分布式存储与处理成为唯一的选项。从2005年开始，Hadoop从最初Nutch项目的一部分，逐步发展成为目前最流行的大数据处理平台。Hadoop生态圈的各个项目，围绕着大数据的存储，计算，分析，展示，安全等各个方面，构建了一个完整的大数据生态系统，并有Cloudera，HortonWorks，MapR等数十家公司基于开源的Hadoop平台构建自己的商业模式，可以认为是最近十年来最成功的开源社区。</p>\n<p>Hadoop 的成功固然是由于其顺应了新世纪以来互联网技术的发展趋势，同时其基于JVM的平台开发也为Hadoop的快速发展起到了促进作用。Hadoop生态圈的项目大都基于Java，Scala，Clojure等JVM语言开发，这些语言良好的语法规范，丰富的第三方类库以及完善的工具支持，为Hadoop这样的超大型项目提供了基础支撑。同时，作为在程序员中普及率最高的语言之一，它也降低了更多程序员使用，或是参与开发Hadoop项目的门槛。同时，基于Scala开发的Spark，甚至因为项目的火热反过来极大的促进了Scala语言的推广。但是随着Hadoop平台的逐步发展，Hadoop生态圈的项目之间的竞争加剧，越来越多的Hadoop项目注意到了这些JVM语言的一些不足之处，希望通过更有效率的处理方式，提升分布式系统的执行效率与健壮性。本文主要以Spark和Flink项目为例，介绍Hadoop社区观察到的一些因为JVM语言的不足导致的问题，以及相应的解决方案与未来可能的发展方向。</p>\n<p>–more–</p>\n<p>注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">本文假设读者对Java和Hadoop系统有基本了解。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-背景\"><a href=\"#1-背景\" class=\"headerlink\" title=\"1. 背景\"></a>1. 背景</h3><p>目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，分布式计算引擎项目之间的竞争也相当激烈。MapReduce作为Hadoop平台的第一个分布式计算引擎，具有非常良好的可扩展性，Yahoo曾成功的搭建了上万台节点的MapReduce系统。但是MapReduce只支持Map和Reduce编程范式，使得复杂数据计算逻辑需要分割为多个Hadoop Job，而每个Hadoop Job都需要从HDFS读取数据，并将Job执行结果写回HDFS，所以会产生大量额外的IO开销，目前MapReduce正在逐渐被其他三个分布式计算引擎替代。Tez,Spark和Flink都支持图结构的分布式计算流，可在同一Job内支持任意复杂逻辑的计算流。Tez的抽象层次较低，用户不易直接使用，Spark与Flink都提供了抽象的分布式数据集以及可在数据集上使用的操作符，用户可以像操作Scala数据集合类似的方式在Spark/FLink中的操作分布式数据集，非常的容易上手，同时，Spark与Flink都在分布式计算引擎之上，提供了针对SQL，流处理，机器学习和图计算等特定数据处理领域的库。</p>\n<p>随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了JVM本身，希望通过解决JVM本身带来的一些问题，提高分布式系统的性能或是健壮性，从而增强自身的竞争力。</p>\n<p>JVM本身作为一个各种类型应用执行的平台，其对Java对象的管理也是基于通用的处理策略，其垃圾回收器通过估算Java对象的生命周期对Java对象进行有效率的管理。针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的JVM参数更有效率的管理Java对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身以及JVM的各参数有深入的了解，极大的提高了分布式计算平台的调优门槛（例如这篇文章中对Spark的调优 <a href=\"https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html\" target=\"_blank\" rel=\"noopener\">Tuning Java Garbage Collection for Spark Applications</a>）。然而类似Spark或是Flink的分布式计算框架，框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的Java对象生命周期，从而为更有效率的管理Java对象提供了可能。</p>\n<h3 id=\"2-JVM存在的问题\"><a href=\"#2-JVM存在的问题\" class=\"headerlink\" title=\"2. JVM存在的问题\"></a>2. JVM存在的问题</h3><h4 id=\"2-1-Java对象开销\"><a href=\"#2-1-Java对象开销\" class=\"headerlink\" title=\"2.1. Java对象开销\"></a>2.1. Java对象开销</h4><p>相对于c/c++等更加接近底层的语言，Java对象的存储密度相对偏低，例如【1】，“abcd”这样简单的字符串在UTF-8编码中需要4个字节存储，但Java采用UTF-16编码存储字符串，需要8个字节存储“abcd”，同时Java对象还对象header等其他额外信息，一个4字节字符串对象，在Java中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率的内存存储，则意味着CPU数据访问吞吐量更高，以及更少的磁盘落地可能。</p>\n<h4 id=\"2-2-对象存储结构引发的cache-miss\"><a href=\"#2-2-对象存储结构引发的cache-miss\" class=\"headerlink\" title=\"2.2. 对象存储结构引发的cache miss\"></a>2.2. 对象存储结构引发的cache miss</h4><p>为了缓解CPU处理速度与内存访问速度的差距【2】，现代CPU数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以cache line为单位加载数据，所以当CPU访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果CPU要访问的数据不在当前缓存所有的cache line中，则需要从内存中加载对应的数据，这被称为一次cache miss。当cache miss非常高的时候，CPU大部分的时间都在等待数据加载，而不是真正的处理数据。Java对象并不是连续的存储在内存上，同时很多的Java数据结构的数据聚集性也不好，在Spark的性能调优中，经常能够观测到大量的cache miss。Java社区有个项目叫做Project Valhalla，可能会部分的解决这个问题，有兴趣的可以看看这儿 <a href=\"http://openjdk.java.net/projects/valhalla/\" target=\"_blank\" rel=\"noopener\">OpenJDK: Valhalla</a>。</p>\n<h4 id=\"2-3-大数据的垃圾回收\"><a href=\"#2-3-大数据的垃圾回收\" class=\"headerlink\" title=\"2.3. 大数据的垃圾回收\"></a>2.3. 大数据的垃圾回收</h4><p>Java的垃圾回收机制，一直让Java开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面，垃圾回收也是Java应用的一颗不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大的影响了Java应用的性能和可用性。在当前的数据中心中，大容量的内存得到了广泛的应用，甚至出现了单台机器配置TB内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换，清洗，处理等步骤。在这个过程中，会产生海量的Java对象，JVM的垃圾回收执行效率对性能有很大影响。通过JVM参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及JVM的各参数有深入的了解，而且有时候这也远远不够。</p>\n<h4 id=\"2-4-OOM问题\"><a href=\"#2-4-OOM问题\" class=\"headerlink\" title=\"2.4. OOM问题\"></a>2.4. OOM问题</h4><p>OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会出现OutOfMemoryError错误，JVM崩溃，分布式框架的健壮性和性能都会受到影响。通过JVM管理内存，同时试图解决OOM问题的应用，通常都需要检查Java对象的大小，并在某些存储Java对象特别多的数据结构中设置阈值进行控制。但是JVM并没有提供官方的检查Java对象大小的工具，第三方的工具类库可能无法准确通用的确定Java对象的大小【6】。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外的业务逻辑无关的代码。</p>\n<h3 id=\"3-解决方案\"><a href=\"#3-解决方案\" class=\"headerlink\" title=\"3. 解决方案\"></a>3. 解决方案</h3><p>为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术：</p>\n<p>(1) 定制的序列化工具。显式内存管理的前提步骤就是序列化，将Java对象序列化成二进制数据存储在内存上（on heap或是off-heap）。通用的序列化框架，如Java默认的java.io.Serializable将Java对象以及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于Spark或是Flink这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如Hadoop的org.apache.hadoop.io.Writable，需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。</p>\n<p>(2) 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有的内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收的压力。</p>\n<p>(3) 缓存友好的数据结构和算法。只将操作相关的数据连续存储，可以最大化的利用L1/L2/L3缓存，减少Cache miss的概率，提升CPU计算的吞吐量。以排序为例，由于排序的主要操作是对Key进行对比，如果将所有排序数据的Key与Value分开，对Key连续存储，则访问Key时的Cache命中率会大大提高。</p>\n<h4 id=\"3-1-定制的序列化工具\"><a href=\"#3-1-定制的序列化工具\" class=\"headerlink\" title=\"3.1 定制的序列化工具\"></a>3.1 定制的序列化工具</h4><p>分布式计算框架可以使用定制序列化工具的前提是要处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。Spark与Flink数据集都支持任意Java或是Scala类型，通过自动生成定制序列化工具，Spark与Flink既保证了API接口对用户的友好度（不用像Hadoop那样数据类型需要继承实现org.apache.hadoop.io.Writable接口），同时也达到了和Hadoop类似的序列化效率。</p>\n<h5 id=\"3-1-1-Spark的序列化框架\"><a href=\"#3-1-1-Spark的序列化框架\" class=\"headerlink\" title=\"3.1.1 Spark的序列化框架\"></a>3.1.1 Spark的序列化框架</h5><p>Spark 支持通用的计算框架，如 Java Serialization和 Kryo。其缺点之前也略有论述，总结如下：</p>\n<ul>\n<li>占用较多内存。Kryo相对于Java Serialization更高，它支持一种类型到Integer的映射机制，序列化时用Integer代替类型信息，但还不及定制的序列化工具效率。</li>\n<li>反序列化时，必须反序列化整个Java对象。</li>\n<li>无法直接操作序列化后的二进制数据。</li>\n</ul>\n<p><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten</a> 提供了一种更好的解决方式，针对于DataFrame API（Spark针对结构化数据的类SQL分析API，参考 <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\" rel=\"noopener\">Spark DataFrame Blog</a>），由于其数据集是有固定Schema的Tuple（可大概类比为数据库中的行），序列化是针对每个Tuple存储其类型信息以及其成员的类型信息是非常浪费内存的，对于Spark来说，Tuple类型信息是全局可知的，所以其定制的序列化工具只存储Tuple的数据，如下图所示</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-1.jpg?raw=true\" alt=\"\"></p>\n<p>对于固定大小的成员，如int，long等，其按照偏移量直接内联存储。对于变长的成员，如String，其存储一个指针，指向真正的数据存储位置，并在数据存储开始处存储其长度。通过这种存储方式，保证了在反序列化时，当只需访问某一个成员时，只需根据偏移量反序列化这个成员，并不需要反序列化整个Tuple。</p>\n<p>Project Tungsten 的定制序列化工具应用在 Sort，HashTable，Shuffle等很多对Spark性能影响最大的地方。比如在Shuffle阶段，定制序列化工具不仅提升了序列化的性能，而且减少了网络传输的数据量，根据DataBricks的Blog介绍，相对于Kryo，Shuffle800万复杂Tuple数据时，其性能至少提高2倍以上。此外，Project Tungsten也计划通过Code generation技术，自动生成序列化代码，将定制序列化工具推广到Spark Core层，从而使得更多的Spark应用受惠于此优化。</p>\n<h5 id=\"3-1-2-Flink的序列化框架\"><a href=\"#3-1-2-Flink的序列化框架\" class=\"headerlink\" title=\"3.1.2 Flink的序列化框架\"></a>3.1.2 Flink的序列化框架</h5><p>Flink在系统设计之初，就借鉴了很多传统 RDBMS 的设计，其中之一就是对数据集的类型信息进行分析，对于特定 Schema 的数据集的处理过程，进行类似RDBMS执行计划优化的优化。同时，数据集的类型信息也可以用来设计定制的序列化工具。和Spark类似，Flink支持任意的Java或是Scala类型，Flink通过Java Reflection框架分析基于Java的Flink程序UDF(User Define Function)的返回类型的类型信息，通过Scala Compiler分析基于Scala的Flink程序UDF的返回类型的类型信息。类型信息由TypeInformation类表示，这个类有诸多具体实现类，例如（更多详情参考Flink官方博客 <a href=\"http://link.zhihu.com/?target=http%3A//flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html\" target=\"_blank\" rel=\"noopener\">Apache Flink: Juggling with Bits and Bytes</a>）：</p>\n<ul>\n<li>BasicTypeInfo: 任意Java基本类型（装包或未装包）和String类型。</li>\n<li>BasicArrayTypeInfo: 任意Java基本类型数组（装包或未装包）和String数组。</li>\n<li>WritableTypeInfo: 任意Hadoop’s Writable接口的实现类.</li>\n<li>TupleTypeInfo: 任意的Flink tuple类型(支持Tuple1 to Tuple25). Flink tuples是固定长度固定类型的Java Tuple实现。</li>\n<li>CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples).</li>\n<li>PojoTypeInfo: 任意的POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是public修饰符定义，要么有getter/setter方法。</li>\n<li>GenericTypeInfo: 任意无法匹配之前几种类型的类。）</li>\n</ul>\n<p>前6种类型数据集几乎覆盖了绝大部分的Flink程序，针对前6种类型数据集，Flink皆可以自动生成对应的TypeSerializer定制序列化工具，非常有效率的对数据集进行序列化和反序列化。对于第7中类型，Flink使用Kryo进行序列化和反序列化。此外，对于可被用作Key的类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据直接进行compare，hash等之类的操作。对于Tuple，CaseClass，Pojo等组合类型，Flink自动生成的TypeSerializer，TypeComparator同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的TypeSerializer，TypeComparator，如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-2.jpg?raw=true\" alt=\"\"></p>\n<p>此外，如有需要，用户可通过集成TypeInformation接口，定制实现自己的序列化工具。</p>\n<h5 id=\"3-2-显式的内存管理\"><a href=\"#3-2-显式的内存管理\" class=\"headerlink\" title=\"3.2 显式的内存管理\"></a>3.2 显式的内存管理</h5><p>垃圾回收的JVM内存管理回避不了的问题，JDK8的G1算法改善了JVM垃圾回收的效率和可用范围，但对于大数据处理的实际环境中，还是远远不够。这也和现在分布式框架的发展趋势有冲突，越来越多的分布式计算框架希望尽可能多的将待处理的数据集放在内存中，而对于JVM垃圾回收来说，内存中Java对象越少，存活时间越短，其效率越高。通过JVM进行内存管理的话，OutOfMemoryError也是一个很难解决的问题。同时，在JVM内存管理中，Java对象有潜在的碎片化存储问题（Java对象所有信息可能不是在内存中连续存储），也有可能在所有Java对象大小没有超过JVM分配内存时，出现OutOfMemoryError问题。</p>\n<h5 id=\"3-2-1-Flink的内存管理\"><a href=\"#3-2-1-Flink的内存管理\" class=\"headerlink\" title=\"3.2.1 Flink的内存管理\"></a>3.2.1 Flink的内存管理</h5><p>Flink将内存分为三个部分，每个部分都有不同的用途：</p>\n<ul>\n<li>Network buffers: 一些以32KB Byte数组为单位的buffer，主要被网络模块用于数据的网络传输。</li>\n<li>Memory Manager pool: 大量以32KB Byte数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。</li>\n<li>Remaining (Free) Heap: 主要留给UDF中用户自己创建的Java对象，由JVM管理。</li>\n</ul>\n<p>Network buffers在Flink中主要基于Netty的网络传输，无需多讲。Remaining Heap用于UDF中用户自己创建的Java对象，在UDF中，用户通常是流式的处理数据，并不需要很多内存，同时Flink也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool（以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。</p>\n<p>在Flink中，内存池由多个MemorySegment组成，每个MemorySegment代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment提供了根据偏移量访问数据的各种方法，如get/put int，long，float，double等，MemorySegment之间数据拷贝等方法，和java.nio.ByteBuffer类似。对于Flink的数据结构，通常包括多个向内存池申请的MemeorySegment，所有要存入的对象，通过TypeSerializer序列化之后，将二进制数据存储在MemorySegment中，在取出时，通过TypeSerializer反序列化。数据结构通过MemorySegment提供的set/get方法访问具体的二进制数据。</p>\n<p>Flink这种看起来比较复杂的内存管理方式带来的好处主要有：</p>\n<ul>\n<li>二进制的数据存储大大提高了数据存储密度，节省了存储空间。</li>\n<li>所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。而对于大部分的分布式计算框架来说，这部分由于要缓存大量数据，是最有可能导致OOM的地方。</li>\n<li>内存池虽然占据了大部分内存，但其中的MemorySegment容量较大(默认32KB)，所以内存池中的Java对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了JVM垃圾回收的压力。</li>\n<li>Remaining Heap的内存虽然由JVM管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的Minor GC就会全部回收掉，一般不会触发Full GC。</li>\n</ul>\n<p>Flink当前的内存管理在最底层是基于byte[]，所以数据最终还是on-heap，最近Flink增加了off-heap的内存管理支持，将会在下一个release中正式出现。Flink off-heap的内存管理相对于on-heap的优点主要在于（更多细节，请参考 <a href=\"http://flink.apache.org/news/2015/09/16/off-heap-memory.html\" target=\"_blank\" rel=\"noopener\">Apache Flink: Off-heap Memory in Apache Flink and the curious JIT compiler</a>）：</p>\n<ul>\n<li>启动分配了大内存(例如100G)的JVM很耗费时间，垃圾回收也很慢。如果采用off-heap，剩下的Network buffer和Remaining heap都会很小，垃圾回收也不用考虑MemorySegment中的Java对象了。</li>\n<li>更有效率的IO操作。在off-heap下，将MemorySegment写到磁盘或是网络，可以支持zeor-copy技术，而on-heap的话，则至少需要一次内存拷贝。</li>\n<li>off-heap可用于错误恢复，比如JVM崩溃，在on-heap时，数据也随之丢失，但在off-heap下，off-heap的数据可能还在。此外，off-heap上的数据还可以和其他程序共享。</li>\n</ul>\n<h5 id=\"3-2-2-Spark的内存管理\"><a href=\"#3-2-2-Spark的内存管理\" class=\"headerlink\" title=\"3.2.2 Spark的内存管理\"></a>3.2.2 Spark的内存管理</h5><p>Spark的off-heap内存管理与Flink off-heap模式比较相似，也是通过Java UnSafe API直接访问off-heap内存，通过定制的序列化工具将序列化后的二进制数据存储与off-heap上，Spark的数据结构和算法直接访问和操作在off-heap上的二进制数据。Project Tungsten是一个正在进行中的项目，想了解具体进展可以访问：<a href=\"https://issues.apache.org/jira/browse/SPARK-7075\" target=\"_blank\" rel=\"noopener\">SPARK-7075 Project Tungsten (Spark 1.5 Phase 1)</a>， <a href=\"https://issues.apache.org/jira/browse/SPARK-9697\" target=\"_blank\" rel=\"noopener\">SPARK-9697 Project Tungsten (Spark 1.6)</a>。</p>\n<h4 id=\"3-3-缓存友好的计算\"><a href=\"#3-3-缓存友好的计算\" class=\"headerlink\" title=\"3.3 缓存友好的计算\"></a>3.3 缓存友好的计算</h4><p>磁盘IO和网络IO之前一直被认为是Hadoop系统的瓶颈，但是随着Spark，Flink等新一代的分布式计算框架的发展，越来越多的趋势使得CPU/Memory逐渐成为瓶颈，这些趋势包括：</p>\n<ul>\n<li>更先进的IO硬件逐渐普及。10GB网络和SSD硬盘等已经被越来越多的数据中心使用。</li>\n<li>更高效的存储格式。Parquet，ORC等列式存储被越来越多的Hadoop项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。</li>\n<li>更高效的执行计划。例如Spark DataFrame的执行计划优化器的Fliter-Push-Down优化会将过滤条件尽可能的提前，甚至提前到Parquet的数据访问层，使得在很多实际的工作负载中，并不需要很多的磁盘IO。</li>\n</ul>\n<p>由于CPU处理速度和内存访问速度的差距，提升CPU的处理效率的关键在于最大化的利用L1/L2/L3/Memory，减少任何不必要的Cache miss。定制的序列化工具给Spark和Flink提供了可能，通过定制的序列化工具，Spark和Flink访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时，尽量连续存储，减少内存碎片化对Cache命中率的影响，甚至更进一步，Spark与Flink可以将需要操作的部分数据（如排序时的Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能的提升Cache命中的概率。</p>\n<h5 id=\"3-3-1-Flink中的数据结构\"><a href=\"#3-3-1-Flink中的数据结构\" class=\"headerlink\" title=\"3.3.1 Flink中的数据结构\"></a>3.3.1 Flink中的数据结构</h5><p>以Flink中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink通过特殊设计的排序算法，获得了非常好了性能，其排序算法的实现如下：</p>\n<ul>\n<li>将待排序的数据经过序列化后存储在两个不同的MemorySegment集中。数据全部的序列化值存放于其中一个MemorySegment集中。数据序列化后的Key和指向第一个MemorySegment集中其值的指针存放于第二个MemorySegment集中。</li>\n<li>对第二个MemorySegment集中的Key进行排序，如需交换Key位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。</li>\n<li>排序完成后，访问数据时，按照第二个MemorySegment集中Key的顺序访问，并通过Pinter值找到数据在第一个MemorySegment集中的位置，通过TypeSerializer反序列化成Java对象返回。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hadoop/hadoop-ecosystem-break-away-jvm-3.jpg?raw=true\" alt=\"\"></p>\n<p>这样实现的好处有：</p>\n<ul>\n<li>通过Key和Full data分离存储的方式，尽量将被操作的数据最小化，提高Cache命中的概率，从而提高CPU的吞吐量。</li>\n<li>移动数据时，只需移动Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。</li>\n<li>TypeComparator直接基于二进制数据进行操作，节省了反序列化的时间。</li>\n</ul>\n<h5 id=\"3-3-2-Spark的数据结构\"><a href=\"#3-3-2-Spark的数据结构\" class=\"headerlink\" title=\"3.3.2 Spark的数据结构\"></a>3.3.2 Spark的数据结构</h5><p>Spark中基于off-heap的排序与Flink几乎一模一样，在这里就不多做介绍了，感兴趣的话，请参考：<a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a>。</p>\n<h3 id=\"4-总结\"><a href=\"#4-总结\" class=\"headerlink\" title=\"4. 总结\"></a>4. 总结</h3><p>本文主要介绍了Hadoop生态圈的一些项目遇到的一些因为JVM内存管理导致的问题，以及社区是如何应对的。基本上，以内存为中心的分布式计算框架，大都开始了部分脱离JVM，走上了自己管理内存的路线，Project Tungsten甚至更进一步，提出了通过LLVM，将部分逻辑编译成本地代码，从而更加深入的挖掘SIMD等CPU潜力。此外，除了Spark，Flink这样的分布式计算框架，HBase（HBASE-11425），HDFS（HDFS-7844）等项目也在部分性能相关的模块通过自己管理内存来规避JVM的一些缺陷，同时提升性能。</p>\n<p>参考：</p>\n<ol>\n<li><p><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html, http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen\" target=\"_blank\" rel=\"noopener\">project tungsten</a></p>\n</li>\n<li><p><a href=\"http://www.lighterra.com/papers/modernmicroprocessors/\" target=\"_blank\" rel=\"noopener\">The “Memory Wall”: Modern Microprocessors</a></p>\n</li>\n<li><p>flink memory management: Apache Flink: <a href=\"http://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html\" target=\"_blank\" rel=\"noopener\">Juggling with Bits and Bytes</a></p>\n</li>\n<li><p><a href=\"https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html\" target=\"_blank\" rel=\"noopener\">java GC：Tuning Java Garbage Collection for Spark Applications</a></p>\n</li>\n<li><p><a href=\"http://openjdk.java.net/projects/valhalla/\" target=\"_blank\" rel=\"noopener\">Project Valhalla: OpenJDK: Valhalla</a></p>\n</li>\n<li><p><a href=\"https://github.com/dweiss/java-sizeof\" target=\"_blank\" rel=\"noopener\">java object size: dweiss/java-sizeof · GitHub</a></p>\n</li>\n<li><p><a href=\"http://www.bigsynapse.com/addressing-big-data-performance\" target=\"_blank\" rel=\"noopener\">Big Data Performance Engineering</a></p>\n</li>\n</ol>\n<p>原文: <a href=\"https://zhuanlan.zhihu.com/hadoop/20228397\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/hadoop/20228397</a></p>\n"},{"title":"Hexo 搭建静态博客","date":"2017-12-01T10:17:23.000Z","archives":"Hexo","_content":"\n### 1.简介\n\nHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章（经常玩CSDN上的人都知道），在几秒内，即可利用靓丽的主题生成静态网页。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，在这里我是基于github搭建的个人博客站点。\n\n### 2. 环境配置\n\n安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序：\n\n- Node.js\n\n- Git\n\n#### 2.1 Git\n\nGit安装参考博文：http://blog.csdn.net/sunnyyoona/article/details/51453880\n\n#### 2.2 Node.js\n\n安装 Node.js 的最佳方式是使用 nvm:\n```\nwget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh\n```\n安装完成后，重启终端并执行下列命令即可安装 Node.js:\n```\nnvm install 4\n```\n\n#### 2.3 Hexo\n\n所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。一般情况下我们机器上没有安装npm，首先要安装npm：\n```\nsudo apt-get install npm\n```\n下面使用npm安装Hexo，安装过程中我们可能会遇到下面的问题:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-1.png?raw=true)\n\n我们需要运行下面的命令，才能安装成功：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-2.png?raw=true)\n\n再重新安装hexo:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-3.png?raw=true)\n\n### 3. 建站\n\n#### 3.1 目录和文件\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。\n```\nhexo init blog  \ncd blog\nnpm install\n```\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-4.png?raw=true)\n\n备注:\n```\n在我这我初始化的目录名称为blog\n```\n新建完成后，指定目录`blog`文件如下：\n```\nxiaosi@yoona:~/blog$ tree -L 2\n.\n├── _config.yml\n├── package.json\n├── scaffolds\n│   ├── draft.md\n│   ├── page.md\n│   └── post.md\n├── source\n│   └── _posts\n└── themes\n    └── landscape\n\n5 directories, 5 files\n```\n文件|说明\n---|---\nscaffolds|脚手架，也就是一个工具模板\nsource|存放博客正文内容\n`_posts`|文件箱\nthemes|存放皮肤的目录  themes/landscape 默认的皮肤\n`_config.yml`|全局的配置文件\n\n\n备注：\n```\n我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。\n\n_posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。\n```\n\n#### 3.2 全局配置\n\n`_config.yml`配置信息：（网站的配置信息，可以在此配置大部分的参数）\n\n配置|说明\n---|---\n站点信息|定义标题，作者，语言\nURL|URL访问路径\n文件目录|正文的存储目录\n写博客配置|文章标题，文章类型，外部链接等\n目录和标签|默认分类，分类图，标签图\n归档设置|归档的类型\n服务器设置|IP，访问端口，日志输出\n时间和日期格式|时间显示格式，日期显示格式\n分页设置|每页显示数量\n评论|外挂的Disqus评论系统\n插件和皮肤|换皮肤，安装插件\nMarkdown语言|markdown的标准\nCSS的stylus格式|是否允许压缩\n部署配置|github发布项目地址\n\n配置`_config.yml`：\n```\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# 站点信息\ntitle: Yoona\nsubtitle:\ndescription: Stay Hungry Stay Foolish\nauthor: sjf0115\nlanguage:\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://sjf0115.club/\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory 文件目录\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing 写博客配置\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Home page setting\n# path: Root path for your blogs index page. (default = '')\n# per_page: Posts displayed per page. (0 = disable pagination)\n# order_by: Posts order. (Order by date descending by default)\nindex_generator:\n  path: ''\n  per_page: 10\n  order_by: -date\n\n# Category & Tag 目录和标签\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format 时间和日期格式\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination 分页设置\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions 插件与皮肤\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape\n\n# Deployment 部署配置\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type:git\n  repo:git@github.com:sjf0115/hexo-blog.git\n```\n\n### 3.3 创建新文章\n\n接下来，我们开始新博客了，创建第一博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。\n\n下面我们创建一篇名为hexo的文章：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-5.png?raw=true)\n\n在_post目录下，就会生成文件：\"hexo.md\":\n```\nxiaosi@yoona:~/blog/source/_posts$ ll\n总用量 5\ndrwxrwxrwx 1 xiaosi xiaosi 256 12月  1 10:17 ./\ndrwxrwxrwx 1 xiaosi xiaosi   0 12月  1 09:59 ../\n-rwxrwxrwx 1 xiaosi xiaosi 826 12月  1 09:59 hello-wor\n```\n然后，我们编辑文件：`hexo.md`，以markdown语法写文章，然后保存。在命令行，启动服务器进行保存:\n```\nxiaosi@yoona:~/blog/source/posts$ hexo s\nNative thread-sleep not available.\nThis will result in much slower performance, but it will still work.\nYou should re-install spawn-sync or upgrade to the lastest version of node if possible.\nCheck /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.\n```\n通过浏览器打开， http://localhost:4000/ ，就出现了我们新写的文章。\n\n### 4. 发布项目到github\n\n#### 4.1 静态化处理\n\n写完文章之后，可以发布到github上面。hexo是一个静态博客框架。静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的：\n```\nxiaosi@yoona:~/blog$ hexo generate\nNative thread-sleep not available.\nThis will result in much slower performance, but it will still work.\nYou should re-install spawn-sync or upgrade to the lastest version of node if possible.\nCheck /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details\nINFO  Start processing\nINFO  Files loaded in 143 ms\nINFO  Generated: index.html\nINFO  Generated: archives/index.html\nINFO  Generated: archives/2016/index.html\nINFO  Generated: categories/diary/index.html\nINFO  Generated: archives/2016/05/index.html\nINFO  Generated: fancybox/blank.gif\nINFO  Generated: archives/2017/12/index.html\nINFO  Generated: fancybox/fancybox_loading.gif\nINFO  Generated: fancybox/fancybox_overlay.png\nINFO  Generated: fancybox/fancybox_sprite@2x.png\nINFO  Generated: archives/2017/index.html\nINFO  Generated: tags/hexo/index.html\nINFO  Generated: fancybox/fancybox_sprite.png\nINFO  Generated: js/script.js\nINFO  Generated: fancybox/jquery.fancybox.css\nINFO  Generated: css/style.css\nINFO  Generated: fancybox/jquery.fancybox.pack.js\nINFO  Generated: fancybox/helpers/jquery.fancybox-buttons.js\nINFO  Generated: fancybox/helpers/jquery.fancybox-media.js\nINFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.css\nINFO  Generated: fancybox/helpers/jquery.fancybox-buttons.css\nINFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.js\nINFO  Generated: css/fonts/fontawesome-webfont.woff\nINFO  Generated: css/fonts/fontawesome-webfont.eot\nINFO  Generated: css/fonts/FontAwesome.otf\nINFO  Generated: fancybox/helpers/fancybox_buttons.png\nINFO  Generated: fancybox/fancybox_loading@2x.gif\nINFO  Generated: fancybox/jquery.fancybox.js\nINFO  Generated: 2017/12/01/hello-world/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.ttf\nINFO  Generated: 2016/05/17/hexo/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.svg\nINFO  Generated: css/images/banner.jpg\nINFO  33 files generated in 1.19 s\n```\n在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件：\n```\nxiaosi@yoona:~/blog/public$ ll\n总用量 24\ndrwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:26 ./\ndrwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 ../\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 2017/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 archives/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 categories/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 css/\ndrwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 fancybox/\n-rwxrwxrwx 1 xiaosi xiaosi 9841 12月  1 10:24 index.html*\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 js/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 tags/\n```\n#### 4.2 发布到github\n\n接下来，我们把这个博客发布到github。\n\n在github中创建一个项目hexo-blog，项目地址：https://github.com/sjf0115/hexo-blog\n\n编辑全局配置文件：`_config.yml`，找到deploy的部分，设置github的项目地址：\n```\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: git@github.com:sjf0115/hexo-blog.git\n```\n然后，通过如下命令进行部署:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-7.png?raw=true)\n\n出现上述问题，可以使用配置ssh秘钥解决。如果出现deployer找不到git: ERROR Deployer not found: git错误，使用下面方式解决：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-8.png?raw=true)\n\n再来一次hexo deploy：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-9.png?raw=true)\n\n到目前为止这个静态的web网站就被部署到了github，检查一下分支是gh-pages。gh-pages是github为了web项目特别设置的分支:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-10.png?raw=true)\n\n然后，点击”Settings”，找到GitHub Pages，提示“Your site is published at `http://sjf0115.github.io/hexo-blog`:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-11.png?raw=true)\n\n打开网页，就是我们刚刚发布站点：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-12.png?raw=true)\n\n可以看到网页样式出现问题，不用担心，我们设置域名之后就OK了。\n\n#### 4.3 设置域名\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-13.png?raw=true)\n\n在dnspod控制台，设置主机记录@，类型A，到IP 23.235.37.133（github地址）:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-14.png?raw=true)\n\n对域名判断是否生效，对域名执行ping：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-15.png?raw=true)\n\n在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名sjf0115.club。通过浏览器，访问http://sjf0115.club ， 就打开了我们建好的博客站点:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-16.png?raw=true)\n","source":"_posts/Hexo/Hexo+Github搭建静态博客.md","raw":"---\ntitle: Hexo 搭建静态博客\ndate: 2017-12-01 18:17:23\ntags:\n- Hexo\n\narchives: Hexo\npermalink: hexo_generate_blog\n---\n\n### 1.简介\n\nHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章（经常玩CSDN上的人都知道），在几秒内，即可利用靓丽的主题生成静态网页。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，在这里我是基于github搭建的个人博客站点。\n\n### 2. 环境配置\n\n安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序：\n\n- Node.js\n\n- Git\n\n#### 2.1 Git\n\nGit安装参考博文：http://blog.csdn.net/sunnyyoona/article/details/51453880\n\n#### 2.2 Node.js\n\n安装 Node.js 的最佳方式是使用 nvm:\n```\nwget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh\n```\n安装完成后，重启终端并执行下列命令即可安装 Node.js:\n```\nnvm install 4\n```\n\n#### 2.3 Hexo\n\n所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。一般情况下我们机器上没有安装npm，首先要安装npm：\n```\nsudo apt-get install npm\n```\n下面使用npm安装Hexo，安装过程中我们可能会遇到下面的问题:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-1.png?raw=true)\n\n我们需要运行下面的命令，才能安装成功：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-2.png?raw=true)\n\n再重新安装hexo:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-3.png?raw=true)\n\n### 3. 建站\n\n#### 3.1 目录和文件\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。\n```\nhexo init blog  \ncd blog\nnpm install\n```\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-4.png?raw=true)\n\n备注:\n```\n在我这我初始化的目录名称为blog\n```\n新建完成后，指定目录`blog`文件如下：\n```\nxiaosi@yoona:~/blog$ tree -L 2\n.\n├── _config.yml\n├── package.json\n├── scaffolds\n│   ├── draft.md\n│   ├── page.md\n│   └── post.md\n├── source\n│   └── _posts\n└── themes\n    └── landscape\n\n5 directories, 5 files\n```\n文件|说明\n---|---\nscaffolds|脚手架，也就是一个工具模板\nsource|存放博客正文内容\n`_posts`|文件箱\nthemes|存放皮肤的目录  themes/landscape 默认的皮肤\n`_config.yml`|全局的配置文件\n\n\n备注：\n```\n我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。\n\n_posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。\n```\n\n#### 3.2 全局配置\n\n`_config.yml`配置信息：（网站的配置信息，可以在此配置大部分的参数）\n\n配置|说明\n---|---\n站点信息|定义标题，作者，语言\nURL|URL访问路径\n文件目录|正文的存储目录\n写博客配置|文章标题，文章类型，外部链接等\n目录和标签|默认分类，分类图，标签图\n归档设置|归档的类型\n服务器设置|IP，访问端口，日志输出\n时间和日期格式|时间显示格式，日期显示格式\n分页设置|每页显示数量\n评论|外挂的Disqus评论系统\n插件和皮肤|换皮肤，安装插件\nMarkdown语言|markdown的标准\nCSS的stylus格式|是否允许压缩\n部署配置|github发布项目地址\n\n配置`_config.yml`：\n```\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# 站点信息\ntitle: Yoona\nsubtitle:\ndescription: Stay Hungry Stay Foolish\nauthor: sjf0115\nlanguage:\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://sjf0115.club/\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory 文件目录\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing 写博客配置\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Home page setting\n# path: Root path for your blogs index page. (default = '')\n# per_page: Posts displayed per page. (0 = disable pagination)\n# order_by: Posts order. (Order by date descending by default)\nindex_generator:\n  path: ''\n  per_page: 10\n  order_by: -date\n\n# Category & Tag 目录和标签\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format 时间和日期格式\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination 分页设置\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions 插件与皮肤\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape\n\n# Deployment 部署配置\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type:git\n  repo:git@github.com:sjf0115/hexo-blog.git\n```\n\n### 3.3 创建新文章\n\n接下来，我们开始新博客了，创建第一博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。\n\n下面我们创建一篇名为hexo的文章：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-5.png?raw=true)\n\n在_post目录下，就会生成文件：\"hexo.md\":\n```\nxiaosi@yoona:~/blog/source/_posts$ ll\n总用量 5\ndrwxrwxrwx 1 xiaosi xiaosi 256 12月  1 10:17 ./\ndrwxrwxrwx 1 xiaosi xiaosi   0 12月  1 09:59 ../\n-rwxrwxrwx 1 xiaosi xiaosi 826 12月  1 09:59 hello-wor\n```\n然后，我们编辑文件：`hexo.md`，以markdown语法写文章，然后保存。在命令行，启动服务器进行保存:\n```\nxiaosi@yoona:~/blog/source/posts$ hexo s\nNative thread-sleep not available.\nThis will result in much slower performance, but it will still work.\nYou should re-install spawn-sync or upgrade to the lastest version of node if possible.\nCheck /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.\n```\n通过浏览器打开， http://localhost:4000/ ，就出现了我们新写的文章。\n\n### 4. 发布项目到github\n\n#### 4.1 静态化处理\n\n写完文章之后，可以发布到github上面。hexo是一个静态博客框架。静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的：\n```\nxiaosi@yoona:~/blog$ hexo generate\nNative thread-sleep not available.\nThis will result in much slower performance, but it will still work.\nYou should re-install spawn-sync or upgrade to the lastest version of node if possible.\nCheck /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details\nINFO  Start processing\nINFO  Files loaded in 143 ms\nINFO  Generated: index.html\nINFO  Generated: archives/index.html\nINFO  Generated: archives/2016/index.html\nINFO  Generated: categories/diary/index.html\nINFO  Generated: archives/2016/05/index.html\nINFO  Generated: fancybox/blank.gif\nINFO  Generated: archives/2017/12/index.html\nINFO  Generated: fancybox/fancybox_loading.gif\nINFO  Generated: fancybox/fancybox_overlay.png\nINFO  Generated: fancybox/fancybox_sprite@2x.png\nINFO  Generated: archives/2017/index.html\nINFO  Generated: tags/hexo/index.html\nINFO  Generated: fancybox/fancybox_sprite.png\nINFO  Generated: js/script.js\nINFO  Generated: fancybox/jquery.fancybox.css\nINFO  Generated: css/style.css\nINFO  Generated: fancybox/jquery.fancybox.pack.js\nINFO  Generated: fancybox/helpers/jquery.fancybox-buttons.js\nINFO  Generated: fancybox/helpers/jquery.fancybox-media.js\nINFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.css\nINFO  Generated: fancybox/helpers/jquery.fancybox-buttons.css\nINFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.js\nINFO  Generated: css/fonts/fontawesome-webfont.woff\nINFO  Generated: css/fonts/fontawesome-webfont.eot\nINFO  Generated: css/fonts/FontAwesome.otf\nINFO  Generated: fancybox/helpers/fancybox_buttons.png\nINFO  Generated: fancybox/fancybox_loading@2x.gif\nINFO  Generated: fancybox/jquery.fancybox.js\nINFO  Generated: 2017/12/01/hello-world/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.ttf\nINFO  Generated: 2016/05/17/hexo/index.html\nINFO  Generated: css/fonts/fontawesome-webfont.svg\nINFO  Generated: css/images/banner.jpg\nINFO  33 files generated in 1.19 s\n```\n在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件：\n```\nxiaosi@yoona:~/blog/public$ ll\n总用量 24\ndrwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:26 ./\ndrwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 ../\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 2017/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 archives/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 categories/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 css/\ndrwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 fancybox/\n-rwxrwxrwx 1 xiaosi xiaosi 9841 12月  1 10:24 index.html*\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 js/\ndrwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 tags/\n```\n#### 4.2 发布到github\n\n接下来，我们把这个博客发布到github。\n\n在github中创建一个项目hexo-blog，项目地址：https://github.com/sjf0115/hexo-blog\n\n编辑全局配置文件：`_config.yml`，找到deploy的部分，设置github的项目地址：\n```\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: git@github.com:sjf0115/hexo-blog.git\n```\n然后，通过如下命令进行部署:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-7.png?raw=true)\n\n出现上述问题，可以使用配置ssh秘钥解决。如果出现deployer找不到git: ERROR Deployer not found: git错误，使用下面方式解决：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-8.png?raw=true)\n\n再来一次hexo deploy：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-9.png?raw=true)\n\n到目前为止这个静态的web网站就被部署到了github，检查一下分支是gh-pages。gh-pages是github为了web项目特别设置的分支:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-10.png?raw=true)\n\n然后，点击”Settings”，找到GitHub Pages，提示“Your site is published at `http://sjf0115.github.io/hexo-blog`:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-11.png?raw=true)\n\n打开网页，就是我们刚刚发布站点：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-12.png?raw=true)\n\n可以看到网页样式出现问题，不用担心，我们设置域名之后就OK了。\n\n#### 4.3 设置域名\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-13.png?raw=true)\n\n在dnspod控制台，设置主机记录@，类型A，到IP 23.235.37.133（github地址）:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-14.png?raw=true)\n\n对域名判断是否生效，对域名执行ping：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-15.png?raw=true)\n\n在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名sjf0115.club。通过浏览器，访问http://sjf0115.club ， 就打开了我们建好的博客站点:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-16.png?raw=true)\n","slug":"hexo_generate_blog","published":1,"updated":"2018-01-29T11:22:21.234Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje58titg003sordbiuj5kvb6","content":"<h3 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h3><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章（经常玩CSDN上的人都知道），在几秒内，即可利用靓丽的主题生成静态网页。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，在这里我是基于github搭建的个人博客站点。</p>\n<h3 id=\"2-环境配置\"><a href=\"#2-环境配置\" class=\"headerlink\" title=\"2. 环境配置\"></a>2. 环境配置</h3><p>安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序：</p>\n<ul>\n<li><p>Node.js</p>\n</li>\n<li><p>Git</p>\n</li>\n</ul>\n<h4 id=\"2-1-Git\"><a href=\"#2-1-Git\" class=\"headerlink\" title=\"2.1 Git\"></a>2.1 Git</h4><p>Git安装参考博文：<a href=\"http://blog.csdn.net/sunnyyoona/article/details/51453880\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/sunnyyoona/article/details/51453880</a></p>\n<h4 id=\"2-2-Node-js\"><a href=\"#2-2-Node-js\" class=\"headerlink\" title=\"2.2 Node.js\"></a>2.2 Node.js</h4><p>安装 Node.js 的最佳方式是使用 nvm:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh</span><br></pre></td></tr></table></figure></p>\n<p>安装完成后，重启终端并执行下列命令即可安装 Node.js:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">nvm install 4</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-Hexo\"><a href=\"#2-3-Hexo\" class=\"headerlink\" title=\"2.3 Hexo\"></a>2.3 Hexo</h4><p>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。一般情况下我们机器上没有安装npm，首先要安装npm：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo apt-get install npm</span><br></pre></td></tr></table></figure></p>\n<p>下面使用npm安装Hexo，安装过程中我们可能会遇到下面的问题:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-1.png?raw=true\" alt=\"\"></p>\n<p>我们需要运行下面的命令，才能安装成功：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-2.png?raw=true\" alt=\"\"></p>\n<p>再重新安装hexo:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-建站\"><a href=\"#3-建站\" class=\"headerlink\" title=\"3. 建站\"></a>3. 建站</h3><h4 id=\"3-1-目录和文件\"><a href=\"#3-1-目录和文件\" class=\"headerlink\" title=\"3.1 目录和文件\"></a>3.1 目录和文件</h4><p>安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hexo init blog  </span><br><span class=\"line\">cd blog</span><br><span class=\"line\">npm install</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-4.png?raw=true\" alt=\"\"></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">在我这我初始化的目录名称为blog</span><br></pre></td></tr></table></figure></p>\n<p>新建完成后，指定目录<code>blog</code>文件如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog$ tree -L 2</span><br><span class=\"line\">.</span><br><span class=\"line\">├── _config.yml</span><br><span class=\"line\">├── package.json</span><br><span class=\"line\">├── scaffolds</span><br><span class=\"line\">│   ├── draft.md</span><br><span class=\"line\">│   ├── page.md</span><br><span class=\"line\">│   └── post.md</span><br><span class=\"line\">├── source</span><br><span class=\"line\">│   └── _posts</span><br><span class=\"line\">└── themes</span><br><span class=\"line\">    └── landscape</span><br><span class=\"line\"></span><br><span class=\"line\">5 directories, 5 files</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>文件</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>scaffolds</td>\n<td>脚手架，也就是一个工具模板</td>\n</tr>\n<tr>\n<td>source</td>\n<td>存放博客正文内容</td>\n</tr>\n<tr>\n<td><code>_posts</code></td>\n<td>文件箱</td>\n</tr>\n<tr>\n<td>themes</td>\n<td>存放皮肤的目录  themes/landscape 默认的皮肤</td>\n</tr>\n<tr>\n<td><code>_config.yml</code></td>\n<td>全局的配置文件</td>\n</tr>\n</tbody>\n</table>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。</span><br><span class=\"line\"></span><br><span class=\"line\">_posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-2-全局配置\"><a href=\"#3-2-全局配置\" class=\"headerlink\" title=\"3.2 全局配置\"></a>3.2 全局配置</h4><p><code>_config.yml</code>配置信息：（网站的配置信息，可以在此配置大部分的参数）</p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>站点信息</td>\n<td>定义标题，作者，语言</td>\n</tr>\n<tr>\n<td>URL</td>\n<td>URL访问路径</td>\n</tr>\n<tr>\n<td>文件目录</td>\n<td>正文的存储目录</td>\n</tr>\n<tr>\n<td>写博客配置</td>\n<td>文章标题，文章类型，外部链接等</td>\n</tr>\n<tr>\n<td>目录和标签</td>\n<td>默认分类，分类图，标签图</td>\n</tr>\n<tr>\n<td>归档设置</td>\n<td>归档的类型</td>\n</tr>\n<tr>\n<td>服务器设置</td>\n<td>IP，访问端口，日志输出</td>\n</tr>\n<tr>\n<td>时间和日期格式</td>\n<td>时间显示格式，日期显示格式</td>\n</tr>\n<tr>\n<td>分页设置</td>\n<td>每页显示数量</td>\n</tr>\n<tr>\n<td>评论</td>\n<td>外挂的Disqus评论系统</td>\n</tr>\n<tr>\n<td>插件和皮肤</td>\n<td>换皮肤，安装插件</td>\n</tr>\n<tr>\n<td>Markdown语言</td>\n<td>markdown的标准</td>\n</tr>\n<tr>\n<td>CSS的stylus格式</td>\n<td>是否允许压缩</td>\n</tr>\n<tr>\n<td>部署配置</td>\n<td>github发布项目地址</td>\n</tr>\n</tbody>\n</table>\n<p>配置<code>_config.yml</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Hexo Configuration</span><br><span class=\"line\">## Docs: https://hexo.io/docs/configuration.html</span><br><span class=\"line\">## Source: https://github.com/hexojs/hexo/</span><br><span class=\"line\"></span><br><span class=\"line\"># 站点信息</span><br><span class=\"line\">title: Yoona</span><br><span class=\"line\">subtitle:</span><br><span class=\"line\">description: Stay Hungry Stay Foolish</span><br><span class=\"line\">author: sjf0115</span><br><span class=\"line\">language:</span><br><span class=\"line\">timezone:</span><br><span class=\"line\"></span><br><span class=\"line\"># URL</span><br><span class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</span><br><span class=\"line\">url: http://sjf0115.club/</span><br><span class=\"line\">root: /</span><br><span class=\"line\">permalink: :year/:month/:day/:title/</span><br><span class=\"line\">permalink_defaults:</span><br><span class=\"line\"></span><br><span class=\"line\"># Directory 文件目录</span><br><span class=\"line\">source_dir: source</span><br><span class=\"line\">public_dir: public</span><br><span class=\"line\">tag_dir: tags</span><br><span class=\"line\">archive_dir: archives</span><br><span class=\"line\">category_dir: categories</span><br><span class=\"line\">code_dir: downloads/code</span><br><span class=\"line\">i18n_dir: :lang</span><br><span class=\"line\">skip_render:</span><br><span class=\"line\"></span><br><span class=\"line\"># Writing 写博客配置</span><br><span class=\"line\">new_post_name: :title.md # File name of new posts</span><br><span class=\"line\">default_layout: post</span><br><span class=\"line\">titlecase: false # Transform title into titlecase</span><br><span class=\"line\">external_link: true # Open external links in new tab</span><br><span class=\"line\">filename_case: 0</span><br><span class=\"line\">render_drafts: false</span><br><span class=\"line\">post_asset_folder: false</span><br><span class=\"line\">relative_link: false</span><br><span class=\"line\">future: true</span><br><span class=\"line\">highlight:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  line_number: true</span><br><span class=\"line\">  auto_detect: false</span><br><span class=\"line\">  tab_replace:</span><br><span class=\"line\"></span><br><span class=\"line\"># Home page setting</span><br><span class=\"line\"># path: Root path for your blogs index page. (default = &apos;&apos;)</span><br><span class=\"line\"># per_page: Posts displayed per page. (0 = disable pagination)</span><br><span class=\"line\"># order_by: Posts order. (Order by date descending by default)</span><br><span class=\"line\">index_generator:</span><br><span class=\"line\">  path: &apos;&apos;</span><br><span class=\"line\">  per_page: 10</span><br><span class=\"line\">  order_by: -date</span><br><span class=\"line\"></span><br><span class=\"line\"># Category &amp; Tag 目录和标签</span><br><span class=\"line\">default_category: uncategorized</span><br><span class=\"line\">category_map:</span><br><span class=\"line\">tag_map:</span><br><span class=\"line\"></span><br><span class=\"line\"># Date / Time format 时间和日期格式</span><br><span class=\"line\">## Hexo uses Moment.js to parse and display date</span><br><span class=\"line\">## You can customize the date format as defined in</span><br><span class=\"line\">## http://momentjs.com/docs/#/displaying/format/</span><br><span class=\"line\">date_format: YYYY-MM-DD</span><br><span class=\"line\">time_format: HH:mm:ss</span><br><span class=\"line\"></span><br><span class=\"line\"># Pagination 分页设置</span><br><span class=\"line\">## Set per_page to 0 to disable pagination</span><br><span class=\"line\">per_page: 10</span><br><span class=\"line\">pagination_dir: page</span><br><span class=\"line\"></span><br><span class=\"line\"># Extensions 插件与皮肤</span><br><span class=\"line\">## Plugins: https://hexo.io/plugins/</span><br><span class=\"line\">## Themes: https://hexo.io/themes/</span><br><span class=\"line\">theme: landscape</span><br><span class=\"line\"></span><br><span class=\"line\"># Deployment 部署配置</span><br><span class=\"line\">## Docs: https://hexo.io/docs/deployment.html</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type:git</span><br><span class=\"line\">  repo:git@github.com:sjf0115/hexo-blog.git</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-3-创建新文章\"><a href=\"#3-3-创建新文章\" class=\"headerlink\" title=\"3.3 创建新文章\"></a>3.3 创建新文章</h3><p>接下来，我们开始新博客了，创建第一博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。</p>\n<p>下面我们创建一篇名为hexo的文章：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-5.png?raw=true\" alt=\"\"></p>\n<p>在_post目录下，就会生成文件：”hexo.md”:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog/source/_posts$ ll</span><br><span class=\"line\">总用量 5</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 256 12月  1 10:17 ./</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi   0 12月  1 09:59 ../</span><br><span class=\"line\">-rwxrwxrwx 1 xiaosi xiaosi 826 12月  1 09:59 hello-wor</span><br></pre></td></tr></table></figure></p>\n<p>然后，我们编辑文件：<code>hexo.md</code>，以markdown语法写文章，然后保存。在命令行，启动服务器进行保存:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog/source/posts$ hexo s</span><br><span class=\"line\">Native thread-sleep not available.</span><br><span class=\"line\">This will result in much slower performance, but it will still work.</span><br><span class=\"line\">You should re-install spawn-sync or upgrade to the lastest version of node if possible.</span><br><span class=\"line\">Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p>\n<p>通过浏览器打开， <a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a> ，就出现了我们新写的文章。</p>\n<h3 id=\"4-发布项目到github\"><a href=\"#4-发布项目到github\" class=\"headerlink\" title=\"4. 发布项目到github\"></a>4. 发布项目到github</h3><h4 id=\"4-1-静态化处理\"><a href=\"#4-1-静态化处理\" class=\"headerlink\" title=\"4.1 静态化处理\"></a>4.1 静态化处理</h4><p>写完文章之后，可以发布到github上面。hexo是一个静态博客框架。静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog$ hexo generate</span><br><span class=\"line\">Native thread-sleep not available.</span><br><span class=\"line\">This will result in much slower performance, but it will still work.</span><br><span class=\"line\">You should re-install spawn-sync or upgrade to the lastest version of node if possible.</span><br><span class=\"line\">Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">INFO  Files loaded in 143 ms</span><br><span class=\"line\">INFO  Generated: index.html</span><br><span class=\"line\">INFO  Generated: archives/index.html</span><br><span class=\"line\">INFO  Generated: archives/2016/index.html</span><br><span class=\"line\">INFO  Generated: categories/diary/index.html</span><br><span class=\"line\">INFO  Generated: archives/2016/05/index.html</span><br><span class=\"line\">INFO  Generated: fancybox/blank.gif</span><br><span class=\"line\">INFO  Generated: archives/2017/12/index.html</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_loading.gif</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_overlay.png</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_sprite@2x.png</span><br><span class=\"line\">INFO  Generated: archives/2017/index.html</span><br><span class=\"line\">INFO  Generated: tags/hexo/index.html</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_sprite.png</span><br><span class=\"line\">INFO  Generated: js/script.js</span><br><span class=\"line\">INFO  Generated: fancybox/jquery.fancybox.css</span><br><span class=\"line\">INFO  Generated: css/style.css</span><br><span class=\"line\">INFO  Generated: fancybox/jquery.fancybox.pack.js</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-buttons.js</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-media.js</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.css</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-buttons.css</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.js</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.woff</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.eot</span><br><span class=\"line\">INFO  Generated: css/fonts/FontAwesome.otf</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/fancybox_buttons.png</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_loading@2x.gif</span><br><span class=\"line\">INFO  Generated: fancybox/jquery.fancybox.js</span><br><span class=\"line\">INFO  Generated: 2017/12/01/hello-world/index.html</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.ttf</span><br><span class=\"line\">INFO  Generated: 2016/05/17/hexo/index.html</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.svg</span><br><span class=\"line\">INFO  Generated: css/images/banner.jpg</span><br><span class=\"line\">INFO  33 files generated in 1.19 s</span><br></pre></td></tr></table></figure></p>\n<p>在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog/public$ ll</span><br><span class=\"line\">总用量 24</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:26 ./</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 ../</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 2017/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 archives/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 categories/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 css/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 fancybox/</span><br><span class=\"line\">-rwxrwxrwx 1 xiaosi xiaosi 9841 12月  1 10:24 index.html*</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 js/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 tags/</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-2-发布到github\"><a href=\"#4-2-发布到github\" class=\"headerlink\" title=\"4.2 发布到github\"></a>4.2 发布到github</h4><p>接下来，我们把这个博客发布到github。</p>\n<p>在github中创建一个项目hexo-blog，项目地址：<a href=\"https://github.com/sjf0115/hexo-blog\" target=\"_blank\" rel=\"noopener\">https://github.com/sjf0115/hexo-blog</a></p>\n<p>编辑全局配置文件：<code>_config.yml</code>，找到deploy的部分，设置github的项目地址：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Deployment</span><br><span class=\"line\">## Docs: https://hexo.io/docs/deployment.html</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: git@github.com:sjf0115/hexo-blog.git</span><br></pre></td></tr></table></figure></p>\n<p>然后，通过如下命令进行部署:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-7.png?raw=true\" alt=\"\"></p>\n<p>出现上述问题，可以使用配置ssh秘钥解决。如果出现deployer找不到git: ERROR Deployer not found: git错误，使用下面方式解决：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-8.png?raw=true\" alt=\"\"></p>\n<p>再来一次hexo deploy：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-9.png?raw=true\" alt=\"\"></p>\n<p>到目前为止这个静态的web网站就被部署到了github，检查一下分支是gh-pages。gh-pages是github为了web项目特别设置的分支:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-10.png?raw=true\" alt=\"\"></p>\n<p>然后，点击”Settings”，找到GitHub Pages，提示“Your site is published at <code>http://sjf0115.github.io/hexo-blog</code>:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-11.png?raw=true\" alt=\"\"></p>\n<p>打开网页，就是我们刚刚发布站点：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-12.png?raw=true\" alt=\"\"></p>\n<p>可以看到网页样式出现问题，不用担心，我们设置域名之后就OK了。</p>\n<h4 id=\"4-3-设置域名\"><a href=\"#4-3-设置域名\" class=\"headerlink\" title=\"4.3 设置域名\"></a>4.3 设置域名</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-13.png?raw=true\" alt=\"\"></p>\n<p>在dnspod控制台，设置主机记录@，类型A，到IP 23.235.37.133（github地址）:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-14.png?raw=true\" alt=\"\"></p>\n<p>对域名判断是否生效，对域名执行ping：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-15.png?raw=true\" alt=\"\"></p>\n<p>在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名sjf0115.club。通过浏览器，访问<a href=\"http://sjf0115.club\" target=\"_blank\" rel=\"noopener\">http://sjf0115.club</a> ， 就打开了我们建好的博客站点:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-16.png?raw=true\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h3><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章（经常玩CSDN上的人都知道），在几秒内，即可利用靓丽的主题生成静态网页。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，在这里我是基于github搭建的个人博客站点。</p>\n<h3 id=\"2-环境配置\"><a href=\"#2-环境配置\" class=\"headerlink\" title=\"2. 环境配置\"></a>2. 环境配置</h3><p>安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序：</p>\n<ul>\n<li><p>Node.js</p>\n</li>\n<li><p>Git</p>\n</li>\n</ul>\n<h4 id=\"2-1-Git\"><a href=\"#2-1-Git\" class=\"headerlink\" title=\"2.1 Git\"></a>2.1 Git</h4><p>Git安装参考博文：<a href=\"http://blog.csdn.net/sunnyyoona/article/details/51453880\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/sunnyyoona/article/details/51453880</a></p>\n<h4 id=\"2-2-Node-js\"><a href=\"#2-2-Node-js\" class=\"headerlink\" title=\"2.2 Node.js\"></a>2.2 Node.js</h4><p>安装 Node.js 的最佳方式是使用 nvm:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh</span><br></pre></td></tr></table></figure></p>\n<p>安装完成后，重启终端并执行下列命令即可安装 Node.js:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">nvm install 4</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-Hexo\"><a href=\"#2-3-Hexo\" class=\"headerlink\" title=\"2.3 Hexo\"></a>2.3 Hexo</h4><p>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。一般情况下我们机器上没有安装npm，首先要安装npm：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo apt-get install npm</span><br></pre></td></tr></table></figure></p>\n<p>下面使用npm安装Hexo，安装过程中我们可能会遇到下面的问题:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-1.png?raw=true\" alt=\"\"></p>\n<p>我们需要运行下面的命令，才能安装成功：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-2.png?raw=true\" alt=\"\"></p>\n<p>再重新安装hexo:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-建站\"><a href=\"#3-建站\" class=\"headerlink\" title=\"3. 建站\"></a>3. 建站</h3><h4 id=\"3-1-目录和文件\"><a href=\"#3-1-目录和文件\" class=\"headerlink\" title=\"3.1 目录和文件\"></a>3.1 目录和文件</h4><p>安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hexo init blog  </span><br><span class=\"line\">cd blog</span><br><span class=\"line\">npm install</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-4.png?raw=true\" alt=\"\"></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">在我这我初始化的目录名称为blog</span><br></pre></td></tr></table></figure></p>\n<p>新建完成后，指定目录<code>blog</code>文件如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog$ tree -L 2</span><br><span class=\"line\">.</span><br><span class=\"line\">├── _config.yml</span><br><span class=\"line\">├── package.json</span><br><span class=\"line\">├── scaffolds</span><br><span class=\"line\">│   ├── draft.md</span><br><span class=\"line\">│   ├── page.md</span><br><span class=\"line\">│   └── post.md</span><br><span class=\"line\">├── source</span><br><span class=\"line\">│   └── _posts</span><br><span class=\"line\">└── themes</span><br><span class=\"line\">    └── landscape</span><br><span class=\"line\"></span><br><span class=\"line\">5 directories, 5 files</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>文件</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>scaffolds</td>\n<td>脚手架，也就是一个工具模板</td>\n</tr>\n<tr>\n<td>source</td>\n<td>存放博客正文内容</td>\n</tr>\n<tr>\n<td><code>_posts</code></td>\n<td>文件箱</td>\n</tr>\n<tr>\n<td>themes</td>\n<td>存放皮肤的目录  themes/landscape 默认的皮肤</td>\n</tr>\n<tr>\n<td><code>_config.yml</code></td>\n<td>全局的配置文件</td>\n</tr>\n</tbody>\n</table>\n<p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。</span><br><span class=\"line\"></span><br><span class=\"line\">_posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-2-全局配置\"><a href=\"#3-2-全局配置\" class=\"headerlink\" title=\"3.2 全局配置\"></a>3.2 全局配置</h4><p><code>_config.yml</code>配置信息：（网站的配置信息，可以在此配置大部分的参数）</p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>站点信息</td>\n<td>定义标题，作者，语言</td>\n</tr>\n<tr>\n<td>URL</td>\n<td>URL访问路径</td>\n</tr>\n<tr>\n<td>文件目录</td>\n<td>正文的存储目录</td>\n</tr>\n<tr>\n<td>写博客配置</td>\n<td>文章标题，文章类型，外部链接等</td>\n</tr>\n<tr>\n<td>目录和标签</td>\n<td>默认分类，分类图，标签图</td>\n</tr>\n<tr>\n<td>归档设置</td>\n<td>归档的类型</td>\n</tr>\n<tr>\n<td>服务器设置</td>\n<td>IP，访问端口，日志输出</td>\n</tr>\n<tr>\n<td>时间和日期格式</td>\n<td>时间显示格式，日期显示格式</td>\n</tr>\n<tr>\n<td>分页设置</td>\n<td>每页显示数量</td>\n</tr>\n<tr>\n<td>评论</td>\n<td>外挂的Disqus评论系统</td>\n</tr>\n<tr>\n<td>插件和皮肤</td>\n<td>换皮肤，安装插件</td>\n</tr>\n<tr>\n<td>Markdown语言</td>\n<td>markdown的标准</td>\n</tr>\n<tr>\n<td>CSS的stylus格式</td>\n<td>是否允许压缩</td>\n</tr>\n<tr>\n<td>部署配置</td>\n<td>github发布项目地址</td>\n</tr>\n</tbody>\n</table>\n<p>配置<code>_config.yml</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Hexo Configuration</span><br><span class=\"line\">## Docs: https://hexo.io/docs/configuration.html</span><br><span class=\"line\">## Source: https://github.com/hexojs/hexo/</span><br><span class=\"line\"></span><br><span class=\"line\"># 站点信息</span><br><span class=\"line\">title: Yoona</span><br><span class=\"line\">subtitle:</span><br><span class=\"line\">description: Stay Hungry Stay Foolish</span><br><span class=\"line\">author: sjf0115</span><br><span class=\"line\">language:</span><br><span class=\"line\">timezone:</span><br><span class=\"line\"></span><br><span class=\"line\"># URL</span><br><span class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</span><br><span class=\"line\">url: http://sjf0115.club/</span><br><span class=\"line\">root: /</span><br><span class=\"line\">permalink: :year/:month/:day/:title/</span><br><span class=\"line\">permalink_defaults:</span><br><span class=\"line\"></span><br><span class=\"line\"># Directory 文件目录</span><br><span class=\"line\">source_dir: source</span><br><span class=\"line\">public_dir: public</span><br><span class=\"line\">tag_dir: tags</span><br><span class=\"line\">archive_dir: archives</span><br><span class=\"line\">category_dir: categories</span><br><span class=\"line\">code_dir: downloads/code</span><br><span class=\"line\">i18n_dir: :lang</span><br><span class=\"line\">skip_render:</span><br><span class=\"line\"></span><br><span class=\"line\"># Writing 写博客配置</span><br><span class=\"line\">new_post_name: :title.md # File name of new posts</span><br><span class=\"line\">default_layout: post</span><br><span class=\"line\">titlecase: false # Transform title into titlecase</span><br><span class=\"line\">external_link: true # Open external links in new tab</span><br><span class=\"line\">filename_case: 0</span><br><span class=\"line\">render_drafts: false</span><br><span class=\"line\">post_asset_folder: false</span><br><span class=\"line\">relative_link: false</span><br><span class=\"line\">future: true</span><br><span class=\"line\">highlight:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  line_number: true</span><br><span class=\"line\">  auto_detect: false</span><br><span class=\"line\">  tab_replace:</span><br><span class=\"line\"></span><br><span class=\"line\"># Home page setting</span><br><span class=\"line\"># path: Root path for your blogs index page. (default = &apos;&apos;)</span><br><span class=\"line\"># per_page: Posts displayed per page. (0 = disable pagination)</span><br><span class=\"line\"># order_by: Posts order. (Order by date descending by default)</span><br><span class=\"line\">index_generator:</span><br><span class=\"line\">  path: &apos;&apos;</span><br><span class=\"line\">  per_page: 10</span><br><span class=\"line\">  order_by: -date</span><br><span class=\"line\"></span><br><span class=\"line\"># Category &amp; Tag 目录和标签</span><br><span class=\"line\">default_category: uncategorized</span><br><span class=\"line\">category_map:</span><br><span class=\"line\">tag_map:</span><br><span class=\"line\"></span><br><span class=\"line\"># Date / Time format 时间和日期格式</span><br><span class=\"line\">## Hexo uses Moment.js to parse and display date</span><br><span class=\"line\">## You can customize the date format as defined in</span><br><span class=\"line\">## http://momentjs.com/docs/#/displaying/format/</span><br><span class=\"line\">date_format: YYYY-MM-DD</span><br><span class=\"line\">time_format: HH:mm:ss</span><br><span class=\"line\"></span><br><span class=\"line\"># Pagination 分页设置</span><br><span class=\"line\">## Set per_page to 0 to disable pagination</span><br><span class=\"line\">per_page: 10</span><br><span class=\"line\">pagination_dir: page</span><br><span class=\"line\"></span><br><span class=\"line\"># Extensions 插件与皮肤</span><br><span class=\"line\">## Plugins: https://hexo.io/plugins/</span><br><span class=\"line\">## Themes: https://hexo.io/themes/</span><br><span class=\"line\">theme: landscape</span><br><span class=\"line\"></span><br><span class=\"line\"># Deployment 部署配置</span><br><span class=\"line\">## Docs: https://hexo.io/docs/deployment.html</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type:git</span><br><span class=\"line\">  repo:git@github.com:sjf0115/hexo-blog.git</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-3-创建新文章\"><a href=\"#3-3-创建新文章\" class=\"headerlink\" title=\"3.3 创建新文章\"></a>3.3 创建新文章</h3><p>接下来，我们开始新博客了，创建第一博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。</p>\n<p>下面我们创建一篇名为hexo的文章：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-5.png?raw=true\" alt=\"\"></p>\n<p>在_post目录下，就会生成文件：”hexo.md”:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog/source/_posts$ ll</span><br><span class=\"line\">总用量 5</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 256 12月  1 10:17 ./</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi   0 12月  1 09:59 ../</span><br><span class=\"line\">-rwxrwxrwx 1 xiaosi xiaosi 826 12月  1 09:59 hello-wor</span><br></pre></td></tr></table></figure></p>\n<p>然后，我们编辑文件：<code>hexo.md</code>，以markdown语法写文章，然后保存。在命令行，启动服务器进行保存:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog/source/posts$ hexo s</span><br><span class=\"line\">Native thread-sleep not available.</span><br><span class=\"line\">This will result in much slower performance, but it will still work.</span><br><span class=\"line\">You should re-install spawn-sync or upgrade to the lastest version of node if possible.</span><br><span class=\"line\">Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p>\n<p>通过浏览器打开， <a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a> ，就出现了我们新写的文章。</p>\n<h3 id=\"4-发布项目到github\"><a href=\"#4-发布项目到github\" class=\"headerlink\" title=\"4. 发布项目到github\"></a>4. 发布项目到github</h3><h4 id=\"4-1-静态化处理\"><a href=\"#4-1-静态化处理\" class=\"headerlink\" title=\"4.1 静态化处理\"></a>4.1 静态化处理</h4><p>写完文章之后，可以发布到github上面。hexo是一个静态博客框架。静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog$ hexo generate</span><br><span class=\"line\">Native thread-sleep not available.</span><br><span class=\"line\">This will result in much slower performance, but it will still work.</span><br><span class=\"line\">You should re-install spawn-sync or upgrade to the lastest version of node if possible.</span><br><span class=\"line\">Check /usr/local/lib/node_modules/hexo/node_modules/hexo-util/node_modules/cross-spawn/node_modules/spawn-sync/error.log for more details</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">INFO  Files loaded in 143 ms</span><br><span class=\"line\">INFO  Generated: index.html</span><br><span class=\"line\">INFO  Generated: archives/index.html</span><br><span class=\"line\">INFO  Generated: archives/2016/index.html</span><br><span class=\"line\">INFO  Generated: categories/diary/index.html</span><br><span class=\"line\">INFO  Generated: archives/2016/05/index.html</span><br><span class=\"line\">INFO  Generated: fancybox/blank.gif</span><br><span class=\"line\">INFO  Generated: archives/2017/12/index.html</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_loading.gif</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_overlay.png</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_sprite@2x.png</span><br><span class=\"line\">INFO  Generated: archives/2017/index.html</span><br><span class=\"line\">INFO  Generated: tags/hexo/index.html</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_sprite.png</span><br><span class=\"line\">INFO  Generated: js/script.js</span><br><span class=\"line\">INFO  Generated: fancybox/jquery.fancybox.css</span><br><span class=\"line\">INFO  Generated: css/style.css</span><br><span class=\"line\">INFO  Generated: fancybox/jquery.fancybox.pack.js</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-buttons.js</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-media.js</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.css</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-buttons.css</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/jquery.fancybox-thumbs.js</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.woff</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.eot</span><br><span class=\"line\">INFO  Generated: css/fonts/FontAwesome.otf</span><br><span class=\"line\">INFO  Generated: fancybox/helpers/fancybox_buttons.png</span><br><span class=\"line\">INFO  Generated: fancybox/fancybox_loading@2x.gif</span><br><span class=\"line\">INFO  Generated: fancybox/jquery.fancybox.js</span><br><span class=\"line\">INFO  Generated: 2017/12/01/hello-world/index.html</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.ttf</span><br><span class=\"line\">INFO  Generated: 2016/05/17/hexo/index.html</span><br><span class=\"line\">INFO  Generated: css/fonts/fontawesome-webfont.svg</span><br><span class=\"line\">INFO  Generated: css/images/banner.jpg</span><br><span class=\"line\">INFO  33 files generated in 1.19 s</span><br></pre></td></tr></table></figure></p>\n<p>在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~/blog/public$ ll</span><br><span class=\"line\">总用量 24</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:26 ./</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 ../</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 2017/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 archives/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 categories/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 css/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi 4096 12月  1 10:24 fancybox/</span><br><span class=\"line\">-rwxrwxrwx 1 xiaosi xiaosi 9841 12月  1 10:24 index.html*</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 js/</span><br><span class=\"line\">drwxrwxrwx 1 xiaosi xiaosi    0 12月  1 10:24 tags/</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-2-发布到github\"><a href=\"#4-2-发布到github\" class=\"headerlink\" title=\"4.2 发布到github\"></a>4.2 发布到github</h4><p>接下来，我们把这个博客发布到github。</p>\n<p>在github中创建一个项目hexo-blog，项目地址：<a href=\"https://github.com/sjf0115/hexo-blog\" target=\"_blank\" rel=\"noopener\">https://github.com/sjf0115/hexo-blog</a></p>\n<p>编辑全局配置文件：<code>_config.yml</code>，找到deploy的部分，设置github的项目地址：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Deployment</span><br><span class=\"line\">## Docs: https://hexo.io/docs/deployment.html</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: git@github.com:sjf0115/hexo-blog.git</span><br></pre></td></tr></table></figure></p>\n<p>然后，通过如下命令进行部署:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-7.png?raw=true\" alt=\"\"></p>\n<p>出现上述问题，可以使用配置ssh秘钥解决。如果出现deployer找不到git: ERROR Deployer not found: git错误，使用下面方式解决：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-8.png?raw=true\" alt=\"\"></p>\n<p>再来一次hexo deploy：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-9.png?raw=true\" alt=\"\"></p>\n<p>到目前为止这个静态的web网站就被部署到了github，检查一下分支是gh-pages。gh-pages是github为了web项目特别设置的分支:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-10.png?raw=true\" alt=\"\"></p>\n<p>然后，点击”Settings”，找到GitHub Pages，提示“Your site is published at <code>http://sjf0115.github.io/hexo-blog</code>:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-11.png?raw=true\" alt=\"\"></p>\n<p>打开网页，就是我们刚刚发布站点：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-12.png?raw=true\" alt=\"\"></p>\n<p>可以看到网页样式出现问题，不用担心，我们设置域名之后就OK了。</p>\n<h4 id=\"4-3-设置域名\"><a href=\"#4-3-设置域名\" class=\"headerlink\" title=\"4.3 设置域名\"></a>4.3 设置域名</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-13.png?raw=true\" alt=\"\"></p>\n<p>在dnspod控制台，设置主机记录@，类型A，到IP 23.235.37.133（github地址）:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-14.png?raw=true\" alt=\"\"></p>\n<p>对域名判断是否生效，对域名执行ping：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-15.png?raw=true\" alt=\"\"></p>\n<p>在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名sjf0115.club。通过浏览器，访问<a href=\"http://sjf0115.club\" target=\"_blank\" rel=\"noopener\">http://sjf0115.club</a> ， 就打开了我们建好的博客站点:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E6%90%AD%E5%BB%BA%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2-16.png?raw=true\" alt=\"\"></p>\n"},{"title":"Hexo+Github为NexT主题添加文章阅读量统计功能","date":"2017-12-01T12:17:23.000Z","author":"夏末","archives":"Hexo","_content":"\n\n在注册完成 `LeanCloud` 帐号并验证邮箱之后，我们就可以登录我们的 `LeanCloud` 帐号，进行一番配置之后拿到 `AppID` 以及 `AppKey` 这两个参数即可正常使用文章阅读量统计的功能了。\n\n### 1. 创建应用\n\n(1) 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-1.png?raw=true)\n\n(2) 在出现的界面点击创建应用：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-2.png?raw=true)\n\n(3) 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-3.png?raw=true)\n\n(4) 创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-4.png?raw=true)\n\n(5) 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建 `Class`。在弹出的选项中选择创建 `Class` 来新建 `Class` 用来专门保存我们博客的文章访问量等数据。\n\n备注:\n```\n点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter。\n```\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-5.png?raw=true)\n\n由于 `LeanCloud` 升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。\n\n(6) 创建完成之后，左侧数据栏应该会多出一栏名为 `Counter` 的栏目，这个时候我们点击左侧的设置，切换到我们创建的应用 `smartsi` 应用的操作界面。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-6.png?raw=true)\n\n在弹出的界面中，选择左侧的 `应用Key` 选项，即可发现我们创建应用的 `AppID` 以及 `AppKey`，有了它，我们就有权限能够通过主题中配置好的 `Javascript` 代码与这个应用的 `Counter`表进行数据存取操作了:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-7.png?raw=true)\n\n复制 `AppID` 以及 `AppKey` 并在 `NexT` 主题的 `_config.yml` 文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:\n```\nleancloud_visitors:\n  enable: true\n  app_id: 你的app_id\n  app_key: 你的app_key\n```\n\n### 2. 后台管理\n\n当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们 `LeanCloud` 对应的应用的 `Counter` 表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的 `Counter` 表中：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-8.png?raw=true)\n\n我们可以修改其中的 `time` 字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。\n- `url` 字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。\n- `title` 字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。\n- 其他字段皆为自动生成，具体作用请查阅 `LeanCloud` 官方文档，如果你不知道有什么作用请不要随意修改。\n\n### 3. Web安全\n\n因为`AppID`以及`AppKey`是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。\n\n选择应用的设置的安全中心选项卡,在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-9.png?raw=true)\n\n如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-10.png?raw=true)\n\n这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。\n\n\n\n原文:https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud\n","source":"_posts/Hexo/[Hexo]Hexo+Github为NexT主题添加文章阅读量统计功能.md","raw":"---\ntitle: Hexo+Github为NexT主题添加文章阅读量统计功能\ndate: 2017-12-01 20:17:23\nauthor: 夏末\ntags:\n- Hexo\n\narchives: Hexo\n---\n\n\n在注册完成 `LeanCloud` 帐号并验证邮箱之后，我们就可以登录我们的 `LeanCloud` 帐号，进行一番配置之后拿到 `AppID` 以及 `AppKey` 这两个参数即可正常使用文章阅读量统计的功能了。\n\n### 1. 创建应用\n\n(1) 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-1.png?raw=true)\n\n(2) 在出现的界面点击创建应用：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-2.png?raw=true)\n\n(3) 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-3.png?raw=true)\n\n(4) 创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-4.png?raw=true)\n\n(5) 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建 `Class`。在弹出的选项中选择创建 `Class` 来新建 `Class` 用来专门保存我们博客的文章访问量等数据。\n\n备注:\n```\n点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter。\n```\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-5.png?raw=true)\n\n由于 `LeanCloud` 升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。\n\n(6) 创建完成之后，左侧数据栏应该会多出一栏名为 `Counter` 的栏目，这个时候我们点击左侧的设置，切换到我们创建的应用 `smartsi` 应用的操作界面。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-6.png?raw=true)\n\n在弹出的界面中，选择左侧的 `应用Key` 选项，即可发现我们创建应用的 `AppID` 以及 `AppKey`，有了它，我们就有权限能够通过主题中配置好的 `Javascript` 代码与这个应用的 `Counter`表进行数据存取操作了:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-7.png?raw=true)\n\n复制 `AppID` 以及 `AppKey` 并在 `NexT` 主题的 `_config.yml` 文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:\n```\nleancloud_visitors:\n  enable: true\n  app_id: 你的app_id\n  app_key: 你的app_key\n```\n\n### 2. 后台管理\n\n当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们 `LeanCloud` 对应的应用的 `Counter` 表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的 `Counter` 表中：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-8.png?raw=true)\n\n我们可以修改其中的 `time` 字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。\n- `url` 字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。\n- `title` 字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。\n- 其他字段皆为自动生成，具体作用请查阅 `LeanCloud` 官方文档，如果你不知道有什么作用请不要随意修改。\n\n### 3. Web安全\n\n因为`AppID`以及`AppKey`是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。\n\n选择应用的设置的安全中心选项卡,在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-9.png?raw=true)\n\n如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-10.png?raw=true)\n\n这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。\n\n\n\n原文:https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud\n","slug":"Hexo/[Hexo]Hexo+Github为NexT主题添加文章阅读量统计功能","published":1,"updated":"2018-01-29T09:36:59.628Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje58titk003vordboecagrrf","content":"<p>在注册完成 <code>LeanCloud</code> 帐号并验证邮箱之后，我们就可以登录我们的 <code>LeanCloud</code> 帐号，进行一番配置之后拿到 <code>AppID</code> 以及 <code>AppKey</code> 这两个参数即可正常使用文章阅读量统计的功能了。</p>\n<h3 id=\"1-创建应用\"><a href=\"#1-创建应用\" class=\"headerlink\" title=\"1. 创建应用\"></a>1. 创建应用</h3><p>(1) 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-1.png?raw=true\" alt=\"\"></p>\n<p>(2) 在出现的界面点击创建应用：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-2.png?raw=true\" alt=\"\"></p>\n<p>(3) 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-3.png?raw=true\" alt=\"\"></p>\n<p>(4) 创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-4.png?raw=true\" alt=\"\"></p>\n<p>(5) 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建 <code>Class</code>。在弹出的选项中选择创建 <code>Class</code> 来新建 <code>Class</code> 用来专门保存我们博客的文章访问量等数据。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter。</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-5.png?raw=true\" alt=\"\"></p>\n<p>由于 <code>LeanCloud</code> 升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。</p>\n<p>(6) 创建完成之后，左侧数据栏应该会多出一栏名为 <code>Counter</code> 的栏目，这个时候我们点击左侧的设置，切换到我们创建的应用 <code>smartsi</code> 应用的操作界面。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-6.png?raw=true\" alt=\"\"></p>\n<p>在弹出的界面中，选择左侧的 <code>应用Key</code> 选项，即可发现我们创建应用的 <code>AppID</code> 以及 <code>AppKey</code>，有了它，我们就有权限能够通过主题中配置好的 <code>Javascript</code> 代码与这个应用的 <code>Counter</code>表进行数据存取操作了:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-7.png?raw=true\" alt=\"\"></p>\n<p>复制 <code>AppID</code> 以及 <code>AppKey</code> 并在 <code>NexT</code> 主题的 <code>_config.yml</code> 文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">leancloud_visitors:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  app_id: 你的app_id</span><br><span class=\"line\">  app_key: 你的app_key</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-后台管理\"><a href=\"#2-后台管理\" class=\"headerlink\" title=\"2. 后台管理\"></a>2. 后台管理</h3><p>当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们 <code>LeanCloud</code> 对应的应用的 <code>Counter</code> 表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的 <code>Counter</code> 表中：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-8.png?raw=true\" alt=\"\"></p>\n<p>我们可以修改其中的 <code>time</code> 字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。</p>\n<ul>\n<li><code>url</code> 字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。</li>\n<li><code>title</code> 字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。</li>\n<li>其他字段皆为自动生成，具体作用请查阅 <code>LeanCloud</code> 官方文档，如果你不知道有什么作用请不要随意修改。</li>\n</ul>\n<h3 id=\"3-Web安全\"><a href=\"#3-Web安全\" class=\"headerlink\" title=\"3. Web安全\"></a>3. Web安全</h3><p>因为<code>AppID</code>以及<code>AppKey</code>是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。</p>\n<p>选择应用的设置的安全中心选项卡,在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-9.png?raw=true\" alt=\"\"></p>\n<p>如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-10.png?raw=true\" alt=\"\"></p>\n<p>这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。</p>\n<p>原文:<a href=\"https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud\" target=\"_blank\" rel=\"noopener\">https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>在注册完成 <code>LeanCloud</code> 帐号并验证邮箱之后，我们就可以登录我们的 <code>LeanCloud</code> 帐号，进行一番配置之后拿到 <code>AppID</code> 以及 <code>AppKey</code> 这两个参数即可正常使用文章阅读量统计的功能了。</p>\n<h3 id=\"1-创建应用\"><a href=\"#1-创建应用\" class=\"headerlink\" title=\"1. 创建应用\"></a>1. 创建应用</h3><p>(1) 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-1.png?raw=true\" alt=\"\"></p>\n<p>(2) 在出现的界面点击创建应用：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-2.png?raw=true\" alt=\"\"></p>\n<p>(3) 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-3.png?raw=true\" alt=\"\"></p>\n<p>(4) 创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-4.png?raw=true\" alt=\"\"></p>\n<p>(5) 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建 <code>Class</code>。在弹出的选项中选择创建 <code>Class</code> 来新建 <code>Class</code> 用来专门保存我们博客的文章访问量等数据。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter。</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-5.png?raw=true\" alt=\"\"></p>\n<p>由于 <code>LeanCloud</code> 升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。</p>\n<p>(6) 创建完成之后，左侧数据栏应该会多出一栏名为 <code>Counter</code> 的栏目，这个时候我们点击左侧的设置，切换到我们创建的应用 <code>smartsi</code> 应用的操作界面。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-6.png?raw=true\" alt=\"\"></p>\n<p>在弹出的界面中，选择左侧的 <code>应用Key</code> 选项，即可发现我们创建应用的 <code>AppID</code> 以及 <code>AppKey</code>，有了它，我们就有权限能够通过主题中配置好的 <code>Javascript</code> 代码与这个应用的 <code>Counter</code>表进行数据存取操作了:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-7.png?raw=true\" alt=\"\"></p>\n<p>复制 <code>AppID</code> 以及 <code>AppKey</code> 并在 <code>NexT</code> 主题的 <code>_config.yml</code> 文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">leancloud_visitors:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  app_id: 你的app_id</span><br><span class=\"line\">  app_key: 你的app_key</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-后台管理\"><a href=\"#2-后台管理\" class=\"headerlink\" title=\"2. 后台管理\"></a>2. 后台管理</h3><p>当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们 <code>LeanCloud</code> 对应的应用的 <code>Counter</code> 表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的 <code>Counter</code> 表中：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-8.png?raw=true\" alt=\"\"></p>\n<p>我们可以修改其中的 <code>time</code> 字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。</p>\n<ul>\n<li><code>url</code> 字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。</li>\n<li><code>title</code> 字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。</li>\n<li>其他字段皆为自动生成，具体作用请查阅 <code>LeanCloud</code> 官方文档，如果你不知道有什么作用请不要随意修改。</li>\n</ul>\n<h3 id=\"3-Web安全\"><a href=\"#3-Web安全\" class=\"headerlink\" title=\"3. Web安全\"></a>3. Web安全</h3><p>因为<code>AppID</code>以及<code>AppKey</code>是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。</p>\n<p>选择应用的设置的安全中心选项卡,在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-9.png?raw=true\" alt=\"\"></p>\n<p>如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD-10.png?raw=true\" alt=\"\"></p>\n<p>这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。</p>\n<p>原文:<a href=\"https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud\" target=\"_blank\" rel=\"noopener\">https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Hive 内部表与外部表","date":"2017-12-12T12:19:01.000Z","_content":"\n托管表(内部表)和外部表是`Hive`中的两种不同类型的表，在这篇文章中，我们将讨论`Hive`中表的类型以及它们之间的差异以及如何创建这些表以及何时将这些表用于特定的数据集。\n\n### 1. 内部表\n\n托管表(`Managed TABLE`)也称为内部表(`Internal TABLE`)。这是Hive中的默认表。当我们在Hive中创建一个表，没有指定为外部表时，默认情况下我们创建的是一个内部表。如果我们创建一个内部表，那么表将在`HDFS`中的特定位置创建。默认情况下，表数据将在`HDFS`的`/usr/hive/warehouse`目录中创建。如果我们删除了一个内部表，那么这个表的表数据和元数据都将从`HDFS`中删除。\n\n#### 1.1 创建表\n\n我们可以用下面的语句在Hive里面创建一个内部表：\n```\nCREATE  TABLE IF NOT EXISTS tb_station_coordinate(\n  station string,\n  lon string,\n  lat string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',';\n```\n我们已经成功创建了表并使用如下命令检查表的详细信息：\n```\nhive> describe formatted tb_station_coordinate;\nOK\n# col_name            \tdata_type           \tcomment             \n\nstation             \tstring              \t                    \nlon                 \tstring              \t                    \nlat                 \tstring              \t                    \n\n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \txiaosi              \t \nCreateTime:         \tTue Dec 12 17:42:09 CST 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t \nTable Type:         \tMANAGED_TABLE       \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tnumFiles            \t0                   \n\tnumRows             \t0                   \n\trawDataSize         \t0                   \n\ttotalSize           \t0                   \n\ttransient_lastDdlTime\t1513071729          \n\n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t \nInputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tfield.delim         \t,                   \n\tserialization.format\t,                   \nTime taken: 0.16 seconds, Fetched: 33 row(s)\n```\n从上面我们可以看到表的类型`Table Type`为`MANAGED_TABLE`，即我们创建了一个托管表(内部表)。\n\n#### 1.2 导入数据\n\n我们使用如下命令将一个样本数据集导入到表中：\n```\nhive> load data local inpath '/home/xiaosi/station_coordinate.txt' overwrite into table tb_station_coordinate;\nLoading data to table default.tb_station_coordinate\nOK\nTime taken: 2.418 seconds\n```\n如果我们在`HDFS`的目录`/user/hive/warehouse/tb_station_coordinate`查看，我们可以得到表中的内容：\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nFound 1 items\n-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 17:50 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\nxiaosi@yoona:~$\nxiaosi@yoona:~$\nxiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\n桂林北站,110.302159,25.329024\n杭州东站,120.213116,30.290998\n山海关站,119.767555,40.000793\n武昌站,114.317576,30.528401\n北京南站,116.378875,39.865052\n...\n```\n\n备注:\n\n`/home/xiaosi/station_coordinate.txt`是本地文件系统路径。从上面的输出我们可以看到数据是从本地的这个路径复制到`HDFS`上的`/user/hive/warehouse/tb_station_coordinate/`目录下。\n为什么会自动复制到`HDFS`这个目录下呢？这个是由`Hive`的配置文件设置的。在`Hive`的`${HIVE_HOME}/conf/hive-site.xml`配置文件中指定，`hive.metastore.warehouse.dir`属性指向的就是`Hive`表数据存放的路径(在这配置的是`/user/hive/warehouse/`)。`Hive`每创建一个表都会在`hive.metastore.warehouse.dir`指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面`/user/hive/warehouse/tb_station_coordinate`。\n\n#### 1.3 删除表\n\n现在让我们使用如下命令删除上面创建的表:\n```\nhive> drop table tb_station_coordinate;\nMoved: 'hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate' to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current\nOK\nTime taken: 1.327 seconds\n```\n从上面的输出我们可以得知，原来属于`tb_station_coordinate`表的数据被移到`hdfs://localhost:9000/user/xiaosi/.Trash/Current`文件夹中(如果你的Hadoop没有采用回收站机制，那么删除操作将会把属于该表的所有数据全部删除)(回收站机制请参阅:[Hadoop Trash回收站使用指南](http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/))。\n\n如果我们在`HDFS`的目录`/user/hive/warehouse/tb_station_coordinate`查看：\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nls: `/user/hive/warehouse/tb_station_coordinate': No such file or directory\n```\n你可以看到输出为`No such file or directory`，因为表及其内容都从HDFS从删除了。\n\n### 2. 外部表\n\n当数据在Hive之外使用时，创建外部表(`EXTERNAL TABLE`)来在外部使用。无论何时我们想要删除表的元数据，并且想保留表中的数据，我们使用外部表。外部表只删除表的`schema`。\n\n#### 2.1 外部普通表\n\n我们使用如下命令创建一个外部表：\n```\nCREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(\n  station string,\n  lon string,\n  lat string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',';\n```\n我们现在已经成功创建了外部表。我们使用如下命令检查关于表的细节：\n```\nhive> describe formatted tb_station_coordinate;\nOK\n# col_name            \tdata_type           \tcomment             \n\nstation             \tstring              \t                    \nlon                 \tstring              \t                    \nlat                 \tstring              \t                    \n\n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \txiaosi              \t \nCreateTime:         \tTue Dec 12 18:16:13 CST 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t \nTable Type:         \tEXTERNAL_TABLE      \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tEXTERNAL            \tTRUE                \n\tnumFiles            \t0                   \n\tnumRows             \t0                   \n\trawDataSize         \t0                   \n\ttotalSize           \t0                   \n\ttransient_lastDdlTime\t1513073773          \n\n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t \nInputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tfield.delim         \t,                   \n\tserialization.format\t,                   \nTime taken: 0.132 seconds, Fetched: 34 row(s)\n\n```\n从上面我们可以看到表的类型`Table Type`为`EXTERNAL_TABLE`，即我们创建了一个外部表。\n\n#### 2.2 导入数据\n\n我们使用如下命令将一个样本数据集导入到表中：\n```\nhive> load data local inpath '/home/xiaosi/station_coordinate.txt' overwrite into table tb_station_coordinate;\nLoading data to table default.tb_station_coordinate\nOK\nTime taken: 2.418 seconds\n```\n\n如果我们在`HDFS`的目录`/user/hive/warehouse/tb_station_coordinate`查看，我们可以得到表中的内容：\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nFound 1 items\n-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\nxiaosi@yoona:~$\nxiaosi@yoona:~$\nxiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\n桂林北站,110.302159,25.329024\n杭州东站,120.213116,30.290998\n山海关站,119.767555,40.000793\n武昌站,114.317576,30.528401\n...\n```\n\n#### 2.3 删除表\n\n现在让我们使用如下命令删除上面创建的表:\n```\nhive> drop table tb_station_coordinate;\nOK\nTime taken: 0.174 seconds\nhive>\n```\n我们的Hadoop已经开启了回收站机制，但是删除操作并没有将数据进行删除，不像删除内部表一样，输出`Moved: 'hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate' to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current`(回收站机制请参阅:[Hadoop Trash回收站使用指南](http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/))。为了验证我们真的没有删除数据，我们在HDFS目录下查看数据:\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nFound 1 items\n-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\nxiaosi@yoona:~$\nxiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\n桂林北站,110.302159,25.329024\n杭州东站,120.213116,30.290998\n山海关站,119.767555,40.000793\n武昌站,114.317576,30.528401\n北京南站,116.378875,39.865052\n...\n```\n你可以看到表中的数据仍然在HDFS中。所以我们得知如果我们创建一个外部表，在删除表之后，只有与表相关的元数据被删除，而不会删除表的内容。\n\n#### 2.4 创建表指定外部目录\n\n只有当你的数据在`/user/hive/warehouse`目录中时，上述方法才能有效。但是，如果你的数据在另一个位置，如果你删除该表，数据也将被删除。所以在这种情况下，你需要在创建表时设置数据的外部位置，如下所示：\n```\nCREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(\n  station string,\n  lon string,\n  lat string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLOCATION '/user/xiaosi/test/coordinate/';\n```\n\n备注:\n```\n你也可以通过在创建表时设置数据存储位置来创建一个内部表。但是，如果删除表，数据将被删除。\n```\n\n备注:\n```\n如果你想要创建外部表，需要在创建表的时候加上 EXTERNAL 关键字，同时指定外部表存放数据的路径(例如2.4所示)，也可以不指定外部表的存放路径(例如2.3所示)，这样Hive将在HDFS上的/user/hive/warehouse/目录下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里。\n```\n\n### 3. 使用场景\n\n#### 3.1 内部表\n\n- 数据是临时的\n- 希望使用`Hive`来管理表和数据的生命周期\n- 删除后不想要数据\n\n#### 3.2 外部表\n\n- 这些数据也在`Hive`之外使用。\n- `Hive`不管理数据和权限设置以及目录等，需要你有另一个程序或过程来做这些事情\n- 不是基于现有表(AS SELECT)来创建的表\n- 可以创建表并使用相同的模式并指向数据的位置\n\n\n\n\n参考:https://acadgild.com/blog/managed-and-external-tables-in-hive/\n\nhttps://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil\n","source":"_posts/Hive/Hive使用指南 内部表与外部表.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Hive 内部表与外部表\ndate: 2017-12-12 20:19:01\ntags:\n  - Hive\n\ncategories: Hive\n---\n\n托管表(内部表)和外部表是`Hive`中的两种不同类型的表，在这篇文章中，我们将讨论`Hive`中表的类型以及它们之间的差异以及如何创建这些表以及何时将这些表用于特定的数据集。\n\n### 1. 内部表\n\n托管表(`Managed TABLE`)也称为内部表(`Internal TABLE`)。这是Hive中的默认表。当我们在Hive中创建一个表，没有指定为外部表时，默认情况下我们创建的是一个内部表。如果我们创建一个内部表，那么表将在`HDFS`中的特定位置创建。默认情况下，表数据将在`HDFS`的`/usr/hive/warehouse`目录中创建。如果我们删除了一个内部表，那么这个表的表数据和元数据都将从`HDFS`中删除。\n\n#### 1.1 创建表\n\n我们可以用下面的语句在Hive里面创建一个内部表：\n```\nCREATE  TABLE IF NOT EXISTS tb_station_coordinate(\n  station string,\n  lon string,\n  lat string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',';\n```\n我们已经成功创建了表并使用如下命令检查表的详细信息：\n```\nhive> describe formatted tb_station_coordinate;\nOK\n# col_name            \tdata_type           \tcomment             \n\nstation             \tstring              \t                    \nlon                 \tstring              \t                    \nlat                 \tstring              \t                    \n\n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \txiaosi              \t \nCreateTime:         \tTue Dec 12 17:42:09 CST 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t \nTable Type:         \tMANAGED_TABLE       \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tnumFiles            \t0                   \n\tnumRows             \t0                   \n\trawDataSize         \t0                   \n\ttotalSize           \t0                   \n\ttransient_lastDdlTime\t1513071729          \n\n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t \nInputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tfield.delim         \t,                   \n\tserialization.format\t,                   \nTime taken: 0.16 seconds, Fetched: 33 row(s)\n```\n从上面我们可以看到表的类型`Table Type`为`MANAGED_TABLE`，即我们创建了一个托管表(内部表)。\n\n#### 1.2 导入数据\n\n我们使用如下命令将一个样本数据集导入到表中：\n```\nhive> load data local inpath '/home/xiaosi/station_coordinate.txt' overwrite into table tb_station_coordinate;\nLoading data to table default.tb_station_coordinate\nOK\nTime taken: 2.418 seconds\n```\n如果我们在`HDFS`的目录`/user/hive/warehouse/tb_station_coordinate`查看，我们可以得到表中的内容：\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nFound 1 items\n-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 17:50 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\nxiaosi@yoona:~$\nxiaosi@yoona:~$\nxiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\n桂林北站,110.302159,25.329024\n杭州东站,120.213116,30.290998\n山海关站,119.767555,40.000793\n武昌站,114.317576,30.528401\n北京南站,116.378875,39.865052\n...\n```\n\n备注:\n\n`/home/xiaosi/station_coordinate.txt`是本地文件系统路径。从上面的输出我们可以看到数据是从本地的这个路径复制到`HDFS`上的`/user/hive/warehouse/tb_station_coordinate/`目录下。\n为什么会自动复制到`HDFS`这个目录下呢？这个是由`Hive`的配置文件设置的。在`Hive`的`${HIVE_HOME}/conf/hive-site.xml`配置文件中指定，`hive.metastore.warehouse.dir`属性指向的就是`Hive`表数据存放的路径(在这配置的是`/user/hive/warehouse/`)。`Hive`每创建一个表都会在`hive.metastore.warehouse.dir`指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面`/user/hive/warehouse/tb_station_coordinate`。\n\n#### 1.3 删除表\n\n现在让我们使用如下命令删除上面创建的表:\n```\nhive> drop table tb_station_coordinate;\nMoved: 'hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate' to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current\nOK\nTime taken: 1.327 seconds\n```\n从上面的输出我们可以得知，原来属于`tb_station_coordinate`表的数据被移到`hdfs://localhost:9000/user/xiaosi/.Trash/Current`文件夹中(如果你的Hadoop没有采用回收站机制，那么删除操作将会把属于该表的所有数据全部删除)(回收站机制请参阅:[Hadoop Trash回收站使用指南](http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/))。\n\n如果我们在`HDFS`的目录`/user/hive/warehouse/tb_station_coordinate`查看：\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nls: `/user/hive/warehouse/tb_station_coordinate': No such file or directory\n```\n你可以看到输出为`No such file or directory`，因为表及其内容都从HDFS从删除了。\n\n### 2. 外部表\n\n当数据在Hive之外使用时，创建外部表(`EXTERNAL TABLE`)来在外部使用。无论何时我们想要删除表的元数据，并且想保留表中的数据，我们使用外部表。外部表只删除表的`schema`。\n\n#### 2.1 外部普通表\n\n我们使用如下命令创建一个外部表：\n```\nCREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(\n  station string,\n  lon string,\n  lat string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',';\n```\n我们现在已经成功创建了外部表。我们使用如下命令检查关于表的细节：\n```\nhive> describe formatted tb_station_coordinate;\nOK\n# col_name            \tdata_type           \tcomment             \n\nstation             \tstring              \t                    \nlon                 \tstring              \t                    \nlat                 \tstring              \t                    \n\n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \txiaosi              \t \nCreateTime:         \tTue Dec 12 18:16:13 CST 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t \nTable Type:         \tEXTERNAL_TABLE      \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tEXTERNAL            \tTRUE                \n\tnumFiles            \t0                   \n\tnumRows             \t0                   \n\trawDataSize         \t0                   \n\ttotalSize           \t0                   \n\ttransient_lastDdlTime\t1513073773          \n\n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t \nInputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tfield.delim         \t,                   \n\tserialization.format\t,                   \nTime taken: 0.132 seconds, Fetched: 34 row(s)\n\n```\n从上面我们可以看到表的类型`Table Type`为`EXTERNAL_TABLE`，即我们创建了一个外部表。\n\n#### 2.2 导入数据\n\n我们使用如下命令将一个样本数据集导入到表中：\n```\nhive> load data local inpath '/home/xiaosi/station_coordinate.txt' overwrite into table tb_station_coordinate;\nLoading data to table default.tb_station_coordinate\nOK\nTime taken: 2.418 seconds\n```\n\n如果我们在`HDFS`的目录`/user/hive/warehouse/tb_station_coordinate`查看，我们可以得到表中的内容：\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nFound 1 items\n-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\nxiaosi@yoona:~$\nxiaosi@yoona:~$\nxiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\n桂林北站,110.302159,25.329024\n杭州东站,120.213116,30.290998\n山海关站,119.767555,40.000793\n武昌站,114.317576,30.528401\n...\n```\n\n#### 2.3 删除表\n\n现在让我们使用如下命令删除上面创建的表:\n```\nhive> drop table tb_station_coordinate;\nOK\nTime taken: 0.174 seconds\nhive>\n```\n我们的Hadoop已经开启了回收站机制，但是删除操作并没有将数据进行删除，不像删除内部表一样，输出`Moved: 'hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate' to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current`(回收站机制请参阅:[Hadoop Trash回收站使用指南](http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/))。为了验证我们真的没有删除数据，我们在HDFS目录下查看数据:\n```\nxiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate\nFound 1 items\n-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\nxiaosi@yoona:~$\nxiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt\n桂林北站,110.302159,25.329024\n杭州东站,120.213116,30.290998\n山海关站,119.767555,40.000793\n武昌站,114.317576,30.528401\n北京南站,116.378875,39.865052\n...\n```\n你可以看到表中的数据仍然在HDFS中。所以我们得知如果我们创建一个外部表，在删除表之后，只有与表相关的元数据被删除，而不会删除表的内容。\n\n#### 2.4 创建表指定外部目录\n\n只有当你的数据在`/user/hive/warehouse`目录中时，上述方法才能有效。但是，如果你的数据在另一个位置，如果你删除该表，数据也将被删除。所以在这种情况下，你需要在创建表时设置数据的外部位置，如下所示：\n```\nCREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(\n  station string,\n  lon string,\n  lat string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLOCATION '/user/xiaosi/test/coordinate/';\n```\n\n备注:\n```\n你也可以通过在创建表时设置数据存储位置来创建一个内部表。但是，如果删除表，数据将被删除。\n```\n\n备注:\n```\n如果你想要创建外部表，需要在创建表的时候加上 EXTERNAL 关键字，同时指定外部表存放数据的路径(例如2.4所示)，也可以不指定外部表的存放路径(例如2.3所示)，这样Hive将在HDFS上的/user/hive/warehouse/目录下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里。\n```\n\n### 3. 使用场景\n\n#### 3.1 内部表\n\n- 数据是临时的\n- 希望使用`Hive`来管理表和数据的生命周期\n- 删除后不想要数据\n\n#### 3.2 外部表\n\n- 这些数据也在`Hive`之外使用。\n- `Hive`不管理数据和权限设置以及目录等，需要你有另一个程序或过程来做这些事情\n- 不是基于现有表(AS SELECT)来创建的表\n- 可以创建表并使用相同的模式并指向数据的位置\n\n\n\n\n参考:https://acadgild.com/blog/managed-and-external-tables-in-hive/\n\nhttps://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil\n","slug":"Hive/Hive使用指南 内部表与外部表","published":1,"updated":"2018-01-29T09:36:59.627Z","comments":1,"photos":[],"link":"","_id":"cje58titn003yordbokrt3vou","content":"<p>托管表(内部表)和外部表是<code>Hive</code>中的两种不同类型的表，在这篇文章中，我们将讨论<code>Hive</code>中表的类型以及它们之间的差异以及如何创建这些表以及何时将这些表用于特定的数据集。</p>\n<h3 id=\"1-内部表\"><a href=\"#1-内部表\" class=\"headerlink\" title=\"1. 内部表\"></a>1. 内部表</h3><p>托管表(<code>Managed TABLE</code>)也称为内部表(<code>Internal TABLE</code>)。这是Hive中的默认表。当我们在Hive中创建一个表，没有指定为外部表时，默认情况下我们创建的是一个内部表。如果我们创建一个内部表，那么表将在<code>HDFS</code>中的特定位置创建。默认情况下，表数据将在<code>HDFS</code>的<code>/usr/hive/warehouse</code>目录中创建。如果我们删除了一个内部表，那么这个表的表数据和元数据都将从<code>HDFS</code>中删除。</p>\n<h4 id=\"1-1-创建表\"><a href=\"#1-1-创建表\" class=\"headerlink\" title=\"1.1 创建表\"></a>1.1 创建表</h4><p>我们可以用下面的语句在Hive里面创建一个内部表：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE  TABLE IF NOT EXISTS tb_station_coordinate(</span><br><span class=\"line\">  station string,</span><br><span class=\"line\">  lon string,</span><br><span class=\"line\">  lat string</span><br><span class=\"line\">)</span><br><span class=\"line\">ROW FORMAT DELIMITED</span><br><span class=\"line\">FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>我们已经成功创建了表并使用如下命令检查表的详细信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; describe formatted tb_station_coordinate;</span><br><span class=\"line\">OK</span><br><span class=\"line\"># col_name            \tdata_type           \tcomment             </span><br><span class=\"line\"></span><br><span class=\"line\">station             \tstring              \t                    </span><br><span class=\"line\">lon                 \tstring              \t                    </span><br><span class=\"line\">lat                 \tstring              \t                    </span><br><span class=\"line\"></span><br><span class=\"line\"># Detailed Table Information\t \t </span><br><span class=\"line\">Database:           \tdefault             \t </span><br><span class=\"line\">Owner:              \txiaosi              \t </span><br><span class=\"line\">CreateTime:         \tTue Dec 12 17:42:09 CST 2017\t </span><br><span class=\"line\">LastAccessTime:     \tUNKNOWN             \t </span><br><span class=\"line\">Retention:          \t0                   \t </span><br><span class=\"line\">Location:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t </span><br><span class=\"line\">Table Type:         \tMANAGED_TABLE       \t </span><br><span class=\"line\">Table Parameters:\t \t </span><br><span class=\"line\">\tCOLUMN_STATS_ACCURATE\t&#123;\\&quot;BASIC_STATS\\&quot;:\\&quot;true\\&quot;&#125;</span><br><span class=\"line\">\tnumFiles            \t0                   </span><br><span class=\"line\">\tnumRows             \t0                   </span><br><span class=\"line\">\trawDataSize         \t0                   </span><br><span class=\"line\">\ttotalSize           \t0                   </span><br><span class=\"line\">\ttransient_lastDdlTime\t1513071729          </span><br><span class=\"line\"></span><br><span class=\"line\"># Storage Information\t \t </span><br><span class=\"line\">SerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t </span><br><span class=\"line\">InputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t </span><br><span class=\"line\">OutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t </span><br><span class=\"line\">Compressed:         \tNo                  \t </span><br><span class=\"line\">Num Buckets:        \t-1                  \t </span><br><span class=\"line\">Bucket Columns:     \t[]                  \t </span><br><span class=\"line\">Sort Columns:       \t[]                  \t </span><br><span class=\"line\">Storage Desc Params:\t \t </span><br><span class=\"line\">\tfield.delim         \t,                   </span><br><span class=\"line\">\tserialization.format\t,                   </span><br><span class=\"line\">Time taken: 0.16 seconds, Fetched: 33 row(s)</span><br></pre></td></tr></table></figure></p>\n<p>从上面我们可以看到表的类型<code>Table Type</code>为<code>MANAGED_TABLE</code>，即我们创建了一个托管表(内部表)。</p>\n<h4 id=\"1-2-导入数据\"><a href=\"#1-2-导入数据\" class=\"headerlink\" title=\"1.2 导入数据\"></a>1.2 导入数据</h4><p>我们使用如下命令将一个样本数据集导入到表中：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;</span><br><span class=\"line\">Loading data to table default.tb_station_coordinate</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 2.418 seconds</span><br></pre></td></tr></table></figure></p>\n<p>如果我们在<code>HDFS</code>的目录<code>/user/hive/warehouse/tb_station_coordinate</code>查看，我们可以得到表中的内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 17:50 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">桂林北站,110.302159,25.329024</span><br><span class=\"line\">杭州东站,120.213116,30.290998</span><br><span class=\"line\">山海关站,119.767555,40.000793</span><br><span class=\"line\">武昌站,114.317576,30.528401</span><br><span class=\"line\">北京南站,116.378875,39.865052</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>备注:</p>\n<p><code>/home/xiaosi/station_coordinate.txt</code>是本地文件系统路径。从上面的输出我们可以看到数据是从本地的这个路径复制到<code>HDFS</code>上的<code>/user/hive/warehouse/tb_station_coordinate/</code>目录下。<br>为什么会自动复制到<code>HDFS</code>这个目录下呢？这个是由<code>Hive</code>的配置文件设置的。在<code>Hive</code>的<code>${HIVE_HOME}/conf/hive-site.xml</code>配置文件中指定，<code>hive.metastore.warehouse.dir</code>属性指向的就是<code>Hive</code>表数据存放的路径(在这配置的是<code>/user/hive/warehouse/</code>)。<code>Hive</code>每创建一个表都会在<code>hive.metastore.warehouse.dir</code>指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面<code>/user/hive/warehouse/tb_station_coordinate</code>。</p>\n<h4 id=\"1-3-删除表\"><a href=\"#1-3-删除表\" class=\"headerlink\" title=\"1.3 删除表\"></a>1.3 删除表</h4><p>现在让我们使用如下命令删除上面创建的表:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; drop table tb_station_coordinate;</span><br><span class=\"line\">Moved: &apos;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&apos; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 1.327 seconds</span><br></pre></td></tr></table></figure></p>\n<p>从上面的输出我们可以得知，原来属于<code>tb_station_coordinate</code>表的数据被移到<code>hdfs://localhost:9000/user/xiaosi/.Trash/Current</code>文件夹中(如果你的Hadoop没有采用回收站机制，那么删除操作将会把属于该表的所有数据全部删除)(回收站机制请参阅:<a href=\"http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/\" target=\"_blank\" rel=\"noopener\">Hadoop Trash回收站使用指南</a>)。</p>\n<p>如果我们在<code>HDFS</code>的目录<code>/user/hive/warehouse/tb_station_coordinate</code>查看：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">ls: `/user/hive/warehouse/tb_station_coordinate&apos;: No such file or directory</span><br></pre></td></tr></table></figure></p>\n<p>你可以看到输出为<code>No such file or directory</code>，因为表及其内容都从HDFS从删除了。</p>\n<h3 id=\"2-外部表\"><a href=\"#2-外部表\" class=\"headerlink\" title=\"2. 外部表\"></a>2. 外部表</h3><p>当数据在Hive之外使用时，创建外部表(<code>EXTERNAL TABLE</code>)来在外部使用。无论何时我们想要删除表的元数据，并且想保留表中的数据，我们使用外部表。外部表只删除表的<code>schema</code>。</p>\n<h4 id=\"2-1-外部普通表\"><a href=\"#2-1-外部普通表\" class=\"headerlink\" title=\"2.1 外部普通表\"></a>2.1 外部普通表</h4><p>我们使用如下命令创建一个外部表：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(</span><br><span class=\"line\">  station string,</span><br><span class=\"line\">  lon string,</span><br><span class=\"line\">  lat string</span><br><span class=\"line\">)</span><br><span class=\"line\">ROW FORMAT DELIMITED</span><br><span class=\"line\">FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>我们现在已经成功创建了外部表。我们使用如下命令检查关于表的细节：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; describe formatted tb_station_coordinate;</span><br><span class=\"line\">OK</span><br><span class=\"line\"># col_name            \tdata_type           \tcomment             </span><br><span class=\"line\"></span><br><span class=\"line\">station             \tstring              \t                    </span><br><span class=\"line\">lon                 \tstring              \t                    </span><br><span class=\"line\">lat                 \tstring              \t                    </span><br><span class=\"line\"></span><br><span class=\"line\"># Detailed Table Information\t \t </span><br><span class=\"line\">Database:           \tdefault             \t </span><br><span class=\"line\">Owner:              \txiaosi              \t </span><br><span class=\"line\">CreateTime:         \tTue Dec 12 18:16:13 CST 2017\t </span><br><span class=\"line\">LastAccessTime:     \tUNKNOWN             \t </span><br><span class=\"line\">Retention:          \t0                   \t </span><br><span class=\"line\">Location:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t </span><br><span class=\"line\">Table Type:         \tEXTERNAL_TABLE      \t </span><br><span class=\"line\">Table Parameters:\t \t </span><br><span class=\"line\">\tCOLUMN_STATS_ACCURATE\t&#123;\\&quot;BASIC_STATS\\&quot;:\\&quot;true\\&quot;&#125;</span><br><span class=\"line\">\tEXTERNAL            \tTRUE                </span><br><span class=\"line\">\tnumFiles            \t0                   </span><br><span class=\"line\">\tnumRows             \t0                   </span><br><span class=\"line\">\trawDataSize         \t0                   </span><br><span class=\"line\">\ttotalSize           \t0                   </span><br><span class=\"line\">\ttransient_lastDdlTime\t1513073773          </span><br><span class=\"line\"></span><br><span class=\"line\"># Storage Information\t \t </span><br><span class=\"line\">SerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t </span><br><span class=\"line\">InputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t </span><br><span class=\"line\">OutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t </span><br><span class=\"line\">Compressed:         \tNo                  \t </span><br><span class=\"line\">Num Buckets:        \t-1                  \t </span><br><span class=\"line\">Bucket Columns:     \t[]                  \t </span><br><span class=\"line\">Sort Columns:       \t[]                  \t </span><br><span class=\"line\">Storage Desc Params:\t \t </span><br><span class=\"line\">\tfield.delim         \t,                   </span><br><span class=\"line\">\tserialization.format\t,                   </span><br><span class=\"line\">Time taken: 0.132 seconds, Fetched: 34 row(s)</span><br></pre></td></tr></table></figure></p>\n<p>从上面我们可以看到表的类型<code>Table Type</code>为<code>EXTERNAL_TABLE</code>，即我们创建了一个外部表。</p>\n<h4 id=\"2-2-导入数据\"><a href=\"#2-2-导入数据\" class=\"headerlink\" title=\"2.2 导入数据\"></a>2.2 导入数据</h4><p>我们使用如下命令将一个样本数据集导入到表中：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;</span><br><span class=\"line\">Loading data to table default.tb_station_coordinate</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 2.418 seconds</span><br></pre></td></tr></table></figure></p>\n<p>如果我们在<code>HDFS</code>的目录<code>/user/hive/warehouse/tb_station_coordinate</code>查看，我们可以得到表中的内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">桂林北站,110.302159,25.329024</span><br><span class=\"line\">杭州东站,120.213116,30.290998</span><br><span class=\"line\">山海关站,119.767555,40.000793</span><br><span class=\"line\">武昌站,114.317576,30.528401</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-删除表\"><a href=\"#2-3-删除表\" class=\"headerlink\" title=\"2.3 删除表\"></a>2.3 删除表</h4><p>现在让我们使用如下命令删除上面创建的表:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; drop table tb_station_coordinate;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.174 seconds</span><br><span class=\"line\">hive&gt;</span><br></pre></td></tr></table></figure></p>\n<p>我们的Hadoop已经开启了回收站机制，但是删除操作并没有将数据进行删除，不像删除内部表一样，输出<code>Moved: &#39;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&#39; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current</code>(回收站机制请参阅:<a href=\"http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/\" target=\"_blank\" rel=\"noopener\">Hadoop Trash回收站使用指南</a>)。为了验证我们真的没有删除数据，我们在HDFS目录下查看数据:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">桂林北站,110.302159,25.329024</span><br><span class=\"line\">杭州东站,120.213116,30.290998</span><br><span class=\"line\">山海关站,119.767555,40.000793</span><br><span class=\"line\">武昌站,114.317576,30.528401</span><br><span class=\"line\">北京南站,116.378875,39.865052</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>你可以看到表中的数据仍然在HDFS中。所以我们得知如果我们创建一个外部表，在删除表之后，只有与表相关的元数据被删除，而不会删除表的内容。</p>\n<h4 id=\"2-4-创建表指定外部目录\"><a href=\"#2-4-创建表指定外部目录\" class=\"headerlink\" title=\"2.4 创建表指定外部目录\"></a>2.4 创建表指定外部目录</h4><p>只有当你的数据在<code>/user/hive/warehouse</code>目录中时，上述方法才能有效。但是，如果你的数据在另一个位置，如果你删除该表，数据也将被删除。所以在这种情况下，你需要在创建表时设置数据的外部位置，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(</span><br><span class=\"line\">  station string,</span><br><span class=\"line\">  lon string,</span><br><span class=\"line\">  lat string</span><br><span class=\"line\">)</span><br><span class=\"line\">ROW FORMAT DELIMITED</span><br><span class=\"line\">FIELDS TERMINATED BY &apos;,&apos;</span><br><span class=\"line\">LOCATION &apos;/user/xiaosi/test/coordinate/&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">你也可以通过在创建表时设置数据存储位置来创建一个内部表。但是，如果删除表，数据将被删除。</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果你想要创建外部表，需要在创建表的时候加上 EXTERNAL 关键字，同时指定外部表存放数据的路径(例如2.4所示)，也可以不指定外部表的存放路径(例如2.3所示)，这样Hive将在HDFS上的/user/hive/warehouse/目录下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-使用场景\"><a href=\"#3-使用场景\" class=\"headerlink\" title=\"3. 使用场景\"></a>3. 使用场景</h3><h4 id=\"3-1-内部表\"><a href=\"#3-1-内部表\" class=\"headerlink\" title=\"3.1 内部表\"></a>3.1 内部表</h4><ul>\n<li>数据是临时的</li>\n<li>希望使用<code>Hive</code>来管理表和数据的生命周期</li>\n<li>删除后不想要数据</li>\n</ul>\n<h4 id=\"3-2-外部表\"><a href=\"#3-2-外部表\" class=\"headerlink\" title=\"3.2 外部表\"></a>3.2 外部表</h4><ul>\n<li>这些数据也在<code>Hive</code>之外使用。</li>\n<li><code>Hive</code>不管理数据和权限设置以及目录等，需要你有另一个程序或过程来做这些事情</li>\n<li>不是基于现有表(AS SELECT)来创建的表</li>\n<li>可以创建表并使用相同的模式并指向数据的位置</li>\n</ul>\n<p>参考:<a href=\"https://acadgild.com/blog/managed-and-external-tables-in-hive/\" target=\"_blank\" rel=\"noopener\">https://acadgild.com/blog/managed-and-external-tables-in-hive/</a></p>\n<p><a href=\"https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil\" target=\"_blank\" rel=\"noopener\">https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>托管表(内部表)和外部表是<code>Hive</code>中的两种不同类型的表，在这篇文章中，我们将讨论<code>Hive</code>中表的类型以及它们之间的差异以及如何创建这些表以及何时将这些表用于特定的数据集。</p>\n<h3 id=\"1-内部表\"><a href=\"#1-内部表\" class=\"headerlink\" title=\"1. 内部表\"></a>1. 内部表</h3><p>托管表(<code>Managed TABLE</code>)也称为内部表(<code>Internal TABLE</code>)。这是Hive中的默认表。当我们在Hive中创建一个表，没有指定为外部表时，默认情况下我们创建的是一个内部表。如果我们创建一个内部表，那么表将在<code>HDFS</code>中的特定位置创建。默认情况下，表数据将在<code>HDFS</code>的<code>/usr/hive/warehouse</code>目录中创建。如果我们删除了一个内部表，那么这个表的表数据和元数据都将从<code>HDFS</code>中删除。</p>\n<h4 id=\"1-1-创建表\"><a href=\"#1-1-创建表\" class=\"headerlink\" title=\"1.1 创建表\"></a>1.1 创建表</h4><p>我们可以用下面的语句在Hive里面创建一个内部表：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE  TABLE IF NOT EXISTS tb_station_coordinate(</span><br><span class=\"line\">  station string,</span><br><span class=\"line\">  lon string,</span><br><span class=\"line\">  lat string</span><br><span class=\"line\">)</span><br><span class=\"line\">ROW FORMAT DELIMITED</span><br><span class=\"line\">FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>我们已经成功创建了表并使用如下命令检查表的详细信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; describe formatted tb_station_coordinate;</span><br><span class=\"line\">OK</span><br><span class=\"line\"># col_name            \tdata_type           \tcomment             </span><br><span class=\"line\"></span><br><span class=\"line\">station             \tstring              \t                    </span><br><span class=\"line\">lon                 \tstring              \t                    </span><br><span class=\"line\">lat                 \tstring              \t                    </span><br><span class=\"line\"></span><br><span class=\"line\"># Detailed Table Information\t \t </span><br><span class=\"line\">Database:           \tdefault             \t </span><br><span class=\"line\">Owner:              \txiaosi              \t </span><br><span class=\"line\">CreateTime:         \tTue Dec 12 17:42:09 CST 2017\t </span><br><span class=\"line\">LastAccessTime:     \tUNKNOWN             \t </span><br><span class=\"line\">Retention:          \t0                   \t </span><br><span class=\"line\">Location:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t </span><br><span class=\"line\">Table Type:         \tMANAGED_TABLE       \t </span><br><span class=\"line\">Table Parameters:\t \t </span><br><span class=\"line\">\tCOLUMN_STATS_ACCURATE\t&#123;\\&quot;BASIC_STATS\\&quot;:\\&quot;true\\&quot;&#125;</span><br><span class=\"line\">\tnumFiles            \t0                   </span><br><span class=\"line\">\tnumRows             \t0                   </span><br><span class=\"line\">\trawDataSize         \t0                   </span><br><span class=\"line\">\ttotalSize           \t0                   </span><br><span class=\"line\">\ttransient_lastDdlTime\t1513071729          </span><br><span class=\"line\"></span><br><span class=\"line\"># Storage Information\t \t </span><br><span class=\"line\">SerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t </span><br><span class=\"line\">InputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t </span><br><span class=\"line\">OutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t </span><br><span class=\"line\">Compressed:         \tNo                  \t </span><br><span class=\"line\">Num Buckets:        \t-1                  \t </span><br><span class=\"line\">Bucket Columns:     \t[]                  \t </span><br><span class=\"line\">Sort Columns:       \t[]                  \t </span><br><span class=\"line\">Storage Desc Params:\t \t </span><br><span class=\"line\">\tfield.delim         \t,                   </span><br><span class=\"line\">\tserialization.format\t,                   </span><br><span class=\"line\">Time taken: 0.16 seconds, Fetched: 33 row(s)</span><br></pre></td></tr></table></figure></p>\n<p>从上面我们可以看到表的类型<code>Table Type</code>为<code>MANAGED_TABLE</code>，即我们创建了一个托管表(内部表)。</p>\n<h4 id=\"1-2-导入数据\"><a href=\"#1-2-导入数据\" class=\"headerlink\" title=\"1.2 导入数据\"></a>1.2 导入数据</h4><p>我们使用如下命令将一个样本数据集导入到表中：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;</span><br><span class=\"line\">Loading data to table default.tb_station_coordinate</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 2.418 seconds</span><br></pre></td></tr></table></figure></p>\n<p>如果我们在<code>HDFS</code>的目录<code>/user/hive/warehouse/tb_station_coordinate</code>查看，我们可以得到表中的内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 17:50 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">桂林北站,110.302159,25.329024</span><br><span class=\"line\">杭州东站,120.213116,30.290998</span><br><span class=\"line\">山海关站,119.767555,40.000793</span><br><span class=\"line\">武昌站,114.317576,30.528401</span><br><span class=\"line\">北京南站,116.378875,39.865052</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>备注:</p>\n<p><code>/home/xiaosi/station_coordinate.txt</code>是本地文件系统路径。从上面的输出我们可以看到数据是从本地的这个路径复制到<code>HDFS</code>上的<code>/user/hive/warehouse/tb_station_coordinate/</code>目录下。<br>为什么会自动复制到<code>HDFS</code>这个目录下呢？这个是由<code>Hive</code>的配置文件设置的。在<code>Hive</code>的<code>${HIVE_HOME}/conf/hive-site.xml</code>配置文件中指定，<code>hive.metastore.warehouse.dir</code>属性指向的就是<code>Hive</code>表数据存放的路径(在这配置的是<code>/user/hive/warehouse/</code>)。<code>Hive</code>每创建一个表都会在<code>hive.metastore.warehouse.dir</code>指向的目录下以表名创建一个文件夹，所有属于这个表的数据都存放在这个文件夹里面<code>/user/hive/warehouse/tb_station_coordinate</code>。</p>\n<h4 id=\"1-3-删除表\"><a href=\"#1-3-删除表\" class=\"headerlink\" title=\"1.3 删除表\"></a>1.3 删除表</h4><p>现在让我们使用如下命令删除上面创建的表:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; drop table tb_station_coordinate;</span><br><span class=\"line\">Moved: &apos;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&apos; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 1.327 seconds</span><br></pre></td></tr></table></figure></p>\n<p>从上面的输出我们可以得知，原来属于<code>tb_station_coordinate</code>表的数据被移到<code>hdfs://localhost:9000/user/xiaosi/.Trash/Current</code>文件夹中(如果你的Hadoop没有采用回收站机制，那么删除操作将会把属于该表的所有数据全部删除)(回收站机制请参阅:<a href=\"http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/\" target=\"_blank\" rel=\"noopener\">Hadoop Trash回收站使用指南</a>)。</p>\n<p>如果我们在<code>HDFS</code>的目录<code>/user/hive/warehouse/tb_station_coordinate</code>查看：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">ls: `/user/hive/warehouse/tb_station_coordinate&apos;: No such file or directory</span><br></pre></td></tr></table></figure></p>\n<p>你可以看到输出为<code>No such file or directory</code>，因为表及其内容都从HDFS从删除了。</p>\n<h3 id=\"2-外部表\"><a href=\"#2-外部表\" class=\"headerlink\" title=\"2. 外部表\"></a>2. 外部表</h3><p>当数据在Hive之外使用时，创建外部表(<code>EXTERNAL TABLE</code>)来在外部使用。无论何时我们想要删除表的元数据，并且想保留表中的数据，我们使用外部表。外部表只删除表的<code>schema</code>。</p>\n<h4 id=\"2-1-外部普通表\"><a href=\"#2-1-外部普通表\" class=\"headerlink\" title=\"2.1 外部普通表\"></a>2.1 外部普通表</h4><p>我们使用如下命令创建一个外部表：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(</span><br><span class=\"line\">  station string,</span><br><span class=\"line\">  lon string,</span><br><span class=\"line\">  lat string</span><br><span class=\"line\">)</span><br><span class=\"line\">ROW FORMAT DELIMITED</span><br><span class=\"line\">FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>我们现在已经成功创建了外部表。我们使用如下命令检查关于表的细节：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; describe formatted tb_station_coordinate;</span><br><span class=\"line\">OK</span><br><span class=\"line\"># col_name            \tdata_type           \tcomment             </span><br><span class=\"line\"></span><br><span class=\"line\">station             \tstring              \t                    </span><br><span class=\"line\">lon                 \tstring              \t                    </span><br><span class=\"line\">lat                 \tstring              \t                    </span><br><span class=\"line\"></span><br><span class=\"line\"># Detailed Table Information\t \t </span><br><span class=\"line\">Database:           \tdefault             \t </span><br><span class=\"line\">Owner:              \txiaosi              \t </span><br><span class=\"line\">CreateTime:         \tTue Dec 12 18:16:13 CST 2017\t </span><br><span class=\"line\">LastAccessTime:     \tUNKNOWN             \t </span><br><span class=\"line\">Retention:          \t0                   \t </span><br><span class=\"line\">Location:           \thdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate\t </span><br><span class=\"line\">Table Type:         \tEXTERNAL_TABLE      \t </span><br><span class=\"line\">Table Parameters:\t \t </span><br><span class=\"line\">\tCOLUMN_STATS_ACCURATE\t&#123;\\&quot;BASIC_STATS\\&quot;:\\&quot;true\\&quot;&#125;</span><br><span class=\"line\">\tEXTERNAL            \tTRUE                </span><br><span class=\"line\">\tnumFiles            \t0                   </span><br><span class=\"line\">\tnumRows             \t0                   </span><br><span class=\"line\">\trawDataSize         \t0                   </span><br><span class=\"line\">\ttotalSize           \t0                   </span><br><span class=\"line\">\ttransient_lastDdlTime\t1513073773          </span><br><span class=\"line\"></span><br><span class=\"line\"># Storage Information\t \t </span><br><span class=\"line\">SerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t </span><br><span class=\"line\">InputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t </span><br><span class=\"line\">OutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t </span><br><span class=\"line\">Compressed:         \tNo                  \t </span><br><span class=\"line\">Num Buckets:        \t-1                  \t </span><br><span class=\"line\">Bucket Columns:     \t[]                  \t </span><br><span class=\"line\">Sort Columns:       \t[]                  \t </span><br><span class=\"line\">Storage Desc Params:\t \t </span><br><span class=\"line\">\tfield.delim         \t,                   </span><br><span class=\"line\">\tserialization.format\t,                   </span><br><span class=\"line\">Time taken: 0.132 seconds, Fetched: 34 row(s)</span><br></pre></td></tr></table></figure></p>\n<p>从上面我们可以看到表的类型<code>Table Type</code>为<code>EXTERNAL_TABLE</code>，即我们创建了一个外部表。</p>\n<h4 id=\"2-2-导入数据\"><a href=\"#2-2-导入数据\" class=\"headerlink\" title=\"2.2 导入数据\"></a>2.2 导入数据</h4><p>我们使用如下命令将一个样本数据集导入到表中：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; load data local inpath &apos;/home/xiaosi/station_coordinate.txt&apos; overwrite into table tb_station_coordinate;</span><br><span class=\"line\">Loading data to table default.tb_station_coordinate</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 2.418 seconds</span><br></pre></td></tr></table></figure></p>\n<p>如果我们在<code>HDFS</code>的目录<code>/user/hive/warehouse/tb_station_coordinate</code>查看，我们可以得到表中的内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">桂林北站,110.302159,25.329024</span><br><span class=\"line\">杭州东站,120.213116,30.290998</span><br><span class=\"line\">山海关站,119.767555,40.000793</span><br><span class=\"line\">武昌站,114.317576,30.528401</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-删除表\"><a href=\"#2-3-删除表\" class=\"headerlink\" title=\"2.3 删除表\"></a>2.3 删除表</h4><p>现在让我们使用如下命令删除上面创建的表:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hive&gt; drop table tb_station_coordinate;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.174 seconds</span><br><span class=\"line\">hive&gt;</span><br></pre></td></tr></table></figure></p>\n<p>我们的Hadoop已经开启了回收站机制，但是删除操作并没有将数据进行删除，不像删除内部表一样，输出<code>Moved: &#39;hdfs://localhost:9000/user/hive/warehouse/tb_station_coordinate&#39; to trash at: hdfs://localhost:9000/user/xiaosi/.Trash/Current</code>(回收站机制请参阅:<a href=\"http://smartying.club/2017/12/07/Hadoop/Hadoop%20Trash%E5%9B%9E%E6%94%B6%E7%AB%99%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/\" target=\"_blank\" rel=\"noopener\">Hadoop Trash回收站使用指南</a>)。为了验证我们真的没有删除数据，我们在HDFS目录下查看数据:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">xiaosi@yoona:~$ hadoop fs -ls  /user/hive/warehouse/tb_station_coordinate</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rwxr-xr-x   1 xiaosi supergroup        374 2017-12-12 18:19 /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">xiaosi@yoona:~$</span><br><span class=\"line\">xiaosi@yoona:~$ hadoop fs -text  /user/hive/warehouse/tb_station_coordinate/station_coordinate.txt</span><br><span class=\"line\">桂林北站,110.302159,25.329024</span><br><span class=\"line\">杭州东站,120.213116,30.290998</span><br><span class=\"line\">山海关站,119.767555,40.000793</span><br><span class=\"line\">武昌站,114.317576,30.528401</span><br><span class=\"line\">北京南站,116.378875,39.865052</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>你可以看到表中的数据仍然在HDFS中。所以我们得知如果我们创建一个外部表，在删除表之后，只有与表相关的元数据被删除，而不会删除表的内容。</p>\n<h4 id=\"2-4-创建表指定外部目录\"><a href=\"#2-4-创建表指定外部目录\" class=\"headerlink\" title=\"2.4 创建表指定外部目录\"></a>2.4 创建表指定外部目录</h4><p>只有当你的数据在<code>/user/hive/warehouse</code>目录中时，上述方法才能有效。但是，如果你的数据在另一个位置，如果你删除该表，数据也将被删除。所以在这种情况下，你需要在创建表时设置数据的外部位置，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">CREATE EXTERNAL TABLE IF NOT EXISTS tb_station_coordinate(</span><br><span class=\"line\">  station string,</span><br><span class=\"line\">  lon string,</span><br><span class=\"line\">  lat string</span><br><span class=\"line\">)</span><br><span class=\"line\">ROW FORMAT DELIMITED</span><br><span class=\"line\">FIELDS TERMINATED BY &apos;,&apos;</span><br><span class=\"line\">LOCATION &apos;/user/xiaosi/test/coordinate/&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">你也可以通过在创建表时设置数据存储位置来创建一个内部表。但是，如果删除表，数据将被删除。</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果你想要创建外部表，需要在创建表的时候加上 EXTERNAL 关键字，同时指定外部表存放数据的路径(例如2.4所示)，也可以不指定外部表的存放路径(例如2.3所示)，这样Hive将在HDFS上的/user/hive/warehouse/目录下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-使用场景\"><a href=\"#3-使用场景\" class=\"headerlink\" title=\"3. 使用场景\"></a>3. 使用场景</h3><h4 id=\"3-1-内部表\"><a href=\"#3-1-内部表\" class=\"headerlink\" title=\"3.1 内部表\"></a>3.1 内部表</h4><ul>\n<li>数据是临时的</li>\n<li>希望使用<code>Hive</code>来管理表和数据的生命周期</li>\n<li>删除后不想要数据</li>\n</ul>\n<h4 id=\"3-2-外部表\"><a href=\"#3-2-外部表\" class=\"headerlink\" title=\"3.2 外部表\"></a>3.2 外部表</h4><ul>\n<li>这些数据也在<code>Hive</code>之外使用。</li>\n<li><code>Hive</code>不管理数据和权限设置以及目录等，需要你有另一个程序或过程来做这些事情</li>\n<li>不是基于现有表(AS SELECT)来创建的表</li>\n<li>可以创建表并使用相同的模式并指向数据的位置</li>\n</ul>\n<p>参考:<a href=\"https://acadgild.com/blog/managed-and-external-tables-in-hive/\" target=\"_blank\" rel=\"noopener\">https://acadgild.com/blog/managed-and-external-tables-in-hive/</a></p>\n<p><a href=\"https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil\" target=\"_blank\" rel=\"noopener\">https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil</a></p>\n"},{"title":"Hexo+Github配置与主题","author":"sjf0115","date":"2017-12-01T11:17:23.000Z","archives":"Hexo","_content":"\n### 1. 基本配置\n\n#### 1.1 语言设置\n\n每个主题都会配置几种界面显示语言，修改语言只要编辑站点配置文件，找到 `language` 字段，并将其值更改为你所需要的语言(例如，简体中文)：\n```\nlanguage: zh-Hans\n```\n\n#### 1.2 网站标题，作者\n\n打开站点配置文件，修改这些值：\n```\ntitle: SmartSi #博客标题\nsubtitle: #博客副标题\ndescription: #博客描述\nauthor: sjf0115 #博客作者\n```\n\n注意：\n```\n配置文件要符合英文标点符号使用规范: 冒号后必须空格，否则会编译错误\n```\n\n#### 1.3 域名与文章链接\n\n```\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://sjf0115.github.io #你的博客网址\nroot: / #博客跟目录，如果你的博客在网址的二级目录下，在这里填上\npermalink: :year/:month/:day/:title/ # 文章链接\npermalink_defaults:\n```\n\n### 2. 安装与启用主题\n\n最简单的安装方法是克隆整个仓库，在这里我们使用的是`NexT`主题：\n```\ncd hexo\ngit clone https://github.com/theme-next/hexo-theme-next themes/next\n```\n或者你可以看到其他详细的[安装说明](https://github.com/theme-next/hexo-theme-next/blob/master/docs/INSTALLATION.md)\n\n安装后，我们要启用我们安装的主题，与所有`Hexo`主题启用的模式一样。 当克隆/下载完成后，打开站点配置文件， 找到 `theme` 字段，并将其值更改为 `next` 。\n```\ntheme: next\n```\n\n### 3. 主题风格\n\n`NexT` 主题目前提供了3中风格类似，但是又有点不同的主题风格，可以通过修改 `主题配置文件` 中的 `Scheme` 值来启用其中一种风格，例如我的博客用的是 `Mist` 风格，只要把另外两个用#注释掉即可:\n```\n# Schemes\n#scheme: Muse\nscheme: Mist\n#scheme: Pisces\n```\n\n### 4. 设置 RSS\n\n`NexT` 中 `RSS` 有三个设置选项，满足特定的使用场景。 更改 `主题配置文件`，设定 `rss` 字段的值：\n- false：禁用 `RSS`，不在页面上显示 `RSS` 连接。\n- 留空：使用 Hexo 生成的 `Feed` 链接。 你可以需要先安装 `hexo-generator-feed` 插件。\n- 具体的链接地址：适用于已经烧制过 Feed 的情形。\n\n### 5. 导航栏添加标签菜单\n\n新建标签页面，并在菜单中显示标签链接。标签页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。\n\n(1) 在终端窗口下，定位到 `Hexo` 站点目录下。使用如下命令新建一名为 `tags` 页面:\n```\nhexo new page \"tags\"\n```\n(2) 编辑刚新建的页面，将页面的类型设置为 `tags` ，主题将自动为这个页面显示标签云。页面内容如下：\n```\ntitle: 标签\ndate: 2017-12-22 12:39:04\ntype: \"tags\"\n```\n(3) 在菜单中添加链接。编辑 `主题配置文件` ，添加 `tags` 到 `menu` 中，如下:\n```\n  menu:\n    home: /\n    archives: /archives\n    tags: /tags\n```\n(4) 使用时在你的文章中添加如下代码：\n```\n---\ntitle: title name\ndate: 2017-12-12-22 12:39:04\ntags:\n  - first tag\n  - second tag\n---\n```\n\n### 6. 添加分类页面\n\n新建分类页面，并在菜单中显示分类链接。分类页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。\n\n(1) 在终端窗口下，定位到 `Hexo` 站点目录下。使用 `hexo new page` 新建一个页面，命名为 `categories` ：\n```\nhexo new page categories\n```\n(2) 编辑刚新建的页面，将页面的 `type` 设置为 `categories` ，主题将自动为这个页面显示分类。页面内容如下：\n```\n---\ntitle: 分类\ndate: 2014-12-22 12:39:04\ntype: \"categories\"\n---\n```\n(3) 在菜单中添加链接。编辑 `主题配置文件` ， 添加 `categories` 到 `menu` 中，如下:\n```\nmenu:\n  home: /\n  archives: /archives\n  categories: /categories\n```\n(4) 使用时在你的文章中添加如下代码：\n```\n---\ntitle: title name\ndate: 2017-12-12-22 12:39:04\ntype: \"categories\"\n---\n```\n### 7. 侧边栏社交链接\n\n侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 `主题配置文件` 中。\n\n(1) 链接放置在 `social` 字段下，一行一个链接。其键值格式是 `显示文本: 链接地址 || 图标`：\n```\nsocial:\n  GitHub: https://github.com/sjf0115 || github\n  E-Mail: mailto:1203745031@qq.com || envelope\n  CSDN: http://blog.csdn.net/sunnyyoona\n```\n备注:\n```\n如果没有指定图标（带或不带分隔符），则会加载默认图标。\n```\nExample:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-1.png?raw=true)\n\n(2) 设定链接的图标，对应的字段是 `social_icons`。其键值格式是: `匹配键: Font Awesome 图标名称`， 匹配键与上一步所配置的链接的显示文本相同（大小写严格匹配），图标名称是 `Font Awesome` 图标的名字（不必带 fa- 前缀）。 `enable` 选项用于控制是否显示图标，你可以设置成 `false` 来去掉图标:\n```\n# Social Icons\nsocial_icons:\n  enable: true\n  # Icon Mappings\n  GitHub: github\n  Twitter: twitter\n  微博: weibo\n```\n\n### 8. 友情链接\n\n编辑 `主题配置文件` 添加：\n```\n友情链接配置示例\n# Blog rolls\nlinks_icon: link\nlinks_title: Links\nlinks_layout: block\n#links_layout: inline\nlinks:\n  CSDN: http://blog.csdn.net/sunnyyoona\n```\nExample:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-2.png?raw=true)\n\n### 9. 站点建立时间\n\n这个时间将在站点的底部显示，例如 `© 2017 - 2018`。 编辑 `主题配置文件`，新增字段 `since`:\n```\n配置示例\nsince: 2017\n```\n\n### 10. 腾讯公益404页面\n\n腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 http://smartsi.club/404.html\n\n(1) 使用方法，新建 `404.html` 页面，放到主题的 `source` 目录下，内容如下：\n```\n<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"https://qzone.qq.com/gy/404/style/404style.css\">\n</head>\n<body>\n  <script type=\"text/plain\" src=\"http://www.qq.com/404/search_children.js\"\n          charset=\"utf-8\" homePageUrl=\"/\"\n          homePageName=\"回到我的主页\">\n  </script>\n  <script src=\"https://qzone.qq.com/gy/404/data.js\" charset=\"utf-8\"></script>\n  <script src=\"https://qzone.qq.com/gy/404/page.js\" charset=\"utf-8\"></script>\n</body>\n</html>\n```\n(2) 开启404页面功能\n\n在 `menu` 下添加 `commonweal: /404/ || heartbeat`：\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n  #schedule: /schedule/ || calendar\n  #sitemap: /sitemap.xml || sitemap\n  commonweal: /404/ || heartbeat\n```\n\n### 11. 开启打赏功能\n\n越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 `主题配置文件` 中填入 `微信` 和 `支付宝` 收款二维码图片地址 即可开启该功能：\n```\n打赏功能配置示例\nreward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！\nwechatpay: /path/to/wechat-reward-image\nalipay: /path/to/alipay-reward-image\n```\n\n### 12. 订阅微信公众号\n\n备注:\n```\n此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后\n```\n在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。\n\n在微信公众号平台下载您的二维码，并将它存放于博客`source/uploads/`目录下。\n\n然后编辑 主题配置文件，如下：\n```\n配置示例\n# Wechat Subscriber\nwechat_subscriber:\n  enabled: true\n  qcode: /uploads/wechat-qcode.jpg\n  description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！\n```\n\n### 13. 设置背景动画\n\n`NexT` 自带两种背景动画效果，编辑 `主题配置文件`， 搜索 `canvas_nest` 或 `three_waves`，根据你的需求设置值为 `true` 或者 `false` 即可：\n\n备注:\n```\nthree_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。\n```\ncanvas_nest 配置示例\n```\n# canvas_nest\ncanvas_nest: true //开启动画\ncanvas_nest: false //关闭动画\n```\nthree_waves 配置示例\n```\n# three_waves\nthree_waves: true //开启动画\nthree_waves: false //关闭动画\n```\n\n### 14. 设置阅读全文\n\n在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 `NexT` 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 `阅读全文` 按钮，可以通过以下方法：\n(1) 在文章中使用 `<!-- more -->` 手动进行截断，`Hexo` 提供的方式 推荐\n(2) 在文章的 [front-matter](https://hexo.io/docs/front-matter.html) 中添加 `description`，并提供文章摘录\n(3) 自动形成摘要，在 `主题配置文件` 中添加：\n```\nauto_excerpt:\n  enable: true\n  length: 150\n```\n默认截取的长度为 150 字符，可以根据需要自行设定\n\n建议使用 `<!-- more -->`（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 `Hexo` 中的插件更好的识别。\n\n### 15. 站内搜索\n\n`NexT` 支持集成 `Swiftype`、 微搜索、`Local Search` 和 `Algolia`。在这里我使用的是`Local Search`，下面将介绍如何使用:\n\n(1) 添加百度/谷歌/本地 自定义站点内容搜索，安装 `hexo-generator-searchdb`，在站点的根目录下执行以下命令：\n```\nnpm install hexo-generator-searchdb --save\n```\n(2) 编辑 `站点配置文件`，新增以下内容到任意位置：\n```\nsearch:\n  path: search.xml\n  field: post\n  format: html\n  limit: 10000\n```\n(3) 编辑 `主题配置文件`，启用本地搜索功能：\n```\n# Local search\nlocal_search:\n  enable: true\n```\n\n其他搜索方式请查看[搜索服务](http://theme-next.iissnan.com/third-party-services.html#search-system)\n\n### 16. 不蒜子统计\n\n备注：\n```\n此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后\n```\n\n(1) 全局配置。编辑 `主题配置文件` 中的 `busuanzi_count` 的配置项。当 `enable: true` 时，代表开启全局开关。若 `site_uv` 、`site_pv` 、 `page_pv` 的值均为 `false` 时，不蒜子仅作记录而不会在页面上显示。\n\n(2) 站点UV配置。当 `site_uv: true` 时，代表在页面底部显示站点的UV值。`site_uv_header` 和 `site_uv_footer` 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）`font-awesome`。显示效果为 `[site_uv_header]UV值[site_uv_footer]`。\n```\n# 效果：本站访客数12345人次\nsite_uv: true\nsite_uv_header: 本站访客数\nsite_uv_footer: 人次\n```\n(3) 站点PV配置。当 `site_pv: true` 时，代表在页面底部显示站点的PV值。`site_pv_header` 和 `site_pv_footer` 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）`font-awesome`。显示效果为 `[site_pv_header]PV值[site_pv_footer]`。\n```\n# 效果：本站总访问量12345次\nsite_pv: true\nsite_pv_header: 本站总访问量\nsite_pv_footer: 次\n```\n(4) Example:\n```\nbusuanzi_count:\n  # count values only if the other configs are false\n  enable: true\n  # custom uv span for the whole site\n  site_uv: true\n  site_uv_header: <i class=\"fa fa-eye\"></i> 本站访客数\n  site_uv_footer: 次\n  # custom pv span for the whole site\n  site_pv: true\n  site_pv_header: 本站总访问量\n  site_pv_footer: 次\n  # custom pv span for one page only\n  page_pv: false\n  page_pv_header: <i class=\"fa fa-file-o\"></i>\n  page_pv_footer:\n```\n\n### 17. 开启about自我介绍页面\n\n(1) 在终端窗口下，定位到 `Hexo` 站点目录下。使用 `hexo new page` 新建一个页面，命名为 `about` ：\n```\ncd your-hexo-site\nhexo new page abhout\n```\n(2) 编辑刚新建的页面，将页面的类型设置为 `about`。页面内容如下：\n```\n---\ntitle: about\ndate: 2014-12-22 12:39:04\ntype: \"about\"\n---\n```\n(3) 在菜单中添加链接。编辑 `主题配置文件` ， 添加 `about` 到 `menu` 中，如下:\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n```\n\n\n\n\n参考: http://theme-next.iissnan.com/theme-settings.html\n\nhttp://theme-next.iissnan.com/third-party-services.html\n","source":"_posts/Hexo/[Hexo]Hexo+Github配置与主题.md","raw":"---\ntitle: Hexo+Github配置与主题\nauthor: sjf0115\ndate: 2017-12-01 19:17:23\ntags:\n- Hexo\n\narchives: Hexo\n---\n\n### 1. 基本配置\n\n#### 1.1 语言设置\n\n每个主题都会配置几种界面显示语言，修改语言只要编辑站点配置文件，找到 `language` 字段，并将其值更改为你所需要的语言(例如，简体中文)：\n```\nlanguage: zh-Hans\n```\n\n#### 1.2 网站标题，作者\n\n打开站点配置文件，修改这些值：\n```\ntitle: SmartSi #博客标题\nsubtitle: #博客副标题\ndescription: #博客描述\nauthor: sjf0115 #博客作者\n```\n\n注意：\n```\n配置文件要符合英文标点符号使用规范: 冒号后必须空格，否则会编译错误\n```\n\n#### 1.3 域名与文章链接\n\n```\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://sjf0115.github.io #你的博客网址\nroot: / #博客跟目录，如果你的博客在网址的二级目录下，在这里填上\npermalink: :year/:month/:day/:title/ # 文章链接\npermalink_defaults:\n```\n\n### 2. 安装与启用主题\n\n最简单的安装方法是克隆整个仓库，在这里我们使用的是`NexT`主题：\n```\ncd hexo\ngit clone https://github.com/theme-next/hexo-theme-next themes/next\n```\n或者你可以看到其他详细的[安装说明](https://github.com/theme-next/hexo-theme-next/blob/master/docs/INSTALLATION.md)\n\n安装后，我们要启用我们安装的主题，与所有`Hexo`主题启用的模式一样。 当克隆/下载完成后，打开站点配置文件， 找到 `theme` 字段，并将其值更改为 `next` 。\n```\ntheme: next\n```\n\n### 3. 主题风格\n\n`NexT` 主题目前提供了3中风格类似，但是又有点不同的主题风格，可以通过修改 `主题配置文件` 中的 `Scheme` 值来启用其中一种风格，例如我的博客用的是 `Mist` 风格，只要把另外两个用#注释掉即可:\n```\n# Schemes\n#scheme: Muse\nscheme: Mist\n#scheme: Pisces\n```\n\n### 4. 设置 RSS\n\n`NexT` 中 `RSS` 有三个设置选项，满足特定的使用场景。 更改 `主题配置文件`，设定 `rss` 字段的值：\n- false：禁用 `RSS`，不在页面上显示 `RSS` 连接。\n- 留空：使用 Hexo 生成的 `Feed` 链接。 你可以需要先安装 `hexo-generator-feed` 插件。\n- 具体的链接地址：适用于已经烧制过 Feed 的情形。\n\n### 5. 导航栏添加标签菜单\n\n新建标签页面，并在菜单中显示标签链接。标签页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。\n\n(1) 在终端窗口下，定位到 `Hexo` 站点目录下。使用如下命令新建一名为 `tags` 页面:\n```\nhexo new page \"tags\"\n```\n(2) 编辑刚新建的页面，将页面的类型设置为 `tags` ，主题将自动为这个页面显示标签云。页面内容如下：\n```\ntitle: 标签\ndate: 2017-12-22 12:39:04\ntype: \"tags\"\n```\n(3) 在菜单中添加链接。编辑 `主题配置文件` ，添加 `tags` 到 `menu` 中，如下:\n```\n  menu:\n    home: /\n    archives: /archives\n    tags: /tags\n```\n(4) 使用时在你的文章中添加如下代码：\n```\n---\ntitle: title name\ndate: 2017-12-12-22 12:39:04\ntags:\n  - first tag\n  - second tag\n---\n```\n\n### 6. 添加分类页面\n\n新建分类页面，并在菜单中显示分类链接。分类页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。\n\n(1) 在终端窗口下，定位到 `Hexo` 站点目录下。使用 `hexo new page` 新建一个页面，命名为 `categories` ：\n```\nhexo new page categories\n```\n(2) 编辑刚新建的页面，将页面的 `type` 设置为 `categories` ，主题将自动为这个页面显示分类。页面内容如下：\n```\n---\ntitle: 分类\ndate: 2014-12-22 12:39:04\ntype: \"categories\"\n---\n```\n(3) 在菜单中添加链接。编辑 `主题配置文件` ， 添加 `categories` 到 `menu` 中，如下:\n```\nmenu:\n  home: /\n  archives: /archives\n  categories: /categories\n```\n(4) 使用时在你的文章中添加如下代码：\n```\n---\ntitle: title name\ndate: 2017-12-12-22 12:39:04\ntype: \"categories\"\n---\n```\n### 7. 侧边栏社交链接\n\n侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 `主题配置文件` 中。\n\n(1) 链接放置在 `social` 字段下，一行一个链接。其键值格式是 `显示文本: 链接地址 || 图标`：\n```\nsocial:\n  GitHub: https://github.com/sjf0115 || github\n  E-Mail: mailto:1203745031@qq.com || envelope\n  CSDN: http://blog.csdn.net/sunnyyoona\n```\n备注:\n```\n如果没有指定图标（带或不带分隔符），则会加载默认图标。\n```\nExample:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-1.png?raw=true)\n\n(2) 设定链接的图标，对应的字段是 `social_icons`。其键值格式是: `匹配键: Font Awesome 图标名称`， 匹配键与上一步所配置的链接的显示文本相同（大小写严格匹配），图标名称是 `Font Awesome` 图标的名字（不必带 fa- 前缀）。 `enable` 选项用于控制是否显示图标，你可以设置成 `false` 来去掉图标:\n```\n# Social Icons\nsocial_icons:\n  enable: true\n  # Icon Mappings\n  GitHub: github\n  Twitter: twitter\n  微博: weibo\n```\n\n### 8. 友情链接\n\n编辑 `主题配置文件` 添加：\n```\n友情链接配置示例\n# Blog rolls\nlinks_icon: link\nlinks_title: Links\nlinks_layout: block\n#links_layout: inline\nlinks:\n  CSDN: http://blog.csdn.net/sunnyyoona\n```\nExample:\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-2.png?raw=true)\n\n### 9. 站点建立时间\n\n这个时间将在站点的底部显示，例如 `© 2017 - 2018`。 编辑 `主题配置文件`，新增字段 `since`:\n```\n配置示例\nsince: 2017\n```\n\n### 10. 腾讯公益404页面\n\n腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 http://smartsi.club/404.html\n\n(1) 使用方法，新建 `404.html` 页面，放到主题的 `source` 目录下，内容如下：\n```\n<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"https://qzone.qq.com/gy/404/style/404style.css\">\n</head>\n<body>\n  <script type=\"text/plain\" src=\"http://www.qq.com/404/search_children.js\"\n          charset=\"utf-8\" homePageUrl=\"/\"\n          homePageName=\"回到我的主页\">\n  </script>\n  <script src=\"https://qzone.qq.com/gy/404/data.js\" charset=\"utf-8\"></script>\n  <script src=\"https://qzone.qq.com/gy/404/page.js\" charset=\"utf-8\"></script>\n</body>\n</html>\n```\n(2) 开启404页面功能\n\n在 `menu` 下添加 `commonweal: /404/ || heartbeat`：\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n  #schedule: /schedule/ || calendar\n  #sitemap: /sitemap.xml || sitemap\n  commonweal: /404/ || heartbeat\n```\n\n### 11. 开启打赏功能\n\n越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 `主题配置文件` 中填入 `微信` 和 `支付宝` 收款二维码图片地址 即可开启该功能：\n```\n打赏功能配置示例\nreward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！\nwechatpay: /path/to/wechat-reward-image\nalipay: /path/to/alipay-reward-image\n```\n\n### 12. 订阅微信公众号\n\n备注:\n```\n此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后\n```\n在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。\n\n在微信公众号平台下载您的二维码，并将它存放于博客`source/uploads/`目录下。\n\n然后编辑 主题配置文件，如下：\n```\n配置示例\n# Wechat Subscriber\nwechat_subscriber:\n  enabled: true\n  qcode: /uploads/wechat-qcode.jpg\n  description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！\n```\n\n### 13. 设置背景动画\n\n`NexT` 自带两种背景动画效果，编辑 `主题配置文件`， 搜索 `canvas_nest` 或 `three_waves`，根据你的需求设置值为 `true` 或者 `false` 即可：\n\n备注:\n```\nthree_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。\n```\ncanvas_nest 配置示例\n```\n# canvas_nest\ncanvas_nest: true //开启动画\ncanvas_nest: false //关闭动画\n```\nthree_waves 配置示例\n```\n# three_waves\nthree_waves: true //开启动画\nthree_waves: false //关闭动画\n```\n\n### 14. 设置阅读全文\n\n在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 `NexT` 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 `阅读全文` 按钮，可以通过以下方法：\n(1) 在文章中使用 `<!-- more -->` 手动进行截断，`Hexo` 提供的方式 推荐\n(2) 在文章的 [front-matter](https://hexo.io/docs/front-matter.html) 中添加 `description`，并提供文章摘录\n(3) 自动形成摘要，在 `主题配置文件` 中添加：\n```\nauto_excerpt:\n  enable: true\n  length: 150\n```\n默认截取的长度为 150 字符，可以根据需要自行设定\n\n建议使用 `<!-- more -->`（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 `Hexo` 中的插件更好的识别。\n\n### 15. 站内搜索\n\n`NexT` 支持集成 `Swiftype`、 微搜索、`Local Search` 和 `Algolia`。在这里我使用的是`Local Search`，下面将介绍如何使用:\n\n(1) 添加百度/谷歌/本地 自定义站点内容搜索，安装 `hexo-generator-searchdb`，在站点的根目录下执行以下命令：\n```\nnpm install hexo-generator-searchdb --save\n```\n(2) 编辑 `站点配置文件`，新增以下内容到任意位置：\n```\nsearch:\n  path: search.xml\n  field: post\n  format: html\n  limit: 10000\n```\n(3) 编辑 `主题配置文件`，启用本地搜索功能：\n```\n# Local search\nlocal_search:\n  enable: true\n```\n\n其他搜索方式请查看[搜索服务](http://theme-next.iissnan.com/third-party-services.html#search-system)\n\n### 16. 不蒜子统计\n\n备注：\n```\n此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后\n```\n\n(1) 全局配置。编辑 `主题配置文件` 中的 `busuanzi_count` 的配置项。当 `enable: true` 时，代表开启全局开关。若 `site_uv` 、`site_pv` 、 `page_pv` 的值均为 `false` 时，不蒜子仅作记录而不会在页面上显示。\n\n(2) 站点UV配置。当 `site_uv: true` 时，代表在页面底部显示站点的UV值。`site_uv_header` 和 `site_uv_footer` 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）`font-awesome`。显示效果为 `[site_uv_header]UV值[site_uv_footer]`。\n```\n# 效果：本站访客数12345人次\nsite_uv: true\nsite_uv_header: 本站访客数\nsite_uv_footer: 人次\n```\n(3) 站点PV配置。当 `site_pv: true` 时，代表在页面底部显示站点的PV值。`site_pv_header` 和 `site_pv_footer` 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）`font-awesome`。显示效果为 `[site_pv_header]PV值[site_pv_footer]`。\n```\n# 效果：本站总访问量12345次\nsite_pv: true\nsite_pv_header: 本站总访问量\nsite_pv_footer: 次\n```\n(4) Example:\n```\nbusuanzi_count:\n  # count values only if the other configs are false\n  enable: true\n  # custom uv span for the whole site\n  site_uv: true\n  site_uv_header: <i class=\"fa fa-eye\"></i> 本站访客数\n  site_uv_footer: 次\n  # custom pv span for the whole site\n  site_pv: true\n  site_pv_header: 本站总访问量\n  site_pv_footer: 次\n  # custom pv span for one page only\n  page_pv: false\n  page_pv_header: <i class=\"fa fa-file-o\"></i>\n  page_pv_footer:\n```\n\n### 17. 开启about自我介绍页面\n\n(1) 在终端窗口下，定位到 `Hexo` 站点目录下。使用 `hexo new page` 新建一个页面，命名为 `about` ：\n```\ncd your-hexo-site\nhexo new page abhout\n```\n(2) 编辑刚新建的页面，将页面的类型设置为 `about`。页面内容如下：\n```\n---\ntitle: about\ndate: 2014-12-22 12:39:04\ntype: \"about\"\n---\n```\n(3) 在菜单中添加链接。编辑 `主题配置文件` ， 添加 `about` 到 `menu` 中，如下:\n```\nmenu:\n  home: / || home\n  about: /about/ || user\n```\n\n\n\n\n参考: http://theme-next.iissnan.com/theme-settings.html\n\nhttp://theme-next.iissnan.com/third-party-services.html\n","slug":"Hexo/[Hexo]Hexo+Github配置与主题","published":1,"updated":"2018-01-22T06:12:36.603Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje58titq0042ordb1phechi4","content":"<h3 id=\"1-基本配置\"><a href=\"#1-基本配置\" class=\"headerlink\" title=\"1. 基本配置\"></a>1. 基本配置</h3><h4 id=\"1-1-语言设置\"><a href=\"#1-1-语言设置\" class=\"headerlink\" title=\"1.1 语言设置\"></a>1.1 语言设置</h4><p>每个主题都会配置几种界面显示语言，修改语言只要编辑站点配置文件，找到 <code>language</code> 字段，并将其值更改为你所需要的语言(例如，简体中文)：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">language: zh-Hans</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-网站标题，作者\"><a href=\"#1-2-网站标题，作者\" class=\"headerlink\" title=\"1.2 网站标题，作者\"></a>1.2 网站标题，作者</h4><p>打开站点配置文件，修改这些值：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">title: SmartSi #博客标题</span><br><span class=\"line\">subtitle: #博客副标题</span><br><span class=\"line\">description: #博客描述</span><br><span class=\"line\">author: sjf0115 #博客作者</span><br></pre></td></tr></table></figure></p>\n<p>注意：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">配置文件要符合英文标点符号使用规范: 冒号后必须空格，否则会编译错误</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-3-域名与文章链接\"><a href=\"#1-3-域名与文章链接\" class=\"headerlink\" title=\"1.3 域名与文章链接\"></a>1.3 域名与文章链接</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># URL</span><br><span class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</span><br><span class=\"line\">url: http://sjf0115.github.io #你的博客网址</span><br><span class=\"line\">root: / #博客跟目录，如果你的博客在网址的二级目录下，在这里填上</span><br><span class=\"line\">permalink: :year/:month/:day/:title/ # 文章链接</span><br><span class=\"line\">permalink_defaults:</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-安装与启用主题\"><a href=\"#2-安装与启用主题\" class=\"headerlink\" title=\"2. 安装与启用主题\"></a>2. 安装与启用主题</h3><p>最简单的安装方法是克隆整个仓库，在这里我们使用的是<code>NexT</code>主题：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">cd hexo</span><br><span class=\"line\">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p>\n<p>或者你可以看到其他详细的<a href=\"https://github.com/theme-next/hexo-theme-next/blob/master/docs/INSTALLATION.md\" target=\"_blank\" rel=\"noopener\">安装说明</a></p>\n<p>安装后，我们要启用我们安装的主题，与所有<code>Hexo</code>主题启用的模式一样。 当克隆/下载完成后，打开站点配置文件， 找到 <code>theme</code> 字段，并将其值更改为 <code>next</code> 。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">theme: next</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-主题风格\"><a href=\"#3-主题风格\" class=\"headerlink\" title=\"3. 主题风格\"></a>3. 主题风格</h3><p><code>NexT</code> 主题目前提供了3中风格类似，但是又有点不同的主题风格，可以通过修改 <code>主题配置文件</code> 中的 <code>Scheme</code> 值来启用其中一种风格，例如我的博客用的是 <code>Mist</code> 风格，只要把另外两个用#注释掉即可:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Schemes</span><br><span class=\"line\">#scheme: Muse</span><br><span class=\"line\">scheme: Mist</span><br><span class=\"line\">#scheme: Pisces</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-设置-RSS\"><a href=\"#4-设置-RSS\" class=\"headerlink\" title=\"4. 设置 RSS\"></a>4. 设置 RSS</h3><p><code>NexT</code> 中 <code>RSS</code> 有三个设置选项，满足特定的使用场景。 更改 <code>主题配置文件</code>，设定 <code>rss</code> 字段的值：</p>\n<ul>\n<li>false：禁用 <code>RSS</code>，不在页面上显示 <code>RSS</code> 连接。</li>\n<li>留空：使用 Hexo 生成的 <code>Feed</code> 链接。 你可以需要先安装 <code>hexo-generator-feed</code> 插件。</li>\n<li>具体的链接地址：适用于已经烧制过 Feed 的情形。</li>\n</ul>\n<h3 id=\"5-导航栏添加标签菜单\"><a href=\"#5-导航栏添加标签菜单\" class=\"headerlink\" title=\"5. 导航栏添加标签菜单\"></a>5. 导航栏添加标签菜单</h3><p>新建标签页面，并在菜单中显示标签链接。标签页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。</p>\n<p>(1) 在终端窗口下，定位到 <code>Hexo</code> 站点目录下。使用如下命令新建一名为 <code>tags</code> 页面:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hexo new page &quot;tags&quot;</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑刚新建的页面，将页面的类型设置为 <code>tags</code> ，主题将自动为这个页面显示标签云。页面内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">title: 标签</span><br><span class=\"line\">date: 2017-12-22 12:39:04</span><br><span class=\"line\">type: &quot;tags&quot;</span><br></pre></td></tr></table></figure></p>\n<p>(3) 在菜单中添加链接。编辑 <code>主题配置文件</code> ，添加 <code>tags</code> 到 <code>menu</code> 中，如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: /</span><br><span class=\"line\">  archives: /archives</span><br><span class=\"line\">  tags: /tags</span><br></pre></td></tr></table></figure></p>\n<p>(4) 使用时在你的文章中添加如下代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: title name</span><br><span class=\"line\">date: 2017-12-12-22 12:39:04</span><br><span class=\"line\">tags:</span><br><span class=\"line\">  - first tag</span><br><span class=\"line\">  - second tag</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-添加分类页面\"><a href=\"#6-添加分类页面\" class=\"headerlink\" title=\"6. 添加分类页面\"></a>6. 添加分类页面</h3><p>新建分类页面，并在菜单中显示分类链接。分类页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。</p>\n<p>(1) 在终端窗口下，定位到 <code>Hexo</code> 站点目录下。使用 <code>hexo new page</code> 新建一个页面，命名为 <code>categories</code> ：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hexo new page categories</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑刚新建的页面，将页面的 <code>type</code> 设置为 <code>categories</code> ，主题将自动为这个页面显示分类。页面内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: 分类</span><br><span class=\"line\">date: 2014-12-22 12:39:04</span><br><span class=\"line\">type: &quot;categories&quot;</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<p>(3) 在菜单中添加链接。编辑 <code>主题配置文件</code> ， 添加 <code>categories</code> 到 <code>menu</code> 中，如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: /</span><br><span class=\"line\">  archives: /archives</span><br><span class=\"line\">  categories: /categories</span><br></pre></td></tr></table></figure></p>\n<p>(4) 使用时在你的文章中添加如下代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: title name</span><br><span class=\"line\">date: 2017-12-12-22 12:39:04</span><br><span class=\"line\">type: &quot;categories&quot;</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-侧边栏社交链接\"><a href=\"#7-侧边栏社交链接\" class=\"headerlink\" title=\"7. 侧边栏社交链接\"></a>7. 侧边栏社交链接</h3><p>侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 <code>主题配置文件</code> 中。</p>\n<p>(1) 链接放置在 <code>social</code> 字段下，一行一个链接。其键值格式是 <code>显示文本: 链接地址 || 图标</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">social:</span><br><span class=\"line\">  GitHub: https://github.com/sjf0115 || github</span><br><span class=\"line\">  E-Mail: mailto:1203745031@qq.com || envelope</span><br><span class=\"line\">  CSDN: http://blog.csdn.net/sunnyyoona</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果没有指定图标（带或不带分隔符），则会加载默认图标。</span><br></pre></td></tr></table></figure></p>\n<p>Example:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-1.png?raw=true\" alt=\"\"></p>\n<p>(2) 设定链接的图标，对应的字段是 <code>social_icons</code>。其键值格式是: <code>匹配键: Font Awesome 图标名称</code>， 匹配键与上一步所配置的链接的显示文本相同（大小写严格匹配），图标名称是 <code>Font Awesome</code> 图标的名字（不必带 fa- 前缀）。 <code>enable</code> 选项用于控制是否显示图标，你可以设置成 <code>false</code> 来去掉图标:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Social Icons</span><br><span class=\"line\">social_icons:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  # Icon Mappings</span><br><span class=\"line\">  GitHub: github</span><br><span class=\"line\">  Twitter: twitter</span><br><span class=\"line\">  微博: weibo</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-友情链接\"><a href=\"#8-友情链接\" class=\"headerlink\" title=\"8. 友情链接\"></a>8. 友情链接</h3><p>编辑 <code>主题配置文件</code> 添加：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">友情链接配置示例</span><br><span class=\"line\"># Blog rolls</span><br><span class=\"line\">links_icon: link</span><br><span class=\"line\">links_title: Links</span><br><span class=\"line\">links_layout: block</span><br><span class=\"line\">#links_layout: inline</span><br><span class=\"line\">links:</span><br><span class=\"line\">  CSDN: http://blog.csdn.net/sunnyyoona</span><br></pre></td></tr></table></figure></p>\n<p>Example:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"9-站点建立时间\"><a href=\"#9-站点建立时间\" class=\"headerlink\" title=\"9. 站点建立时间\"></a>9. 站点建立时间</h3><p>这个时间将在站点的底部显示，例如 <code>© 2017 - 2018</code>。 编辑 <code>主题配置文件</code>，新增字段 <code>since</code>:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">配置示例</span><br><span class=\"line\">since: 2017</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"10-腾讯公益404页面\"><a href=\"#10-腾讯公益404页面\" class=\"headerlink\" title=\"10. 腾讯公益404页面\"></a>10. 腾讯公益404页面</h3><p>腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 <a href=\"http://smartsi.club/404.html\" target=\"_blank\" rel=\"noopener\">http://smartsi.club/404.html</a></p>\n<p>(1) 使用方法，新建 <code>404.html</code> 页面，放到主题的 <code>source</code> 目录下，内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE HTML&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt;</span><br><span class=\"line\">  &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt;</span><br><span class=\"line\">  &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt;</span><br><span class=\"line\">  &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt;</span><br><span class=\"line\">  &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot;</span><br><span class=\"line\">          charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot;</span><br><span class=\"line\">          homePageName=&quot;回到我的主页&quot;&gt;</span><br><span class=\"line\">  &lt;/script&gt;</span><br><span class=\"line\">  &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">  &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>(2) 开启404页面功能</p>\n<p>在 <code>menu</code> 下添加 <code>commonweal: /404/ || heartbeat</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: / || home</span><br><span class=\"line\">  about: /about/ || user</span><br><span class=\"line\">  tags: /tags/ || tags</span><br><span class=\"line\">  categories: /categories/ || th</span><br><span class=\"line\">  archives: /archives/ || archive</span><br><span class=\"line\">  #schedule: /schedule/ || calendar</span><br><span class=\"line\">  #sitemap: /sitemap.xml || sitemap</span><br><span class=\"line\">  commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"11-开启打赏功能\"><a href=\"#11-开启打赏功能\" class=\"headerlink\" title=\"11. 开启打赏功能\"></a>11. 开启打赏功能</h3><p>越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 <code>主题配置文件</code> 中填入 <code>微信</code> 和 <code>支付宝</code> 收款二维码图片地址 即可开启该功能：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">打赏功能配置示例</span><br><span class=\"line\">reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！</span><br><span class=\"line\">wechatpay: /path/to/wechat-reward-image</span><br><span class=\"line\">alipay: /path/to/alipay-reward-image</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"12-订阅微信公众号\"><a href=\"#12-订阅微信公众号\" class=\"headerlink\" title=\"12. 订阅微信公众号\"></a>12. 订阅微信公众号</h3><p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后</span><br></pre></td></tr></table></figure></p>\n<p>在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。</p>\n<p>在微信公众号平台下载您的二维码，并将它存放于博客<code>source/uploads/</code>目录下。</p>\n<p>然后编辑 主题配置文件，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">配置示例</span><br><span class=\"line\"># Wechat Subscriber</span><br><span class=\"line\">wechat_subscriber:</span><br><span class=\"line\">  enabled: true</span><br><span class=\"line\">  qcode: /uploads/wechat-qcode.jpg</span><br><span class=\"line\">  description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"13-设置背景动画\"><a href=\"#13-设置背景动画\" class=\"headerlink\" title=\"13. 设置背景动画\"></a>13. 设置背景动画</h3><p><code>NexT</code> 自带两种背景动画效果，编辑 <code>主题配置文件</code>， 搜索 <code>canvas_nest</code> 或 <code>three_waves</code>，根据你的需求设置值为 <code>true</code> 或者 <code>false</code> 即可：</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">three_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。</span><br></pre></td></tr></table></figure></p>\n<p>canvas_nest 配置示例<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># canvas_nest</span><br><span class=\"line\">canvas_nest: true //开启动画</span><br><span class=\"line\">canvas_nest: false //关闭动画</span><br></pre></td></tr></table></figure></p>\n<p>three_waves 配置示例<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># three_waves</span><br><span class=\"line\">three_waves: true //开启动画</span><br><span class=\"line\">three_waves: false //关闭动画</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"14-设置阅读全文\"><a href=\"#14-设置阅读全文\" class=\"headerlink\" title=\"14. 设置阅读全文\"></a>14. 设置阅读全文</h3><p>在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 <code>NexT</code> 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 <code>阅读全文</code> 按钮，可以通过以下方法：<br>(1) 在文章中使用 <code>&lt;!-- more --&gt;</code> 手动进行截断，<code>Hexo</code> 提供的方式 推荐<br>(2) 在文章的 <a href=\"https://hexo.io/docs/front-matter.html\" target=\"_blank\" rel=\"noopener\">front-matter</a> 中添加 <code>description</code>，并提供文章摘录<br>(3) 自动形成摘要，在 <code>主题配置文件</code> 中添加：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">auto_excerpt:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  length: 150</span><br></pre></td></tr></table></figure></p>\n<p>默认截取的长度为 150 字符，可以根据需要自行设定</p>\n<p>建议使用 <code>&lt;!-- more --&gt;</code>（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 <code>Hexo</code> 中的插件更好的识别。</p>\n<h3 id=\"15-站内搜索\"><a href=\"#15-站内搜索\" class=\"headerlink\" title=\"15. 站内搜索\"></a>15. 站内搜索</h3><p><code>NexT</code> 支持集成 <code>Swiftype</code>、 微搜索、<code>Local Search</code> 和 <code>Algolia</code>。在这里我使用的是<code>Local Search</code>，下面将介绍如何使用:</p>\n<p>(1) 添加百度/谷歌/本地 自定义站点内容搜索，安装 <code>hexo-generator-searchdb</code>，在站点的根目录下执行以下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑 <code>站点配置文件</code>，新增以下内容到任意位置：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">search:</span><br><span class=\"line\">  path: search.xml</span><br><span class=\"line\">  field: post</span><br><span class=\"line\">  format: html</span><br><span class=\"line\">  limit: 10000</span><br></pre></td></tr></table></figure></p>\n<p>(3) 编辑 <code>主题配置文件</code>，启用本地搜索功能：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Local search</span><br><span class=\"line\">local_search:</span><br><span class=\"line\">  enable: true</span><br></pre></td></tr></table></figure></p>\n<p>其他搜索方式请查看<a href=\"http://theme-next.iissnan.com/third-party-services.html#search-system\" target=\"_blank\" rel=\"noopener\">搜索服务</a></p>\n<h3 id=\"16-不蒜子统计\"><a href=\"#16-不蒜子统计\" class=\"headerlink\" title=\"16. 不蒜子统计\"></a>16. 不蒜子统计</h3><p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后</span><br></pre></td></tr></table></figure></p>\n<p>(1) 全局配置。编辑 <code>主题配置文件</code> 中的 <code>busuanzi_count</code> 的配置项。当 <code>enable: true</code> 时，代表开启全局开关。若 <code>site_uv</code> 、<code>site_pv</code> 、 <code>page_pv</code> 的值均为 <code>false</code> 时，不蒜子仅作记录而不会在页面上显示。</p>\n<p>(2) 站点UV配置。当 <code>site_uv: true</code> 时，代表在页面底部显示站点的UV值。<code>site_uv_header</code> 和 <code>site_uv_footer</code> 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）<code>font-awesome</code>。显示效果为 <code>[site_uv_header]UV值[site_uv_footer]</code>。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># 效果：本站访客数12345人次</span><br><span class=\"line\">site_uv: true</span><br><span class=\"line\">site_uv_header: 本站访客数</span><br><span class=\"line\">site_uv_footer: 人次</span><br></pre></td></tr></table></figure></p>\n<p>(3) 站点PV配置。当 <code>site_pv: true</code> 时，代表在页面底部显示站点的PV值。<code>site_pv_header</code> 和 <code>site_pv_footer</code> 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）<code>font-awesome</code>。显示效果为 <code>[site_pv_header]PV值[site_pv_footer]</code>。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># 效果：本站总访问量12345次</span><br><span class=\"line\">site_pv: true</span><br><span class=\"line\">site_pv_header: 本站总访问量</span><br><span class=\"line\">site_pv_footer: 次</span><br></pre></td></tr></table></figure></p>\n<p>(4) Example:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">busuanzi_count:</span><br><span class=\"line\">  # count values only if the other configs are false</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  # custom uv span for the whole site</span><br><span class=\"line\">  site_uv: true</span><br><span class=\"line\">  site_uv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 本站访客数</span><br><span class=\"line\">  site_uv_footer: 次</span><br><span class=\"line\">  # custom pv span for the whole site</span><br><span class=\"line\">  site_pv: true</span><br><span class=\"line\">  site_pv_header: 本站总访问量</span><br><span class=\"line\">  site_pv_footer: 次</span><br><span class=\"line\">  # custom pv span for one page only</span><br><span class=\"line\">  page_pv: false</span><br><span class=\"line\">  page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt;</span><br><span class=\"line\">  page_pv_footer:</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"17-开启about自我介绍页面\"><a href=\"#17-开启about自我介绍页面\" class=\"headerlink\" title=\"17. 开启about自我介绍页面\"></a>17. 开启about自我介绍页面</h3><p>(1) 在终端窗口下，定位到 <code>Hexo</code> 站点目录下。使用 <code>hexo new page</code> 新建一个页面，命名为 <code>about</code> ：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">cd your-hexo-site</span><br><span class=\"line\">hexo new page abhout</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑刚新建的页面，将页面的类型设置为 <code>about</code>。页面内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: about</span><br><span class=\"line\">date: 2014-12-22 12:39:04</span><br><span class=\"line\">type: &quot;about&quot;</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<p>(3) 在菜单中添加链接。编辑 <code>主题配置文件</code> ， 添加 <code>about</code> 到 <code>menu</code> 中，如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: / || home</span><br><span class=\"line\">  about: /about/ || user</span><br></pre></td></tr></table></figure></p>\n<p>参考: <a href=\"http://theme-next.iissnan.com/theme-settings.html\" target=\"_blank\" rel=\"noopener\">http://theme-next.iissnan.com/theme-settings.html</a></p>\n<p><a href=\"http://theme-next.iissnan.com/third-party-services.html\" target=\"_blank\" rel=\"noopener\">http://theme-next.iissnan.com/third-party-services.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-基本配置\"><a href=\"#1-基本配置\" class=\"headerlink\" title=\"1. 基本配置\"></a>1. 基本配置</h3><h4 id=\"1-1-语言设置\"><a href=\"#1-1-语言设置\" class=\"headerlink\" title=\"1.1 语言设置\"></a>1.1 语言设置</h4><p>每个主题都会配置几种界面显示语言，修改语言只要编辑站点配置文件，找到 <code>language</code> 字段，并将其值更改为你所需要的语言(例如，简体中文)：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">language: zh-Hans</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-网站标题，作者\"><a href=\"#1-2-网站标题，作者\" class=\"headerlink\" title=\"1.2 网站标题，作者\"></a>1.2 网站标题，作者</h4><p>打开站点配置文件，修改这些值：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">title: SmartSi #博客标题</span><br><span class=\"line\">subtitle: #博客副标题</span><br><span class=\"line\">description: #博客描述</span><br><span class=\"line\">author: sjf0115 #博客作者</span><br></pre></td></tr></table></figure></p>\n<p>注意：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">配置文件要符合英文标点符号使用规范: 冒号后必须空格，否则会编译错误</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-3-域名与文章链接\"><a href=\"#1-3-域名与文章链接\" class=\"headerlink\" title=\"1.3 域名与文章链接\"></a>1.3 域名与文章链接</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># URL</span><br><span class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</span><br><span class=\"line\">url: http://sjf0115.github.io #你的博客网址</span><br><span class=\"line\">root: / #博客跟目录，如果你的博客在网址的二级目录下，在这里填上</span><br><span class=\"line\">permalink: :year/:month/:day/:title/ # 文章链接</span><br><span class=\"line\">permalink_defaults:</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-安装与启用主题\"><a href=\"#2-安装与启用主题\" class=\"headerlink\" title=\"2. 安装与启用主题\"></a>2. 安装与启用主题</h3><p>最简单的安装方法是克隆整个仓库，在这里我们使用的是<code>NexT</code>主题：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">cd hexo</span><br><span class=\"line\">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p>\n<p>或者你可以看到其他详细的<a href=\"https://github.com/theme-next/hexo-theme-next/blob/master/docs/INSTALLATION.md\" target=\"_blank\" rel=\"noopener\">安装说明</a></p>\n<p>安装后，我们要启用我们安装的主题，与所有<code>Hexo</code>主题启用的模式一样。 当克隆/下载完成后，打开站点配置文件， 找到 <code>theme</code> 字段，并将其值更改为 <code>next</code> 。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">theme: next</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-主题风格\"><a href=\"#3-主题风格\" class=\"headerlink\" title=\"3. 主题风格\"></a>3. 主题风格</h3><p><code>NexT</code> 主题目前提供了3中风格类似，但是又有点不同的主题风格，可以通过修改 <code>主题配置文件</code> 中的 <code>Scheme</code> 值来启用其中一种风格，例如我的博客用的是 <code>Mist</code> 风格，只要把另外两个用#注释掉即可:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Schemes</span><br><span class=\"line\">#scheme: Muse</span><br><span class=\"line\">scheme: Mist</span><br><span class=\"line\">#scheme: Pisces</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-设置-RSS\"><a href=\"#4-设置-RSS\" class=\"headerlink\" title=\"4. 设置 RSS\"></a>4. 设置 RSS</h3><p><code>NexT</code> 中 <code>RSS</code> 有三个设置选项，满足特定的使用场景。 更改 <code>主题配置文件</code>，设定 <code>rss</code> 字段的值：</p>\n<ul>\n<li>false：禁用 <code>RSS</code>，不在页面上显示 <code>RSS</code> 连接。</li>\n<li>留空：使用 Hexo 生成的 <code>Feed</code> 链接。 你可以需要先安装 <code>hexo-generator-feed</code> 插件。</li>\n<li>具体的链接地址：适用于已经烧制过 Feed 的情形。</li>\n</ul>\n<h3 id=\"5-导航栏添加标签菜单\"><a href=\"#5-导航栏添加标签菜单\" class=\"headerlink\" title=\"5. 导航栏添加标签菜单\"></a>5. 导航栏添加标签菜单</h3><p>新建标签页面，并在菜单中显示标签链接。标签页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。</p>\n<p>(1) 在终端窗口下，定位到 <code>Hexo</code> 站点目录下。使用如下命令新建一名为 <code>tags</code> 页面:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hexo new page &quot;tags&quot;</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑刚新建的页面，将页面的类型设置为 <code>tags</code> ，主题将自动为这个页面显示标签云。页面内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">title: 标签</span><br><span class=\"line\">date: 2017-12-22 12:39:04</span><br><span class=\"line\">type: &quot;tags&quot;</span><br></pre></td></tr></table></figure></p>\n<p>(3) 在菜单中添加链接。编辑 <code>主题配置文件</code> ，添加 <code>tags</code> 到 <code>menu</code> 中，如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: /</span><br><span class=\"line\">  archives: /archives</span><br><span class=\"line\">  tags: /tags</span><br></pre></td></tr></table></figure></p>\n<p>(4) 使用时在你的文章中添加如下代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: title name</span><br><span class=\"line\">date: 2017-12-12-22 12:39:04</span><br><span class=\"line\">tags:</span><br><span class=\"line\">  - first tag</span><br><span class=\"line\">  - second tag</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-添加分类页面\"><a href=\"#6-添加分类页面\" class=\"headerlink\" title=\"6. 添加分类页面\"></a>6. 添加分类页面</h3><p>新建分类页面，并在菜单中显示分类链接。分类页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。</p>\n<p>(1) 在终端窗口下，定位到 <code>Hexo</code> 站点目录下。使用 <code>hexo new page</code> 新建一个页面，命名为 <code>categories</code> ：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hexo new page categories</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑刚新建的页面，将页面的 <code>type</code> 设置为 <code>categories</code> ，主题将自动为这个页面显示分类。页面内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: 分类</span><br><span class=\"line\">date: 2014-12-22 12:39:04</span><br><span class=\"line\">type: &quot;categories&quot;</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<p>(3) 在菜单中添加链接。编辑 <code>主题配置文件</code> ， 添加 <code>categories</code> 到 <code>menu</code> 中，如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: /</span><br><span class=\"line\">  archives: /archives</span><br><span class=\"line\">  categories: /categories</span><br></pre></td></tr></table></figure></p>\n<p>(4) 使用时在你的文章中添加如下代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: title name</span><br><span class=\"line\">date: 2017-12-12-22 12:39:04</span><br><span class=\"line\">type: &quot;categories&quot;</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-侧边栏社交链接\"><a href=\"#7-侧边栏社交链接\" class=\"headerlink\" title=\"7. 侧边栏社交链接\"></a>7. 侧边栏社交链接</h3><p>侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 <code>主题配置文件</code> 中。</p>\n<p>(1) 链接放置在 <code>social</code> 字段下，一行一个链接。其键值格式是 <code>显示文本: 链接地址 || 图标</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">social:</span><br><span class=\"line\">  GitHub: https://github.com/sjf0115 || github</span><br><span class=\"line\">  E-Mail: mailto:1203745031@qq.com || envelope</span><br><span class=\"line\">  CSDN: http://blog.csdn.net/sunnyyoona</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">如果没有指定图标（带或不带分隔符），则会加载默认图标。</span><br></pre></td></tr></table></figure></p>\n<p>Example:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-1.png?raw=true\" alt=\"\"></p>\n<p>(2) 设定链接的图标，对应的字段是 <code>social_icons</code>。其键值格式是: <code>匹配键: Font Awesome 图标名称</code>， 匹配键与上一步所配置的链接的显示文本相同（大小写严格匹配），图标名称是 <code>Font Awesome</code> 图标的名字（不必带 fa- 前缀）。 <code>enable</code> 选项用于控制是否显示图标，你可以设置成 <code>false</code> 来去掉图标:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Social Icons</span><br><span class=\"line\">social_icons:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  # Icon Mappings</span><br><span class=\"line\">  GitHub: github</span><br><span class=\"line\">  Twitter: twitter</span><br><span class=\"line\">  微博: weibo</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-友情链接\"><a href=\"#8-友情链接\" class=\"headerlink\" title=\"8. 友情链接\"></a>8. 友情链接</h3><p>编辑 <code>主题配置文件</code> 添加：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">友情链接配置示例</span><br><span class=\"line\"># Blog rolls</span><br><span class=\"line\">links_icon: link</span><br><span class=\"line\">links_title: Links</span><br><span class=\"line\">links_layout: block</span><br><span class=\"line\">#links_layout: inline</span><br><span class=\"line\">links:</span><br><span class=\"line\">  CSDN: http://blog.csdn.net/sunnyyoona</span><br></pre></td></tr></table></figure></p>\n<p>Example:</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Hexo/Hexo+Github%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%B8%BB%E9%A2%98-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"9-站点建立时间\"><a href=\"#9-站点建立时间\" class=\"headerlink\" title=\"9. 站点建立时间\"></a>9. 站点建立时间</h3><p>这个时间将在站点的底部显示，例如 <code>© 2017 - 2018</code>。 编辑 <code>主题配置文件</code>，新增字段 <code>since</code>:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">配置示例</span><br><span class=\"line\">since: 2017</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"10-腾讯公益404页面\"><a href=\"#10-腾讯公益404页面\" class=\"headerlink\" title=\"10. 腾讯公益404页面\"></a>10. 腾讯公益404页面</h3><p>腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 <a href=\"http://smartsi.club/404.html\" target=\"_blank\" rel=\"noopener\">http://smartsi.club/404.html</a></p>\n<p>(1) 使用方法，新建 <code>404.html</code> 页面，放到主题的 <code>source</code> 目录下，内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE HTML&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt;</span><br><span class=\"line\">  &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt;</span><br><span class=\"line\">  &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt;</span><br><span class=\"line\">  &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt;</span><br><span class=\"line\">  &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot;</span><br><span class=\"line\">          charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot;</span><br><span class=\"line\">          homePageName=&quot;回到我的主页&quot;&gt;</span><br><span class=\"line\">  &lt;/script&gt;</span><br><span class=\"line\">  &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">  &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>(2) 开启404页面功能</p>\n<p>在 <code>menu</code> 下添加 <code>commonweal: /404/ || heartbeat</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: / || home</span><br><span class=\"line\">  about: /about/ || user</span><br><span class=\"line\">  tags: /tags/ || tags</span><br><span class=\"line\">  categories: /categories/ || th</span><br><span class=\"line\">  archives: /archives/ || archive</span><br><span class=\"line\">  #schedule: /schedule/ || calendar</span><br><span class=\"line\">  #sitemap: /sitemap.xml || sitemap</span><br><span class=\"line\">  commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"11-开启打赏功能\"><a href=\"#11-开启打赏功能\" class=\"headerlink\" title=\"11. 开启打赏功能\"></a>11. 开启打赏功能</h3><p>越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 <code>主题配置文件</code> 中填入 <code>微信</code> 和 <code>支付宝</code> 收款二维码图片地址 即可开启该功能：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">打赏功能配置示例</span><br><span class=\"line\">reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！</span><br><span class=\"line\">wechatpay: /path/to/wechat-reward-image</span><br><span class=\"line\">alipay: /path/to/alipay-reward-image</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"12-订阅微信公众号\"><a href=\"#12-订阅微信公众号\" class=\"headerlink\" title=\"12. 订阅微信公众号\"></a>12. 订阅微信公众号</h3><p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后</span><br></pre></td></tr></table></figure></p>\n<p>在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。</p>\n<p>在微信公众号平台下载您的二维码，并将它存放于博客<code>source/uploads/</code>目录下。</p>\n<p>然后编辑 主题配置文件，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">配置示例</span><br><span class=\"line\"># Wechat Subscriber</span><br><span class=\"line\">wechat_subscriber:</span><br><span class=\"line\">  enabled: true</span><br><span class=\"line\">  qcode: /uploads/wechat-qcode.jpg</span><br><span class=\"line\">  description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"13-设置背景动画\"><a href=\"#13-设置背景动画\" class=\"headerlink\" title=\"13. 设置背景动画\"></a>13. 设置背景动画</h3><p><code>NexT</code> 自带两种背景动画效果，编辑 <code>主题配置文件</code>， 搜索 <code>canvas_nest</code> 或 <code>three_waves</code>，根据你的需求设置值为 <code>true</code> 或者 <code>false</code> 即可：</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">three_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。</span><br></pre></td></tr></table></figure></p>\n<p>canvas_nest 配置示例<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># canvas_nest</span><br><span class=\"line\">canvas_nest: true //开启动画</span><br><span class=\"line\">canvas_nest: false //关闭动画</span><br></pre></td></tr></table></figure></p>\n<p>three_waves 配置示例<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># three_waves</span><br><span class=\"line\">three_waves: true //开启动画</span><br><span class=\"line\">three_waves: false //关闭动画</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"14-设置阅读全文\"><a href=\"#14-设置阅读全文\" class=\"headerlink\" title=\"14. 设置阅读全文\"></a>14. 设置阅读全文</h3><p>在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 <code>NexT</code> 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 <code>阅读全文</code> 按钮，可以通过以下方法：<br>(1) 在文章中使用 <code>&lt;!-- more --&gt;</code> 手动进行截断，<code>Hexo</code> 提供的方式 推荐<br>(2) 在文章的 <a href=\"https://hexo.io/docs/front-matter.html\" target=\"_blank\" rel=\"noopener\">front-matter</a> 中添加 <code>description</code>，并提供文章摘录<br>(3) 自动形成摘要，在 <code>主题配置文件</code> 中添加：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">auto_excerpt:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  length: 150</span><br></pre></td></tr></table></figure></p>\n<p>默认截取的长度为 150 字符，可以根据需要自行设定</p>\n<p>建议使用 <code>&lt;!-- more --&gt;</code>（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 <code>Hexo</code> 中的插件更好的识别。</p>\n<h3 id=\"15-站内搜索\"><a href=\"#15-站内搜索\" class=\"headerlink\" title=\"15. 站内搜索\"></a>15. 站内搜索</h3><p><code>NexT</code> 支持集成 <code>Swiftype</code>、 微搜索、<code>Local Search</code> 和 <code>Algolia</code>。在这里我使用的是<code>Local Search</code>，下面将介绍如何使用:</p>\n<p>(1) 添加百度/谷歌/本地 自定义站点内容搜索，安装 <code>hexo-generator-searchdb</code>，在站点的根目录下执行以下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑 <code>站点配置文件</code>，新增以下内容到任意位置：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">search:</span><br><span class=\"line\">  path: search.xml</span><br><span class=\"line\">  field: post</span><br><span class=\"line\">  format: html</span><br><span class=\"line\">  limit: 10000</span><br></pre></td></tr></table></figure></p>\n<p>(3) 编辑 <code>主题配置文件</code>，启用本地搜索功能：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Local search</span><br><span class=\"line\">local_search:</span><br><span class=\"line\">  enable: true</span><br></pre></td></tr></table></figure></p>\n<p>其他搜索方式请查看<a href=\"http://theme-next.iissnan.com/third-party-services.html#search-system\" target=\"_blank\" rel=\"noopener\">搜索服务</a></p>\n<h3 id=\"16-不蒜子统计\"><a href=\"#16-不蒜子统计\" class=\"headerlink\" title=\"16. 不蒜子统计\"></a>16. 不蒜子统计</h3><p>备注：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后</span><br></pre></td></tr></table></figure></p>\n<p>(1) 全局配置。编辑 <code>主题配置文件</code> 中的 <code>busuanzi_count</code> 的配置项。当 <code>enable: true</code> 时，代表开启全局开关。若 <code>site_uv</code> 、<code>site_pv</code> 、 <code>page_pv</code> 的值均为 <code>false</code> 时，不蒜子仅作记录而不会在页面上显示。</p>\n<p>(2) 站点UV配置。当 <code>site_uv: true</code> 时，代表在页面底部显示站点的UV值。<code>site_uv_header</code> 和 <code>site_uv_footer</code> 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）<code>font-awesome</code>。显示效果为 <code>[site_uv_header]UV值[site_uv_footer]</code>。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># 效果：本站访客数12345人次</span><br><span class=\"line\">site_uv: true</span><br><span class=\"line\">site_uv_header: 本站访客数</span><br><span class=\"line\">site_uv_footer: 人次</span><br></pre></td></tr></table></figure></p>\n<p>(3) 站点PV配置。当 <code>site_pv: true</code> 时，代表在页面底部显示站点的PV值。<code>site_pv_header</code> 和 <code>site_pv_footer</code> 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）<code>font-awesome</code>。显示效果为 <code>[site_pv_header]PV值[site_pv_footer]</code>。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># 效果：本站总访问量12345次</span><br><span class=\"line\">site_pv: true</span><br><span class=\"line\">site_pv_header: 本站总访问量</span><br><span class=\"line\">site_pv_footer: 次</span><br></pre></td></tr></table></figure></p>\n<p>(4) Example:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">busuanzi_count:</span><br><span class=\"line\">  # count values only if the other configs are false</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  # custom uv span for the whole site</span><br><span class=\"line\">  site_uv: true</span><br><span class=\"line\">  site_uv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 本站访客数</span><br><span class=\"line\">  site_uv_footer: 次</span><br><span class=\"line\">  # custom pv span for the whole site</span><br><span class=\"line\">  site_pv: true</span><br><span class=\"line\">  site_pv_header: 本站总访问量</span><br><span class=\"line\">  site_pv_footer: 次</span><br><span class=\"line\">  # custom pv span for one page only</span><br><span class=\"line\">  page_pv: false</span><br><span class=\"line\">  page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt;</span><br><span class=\"line\">  page_pv_footer:</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"17-开启about自我介绍页面\"><a href=\"#17-开启about自我介绍页面\" class=\"headerlink\" title=\"17. 开启about自我介绍页面\"></a>17. 开启about自我介绍页面</h3><p>(1) 在终端窗口下，定位到 <code>Hexo</code> 站点目录下。使用 <code>hexo new page</code> 新建一个页面，命名为 <code>about</code> ：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">cd your-hexo-site</span><br><span class=\"line\">hexo new page abhout</span><br></pre></td></tr></table></figure></p>\n<p>(2) 编辑刚新建的页面，将页面的类型设置为 <code>about</code>。页面内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: about</span><br><span class=\"line\">date: 2014-12-22 12:39:04</span><br><span class=\"line\">type: &quot;about&quot;</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure></p>\n<p>(3) 在菜单中添加链接。编辑 <code>主题配置文件</code> ， 添加 <code>about</code> 到 <code>menu</code> 中，如下:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">menu:</span><br><span class=\"line\">  home: / || home</span><br><span class=\"line\">  about: /about/ || user</span><br></pre></td></tr></table></figure></p>\n<p>参考: <a href=\"http://theme-next.iissnan.com/theme-settings.html\" target=\"_blank\" rel=\"noopener\">http://theme-next.iissnan.com/theme-settings.html</a></p>\n<p><a href=\"http://theme-next.iissnan.com/third-party-services.html\" target=\"_blank\" rel=\"noopener\">http://theme-next.iissnan.com/third-party-services.html</a></p>\n"},{"layout":"post","author":"周志明","title":"JVM 垃圾收集算法","date":"2018-02-04T09:40:01.000Z","_content":"\n本文“垃圾收集算法”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】\n\n由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本节不打算过多地讨论算法的实现，只是介绍几种算法的思想及其发展过程。\n\n### 1. 标记-清除算法\n\n最基础的收集算法是 `标记-清除` （`Mark-Sweep`）算法，如它的名字一样，算法分为 `标记` 和 `清除` 两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。\n\n它的主要缺点有两个：\n- 一个是效率问题，标记和清除过程的效率都不高；\n- 另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。\n\n标记-清除算法的执行过程如下图所示：\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-1.jpg?raw=true)\n\n### 2. 复制算法\n\n为了解决效率问题，一种称为 `复制` （`Copying`）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。复制算法的执行过程如下图所示：\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-2.jpg?raw=true)\n\n现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 的专门研究表明，新生代中的对象 `98%` 是朝生夕死的，所以并不需要按照 `1∶1` 的比例来划分内存空间，而是将内存分为一块较大的 `Eden` 空间和两块较小的 `Survivor` 空间，每次使用 `Eden` 和其中的一块 `Survivor`。当回收时，将 `Eden` 和 `Survivor` 中还存活着的对象一次性地拷贝到另外一块 `Survivor` 空间上，最后清理掉 `Eden` 和刚才用过的 `Survivor` 的空间。`HotSpot` 虚拟机默认 `Eden` 和 `Survivor` 的大小比例是 `8∶1`，也就是每次新生代中可用内存空间为整个新生代容量的 `90%`（`80%+10%`），只有 `10%` 的内存是会被 `浪费` 的。当然，`98%` 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 `10%` 的对象存活，当 `Survivor` 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。\n\n内存的分配担保就好比我们去银行借款，如果我们信誉很好，在 `98%` 的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块 `Survivor` 空间没有足够的空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。关于对新生代进行分配担保的内容，本章稍后在讲解垃圾收集器执行规则时还会再详细讲解。\n\n### 3. 标记-整理算法\n\n复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 `50%` 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 `100%` 存活的极端情况，所以在老年代一般不能直接选用这种算法。\n\n根据老年代的特点，有人提出了另外一种 `标记-整理` （`Mark-Compact`）算法，标记过程仍然与 `标记-清除` 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，`标记-整理` 算法的示意图如下图所示:\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-3.jpg?raw=true)\n\n### 4. 分代收集算法\n\n当前商业虚拟机的垃圾收集都采用 `分代收集` （`Generational Collection`）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把 `Java` 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 `标记-清理` 或 `标记-整理` 算法来进行回收。\n","source":"_posts/Java/[JVM]JVM 垃圾收集器.md","raw":"---\nlayout: post\nauthor: 周志明\ntitle: JVM 垃圾收集算法\ndate: 2018-02-04 17:40:01\ntags:\n  - Java\n  - JVM\n\ncategories: Java\npermalink: jvm-garbage-collection-algorithm\n---\n\n本文“垃圾收集算法”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】\n\n由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本节不打算过多地讨论算法的实现，只是介绍几种算法的思想及其发展过程。\n\n### 1. 标记-清除算法\n\n最基础的收集算法是 `标记-清除` （`Mark-Sweep`）算法，如它的名字一样，算法分为 `标记` 和 `清除` 两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。\n\n它的主要缺点有两个：\n- 一个是效率问题，标记和清除过程的效率都不高；\n- 另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。\n\n标记-清除算法的执行过程如下图所示：\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-1.jpg?raw=true)\n\n### 2. 复制算法\n\n为了解决效率问题，一种称为 `复制` （`Copying`）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。复制算法的执行过程如下图所示：\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-2.jpg?raw=true)\n\n现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 的专门研究表明，新生代中的对象 `98%` 是朝生夕死的，所以并不需要按照 `1∶1` 的比例来划分内存空间，而是将内存分为一块较大的 `Eden` 空间和两块较小的 `Survivor` 空间，每次使用 `Eden` 和其中的一块 `Survivor`。当回收时，将 `Eden` 和 `Survivor` 中还存活着的对象一次性地拷贝到另外一块 `Survivor` 空间上，最后清理掉 `Eden` 和刚才用过的 `Survivor` 的空间。`HotSpot` 虚拟机默认 `Eden` 和 `Survivor` 的大小比例是 `8∶1`，也就是每次新生代中可用内存空间为整个新生代容量的 `90%`（`80%+10%`），只有 `10%` 的内存是会被 `浪费` 的。当然，`98%` 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 `10%` 的对象存活，当 `Survivor` 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。\n\n内存的分配担保就好比我们去银行借款，如果我们信誉很好，在 `98%` 的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块 `Survivor` 空间没有足够的空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。关于对新生代进行分配担保的内容，本章稍后在讲解垃圾收集器执行规则时还会再详细讲解。\n\n### 3. 标记-整理算法\n\n复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 `50%` 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 `100%` 存活的极端情况，所以在老年代一般不能直接选用这种算法。\n\n根据老年代的特点，有人提出了另外一种 `标记-整理` （`Mark-Compact`）算法，标记过程仍然与 `标记-清除` 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，`标记-整理` 算法的示意图如下图所示:\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-3.jpg?raw=true)\n\n### 4. 分代收集算法\n\n当前商业虚拟机的垃圾收集都采用 `分代收集` （`Generational Collection`）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把 `Java` 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 `标记-清理` 或 `标记-整理` 算法来进行回收。\n","slug":"jvm-garbage-collection-algorithm","published":1,"updated":"2018-02-11T02:38:51.187Z","comments":1,"photos":[],"link":"","_id":"cje58tits0046ordbvo3w0lz5","content":"<p>本文“垃圾收集算法”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】</p>\n<p>由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本节不打算过多地讨论算法的实现，只是介绍几种算法的思想及其发展过程。</p>\n<h3 id=\"1-标记-清除算法\"><a href=\"#1-标记-清除算法\" class=\"headerlink\" title=\"1. 标记-清除算法\"></a>1. 标记-清除算法</h3><p>最基础的收集算法是 <code>标记-清除</code> （<code>Mark-Sweep</code>）算法，如它的名字一样，算法分为 <code>标记</code> 和 <code>清除</code> 两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。</p>\n<p>它的主要缺点有两个：</p>\n<ul>\n<li>一个是效率问题，标记和清除过程的效率都不高；</li>\n<li>另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</li>\n</ul>\n<p>标记-清除算法的执行过程如下图所示：<br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-1.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"2-复制算法\"><a href=\"#2-复制算法\" class=\"headerlink\" title=\"2. 复制算法\"></a>2. 复制算法</h3><p>为了解决效率问题，一种称为 <code>复制</code> （<code>Copying</code>）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。复制算法的执行过程如下图所示：<br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-2.jpg?raw=true\" alt=\"\"></p>\n<p>现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 的专门研究表明，新生代中的对象 <code>98%</code> 是朝生夕死的，所以并不需要按照 <code>1∶1</code> 的比例来划分内存空间，而是将内存分为一块较大的 <code>Eden</code> 空间和两块较小的 <code>Survivor</code> 空间，每次使用 <code>Eden</code> 和其中的一块 <code>Survivor</code>。当回收时，将 <code>Eden</code> 和 <code>Survivor</code> 中还存活着的对象一次性地拷贝到另外一块 <code>Survivor</code> 空间上，最后清理掉 <code>Eden</code> 和刚才用过的 <code>Survivor</code> 的空间。<code>HotSpot</code> 虚拟机默认 <code>Eden</code> 和 <code>Survivor</code> 的大小比例是 <code>8∶1</code>，也就是每次新生代中可用内存空间为整个新生代容量的 <code>90%</code>（<code>80%+10%</code>），只有 <code>10%</code> 的内存是会被 <code>浪费</code> 的。当然，<code>98%</code> 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 <code>10%</code> 的对象存活，当 <code>Survivor</code> 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。</p>\n<p>内存的分配担保就好比我们去银行借款，如果我们信誉很好，在 <code>98%</code> 的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块 <code>Survivor</code> 空间没有足够的空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。关于对新生代进行分配担保的内容，本章稍后在讲解垃圾收集器执行规则时还会再详细讲解。</p>\n<h3 id=\"3-标记-整理算法\"><a href=\"#3-标记-整理算法\" class=\"headerlink\" title=\"3. 标记-整理算法\"></a>3. 标记-整理算法</h3><p>复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 <code>50%</code> 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 <code>100%</code> 存活的极端情况，所以在老年代一般不能直接选用这种算法。</p>\n<p>根据老年代的特点，有人提出了另外一种 <code>标记-整理</code> （<code>Mark-Compact</code>）算法，标记过程仍然与 <code>标记-清除</code> 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，<code>标记-整理</code> 算法的示意图如下图所示:<br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-3.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"4-分代收集算法\"><a href=\"#4-分代收集算法\" class=\"headerlink\" title=\"4. 分代收集算法\"></a>4. 分代收集算法</h3><p>当前商业虚拟机的垃圾收集都采用 <code>分代收集</code> （<code>Generational Collection</code>）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把 <code>Java</code> 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 <code>标记-清理</code> 或 <code>标记-整理</code> 算法来进行回收。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文“垃圾收集算法”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】</p>\n<p>由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法又各不相同，因此本节不打算过多地讨论算法的实现，只是介绍几种算法的思想及其发展过程。</p>\n<h3 id=\"1-标记-清除算法\"><a href=\"#1-标记-清除算法\" class=\"headerlink\" title=\"1. 标记-清除算法\"></a>1. 标记-清除算法</h3><p>最基础的收集算法是 <code>标记-清除</code> （<code>Mark-Sweep</code>）算法，如它的名字一样，算法分为 <code>标记</code> 和 <code>清除</code> 两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。</p>\n<p>它的主要缺点有两个：</p>\n<ul>\n<li>一个是效率问题，标记和清除过程的效率都不高；</li>\n<li>另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</li>\n</ul>\n<p>标记-清除算法的执行过程如下图所示：<br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-1.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"2-复制算法\"><a href=\"#2-复制算法\" class=\"headerlink\" title=\"2. 复制算法\"></a>2. 复制算法</h3><p>为了解决效率问题，一种称为 <code>复制</code> （<code>Copying</code>）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。复制算法的执行过程如下图所示：<br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-2.jpg?raw=true\" alt=\"\"></p>\n<p>现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 的专门研究表明，新生代中的对象 <code>98%</code> 是朝生夕死的，所以并不需要按照 <code>1∶1</code> 的比例来划分内存空间，而是将内存分为一块较大的 <code>Eden</code> 空间和两块较小的 <code>Survivor</code> 空间，每次使用 <code>Eden</code> 和其中的一块 <code>Survivor</code>。当回收时，将 <code>Eden</code> 和 <code>Survivor</code> 中还存活着的对象一次性地拷贝到另外一块 <code>Survivor</code> 空间上，最后清理掉 <code>Eden</code> 和刚才用过的 <code>Survivor</code> 的空间。<code>HotSpot</code> 虚拟机默认 <code>Eden</code> 和 <code>Survivor</code> 的大小比例是 <code>8∶1</code>，也就是每次新生代中可用内存空间为整个新生代容量的 <code>90%</code>（<code>80%+10%</code>），只有 <code>10%</code> 的内存是会被 <code>浪费</code> 的。当然，<code>98%</code> 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 <code>10%</code> 的对象存活，当 <code>Survivor</code> 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。</p>\n<p>内存的分配担保就好比我们去银行借款，如果我们信誉很好，在 <code>98%</code> 的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块 <code>Survivor</code> 空间没有足够的空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。关于对新生代进行分配担保的内容，本章稍后在讲解垃圾收集器执行规则时还会再详细讲解。</p>\n<h3 id=\"3-标记-整理算法\"><a href=\"#3-标记-整理算法\" class=\"headerlink\" title=\"3. 标记-整理算法\"></a>3. 标记-整理算法</h3><p>复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 <code>50%</code> 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 <code>100%</code> 存活的极端情况，所以在老年代一般不能直接选用这种算法。</p>\n<p>根据老年代的特点，有人提出了另外一种 <code>标记-整理</code> （<code>Mark-Compact</code>）算法，标记过程仍然与 <code>标记-清除</code> 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，<code>标记-整理</code> 算法的示意图如下图所示:<br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-garbage-collection-algorithm-3.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"4-分代收集算法\"><a href=\"#4-分代收集算法\" class=\"headerlink\" title=\"4. 分代收集算法\"></a>4. 分代收集算法</h3><p>当前商业虚拟机的垃圾收集都采用 <code>分代收集</code> （<code>Generational Collection</code>）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把 <code>Java</code> 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 <code>标记-清理</code> 或 <code>标记-整理</code> 算法来进行回收。</p>\n"},{"layout":"post","author":"周志明","title":"JVM 垃圾收集器","date":"2018-02-10T09:40:01.000Z","_content":"\n本文“垃圾收集器”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】\n\n如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。`Java` 虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。下面讨论的是基于 `JDK 1.7 Update 14` 之后的 `HotSpot` 虚拟机。这个虚拟机包含的所有收集器如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-common-garbage-collector-1.png?raw=true)\n\n图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。下面会介绍这些收集器的特性，基本原理和使用场景。\n\n### 1. Serial 收集器\n\n`Serial` 收集器是最基本，发展历史最悠久的收集器，曾经（JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，只会使用一个CPU 或一条收集线程去完成垃圾收集工作。重要的是，它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。在用户不可见的情况下把用户正常工作的线程全部停掉，这对应用程序来说都是很难接受的。但是虚拟机的设计者表示完全理解，却又表示很无奈。举个例子来说：你妈妈给你打扫房间的时候，肯定也会让你老老实实的在椅子或房间外待着，如果她一边打扫卫生，你一边乱扔垃圾，这房间还嫩打扫完吗？\n\n`Serial` 收集器看起来是一个鸡肋收集器，但是到现在为止，它依然是虚拟机运行在 `Client` 模式下的默认新生代收集器。这是因为它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说， 该收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集的效率。\n\n`Serial` 收集器在新生代收集时采用复制算法。\n\n### 2. ParNew 收集器\n\n`ParNew` 收集器其实是 `Serial` 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 `Serial`收集器可用的所有控制参数，收集算法，`Stop The World`，对象分配规则，回收策略等都与 `Serial` 收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。\n\n`ParNew` 收集器除了多线程收集之外，其他与 `Serial`收集器相比并没有太多的创新之处，但它却是许多运行在 `Server` 模型下的虚拟机首选的新生代收集器。其中一个与性能无关但是很重要的原因是，除了 `Serial`收集器之外，目前只有它能与 `CMS` 收集器配合工作。在 `JDK 1.5` `HotSpot`推出了一款强交互应用中划时代的垃圾收集器 - `CMS` 收集器 （`Concurrent Mark Sweep`）。这款收集器是 `HotSpot` 虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。不幸的是 `CMS` 收集器作为老年代的收集器，却无法与 `JDK 1.4.0`中已经存在的 `Parallel Scavenge`收集器配合工作，所以在 `JDK 1.5` 中使用 `CMS` 来收集老年代的时候，新生代智能选择 `ParNew` 或者 `Serial`收集器。\n\n`ParNew` 收集器在单 `CPU` 的环境中绝对不会有比 `Serial` 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个 `CPU` 的环境中都不能百分百的保证可以超越 `Serial` 收集器。  随着可以使用的 `CPU` 的数量的增加，它对于 `GC` 时系统资源的有效利用还是很有好处的。\n\n与 `Serial` 收集器一样，也是在新生代收集时采用复制算法。\n\n### 3. Parallel Scavenge 收集器\n\n`Parallel Scavenge` 收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去跟 `ParNew` 收集器一样。\n\n`Parallel Scavenge` 收集器的特点是它的关注点与其他的收集器不同， `CMS` 等收集器的关注点是尽可能的缩短垃圾收集时的用户线程的停顿时间， 而 `Parallel Scavenge` 收集器的目标则是达到一个可控制的吞吐量。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。由于与吞吐量关系密切，该收集器也经常成为 `吞吐量优先` 收集器。`Parallel Scavenge` 收集器可以通过设置 `-XX:+UseAdaptivSizePolicy` 参数开启 `GC` 自适应调节策略，不需要手工指定新生代的大小(`-Xmn`)，Eden与Survivor区的比例(`-XX:SurvivorRatio`)，晋升老年代对象年龄等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。自适应调节策略也是`Parallel Scavenge` 收集器与 `ParNew`收集器的一个重要区别。\n\n备注:\n```\n吞吐量 = 运行用户代码的时间 / （运行用户代码的时间 + 垃圾收集的时间）\n\nExample:\n虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，则吞吐量为99%\n\n```\n\n### 4. Serial Old 收集器\n\n`Serial Old` 收集器是 `Serial` 收集器的老年代版本，它同样是一个单线程收集器，使用`标记-整理`算法。这个收集器的主要意义也是在于 `Client` 模式下的虚拟机使用。如果在 `Server` 模式下，那么它主要有两大用途： 一种用途是在 `JDK1.5` 以及之前的版本中与 `Parallel Scavenge` 收集器搭配使用， 另一种用途是作为 `CMS` 收集器的后备预案，在并发收集发生 `Concurrent Mode Failure` 时使用。\n\n### 5. Parallel Old 收集器\n\n`Parallel Old` 收集器是 `Parallel Scavenge` 收集器的老年代版本，使用多线程和 `标记-整理`算法。这个收集器是在 `JDK 1.6` 中才开始提供的。在此之前新生代 `Parallel Scavenge` 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 `Parallel Scavenge` 收集器，老年代除了 `Serial Old`收集器外别无选择。由于老年代 `Serial Old` 收集器在服务端应用性能上 `拖累`，使用了 `Parallel Scavenge` 收集器也未必能在整体应用上获得吞吐量最大化的效果。由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有`ParNew` 加 `CMS` 的组合给力。\n\n直到 `Parallel Old` 收集器出现后。`吞吐量优先` 收集器终于有了名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑 `Parallel Scavenge` 加 `Parallel Old` 收集器。\n\n### 6. CMS 收集器\n\n`CMS` (`Concurrent Mark Sweep`) 收集器是一种以获得最短回收停顿时间为目标的收集器。`CMS` 收集器非常符合重视响应时间速度以及希望系统停顿时间最短的应用。从名字上就可以看出，`CMS` 收集器是基于 `标记-清除` 算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些。整个过程可以分为4个步骤：\n- 初始标记\n- 并发标记\n- 重新标记\n- 并发清除\n\n初始标记与重新标记这两个步骤仍然需要 `Stop The World`。 初始标记仅仅是标记一下 `GC Roots` 能直接关联到的对象，速度很快，并发标记是进行 `GC Roots Tracing` 的过程，而重新标记则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发比较的时间短。\n\n由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，`CMS` 收集器的内存回收过程是与用户线程一起并发执行的。\n\n`CMS` 是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。但是 `CMS` 收集器还远达不到完美的程度，它有以下三个明显的缺点：\n\n(1) `CMS` 收集器对CPU资源非常敏感\n\n其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。`CMS` 默认启动的回收线程数是 `（CPU数量+3）/ 4`，也就是当CPU在4个以上时，并发回收时垃圾收集线程多于 `25%`的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，`CMS` 对用户程序的影响就可能变得很大。\n\n(2) `CMS` 收集器无法处理浮动垃圾\n\n`CMS` 收集器无法处理浮动垃圾，可能出现 `Concurrent Mode Failure` 失败而导致另一次 `Full GC` 的产生。由于 `CMS` 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，`CMS` 无法在当次收集中处理掉它们，只好留待下一次 `GC` 时再清理掉。这一部分垃圾就称为 `浮动垃圾`。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此 `CMS` 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 `JDK 1.5` 默认设置下，`CMS` 收集器当老年代使用了 `68%` 的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高触发百发比。在 `JDK1.6` 中， `CMS`收集器的启动阈值已经提升至 `92%`。运行期间预留的内存无法满足程序需要，就会出现一次 `Concurrent Mode Failure` 失败，这时虚拟机将启动后备预案：临时启用 `Serial Old` 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说，如果启动阈值设置的太高很容易导致大量这样的失败，性能反而会降低。\n\n(3) `CMS` 收集器会产生大量空间碎片\n\n`CMS` 是一款基于 `标记—清除` 算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 `Full GC`。\n\n### 7. G1 收集器\n\n`G1`（`Garbage-First`）是一款面向服务端应用的垃圾收集器。`HotSpot` 开发团队赋予它的使命是未来可以替换掉 `JDK 1.5` 中发布的 `CMS` 收集器。与其他 `GC` 收集器相比，`G1` 具备如下特点：\n\n(1) 并行与并发。`G1` 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短 `Stop-The-World` 停顿的时间，部分其他收集器原本需要停顿 `Java` 线程执行的 `GC` 动作，`G1` 收集器仍然可以通过并发的方式让 `Java` 程序继续执行。\n\n(2) 分代收集。与其他收集器一样，分代概念在 `G1` 中依然得以保留。虽然 `G1` 可以不需要其他收集器配合就能独立管理整个 `GC` 堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 `GC` 的旧对象以获取更好的收集效果。\n\n(3) 空间整合。与 `CMS` 的 `标记—清理` 算法不同，`G1` 从整体来看是基于 `标记—整理` 算法实现的收集器，从局部（两个 `Region` 之间）上来看是基于 `复制` 算法实现的，但无论如何，这两种算法都意味着 `G1` 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 `GC`。\n\n(4) 可预测的停顿。这是 `G1` 相对于 `CMS` 的另一大优势，降低停顿时间是 `G1` 和 `CMS` 共同的关注点，但 `G1` 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 `M` 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 `N` 毫秒。\n\n在 `G1` 之前的其他收集器进行收集的范围都是整个新生代或者老年代，而 `G1` 不再是这样。使用 `G1` 收集器时，`Java` 堆的内存布局就与其他收集器有很大差别，它将整个 `Java` 堆划分为多个大小相等的独立区域（`Region`），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 `Region`（不需要连续）的集合。\n\n`G1` 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 `Java` 堆中进行全区域的垃圾收集。`G1` 跟踪各个 `Region` 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 `Region`（这也就是 `Garbage-First` 名称的来由）。这种使用 `Region` 划分内存空间以及有优先级的区域回收方式，保证了 `G1` 收集器在有限的时间内可以获取尽可能高的收集效率。\n\n如果不计算维护 `Remembered Set`的操作， `G1` 收集器的运作大致可划分为以下几个步骤：\n\n(1) 初始标记。初始标记阶段仅仅只是标记一下 `GC Roots` 能直接关联到的对象，并且修改 `TAMS`（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 `Region` 中创建新对象，这阶段需要停顿线程，但耗时很短。\n\n(2) 并发标记。并发标记阶段是从 `GC Root` 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。\n\n(3) 最终标记。最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 `Remembered Set Logs` 里面，最终标记阶段需要把 `Remembered Set Logs` 的数据合并到 `Remembered Set` 中，这阶段需要停顿线程，但是可并行执行。\n\n(4) 筛选回收。筛选回收阶段首先对各个 `Region` 的回收价值和成本进行排序，根据用户所期望的 `GC` 停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 `Region`，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。\n","source":"_posts/Java/[JVM]JVM 垃圾收集算法.md","raw":"---\nlayout: post\nauthor: 周志明\ntitle: JVM 垃圾收集器\ndate: 2018-02-10 17:40:01\ntags:\n  - Java\n  - JVM\n\ncategories: Java\npermalink: jvm-common-garbage-collector\n---\n\n本文“垃圾收集器”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】\n\n如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。`Java` 虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。下面讨论的是基于 `JDK 1.7 Update 14` 之后的 `HotSpot` 虚拟机。这个虚拟机包含的所有收集器如下图所示：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-common-garbage-collector-1.png?raw=true)\n\n图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。下面会介绍这些收集器的特性，基本原理和使用场景。\n\n### 1. Serial 收集器\n\n`Serial` 收集器是最基本，发展历史最悠久的收集器，曾经（JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，只会使用一个CPU 或一条收集线程去完成垃圾收集工作。重要的是，它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。在用户不可见的情况下把用户正常工作的线程全部停掉，这对应用程序来说都是很难接受的。但是虚拟机的设计者表示完全理解，却又表示很无奈。举个例子来说：你妈妈给你打扫房间的时候，肯定也会让你老老实实的在椅子或房间外待着，如果她一边打扫卫生，你一边乱扔垃圾，这房间还嫩打扫完吗？\n\n`Serial` 收集器看起来是一个鸡肋收集器，但是到现在为止，它依然是虚拟机运行在 `Client` 模式下的默认新生代收集器。这是因为它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说， 该收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集的效率。\n\n`Serial` 收集器在新生代收集时采用复制算法。\n\n### 2. ParNew 收集器\n\n`ParNew` 收集器其实是 `Serial` 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 `Serial`收集器可用的所有控制参数，收集算法，`Stop The World`，对象分配规则，回收策略等都与 `Serial` 收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。\n\n`ParNew` 收集器除了多线程收集之外，其他与 `Serial`收集器相比并没有太多的创新之处，但它却是许多运行在 `Server` 模型下的虚拟机首选的新生代收集器。其中一个与性能无关但是很重要的原因是，除了 `Serial`收集器之外，目前只有它能与 `CMS` 收集器配合工作。在 `JDK 1.5` `HotSpot`推出了一款强交互应用中划时代的垃圾收集器 - `CMS` 收集器 （`Concurrent Mark Sweep`）。这款收集器是 `HotSpot` 虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。不幸的是 `CMS` 收集器作为老年代的收集器，却无法与 `JDK 1.4.0`中已经存在的 `Parallel Scavenge`收集器配合工作，所以在 `JDK 1.5` 中使用 `CMS` 来收集老年代的时候，新生代智能选择 `ParNew` 或者 `Serial`收集器。\n\n`ParNew` 收集器在单 `CPU` 的环境中绝对不会有比 `Serial` 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个 `CPU` 的环境中都不能百分百的保证可以超越 `Serial` 收集器。  随着可以使用的 `CPU` 的数量的增加，它对于 `GC` 时系统资源的有效利用还是很有好处的。\n\n与 `Serial` 收集器一样，也是在新生代收集时采用复制算法。\n\n### 3. Parallel Scavenge 收集器\n\n`Parallel Scavenge` 收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去跟 `ParNew` 收集器一样。\n\n`Parallel Scavenge` 收集器的特点是它的关注点与其他的收集器不同， `CMS` 等收集器的关注点是尽可能的缩短垃圾收集时的用户线程的停顿时间， 而 `Parallel Scavenge` 收集器的目标则是达到一个可控制的吞吐量。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。由于与吞吐量关系密切，该收集器也经常成为 `吞吐量优先` 收集器。`Parallel Scavenge` 收集器可以通过设置 `-XX:+UseAdaptivSizePolicy` 参数开启 `GC` 自适应调节策略，不需要手工指定新生代的大小(`-Xmn`)，Eden与Survivor区的比例(`-XX:SurvivorRatio`)，晋升老年代对象年龄等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。自适应调节策略也是`Parallel Scavenge` 收集器与 `ParNew`收集器的一个重要区别。\n\n备注:\n```\n吞吐量 = 运行用户代码的时间 / （运行用户代码的时间 + 垃圾收集的时间）\n\nExample:\n虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，则吞吐量为99%\n\n```\n\n### 4. Serial Old 收集器\n\n`Serial Old` 收集器是 `Serial` 收集器的老年代版本，它同样是一个单线程收集器，使用`标记-整理`算法。这个收集器的主要意义也是在于 `Client` 模式下的虚拟机使用。如果在 `Server` 模式下，那么它主要有两大用途： 一种用途是在 `JDK1.5` 以及之前的版本中与 `Parallel Scavenge` 收集器搭配使用， 另一种用途是作为 `CMS` 收集器的后备预案，在并发收集发生 `Concurrent Mode Failure` 时使用。\n\n### 5. Parallel Old 收集器\n\n`Parallel Old` 收集器是 `Parallel Scavenge` 收集器的老年代版本，使用多线程和 `标记-整理`算法。这个收集器是在 `JDK 1.6` 中才开始提供的。在此之前新生代 `Parallel Scavenge` 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 `Parallel Scavenge` 收集器，老年代除了 `Serial Old`收集器外别无选择。由于老年代 `Serial Old` 收集器在服务端应用性能上 `拖累`，使用了 `Parallel Scavenge` 收集器也未必能在整体应用上获得吞吐量最大化的效果。由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有`ParNew` 加 `CMS` 的组合给力。\n\n直到 `Parallel Old` 收集器出现后。`吞吐量优先` 收集器终于有了名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑 `Parallel Scavenge` 加 `Parallel Old` 收集器。\n\n### 6. CMS 收集器\n\n`CMS` (`Concurrent Mark Sweep`) 收集器是一种以获得最短回收停顿时间为目标的收集器。`CMS` 收集器非常符合重视响应时间速度以及希望系统停顿时间最短的应用。从名字上就可以看出，`CMS` 收集器是基于 `标记-清除` 算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些。整个过程可以分为4个步骤：\n- 初始标记\n- 并发标记\n- 重新标记\n- 并发清除\n\n初始标记与重新标记这两个步骤仍然需要 `Stop The World`。 初始标记仅仅是标记一下 `GC Roots` 能直接关联到的对象，速度很快，并发标记是进行 `GC Roots Tracing` 的过程，而重新标记则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发比较的时间短。\n\n由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，`CMS` 收集器的内存回收过程是与用户线程一起并发执行的。\n\n`CMS` 是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。但是 `CMS` 收集器还远达不到完美的程度，它有以下三个明显的缺点：\n\n(1) `CMS` 收集器对CPU资源非常敏感\n\n其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。`CMS` 默认启动的回收线程数是 `（CPU数量+3）/ 4`，也就是当CPU在4个以上时，并发回收时垃圾收集线程多于 `25%`的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，`CMS` 对用户程序的影响就可能变得很大。\n\n(2) `CMS` 收集器无法处理浮动垃圾\n\n`CMS` 收集器无法处理浮动垃圾，可能出现 `Concurrent Mode Failure` 失败而导致另一次 `Full GC` 的产生。由于 `CMS` 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，`CMS` 无法在当次收集中处理掉它们，只好留待下一次 `GC` 时再清理掉。这一部分垃圾就称为 `浮动垃圾`。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此 `CMS` 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 `JDK 1.5` 默认设置下，`CMS` 收集器当老年代使用了 `68%` 的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高触发百发比。在 `JDK1.6` 中， `CMS`收集器的启动阈值已经提升至 `92%`。运行期间预留的内存无法满足程序需要，就会出现一次 `Concurrent Mode Failure` 失败，这时虚拟机将启动后备预案：临时启用 `Serial Old` 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说，如果启动阈值设置的太高很容易导致大量这样的失败，性能反而会降低。\n\n(3) `CMS` 收集器会产生大量空间碎片\n\n`CMS` 是一款基于 `标记—清除` 算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 `Full GC`。\n\n### 7. G1 收集器\n\n`G1`（`Garbage-First`）是一款面向服务端应用的垃圾收集器。`HotSpot` 开发团队赋予它的使命是未来可以替换掉 `JDK 1.5` 中发布的 `CMS` 收集器。与其他 `GC` 收集器相比，`G1` 具备如下特点：\n\n(1) 并行与并发。`G1` 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短 `Stop-The-World` 停顿的时间，部分其他收集器原本需要停顿 `Java` 线程执行的 `GC` 动作，`G1` 收集器仍然可以通过并发的方式让 `Java` 程序继续执行。\n\n(2) 分代收集。与其他收集器一样，分代概念在 `G1` 中依然得以保留。虽然 `G1` 可以不需要其他收集器配合就能独立管理整个 `GC` 堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 `GC` 的旧对象以获取更好的收集效果。\n\n(3) 空间整合。与 `CMS` 的 `标记—清理` 算法不同，`G1` 从整体来看是基于 `标记—整理` 算法实现的收集器，从局部（两个 `Region` 之间）上来看是基于 `复制` 算法实现的，但无论如何，这两种算法都意味着 `G1` 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 `GC`。\n\n(4) 可预测的停顿。这是 `G1` 相对于 `CMS` 的另一大优势，降低停顿时间是 `G1` 和 `CMS` 共同的关注点，但 `G1` 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 `M` 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 `N` 毫秒。\n\n在 `G1` 之前的其他收集器进行收集的范围都是整个新生代或者老年代，而 `G1` 不再是这样。使用 `G1` 收集器时，`Java` 堆的内存布局就与其他收集器有很大差别，它将整个 `Java` 堆划分为多个大小相等的独立区域（`Region`），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 `Region`（不需要连续）的集合。\n\n`G1` 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 `Java` 堆中进行全区域的垃圾收集。`G1` 跟踪各个 `Region` 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 `Region`（这也就是 `Garbage-First` 名称的来由）。这种使用 `Region` 划分内存空间以及有优先级的区域回收方式，保证了 `G1` 收集器在有限的时间内可以获取尽可能高的收集效率。\n\n如果不计算维护 `Remembered Set`的操作， `G1` 收集器的运作大致可划分为以下几个步骤：\n\n(1) 初始标记。初始标记阶段仅仅只是标记一下 `GC Roots` 能直接关联到的对象，并且修改 `TAMS`（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 `Region` 中创建新对象，这阶段需要停顿线程，但耗时很短。\n\n(2) 并发标记。并发标记阶段是从 `GC Root` 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。\n\n(3) 最终标记。最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 `Remembered Set Logs` 里面，最终标记阶段需要把 `Remembered Set Logs` 的数据合并到 `Remembered Set` 中，这阶段需要停顿线程，但是可并行执行。\n\n(4) 筛选回收。筛选回收阶段首先对各个 `Region` 的回收价值和成本进行排序，根据用户所期望的 `GC` 停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 `Region`，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。\n","slug":"jvm-common-garbage-collector","published":1,"updated":"2018-02-11T08:48:15.570Z","comments":1,"photos":[],"link":"","_id":"cje58titv004aordbcgy2y0xk","content":"<p>本文“垃圾收集器”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】</p>\n<p>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<code>Java</code> 虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。下面讨论的是基于 <code>JDK 1.7 Update 14</code> 之后的 <code>HotSpot</code> 虚拟机。这个虚拟机包含的所有收集器如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-common-garbage-collector-1.png?raw=true\" alt=\"\"></p>\n<p>图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。下面会介绍这些收集器的特性，基本原理和使用场景。</p>\n<h3 id=\"1-Serial-收集器\"><a href=\"#1-Serial-收集器\" class=\"headerlink\" title=\"1. Serial 收集器\"></a>1. Serial 收集器</h3><p><code>Serial</code> 收集器是最基本，发展历史最悠久的收集器，曾经（JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，只会使用一个CPU 或一条收集线程去完成垃圾收集工作。重要的是，它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。在用户不可见的情况下把用户正常工作的线程全部停掉，这对应用程序来说都是很难接受的。但是虚拟机的设计者表示完全理解，却又表示很无奈。举个例子来说：你妈妈给你打扫房间的时候，肯定也会让你老老实实的在椅子或房间外待着，如果她一边打扫卫生，你一边乱扔垃圾，这房间还嫩打扫完吗？</p>\n<p><code>Serial</code> 收集器看起来是一个鸡肋收集器，但是到现在为止，它依然是虚拟机运行在 <code>Client</code> 模式下的默认新生代收集器。这是因为它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说， 该收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集的效率。</p>\n<p><code>Serial</code> 收集器在新生代收集时采用复制算法。</p>\n<h3 id=\"2-ParNew-收集器\"><a href=\"#2-ParNew-收集器\" class=\"headerlink\" title=\"2. ParNew 收集器\"></a>2. ParNew 收集器</h3><p><code>ParNew</code> 收集器其实是 <code>Serial</code> 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 <code>Serial</code>收集器可用的所有控制参数，收集算法，<code>Stop The World</code>，对象分配规则，回收策略等都与 <code>Serial</code> 收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。</p>\n<p><code>ParNew</code> 收集器除了多线程收集之外，其他与 <code>Serial</code>收集器相比并没有太多的创新之处，但它却是许多运行在 <code>Server</code> 模型下的虚拟机首选的新生代收集器。其中一个与性能无关但是很重要的原因是，除了 <code>Serial</code>收集器之外，目前只有它能与 <code>CMS</code> 收集器配合工作。在 <code>JDK 1.5</code> <code>HotSpot</code>推出了一款强交互应用中划时代的垃圾收集器 - <code>CMS</code> 收集器 （<code>Concurrent Mark Sweep</code>）。这款收集器是 <code>HotSpot</code> 虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。不幸的是 <code>CMS</code> 收集器作为老年代的收集器，却无法与 <code>JDK 1.4.0</code>中已经存在的 <code>Parallel Scavenge</code>收集器配合工作，所以在 <code>JDK 1.5</code> 中使用 <code>CMS</code> 来收集老年代的时候，新生代智能选择 <code>ParNew</code> 或者 <code>Serial</code>收集器。</p>\n<p><code>ParNew</code> 收集器在单 <code>CPU</code> 的环境中绝对不会有比 <code>Serial</code> 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个 <code>CPU</code> 的环境中都不能百分百的保证可以超越 <code>Serial</code> 收集器。  随着可以使用的 <code>CPU</code> 的数量的增加，它对于 <code>GC</code> 时系统资源的有效利用还是很有好处的。</p>\n<p>与 <code>Serial</code> 收集器一样，也是在新生代收集时采用复制算法。</p>\n<h3 id=\"3-Parallel-Scavenge-收集器\"><a href=\"#3-Parallel-Scavenge-收集器\" class=\"headerlink\" title=\"3. Parallel Scavenge 收集器\"></a>3. Parallel Scavenge 收集器</h3><p><code>Parallel Scavenge</code> 收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去跟 <code>ParNew</code> 收集器一样。</p>\n<p><code>Parallel Scavenge</code> 收集器的特点是它的关注点与其他的收集器不同， <code>CMS</code> 等收集器的关注点是尽可能的缩短垃圾收集时的用户线程的停顿时间， 而 <code>Parallel Scavenge</code> 收集器的目标则是达到一个可控制的吞吐量。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。由于与吞吐量关系密切，该收集器也经常成为 <code>吞吐量优先</code> 收集器。<code>Parallel Scavenge</code> 收集器可以通过设置 <code>-XX:+UseAdaptivSizePolicy</code> 参数开启 <code>GC</code> 自适应调节策略，不需要手工指定新生代的大小(<code>-Xmn</code>)，Eden与Survivor区的比例(<code>-XX:SurvivorRatio</code>)，晋升老年代对象年龄等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。自适应调节策略也是<code>Parallel Scavenge</code> 收集器与 <code>ParNew</code>收集器的一个重要区别。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">吞吐量 = 运行用户代码的时间 / （运行用户代码的时间 + 垃圾收集的时间）</span><br><span class=\"line\"></span><br><span class=\"line\">Example:</span><br><span class=\"line\">虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，则吞吐量为99%</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-Serial-Old-收集器\"><a href=\"#4-Serial-Old-收集器\" class=\"headerlink\" title=\"4. Serial Old 收集器\"></a>4. Serial Old 收集器</h3><p><code>Serial Old</code> 收集器是 <code>Serial</code> 收集器的老年代版本，它同样是一个单线程收集器，使用<code>标记-整理</code>算法。这个收集器的主要意义也是在于 <code>Client</code> 模式下的虚拟机使用。如果在 <code>Server</code> 模式下，那么它主要有两大用途： 一种用途是在 <code>JDK1.5</code> 以及之前的版本中与 <code>Parallel Scavenge</code> 收集器搭配使用， 另一种用途是作为 <code>CMS</code> 收集器的后备预案，在并发收集发生 <code>Concurrent Mode Failure</code> 时使用。</p>\n<h3 id=\"5-Parallel-Old-收集器\"><a href=\"#5-Parallel-Old-收集器\" class=\"headerlink\" title=\"5. Parallel Old 收集器\"></a>5. Parallel Old 收集器</h3><p><code>Parallel Old</code> 收集器是 <code>Parallel Scavenge</code> 收集器的老年代版本，使用多线程和 <code>标记-整理</code>算法。这个收集器是在 <code>JDK 1.6</code> 中才开始提供的。在此之前新生代 <code>Parallel Scavenge</code> 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 <code>Parallel Scavenge</code> 收集器，老年代除了 <code>Serial Old</code>收集器外别无选择。由于老年代 <code>Serial Old</code> 收集器在服务端应用性能上 <code>拖累</code>，使用了 <code>Parallel Scavenge</code> 收集器也未必能在整体应用上获得吞吐量最大化的效果。由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有<code>ParNew</code> 加 <code>CMS</code> 的组合给力。</p>\n<p>直到 <code>Parallel Old</code> 收集器出现后。<code>吞吐量优先</code> 收集器终于有了名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑 <code>Parallel Scavenge</code> 加 <code>Parallel Old</code> 收集器。</p>\n<h3 id=\"6-CMS-收集器\"><a href=\"#6-CMS-收集器\" class=\"headerlink\" title=\"6. CMS 收集器\"></a>6. CMS 收集器</h3><p><code>CMS</code> (<code>Concurrent Mark Sweep</code>) 收集器是一种以获得最短回收停顿时间为目标的收集器。<code>CMS</code> 收集器非常符合重视响应时间速度以及希望系统停顿时间最短的应用。从名字上就可以看出，<code>CMS</code> 收集器是基于 <code>标记-清除</code> 算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些。整个过程可以分为4个步骤：</p>\n<ul>\n<li>初始标记</li>\n<li>并发标记</li>\n<li>重新标记</li>\n<li>并发清除</li>\n</ul>\n<p>初始标记与重新标记这两个步骤仍然需要 <code>Stop The World</code>。 初始标记仅仅是标记一下 <code>GC Roots</code> 能直接关联到的对象，速度很快，并发标记是进行 <code>GC Roots Tracing</code> 的过程，而重新标记则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发比较的时间短。</p>\n<p>由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，<code>CMS</code> 收集器的内存回收过程是与用户线程一起并发执行的。</p>\n<p><code>CMS</code> 是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。但是 <code>CMS</code> 收集器还远达不到完美的程度，它有以下三个明显的缺点：</p>\n<p>(1) <code>CMS</code> 收集器对CPU资源非常敏感</p>\n<p>其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。<code>CMS</code> 默认启动的回收线程数是 <code>（CPU数量+3）/ 4</code>，也就是当CPU在4个以上时，并发回收时垃圾收集线程多于 <code>25%</code>的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，<code>CMS</code> 对用户程序的影响就可能变得很大。</p>\n<p>(2) <code>CMS</code> 收集器无法处理浮动垃圾</p>\n<p><code>CMS</code> 收集器无法处理浮动垃圾，可能出现 <code>Concurrent Mode Failure</code> 失败而导致另一次 <code>Full GC</code> 的产生。由于 <code>CMS</code> 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，<code>CMS</code> 无法在当次收集中处理掉它们，只好留待下一次 <code>GC</code> 时再清理掉。这一部分垃圾就称为 <code>浮动垃圾</code>。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此 <code>CMS</code> 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 <code>JDK 1.5</code> 默认设置下，<code>CMS</code> 收集器当老年代使用了 <code>68%</code> 的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高触发百发比。在 <code>JDK1.6</code> 中， <code>CMS</code>收集器的启动阈值已经提升至 <code>92%</code>。运行期间预留的内存无法满足程序需要，就会出现一次 <code>Concurrent Mode Failure</code> 失败，这时虚拟机将启动后备预案：临时启用 <code>Serial Old</code> 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说，如果启动阈值设置的太高很容易导致大量这样的失败，性能反而会降低。</p>\n<p>(3) <code>CMS</code> 收集器会产生大量空间碎片</p>\n<p><code>CMS</code> 是一款基于 <code>标记—清除</code> 算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 <code>Full GC</code>。</p>\n<h3 id=\"7-G1-收集器\"><a href=\"#7-G1-收集器\" class=\"headerlink\" title=\"7. G1 收集器\"></a>7. G1 收集器</h3><p><code>G1</code>（<code>Garbage-First</code>）是一款面向服务端应用的垃圾收集器。<code>HotSpot</code> 开发团队赋予它的使命是未来可以替换掉 <code>JDK 1.5</code> 中发布的 <code>CMS</code> 收集器。与其他 <code>GC</code> 收集器相比，<code>G1</code> 具备如下特点：</p>\n<p>(1) 并行与并发。<code>G1</code> 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短 <code>Stop-The-World</code> 停顿的时间，部分其他收集器原本需要停顿 <code>Java</code> 线程执行的 <code>GC</code> 动作，<code>G1</code> 收集器仍然可以通过并发的方式让 <code>Java</code> 程序继续执行。</p>\n<p>(2) 分代收集。与其他收集器一样，分代概念在 <code>G1</code> 中依然得以保留。虽然 <code>G1</code> 可以不需要其他收集器配合就能独立管理整个 <code>GC</code> 堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 <code>GC</code> 的旧对象以获取更好的收集效果。</p>\n<p>(3) 空间整合。与 <code>CMS</code> 的 <code>标记—清理</code> 算法不同，<code>G1</code> 从整体来看是基于 <code>标记—整理</code> 算法实现的收集器，从局部（两个 <code>Region</code> 之间）上来看是基于 <code>复制</code> 算法实现的，但无论如何，这两种算法都意味着 <code>G1</code> 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 <code>GC</code>。</p>\n<p>(4) 可预测的停顿。这是 <code>G1</code> 相对于 <code>CMS</code> 的另一大优势，降低停顿时间是 <code>G1</code> 和 <code>CMS</code> 共同的关注点，但 <code>G1</code> 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 <code>M</code> 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 <code>N</code> 毫秒。</p>\n<p>在 <code>G1</code> 之前的其他收集器进行收集的范围都是整个新生代或者老年代，而 <code>G1</code> 不再是这样。使用 <code>G1</code> 收集器时，<code>Java</code> 堆的内存布局就与其他收集器有很大差别，它将整个 <code>Java</code> 堆划分为多个大小相等的独立区域（<code>Region</code>），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 <code>Region</code>（不需要连续）的集合。</p>\n<p><code>G1</code> 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 <code>Java</code> 堆中进行全区域的垃圾收集。<code>G1</code> 跟踪各个 <code>Region</code> 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 <code>Region</code>（这也就是 <code>Garbage-First</code> 名称的来由）。这种使用 <code>Region</code> 划分内存空间以及有优先级的区域回收方式，保证了 <code>G1</code> 收集器在有限的时间内可以获取尽可能高的收集效率。</p>\n<p>如果不计算维护 <code>Remembered Set</code>的操作， <code>G1</code> 收集器的运作大致可划分为以下几个步骤：</p>\n<p>(1) 初始标记。初始标记阶段仅仅只是标记一下 <code>GC Roots</code> 能直接关联到的对象，并且修改 <code>TAMS</code>（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 <code>Region</code> 中创建新对象，这阶段需要停顿线程，但耗时很短。</p>\n<p>(2) 并发标记。并发标记阶段是从 <code>GC Root</code> 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。</p>\n<p>(3) 最终标记。最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 <code>Remembered Set Logs</code> 里面，最终标记阶段需要把 <code>Remembered Set Logs</code> 的数据合并到 <code>Remembered Set</code> 中，这阶段需要停顿线程，但是可并行执行。</p>\n<p>(4) 筛选回收。筛选回收阶段首先对各个 <code>Region</code> 的回收价值和成本进行排序，根据用户所期望的 <code>GC</code> 停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 <code>Region</code>，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文“垃圾收集器”节选自《深入理解Java虚拟机:JVM高级特性与最佳实践》【作者：周志明】</p>\n<p>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<code>Java</code> 虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。下面讨论的是基于 <code>JDK 1.7 Update 14</code> 之后的 <code>HotSpot</code> 虚拟机。这个虚拟机包含的所有收集器如下图所示：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Java/jvm-common-garbage-collector-1.png?raw=true\" alt=\"\"></p>\n<p>图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。下面会介绍这些收集器的特性，基本原理和使用场景。</p>\n<h3 id=\"1-Serial-收集器\"><a href=\"#1-Serial-收集器\" class=\"headerlink\" title=\"1. Serial 收集器\"></a>1. Serial 收集器</h3><p><code>Serial</code> 收集器是最基本，发展历史最悠久的收集器，曾经（JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，只会使用一个CPU 或一条收集线程去完成垃圾收集工作。重要的是，它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。在用户不可见的情况下把用户正常工作的线程全部停掉，这对应用程序来说都是很难接受的。但是虚拟机的设计者表示完全理解，却又表示很无奈。举个例子来说：你妈妈给你打扫房间的时候，肯定也会让你老老实实的在椅子或房间外待着，如果她一边打扫卫生，你一边乱扔垃圾，这房间还嫩打扫完吗？</p>\n<p><code>Serial</code> 收集器看起来是一个鸡肋收集器，但是到现在为止，它依然是虚拟机运行在 <code>Client</code> 模式下的默认新生代收集器。这是因为它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说， 该收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集的效率。</p>\n<p><code>Serial</code> 收集器在新生代收集时采用复制算法。</p>\n<h3 id=\"2-ParNew-收集器\"><a href=\"#2-ParNew-收集器\" class=\"headerlink\" title=\"2. ParNew 收集器\"></a>2. ParNew 收集器</h3><p><code>ParNew</code> 收集器其实是 <code>Serial</code> 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 <code>Serial</code>收集器可用的所有控制参数，收集算法，<code>Stop The World</code>，对象分配规则，回收策略等都与 <code>Serial</code> 收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。</p>\n<p><code>ParNew</code> 收集器除了多线程收集之外，其他与 <code>Serial</code>收集器相比并没有太多的创新之处，但它却是许多运行在 <code>Server</code> 模型下的虚拟机首选的新生代收集器。其中一个与性能无关但是很重要的原因是，除了 <code>Serial</code>收集器之外，目前只有它能与 <code>CMS</code> 收集器配合工作。在 <code>JDK 1.5</code> <code>HotSpot</code>推出了一款强交互应用中划时代的垃圾收集器 - <code>CMS</code> 收集器 （<code>Concurrent Mark Sweep</code>）。这款收集器是 <code>HotSpot</code> 虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。不幸的是 <code>CMS</code> 收集器作为老年代的收集器，却无法与 <code>JDK 1.4.0</code>中已经存在的 <code>Parallel Scavenge</code>收集器配合工作，所以在 <code>JDK 1.5</code> 中使用 <code>CMS</code> 来收集老年代的时候，新生代智能选择 <code>ParNew</code> 或者 <code>Serial</code>收集器。</p>\n<p><code>ParNew</code> 收集器在单 <code>CPU</code> 的环境中绝对不会有比 <code>Serial</code> 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个 <code>CPU</code> 的环境中都不能百分百的保证可以超越 <code>Serial</code> 收集器。  随着可以使用的 <code>CPU</code> 的数量的增加，它对于 <code>GC</code> 时系统资源的有效利用还是很有好处的。</p>\n<p>与 <code>Serial</code> 收集器一样，也是在新生代收集时采用复制算法。</p>\n<h3 id=\"3-Parallel-Scavenge-收集器\"><a href=\"#3-Parallel-Scavenge-收集器\" class=\"headerlink\" title=\"3. Parallel Scavenge 收集器\"></a>3. Parallel Scavenge 收集器</h3><p><code>Parallel Scavenge</code> 收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去跟 <code>ParNew</code> 收集器一样。</p>\n<p><code>Parallel Scavenge</code> 收集器的特点是它的关注点与其他的收集器不同， <code>CMS</code> 等收集器的关注点是尽可能的缩短垃圾收集时的用户线程的停顿时间， 而 <code>Parallel Scavenge</code> 收集器的目标则是达到一个可控制的吞吐量。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。由于与吞吐量关系密切，该收集器也经常成为 <code>吞吐量优先</code> 收集器。<code>Parallel Scavenge</code> 收集器可以通过设置 <code>-XX:+UseAdaptivSizePolicy</code> 参数开启 <code>GC</code> 自适应调节策略，不需要手工指定新生代的大小(<code>-Xmn</code>)，Eden与Survivor区的比例(<code>-XX:SurvivorRatio</code>)，晋升老年代对象年龄等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。自适应调节策略也是<code>Parallel Scavenge</code> 收集器与 <code>ParNew</code>收集器的一个重要区别。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">吞吐量 = 运行用户代码的时间 / （运行用户代码的时间 + 垃圾收集的时间）</span><br><span class=\"line\"></span><br><span class=\"line\">Example:</span><br><span class=\"line\">虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，则吞吐量为99%</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-Serial-Old-收集器\"><a href=\"#4-Serial-Old-收集器\" class=\"headerlink\" title=\"4. Serial Old 收集器\"></a>4. Serial Old 收集器</h3><p><code>Serial Old</code> 收集器是 <code>Serial</code> 收集器的老年代版本，它同样是一个单线程收集器，使用<code>标记-整理</code>算法。这个收集器的主要意义也是在于 <code>Client</code> 模式下的虚拟机使用。如果在 <code>Server</code> 模式下，那么它主要有两大用途： 一种用途是在 <code>JDK1.5</code> 以及之前的版本中与 <code>Parallel Scavenge</code> 收集器搭配使用， 另一种用途是作为 <code>CMS</code> 收集器的后备预案，在并发收集发生 <code>Concurrent Mode Failure</code> 时使用。</p>\n<h3 id=\"5-Parallel-Old-收集器\"><a href=\"#5-Parallel-Old-收集器\" class=\"headerlink\" title=\"5. Parallel Old 收集器\"></a>5. Parallel Old 收集器</h3><p><code>Parallel Old</code> 收集器是 <code>Parallel Scavenge</code> 收集器的老年代版本，使用多线程和 <code>标记-整理</code>算法。这个收集器是在 <code>JDK 1.6</code> 中才开始提供的。在此之前新生代 <code>Parallel Scavenge</code> 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 <code>Parallel Scavenge</code> 收集器，老年代除了 <code>Serial Old</code>收集器外别无选择。由于老年代 <code>Serial Old</code> 收集器在服务端应用性能上 <code>拖累</code>，使用了 <code>Parallel Scavenge</code> 收集器也未必能在整体应用上获得吞吐量最大化的效果。由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有<code>ParNew</code> 加 <code>CMS</code> 的组合给力。</p>\n<p>直到 <code>Parallel Old</code> 收集器出现后。<code>吞吐量优先</code> 收集器终于有了名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑 <code>Parallel Scavenge</code> 加 <code>Parallel Old</code> 收集器。</p>\n<h3 id=\"6-CMS-收集器\"><a href=\"#6-CMS-收集器\" class=\"headerlink\" title=\"6. CMS 收集器\"></a>6. CMS 收集器</h3><p><code>CMS</code> (<code>Concurrent Mark Sweep</code>) 收集器是一种以获得最短回收停顿时间为目标的收集器。<code>CMS</code> 收集器非常符合重视响应时间速度以及希望系统停顿时间最短的应用。从名字上就可以看出，<code>CMS</code> 收集器是基于 <code>标记-清除</code> 算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些。整个过程可以分为4个步骤：</p>\n<ul>\n<li>初始标记</li>\n<li>并发标记</li>\n<li>重新标记</li>\n<li>并发清除</li>\n</ul>\n<p>初始标记与重新标记这两个步骤仍然需要 <code>Stop The World</code>。 初始标记仅仅是标记一下 <code>GC Roots</code> 能直接关联到的对象，速度很快，并发标记是进行 <code>GC Roots Tracing</code> 的过程，而重新标记则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发比较的时间短。</p>\n<p>由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，<code>CMS</code> 收集器的内存回收过程是与用户线程一起并发执行的。</p>\n<p><code>CMS</code> 是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿。但是 <code>CMS</code> 收集器还远达不到完美的程度，它有以下三个明显的缺点：</p>\n<p>(1) <code>CMS</code> 收集器对CPU资源非常敏感</p>\n<p>其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。<code>CMS</code> 默认启动的回收线程数是 <code>（CPU数量+3）/ 4</code>，也就是当CPU在4个以上时，并发回收时垃圾收集线程多于 <code>25%</code>的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，<code>CMS</code> 对用户程序的影响就可能变得很大。</p>\n<p>(2) <code>CMS</code> 收集器无法处理浮动垃圾</p>\n<p><code>CMS</code> 收集器无法处理浮动垃圾，可能出现 <code>Concurrent Mode Failure</code> 失败而导致另一次 <code>Full GC</code> 的产生。由于 <code>CMS</code> 并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，<code>CMS</code> 无法在当次收集中处理掉它们，只好留待下一次 <code>GC</code> 时再清理掉。这一部分垃圾就称为 <code>浮动垃圾</code>。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此 <code>CMS</code> 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 <code>JDK 1.5</code> 默认设置下，<code>CMS</code> 收集器当老年代使用了 <code>68%</code> 的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高触发百发比。在 <code>JDK1.6</code> 中， <code>CMS</code>收集器的启动阈值已经提升至 <code>92%</code>。运行期间预留的内存无法满足程序需要，就会出现一次 <code>Concurrent Mode Failure</code> 失败，这时虚拟机将启动后备预案：临时启用 <code>Serial Old</code> 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说，如果启动阈值设置的太高很容易导致大量这样的失败，性能反而会降低。</p>\n<p>(3) <code>CMS</code> 收集器会产生大量空间碎片</p>\n<p><code>CMS</code> 是一款基于 <code>标记—清除</code> 算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 <code>Full GC</code>。</p>\n<h3 id=\"7-G1-收集器\"><a href=\"#7-G1-收集器\" class=\"headerlink\" title=\"7. G1 收集器\"></a>7. G1 收集器</h3><p><code>G1</code>（<code>Garbage-First</code>）是一款面向服务端应用的垃圾收集器。<code>HotSpot</code> 开发团队赋予它的使命是未来可以替换掉 <code>JDK 1.5</code> 中发布的 <code>CMS</code> 收集器。与其他 <code>GC</code> 收集器相比，<code>G1</code> 具备如下特点：</p>\n<p>(1) 并行与并发。<code>G1</code> 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短 <code>Stop-The-World</code> 停顿的时间，部分其他收集器原本需要停顿 <code>Java</code> 线程执行的 <code>GC</code> 动作，<code>G1</code> 收集器仍然可以通过并发的方式让 <code>Java</code> 程序继续执行。</p>\n<p>(2) 分代收集。与其他收集器一样，分代概念在 <code>G1</code> 中依然得以保留。虽然 <code>G1</code> 可以不需要其他收集器配合就能独立管理整个 <code>GC</code> 堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 <code>GC</code> 的旧对象以获取更好的收集效果。</p>\n<p>(3) 空间整合。与 <code>CMS</code> 的 <code>标记—清理</code> 算法不同，<code>G1</code> 从整体来看是基于 <code>标记—整理</code> 算法实现的收集器，从局部（两个 <code>Region</code> 之间）上来看是基于 <code>复制</code> 算法实现的，但无论如何，这两种算法都意味着 <code>G1</code> 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 <code>GC</code>。</p>\n<p>(4) 可预测的停顿。这是 <code>G1</code> 相对于 <code>CMS</code> 的另一大优势，降低停顿时间是 <code>G1</code> 和 <code>CMS</code> 共同的关注点，但 <code>G1</code> 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 <code>M</code> 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 <code>N</code> 毫秒。</p>\n<p>在 <code>G1</code> 之前的其他收集器进行收集的范围都是整个新生代或者老年代，而 <code>G1</code> 不再是这样。使用 <code>G1</code> 收集器时，<code>Java</code> 堆的内存布局就与其他收集器有很大差别，它将整个 <code>Java</code> 堆划分为多个大小相等的独立区域（<code>Region</code>），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 <code>Region</code>（不需要连续）的集合。</p>\n<p><code>G1</code> 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 <code>Java</code> 堆中进行全区域的垃圾收集。<code>G1</code> 跟踪各个 <code>Region</code> 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 <code>Region</code>（这也就是 <code>Garbage-First</code> 名称的来由）。这种使用 <code>Region</code> 划分内存空间以及有优先级的区域回收方式，保证了 <code>G1</code> 收集器在有限的时间内可以获取尽可能高的收集效率。</p>\n<p>如果不计算维护 <code>Remembered Set</code>的操作， <code>G1</code> 收集器的运作大致可划分为以下几个步骤：</p>\n<p>(1) 初始标记。初始标记阶段仅仅只是标记一下 <code>GC Roots</code> 能直接关联到的对象，并且修改 <code>TAMS</code>（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 <code>Region</code> 中创建新对象，这阶段需要停顿线程，但耗时很短。</p>\n<p>(2) 并发标记。并发标记阶段是从 <code>GC Root</code> 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。</p>\n<p>(3) 最终标记。最终标记阶段是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 <code>Remembered Set Logs</code> 里面，最终标记阶段需要把 <code>Remembered Set Logs</code> 的数据合并到 <code>Remembered Set</code> 中，这阶段需要停顿线程，但是可并行执行。</p>\n<p>(4) 筛选回收。筛选回收阶段首先对各个 <code>Region</code> 的回收价值和成本进行排序，根据用户所期望的 <code>GC</code> 停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 <code>Region</code>，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。</p>\n"},{"layout":"post","author":"sjf0115","title":"Java 堆内内存与堆外内存","date":"2018-02-04T09:40:01.000Z","_content":"\n一般情况下，Java 中分配的非空对象都是由 Java 虚拟机的垃圾收集器管理的，也称为堆内内存（`on-heap memory`）。虚拟机会定期对垃圾内存进行回收，在某些特定的时间点，它会进行一次彻底的回收（`full gc`）。彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对 Java 应用造成的影响，跟堆的大小是成正比的。过大的堆会影响 Java 应用的性能。\n\n对于这个问题，一种解决方案就是使用堆外内存（`off-heap memory`）。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。\n\n但是 Java 本身也在不断对堆内内存的实现方式做改进。两者各有什么优缺点？ [Vanilla Java](http://vanillajava.blogspot.com/) 博客作者 Peter Lawrey 撰写了一篇[文章](http://vanillajava.blogspot.com/2014/12/on-heap-vs-off-heap-memory-usage.html)，在文中他对三种方式：用new来分配对象、对象池（object pool）和堆外内存，进行了详细的分析。\n\n用new来分配对象内存是最基本的一种方式，Lawery提到：\n\n在Java 5.0之前，分配对象的代价很大，以至于大家都使用内存池。但是从5.0开始，对象分配和垃圾回收变得快多了，研发人员发现了性能的提升，纷纷简化他们的代码，不再使用内存池，而直接用new来分配对象。从5.0开始，只有一些分配代价较大的对象，比如线程、套接字和数据库链接，用内存池才会有明显的性能提升。\n\n对于内存池，Lawery认为它主要用于两类对象。第一类是生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能。第二种情况是加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。对此，Lawery还以 [StringInterner](https://github.com/OpenHFT/Java-Lang/blob/master/lang/src/main/java/net/openhft/lang/pool/StringInterner.java) 为例进行了说明。\n\n最后Lawery分析了堆外内存，它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。堆外内存有以下特点：\n- 对于大内存有良好的伸缩性\n- 对垃圾回收停顿的改善可以明显感觉到\n- 在进程间可以共享，减少虚拟机间的复制\n\nLawery还提到堆外内存最重要的还不是它能改进性能，而是它的确定性。\n\n当然堆外内存也有它自己的问题，最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行序列化（serialization），而序列化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。\n\nLawery还介绍了OpenHFT公司提供三个开源库：[Chronicle Queue](http://openhft.net/products/chronicle-queue/)、[Chronicle Map](http://openhft.net/products/chronicle-map/)和 [Thread Affinity](http://openhft.net/products/thread-affinity/)，这些库可以帮助开发人员使用堆外内存来保存数据。采用堆外内存有很多好处，同时也带来挑战，对堆外内存感兴趣的读者可以阅读Lawery的原文来了解更多信息。\n\n转载于： http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/\n","source":"_posts/Java/[Java]Java 堆内内存与堆外内存.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Java 堆内内存与堆外内存\ndate: 2018-02-04 17:40:01\ntags:\n  - Java\n\ncategories: Java\npermalink: java-on-off-heap-memory\n---\n\n一般情况下，Java 中分配的非空对象都是由 Java 虚拟机的垃圾收集器管理的，也称为堆内内存（`on-heap memory`）。虚拟机会定期对垃圾内存进行回收，在某些特定的时间点，它会进行一次彻底的回收（`full gc`）。彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对 Java 应用造成的影响，跟堆的大小是成正比的。过大的堆会影响 Java 应用的性能。\n\n对于这个问题，一种解决方案就是使用堆外内存（`off-heap memory`）。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。\n\n但是 Java 本身也在不断对堆内内存的实现方式做改进。两者各有什么优缺点？ [Vanilla Java](http://vanillajava.blogspot.com/) 博客作者 Peter Lawrey 撰写了一篇[文章](http://vanillajava.blogspot.com/2014/12/on-heap-vs-off-heap-memory-usage.html)，在文中他对三种方式：用new来分配对象、对象池（object pool）和堆外内存，进行了详细的分析。\n\n用new来分配对象内存是最基本的一种方式，Lawery提到：\n\n在Java 5.0之前，分配对象的代价很大，以至于大家都使用内存池。但是从5.0开始，对象分配和垃圾回收变得快多了，研发人员发现了性能的提升，纷纷简化他们的代码，不再使用内存池，而直接用new来分配对象。从5.0开始，只有一些分配代价较大的对象，比如线程、套接字和数据库链接，用内存池才会有明显的性能提升。\n\n对于内存池，Lawery认为它主要用于两类对象。第一类是生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能。第二种情况是加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。对此，Lawery还以 [StringInterner](https://github.com/OpenHFT/Java-Lang/blob/master/lang/src/main/java/net/openhft/lang/pool/StringInterner.java) 为例进行了说明。\n\n最后Lawery分析了堆外内存，它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。堆外内存有以下特点：\n- 对于大内存有良好的伸缩性\n- 对垃圾回收停顿的改善可以明显感觉到\n- 在进程间可以共享，减少虚拟机间的复制\n\nLawery还提到堆外内存最重要的还不是它能改进性能，而是它的确定性。\n\n当然堆外内存也有它自己的问题，最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行序列化（serialization），而序列化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。\n\nLawery还介绍了OpenHFT公司提供三个开源库：[Chronicle Queue](http://openhft.net/products/chronicle-queue/)、[Chronicle Map](http://openhft.net/products/chronicle-map/)和 [Thread Affinity](http://openhft.net/products/thread-affinity/)，这些库可以帮助开发人员使用堆外内存来保存数据。采用堆外内存有很多好处，同时也带来挑战，对堆外内存感兴趣的读者可以阅读Lawery的原文来了解更多信息。\n\n转载于： http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/\n","slug":"java-on-off-heap-memory","published":1,"updated":"2018-02-06T11:51:17.880Z","comments":1,"photos":[],"link":"","_id":"cje58titx004cordb6c8s6dju","content":"<p>一般情况下，Java 中分配的非空对象都是由 Java 虚拟机的垃圾收集器管理的，也称为堆内内存（<code>on-heap memory</code>）。虚拟机会定期对垃圾内存进行回收，在某些特定的时间点，它会进行一次彻底的回收（<code>full gc</code>）。彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对 Java 应用造成的影响，跟堆的大小是成正比的。过大的堆会影响 Java 应用的性能。</p>\n<p>对于这个问题，一种解决方案就是使用堆外内存（<code>off-heap memory</code>）。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</p>\n<p>但是 Java 本身也在不断对堆内内存的实现方式做改进。两者各有什么优缺点？ <a href=\"http://vanillajava.blogspot.com/\" target=\"_blank\" rel=\"noopener\">Vanilla Java</a> 博客作者 Peter Lawrey 撰写了一篇<a href=\"http://vanillajava.blogspot.com/2014/12/on-heap-vs-off-heap-memory-usage.html\" target=\"_blank\" rel=\"noopener\">文章</a>，在文中他对三种方式：用new来分配对象、对象池（object pool）和堆外内存，进行了详细的分析。</p>\n<p>用new来分配对象内存是最基本的一种方式，Lawery提到：</p>\n<p>在Java 5.0之前，分配对象的代价很大，以至于大家都使用内存池。但是从5.0开始，对象分配和垃圾回收变得快多了，研发人员发现了性能的提升，纷纷简化他们的代码，不再使用内存池，而直接用new来分配对象。从5.0开始，只有一些分配代价较大的对象，比如线程、套接字和数据库链接，用内存池才会有明显的性能提升。</p>\n<p>对于内存池，Lawery认为它主要用于两类对象。第一类是生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能。第二种情况是加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。对此，Lawery还以 <a href=\"https://github.com/OpenHFT/Java-Lang/blob/master/lang/src/main/java/net/openhft/lang/pool/StringInterner.java\" target=\"_blank\" rel=\"noopener\">StringInterner</a> 为例进行了说明。</p>\n<p>最后Lawery分析了堆外内存，它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。堆外内存有以下特点：</p>\n<ul>\n<li>对于大内存有良好的伸缩性</li>\n<li>对垃圾回收停顿的改善可以明显感觉到</li>\n<li>在进程间可以共享，减少虚拟机间的复制</li>\n</ul>\n<p>Lawery还提到堆外内存最重要的还不是它能改进性能，而是它的确定性。</p>\n<p>当然堆外内存也有它自己的问题，最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行序列化（serialization），而序列化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。</p>\n<p>Lawery还介绍了OpenHFT公司提供三个开源库：<a href=\"http://openhft.net/products/chronicle-queue/\" target=\"_blank\" rel=\"noopener\">Chronicle Queue</a>、<a href=\"http://openhft.net/products/chronicle-map/\" target=\"_blank\" rel=\"noopener\">Chronicle Map</a>和 <a href=\"http://openhft.net/products/thread-affinity/\" target=\"_blank\" rel=\"noopener\">Thread Affinity</a>，这些库可以帮助开发人员使用堆外内存来保存数据。采用堆外内存有很多好处，同时也带来挑战，对堆外内存感兴趣的读者可以阅读Lawery的原文来了解更多信息。</p>\n<p>转载于： <a href=\"http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>一般情况下，Java 中分配的非空对象都是由 Java 虚拟机的垃圾收集器管理的，也称为堆内内存（<code>on-heap memory</code>）。虚拟机会定期对垃圾内存进行回收，在某些特定的时间点，它会进行一次彻底的回收（<code>full gc</code>）。彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对 Java 应用造成的影响，跟堆的大小是成正比的。过大的堆会影响 Java 应用的性能。</p>\n<p>对于这个问题，一种解决方案就是使用堆外内存（<code>off-heap memory</code>）。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</p>\n<p>但是 Java 本身也在不断对堆内内存的实现方式做改进。两者各有什么优缺点？ <a href=\"http://vanillajava.blogspot.com/\" target=\"_blank\" rel=\"noopener\">Vanilla Java</a> 博客作者 Peter Lawrey 撰写了一篇<a href=\"http://vanillajava.blogspot.com/2014/12/on-heap-vs-off-heap-memory-usage.html\" target=\"_blank\" rel=\"noopener\">文章</a>，在文中他对三种方式：用new来分配对象、对象池（object pool）和堆外内存，进行了详细的分析。</p>\n<p>用new来分配对象内存是最基本的一种方式，Lawery提到：</p>\n<p>在Java 5.0之前，分配对象的代价很大，以至于大家都使用内存池。但是从5.0开始，对象分配和垃圾回收变得快多了，研发人员发现了性能的提升，纷纷简化他们的代码，不再使用内存池，而直接用new来分配对象。从5.0开始，只有一些分配代价较大的对象，比如线程、套接字和数据库链接，用内存池才会有明显的性能提升。</p>\n<p>对于内存池，Lawery认为它主要用于两类对象。第一类是生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能。第二种情况是加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。对此，Lawery还以 <a href=\"https://github.com/OpenHFT/Java-Lang/blob/master/lang/src/main/java/net/openhft/lang/pool/StringInterner.java\" target=\"_blank\" rel=\"noopener\">StringInterner</a> 为例进行了说明。</p>\n<p>最后Lawery分析了堆外内存，它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。堆外内存有以下特点：</p>\n<ul>\n<li>对于大内存有良好的伸缩性</li>\n<li>对垃圾回收停顿的改善可以明显感觉到</li>\n<li>在进程间可以共享，减少虚拟机间的复制</li>\n</ul>\n<p>Lawery还提到堆外内存最重要的还不是它能改进性能，而是它的确定性。</p>\n<p>当然堆外内存也有它自己的问题，最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行序列化（serialization），而序列化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。</p>\n<p>Lawery还介绍了OpenHFT公司提供三个开源库：<a href=\"http://openhft.net/products/chronicle-queue/\" target=\"_blank\" rel=\"noopener\">Chronicle Queue</a>、<a href=\"http://openhft.net/products/chronicle-map/\" target=\"_blank\" rel=\"noopener\">Chronicle Map</a>和 <a href=\"http://openhft.net/products/thread-affinity/\" target=\"_blank\" rel=\"noopener\">Thread Affinity</a>，这些库可以帮助开发人员使用堆外内存来保存数据。采用堆外内存有很多好处，同时也带来挑战，对堆外内存感兴趣的读者可以阅读Lawery的原文来了解更多信息。</p>\n<p>转载于： <a href=\"http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/news/2014/12/external-memory-heap-memory/</a></p>\n"},{"layout":"post","author":"sjf0115","title":"MySQL 配置远程登录","date":"2017-12-29T05:29:01.000Z","_content":"\n### 1. 修改配置\n\n修改`/etc/mysql/mysql.conf.d`目录下的`mysqld.cnf`配置文件:\n```\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\n#bind-address           = 127.0.0.1\n```\n在`bind-address`前面加个`#`进行注释，允许任意IP访问。或者指定自己需要远程访问的IP地址。然后重启`mysql`:\n```\nubuntu@VM-0-7-ubuntu:/etc/mysql/mysql.conf.d$ sudo /etc/init.d/mysql restart\nRestarting mysql (via systemctl): mysql.service.\n```\n### 2. 授权用户\n\n我们先看一下当前能登录到我们数据的用户以及允许连接的IP:\n```sql\nmysql> USE mysql;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\n\nmysql> select User,Host from user;\n+------------------+-----------+\n| User             | Host      |\n+------------------+-----------+\n| debian-sys-maint | localhost |\n| mysql.session    | localhost |\n| mysql.sys        | localhost |\n| root             | localhost |\n+------------------+-----------+\n4 rows in set (0.00 sec)\n```\n我们可以看到只有一个默认的`root`用户，且只允许使用`localhost`连接。下面我们另外添加一个新的`root`用户在指定IP下使用指定密码来访问数据库:\n```sql\nmysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n```\n\n`*.*`中第一个`*`代表数据库名称，第二个`*`代表表名。在这里我们设置的是所有数据库里的所有表都授权给用户，如果只想授权某数据库或某些数据库下某些表，可以把`*`替换成你所需的数据库名和表明即可:\n```\nmysql> GRANT ALL PRIVILEGES ON test_db.user TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;\n```\n上述表示是把`test_db`数据下的`user`数据表授权给用户。\n\n`root`表示授予`root`用户可以登录数据库。`%`表示授权的用户使用哪些IP可以登录，这里表示可以使用用户`root`在任意IP地址来访问数据库。`dev`表示分配`root`用户对应的密码。\n\n当然我们也可以直接用`UPDATE`更新`root`用户`Host`, 但不推荐：\n```\nUPDATE user SET Host='%' WHERE User='root' AND Host='localhost';\n```\n授权用户之后，执行如下命令刷新一下权限:\n```sql\n\nmysql> flush privileges;\nQuery OK, 0 rows affected (0.00 sec)\n```\n至此我们已经完成了配置远程访问数据的所有操作，我们在看一下当前能访问我们数据库的用户:\n```\nmysql> select User,Host from user;\n+------------------+-----------+\n| User             | Host      |\n+------------------+-----------+\n| root             | %         |\n| debian-sys-maint | localhost |\n| mysql.session    | localhost |\n| mysql.sys        | localhost |\n| root             | localhost |\n+------------------+-----------+\n5 rows in set (0.00 sec)\n```\n","source":"_posts/MySQL/[MySQL]MySQL配置远程登录.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: MySQL 配置远程登录\ndate: 2017-12-29 13:29:01\ntags:\n  - MySQL\n\ncategories: MySQL\n---\n\n### 1. 修改配置\n\n修改`/etc/mysql/mysql.conf.d`目录下的`mysqld.cnf`配置文件:\n```\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\n#bind-address           = 127.0.0.1\n```\n在`bind-address`前面加个`#`进行注释，允许任意IP访问。或者指定自己需要远程访问的IP地址。然后重启`mysql`:\n```\nubuntu@VM-0-7-ubuntu:/etc/mysql/mysql.conf.d$ sudo /etc/init.d/mysql restart\nRestarting mysql (via systemctl): mysql.service.\n```\n### 2. 授权用户\n\n我们先看一下当前能登录到我们数据的用户以及允许连接的IP:\n```sql\nmysql> USE mysql;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\n\nmysql> select User,Host from user;\n+------------------+-----------+\n| User             | Host      |\n+------------------+-----------+\n| debian-sys-maint | localhost |\n| mysql.session    | localhost |\n| mysql.sys        | localhost |\n| root             | localhost |\n+------------------+-----------+\n4 rows in set (0.00 sec)\n```\n我们可以看到只有一个默认的`root`用户，且只允许使用`localhost`连接。下面我们另外添加一个新的`root`用户在指定IP下使用指定密码来访问数据库:\n```sql\nmysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n```\n\n`*.*`中第一个`*`代表数据库名称，第二个`*`代表表名。在这里我们设置的是所有数据库里的所有表都授权给用户，如果只想授权某数据库或某些数据库下某些表，可以把`*`替换成你所需的数据库名和表明即可:\n```\nmysql> GRANT ALL PRIVILEGES ON test_db.user TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;\n```\n上述表示是把`test_db`数据下的`user`数据表授权给用户。\n\n`root`表示授予`root`用户可以登录数据库。`%`表示授权的用户使用哪些IP可以登录，这里表示可以使用用户`root`在任意IP地址来访问数据库。`dev`表示分配`root`用户对应的密码。\n\n当然我们也可以直接用`UPDATE`更新`root`用户`Host`, 但不推荐：\n```\nUPDATE user SET Host='%' WHERE User='root' AND Host='localhost';\n```\n授权用户之后，执行如下命令刷新一下权限:\n```sql\n\nmysql> flush privileges;\nQuery OK, 0 rows affected (0.00 sec)\n```\n至此我们已经完成了配置远程访问数据的所有操作，我们在看一下当前能访问我们数据库的用户:\n```\nmysql> select User,Host from user;\n+------------------+-----------+\n| User             | Host      |\n+------------------+-----------+\n| root             | %         |\n| debian-sys-maint | localhost |\n| mysql.session    | localhost |\n| mysql.sys        | localhost |\n| root             | localhost |\n+------------------+-----------+\n5 rows in set (0.00 sec)\n```\n","slug":"MySQL/[MySQL]MySQL配置远程登录","published":1,"updated":"2018-01-29T09:36:59.626Z","comments":1,"photos":[],"link":"","_id":"cje58titz004gordbuxz3q6lr","content":"<h3 id=\"1-修改配置\"><a href=\"#1-修改配置\" class=\"headerlink\" title=\"1. 修改配置\"></a>1. 修改配置</h3><p>修改<code>/etc/mysql/mysql.conf.d</code>目录下的<code>mysqld.cnf</code>配置文件:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Instead of skip-networking the default is now to listen only on</span><br><span class=\"line\"># localhost which is more compatible and is not less secure.</span><br><span class=\"line\">#bind-address           = 127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>在<code>bind-address</code>前面加个<code>#</code>进行注释，允许任意IP访问。或者指定自己需要远程访问的IP地址。然后重启<code>mysql</code>:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">ubuntu@VM-0-7-ubuntu:/etc/mysql/mysql.conf.d$ sudo /etc/init.d/mysql restart</span><br><span class=\"line\">Restarting mysql (via systemctl): mysql.service.</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-授权用户\"><a href=\"#2-授权用户\" class=\"headerlink\" title=\"2. 授权用户\"></a>2. 授权用户</h3><p>我们先看一下当前能登录到我们数据的用户以及允许连接的IP:<br><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; USE mysql;</span><br><span class=\"line\">Reading table information for completion of table and column names</span><br><span class=\"line\">You can turn off this feature to get a quicker startup with -A</span><br><span class=\"line\"></span><br><span class=\"line\">Database changed</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; select User,Host from user;</span><br><span class=\"line\">+<span class=\"comment\">------------------+-----------+</span></span><br><span class=\"line\">| User             | Host      |</span><br><span class=\"line\">+<span class=\"comment\">------------------+-----------+</span></span><br><span class=\"line\">| debian-sys-maint | localhost |</span><br><span class=\"line\">| mysql.session    | localhost |</span><br><span class=\"line\">| mysql.sys        | localhost |</span><br><span class=\"line\">| root             | localhost |</span><br><span class=\"line\">+<span class=\"comment\">------------------+-----------+</span></span><br><span class=\"line\">4 rows in <span class=\"keyword\">set</span> (<span class=\"number\">0.00</span> sec)</span><br></pre></td></tr></table></figure></p>\n<p>我们可以看到只有一个默认的<code>root</code>用户，且只允许使用<code>localhost</code>连接。下面我们另外添加一个新的<code>root</code>用户在指定IP下使用指定密码来访问数据库:<br><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;</span><br><span class=\"line\">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p><code>*.*</code>中第一个<code>*</code>代表数据库名称，第二个<code>*</code>代表表名。在这里我们设置的是所有数据库里的所有表都授权给用户，如果只想授权某数据库或某些数据库下某些表，可以把<code>*</code>替换成你所需的数据库名和表明即可:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; GRANT ALL PRIVILEGES ON test_db.user TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;dev&apos; WITH GRANT OPTION;</span><br></pre></td></tr></table></figure></p>\n<p>上述表示是把<code>test_db</code>数据下的<code>user</code>数据表授权给用户。</p>\n<p><code>root</code>表示授予<code>root</code>用户可以登录数据库。<code>%</code>表示授权的用户使用哪些IP可以登录，这里表示可以使用用户<code>root</code>在任意IP地址来访问数据库。<code>dev</code>表示分配<code>root</code>用户对应的密码。</p>\n<p>当然我们也可以直接用<code>UPDATE</code>更新<code>root</code>用户<code>Host</code>, 但不推荐：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">UPDATE user SET Host=&apos;%&apos; WHERE User=&apos;root&apos; AND Host=&apos;localhost&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>授权用户之后，执行如下命令刷新一下权限:<br><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">mysql&gt; flush privileges;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>至此我们已经完成了配置远程访问数据的所有操作，我们在看一下当前能访问我们数据库的用户:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; select User,Host from user;</span><br><span class=\"line\">+------------------+-----------+</span><br><span class=\"line\">| User             | Host      |</span><br><span class=\"line\">+------------------+-----------+</span><br><span class=\"line\">| root             | %         |</span><br><span class=\"line\">| debian-sys-maint | localhost |</span><br><span class=\"line\">| mysql.session    | localhost |</span><br><span class=\"line\">| mysql.sys        | localhost |</span><br><span class=\"line\">| root             | localhost |</span><br><span class=\"line\">+------------------+-----------+</span><br><span class=\"line\">5 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-修改配置\"><a href=\"#1-修改配置\" class=\"headerlink\" title=\"1. 修改配置\"></a>1. 修改配置</h3><p>修改<code>/etc/mysql/mysql.conf.d</code>目录下的<code>mysqld.cnf</code>配置文件:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\"># Instead of skip-networking the default is now to listen only on</span><br><span class=\"line\"># localhost which is more compatible and is not less secure.</span><br><span class=\"line\">#bind-address           = 127.0.0.1</span><br></pre></td></tr></table></figure></p>\n<p>在<code>bind-address</code>前面加个<code>#</code>进行注释，允许任意IP访问。或者指定自己需要远程访问的IP地址。然后重启<code>mysql</code>:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">ubuntu@VM-0-7-ubuntu:/etc/mysql/mysql.conf.d$ sudo /etc/init.d/mysql restart</span><br><span class=\"line\">Restarting mysql (via systemctl): mysql.service.</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-授权用户\"><a href=\"#2-授权用户\" class=\"headerlink\" title=\"2. 授权用户\"></a>2. 授权用户</h3><p>我们先看一下当前能登录到我们数据的用户以及允许连接的IP:<br><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; USE mysql;</span><br><span class=\"line\">Reading table information for completion of table and column names</span><br><span class=\"line\">You can turn off this feature to get a quicker startup with -A</span><br><span class=\"line\"></span><br><span class=\"line\">Database changed</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; select User,Host from user;</span><br><span class=\"line\">+<span class=\"comment\">------------------+-----------+</span></span><br><span class=\"line\">| User             | Host      |</span><br><span class=\"line\">+<span class=\"comment\">------------------+-----------+</span></span><br><span class=\"line\">| debian-sys-maint | localhost |</span><br><span class=\"line\">| mysql.session    | localhost |</span><br><span class=\"line\">| mysql.sys        | localhost |</span><br><span class=\"line\">| root             | localhost |</span><br><span class=\"line\">+<span class=\"comment\">------------------+-----------+</span></span><br><span class=\"line\">4 rows in <span class=\"keyword\">set</span> (<span class=\"number\">0.00</span> sec)</span><br></pre></td></tr></table></figure></p>\n<p>我们可以看到只有一个默认的<code>root</code>用户，且只允许使用<code>localhost</code>连接。下面我们另外添加一个新的<code>root</code>用户在指定IP下使用指定密码来访问数据库:<br><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'dev' WITH GRANT OPTION;</span><br><span class=\"line\">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p><code>*.*</code>中第一个<code>*</code>代表数据库名称，第二个<code>*</code>代表表名。在这里我们设置的是所有数据库里的所有表都授权给用户，如果只想授权某数据库或某些数据库下某些表，可以把<code>*</code>替换成你所需的数据库名和表明即可:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; GRANT ALL PRIVILEGES ON test_db.user TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;dev&apos; WITH GRANT OPTION;</span><br></pre></td></tr></table></figure></p>\n<p>上述表示是把<code>test_db</code>数据下的<code>user</code>数据表授权给用户。</p>\n<p><code>root</code>表示授予<code>root</code>用户可以登录数据库。<code>%</code>表示授权的用户使用哪些IP可以登录，这里表示可以使用用户<code>root</code>在任意IP地址来访问数据库。<code>dev</code>表示分配<code>root</code>用户对应的密码。</p>\n<p>当然我们也可以直接用<code>UPDATE</code>更新<code>root</code>用户<code>Host</code>, 但不推荐：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">UPDATE user SET Host=&apos;%&apos; WHERE User=&apos;root&apos; AND Host=&apos;localhost&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>授权用户之后，执行如下命令刷新一下权限:<br><figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">mysql&gt; flush privileges;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>至此我们已经完成了配置远程访问数据的所有操作，我们在看一下当前能访问我们数据库的用户:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">mysql&gt; select User,Host from user;</span><br><span class=\"line\">+------------------+-----------+</span><br><span class=\"line\">| User             | Host      |</span><br><span class=\"line\">+------------------+-----------+</span><br><span class=\"line\">| root             | %         |</span><br><span class=\"line\">| debian-sys-maint | localhost |</span><br><span class=\"line\">| mysql.session    | localhost |</span><br><span class=\"line\">| mysql.sys        | localhost |</span><br><span class=\"line\">| root             | localhost |</span><br><span class=\"line\">+------------------+-----------+</span><br><span class=\"line\">5 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Python 字符串操作","date":"2016-05-17T09:39:17.000Z","updated":"2017-12-01T02:20:17.000Z","_content":"\n字符串是 Python 中最常用的数据类型。我们可以使用引号('或\")来创建字符串。\n创建字符串很简单，只要为变量分配一个值即可。例如：\n```\ns = \"Hello World\"\nprint s  # Hello World\n```\n### 1. 大小写转换函数\n\n大小写转换函数返回原始字符串s的一个副本：\n\n函数|说明\n---|---\nlower()|将所有字符转换为小写\nupper()|将所有字符转换为大写\ncapitalize()|将第一个字符转换为大写，同时将其他所有字符转换为小写\n\n这些函数不会影响非字母字符。大小写转换函数是规范化的一个重要元素。\n\nExample:\n```Python\ns = \"Hello World\"\nprint s  # Hello World\n\n# 转为大写\nus = s.upper()\nprint us  # HELLO WORLD\n\n# 转为小写\nls = s.lower()\nprint ls  # hello world\n\n# 首字母大写 其余小写\ncs = s.capitalize()\nprint cs  # Hello world\n```\n\n### 2. 判定函数\n\n判断函数根据字符串s是否属于适当的类而返回True或False：\n\n函数|说明\n---|---\nislower()|检查所有字母字符是否为小写\nisupper()|检查所有字母字符是否为大写\nisspace()|检查所有字符是否为空格\nisdigit()|检查所有字符是否为范围0～9中的十进制数字\nisalpha()|检查所有字符是否为a～z或A～Z范围内的字母字符\n\n使用这些函数，你可以识别有效的单词、非负整数、标点符号等。\n\nExample:\n```Python\n# 是否为大写\nius = \"HELLO\".isupper()\nprint ius  # True\n\n# 是否为小写\nils = \"hello\".islower()\nprint ils  # True\n\n# 是否为空格\niss = \" \".isspace()\nprint iss  # True\n\n# 是否为范围0～9中的十进制数字\nids = \"232\".isdigit()\nprint ids  # True\n\n# 是否为a～z或A～Z范围内的字母字符\nias = \"a2\".isalpha()\nprint ias  # False\n```\n\n### 3. 解码函数\n\nPython有时会将字符串数据表示为原始的二进制数组，而非字符串，尤其是当数据来自外部源（外部文件、数据库或Web）时。Python使用符号b来标识二进制数组。例如:\n```Python\n# 二进制数组\nbin = b\"Hello\"\n# 字符串  \ns = \"Hello\"\nprint bin[0]\nprint s[0]\n```\n`s[0]`和`bin[0]`分别是'H'和72，其中72是字符'H'的ASCII码。\n\n解码函数将二进制数组转换为字符串或反之：\n\n函数|说明\n---|---\ndecode()|将二进制数组转换为字符串\nencode()|将字符串转换为二进制数组\n\n许多Python函数都需要将二进制数据转换为字符串，然后再做处理。\n\n### 4. 去除空白函数\n\n字符串处理的第一步是去除不需要的空白(包括换行符和制表符)。\n\n函数|说明\n---|---\nlstrip()|left strip 在字符串的开始处删除所有空格\nrstrip()|right strip 在字符串的结束处删除所有空格\nstrip()|对整个字符串删除所有空格(不删除字符串内部空格)\n\n经过这些删除操作后，得到的可能会是一个空字符串！\n\nExample:\n```Python\nls = \" Hello world \".lstrip()\nprint ls + \",\" + str(len(ls))  # Hello world ,12\n\nrs = \" Hello World \".rstrip()\nprint rs + \",\" + str(len(rs))  #  Hello World,12\n\nss = \" Hello World \".strip()\nprint ss + \",\" + str(len(ss))  # Hello World,1\n```\n### 5. 分割函数\n\n字符串通常包含多个标记符，用空格、冒号和逗号这样的分隔符分割。函数split(delim='')使用delim作为分隔符，将字符串s分割为子字符串组成的一个列表。如果未指定分隔符，Python会使用空白字符来分割字符串，并将所有连续的空白合并：\n```Python\nss = \"Hello World\".split()\nprint ss  # ['Hello', 'World']\n\nss = \"Hello,World\".split(\",\")\nprint ss  # ['Hello', 'World']\n```\n\n### 6. 连接函数\n\n连接函数join(ls)，将字符串列表ls连接在一起，形成一个字符串，并使用特定的对象字符串作为连接符：\n```Python\ns = \",\".join(\"b\")\nprint s  # b\n\ns = \",\".join([\"a\", \"b\", \"c\", \"d\"])\nprint s  # a,b,c,d\n```\n\n备注:\n```\njoin()函数仅在字符串之间插入连接符，而在第一个字符串前或最后一个字符串后都不插入连接符。\n```\n\n### 7. 查找函数\n\nfind(needle)函数返回对象字符串中子字符串needle第一次出现的索引值(下标从0开始)，当子字符串不存在时，返回-1。该函数区分大小写。\n```\nindex = \"Hello World\".find(\"o\")\nprint index  # 4\n```\n\n\n\n来自于<Python数据科学入门>\n","source":"_posts/Python/Python 字符串操作.md","raw":"---\ntitle: Python 字符串操作\ndate: 2016-05-17 17:39:17\nupdated: 2017-12-01 10:20:17\ntags:\n- Python\n\ncategories: Python\n---\n\n字符串是 Python 中最常用的数据类型。我们可以使用引号('或\")来创建字符串。\n创建字符串很简单，只要为变量分配一个值即可。例如：\n```\ns = \"Hello World\"\nprint s  # Hello World\n```\n### 1. 大小写转换函数\n\n大小写转换函数返回原始字符串s的一个副本：\n\n函数|说明\n---|---\nlower()|将所有字符转换为小写\nupper()|将所有字符转换为大写\ncapitalize()|将第一个字符转换为大写，同时将其他所有字符转换为小写\n\n这些函数不会影响非字母字符。大小写转换函数是规范化的一个重要元素。\n\nExample:\n```Python\ns = \"Hello World\"\nprint s  # Hello World\n\n# 转为大写\nus = s.upper()\nprint us  # HELLO WORLD\n\n# 转为小写\nls = s.lower()\nprint ls  # hello world\n\n# 首字母大写 其余小写\ncs = s.capitalize()\nprint cs  # Hello world\n```\n\n### 2. 判定函数\n\n判断函数根据字符串s是否属于适当的类而返回True或False：\n\n函数|说明\n---|---\nislower()|检查所有字母字符是否为小写\nisupper()|检查所有字母字符是否为大写\nisspace()|检查所有字符是否为空格\nisdigit()|检查所有字符是否为范围0～9中的十进制数字\nisalpha()|检查所有字符是否为a～z或A～Z范围内的字母字符\n\n使用这些函数，你可以识别有效的单词、非负整数、标点符号等。\n\nExample:\n```Python\n# 是否为大写\nius = \"HELLO\".isupper()\nprint ius  # True\n\n# 是否为小写\nils = \"hello\".islower()\nprint ils  # True\n\n# 是否为空格\niss = \" \".isspace()\nprint iss  # True\n\n# 是否为范围0～9中的十进制数字\nids = \"232\".isdigit()\nprint ids  # True\n\n# 是否为a～z或A～Z范围内的字母字符\nias = \"a2\".isalpha()\nprint ias  # False\n```\n\n### 3. 解码函数\n\nPython有时会将字符串数据表示为原始的二进制数组，而非字符串，尤其是当数据来自外部源（外部文件、数据库或Web）时。Python使用符号b来标识二进制数组。例如:\n```Python\n# 二进制数组\nbin = b\"Hello\"\n# 字符串  \ns = \"Hello\"\nprint bin[0]\nprint s[0]\n```\n`s[0]`和`bin[0]`分别是'H'和72，其中72是字符'H'的ASCII码。\n\n解码函数将二进制数组转换为字符串或反之：\n\n函数|说明\n---|---\ndecode()|将二进制数组转换为字符串\nencode()|将字符串转换为二进制数组\n\n许多Python函数都需要将二进制数据转换为字符串，然后再做处理。\n\n### 4. 去除空白函数\n\n字符串处理的第一步是去除不需要的空白(包括换行符和制表符)。\n\n函数|说明\n---|---\nlstrip()|left strip 在字符串的开始处删除所有空格\nrstrip()|right strip 在字符串的结束处删除所有空格\nstrip()|对整个字符串删除所有空格(不删除字符串内部空格)\n\n经过这些删除操作后，得到的可能会是一个空字符串！\n\nExample:\n```Python\nls = \" Hello world \".lstrip()\nprint ls + \",\" + str(len(ls))  # Hello world ,12\n\nrs = \" Hello World \".rstrip()\nprint rs + \",\" + str(len(rs))  #  Hello World,12\n\nss = \" Hello World \".strip()\nprint ss + \",\" + str(len(ss))  # Hello World,1\n```\n### 5. 分割函数\n\n字符串通常包含多个标记符，用空格、冒号和逗号这样的分隔符分割。函数split(delim='')使用delim作为分隔符，将字符串s分割为子字符串组成的一个列表。如果未指定分隔符，Python会使用空白字符来分割字符串，并将所有连续的空白合并：\n```Python\nss = \"Hello World\".split()\nprint ss  # ['Hello', 'World']\n\nss = \"Hello,World\".split(\",\")\nprint ss  # ['Hello', 'World']\n```\n\n### 6. 连接函数\n\n连接函数join(ls)，将字符串列表ls连接在一起，形成一个字符串，并使用特定的对象字符串作为连接符：\n```Python\ns = \",\".join(\"b\")\nprint s  # b\n\ns = \",\".join([\"a\", \"b\", \"c\", \"d\"])\nprint s  # a,b,c,d\n```\n\n备注:\n```\njoin()函数仅在字符串之间插入连接符，而在第一个字符串前或最后一个字符串后都不插入连接符。\n```\n\n### 7. 查找函数\n\nfind(needle)函数返回对象字符串中子字符串needle第一次出现的索引值(下标从0开始)，当子字符串不存在时，返回-1。该函数区分大小写。\n```\nindex = \"Hello World\".find(\"o\")\nprint index  # 4\n```\n\n\n\n来自于<Python数据科学入门>\n","slug":"Python/Python 字符串操作","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cje58tiu9004iordbjhpc5kqp","content":"<p>字符串是 Python 中最常用的数据类型。我们可以使用引号(‘或”)来创建字符串。<br>创建字符串很简单，只要为变量分配一个值即可。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">s = &quot;Hello World&quot;</span><br><span class=\"line\">print s  # Hello World</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-大小写转换函数\"><a href=\"#1-大小写转换函数\" class=\"headerlink\" title=\"1. 大小写转换函数\"></a>1. 大小写转换函数</h3><p>大小写转换函数返回原始字符串s的一个副本：</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>lower()</td>\n<td>将所有字符转换为小写</td>\n</tr>\n<tr>\n<td>upper()</td>\n<td>将所有字符转换为大写</td>\n</tr>\n<tr>\n<td>capitalize()</td>\n<td>将第一个字符转换为大写，同时将其他所有字符转换为小写</td>\n</tr>\n</tbody>\n</table>\n<p>这些函数不会影响非字母字符。大小写转换函数是规范化的一个重要元素。</p>\n<p>Example:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">s = <span class=\"string\">\"Hello World\"</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># Hello World</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 转为大写</span></span><br><span class=\"line\">us = s.upper()</span><br><span class=\"line\"><span class=\"keyword\">print</span> us  <span class=\"comment\"># HELLO WORLD</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 转为小写</span></span><br><span class=\"line\">ls = s.lower()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ls  <span class=\"comment\"># hello world</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 首字母大写 其余小写</span></span><br><span class=\"line\">cs = s.capitalize()</span><br><span class=\"line\"><span class=\"keyword\">print</span> cs  <span class=\"comment\"># Hello world</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-判定函数\"><a href=\"#2-判定函数\" class=\"headerlink\" title=\"2. 判定函数\"></a>2. 判定函数</h3><p>判断函数根据字符串s是否属于适当的类而返回True或False：</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>islower()</td>\n<td>检查所有字母字符是否为小写</td>\n</tr>\n<tr>\n<td>isupper()</td>\n<td>检查所有字母字符是否为大写</td>\n</tr>\n<tr>\n<td>isspace()</td>\n<td>检查所有字符是否为空格</td>\n</tr>\n<tr>\n<td>isdigit()</td>\n<td>检查所有字符是否为范围0～9中的十进制数字</td>\n</tr>\n<tr>\n<td>isalpha()</td>\n<td>检查所有字符是否为a～z或A～Z范围内的字母字符</td>\n</tr>\n</tbody>\n</table>\n<p>使用这些函数，你可以识别有效的单词、非负整数、标点符号等。</p>\n<p>Example:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 是否为大写</span></span><br><span class=\"line\">ius = <span class=\"string\">\"HELLO\"</span>.isupper()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ius  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为小写</span></span><br><span class=\"line\">ils = <span class=\"string\">\"hello\"</span>.islower()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ils  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为空格</span></span><br><span class=\"line\">iss = <span class=\"string\">\" \"</span>.isspace()</span><br><span class=\"line\"><span class=\"keyword\">print</span> iss  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为范围0～9中的十进制数字</span></span><br><span class=\"line\">ids = <span class=\"string\">\"232\"</span>.isdigit()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ids  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为a～z或A～Z范围内的字母字符</span></span><br><span class=\"line\">ias = <span class=\"string\">\"a2\"</span>.isalpha()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ias  <span class=\"comment\"># False</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-解码函数\"><a href=\"#3-解码函数\" class=\"headerlink\" title=\"3. 解码函数\"></a>3. 解码函数</h3><p>Python有时会将字符串数据表示为原始的二进制数组，而非字符串，尤其是当数据来自外部源（外部文件、数据库或Web）时。Python使用符号b来标识二进制数组。例如:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 二进制数组</span></span><br><span class=\"line\">bin = <span class=\"string\">b\"Hello\"</span></span><br><span class=\"line\"><span class=\"comment\"># 字符串  </span></span><br><span class=\"line\">s = <span class=\"string\">\"Hello\"</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> bin[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"keyword\">print</span> s[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></p>\n<p><code>s[0]</code>和<code>bin[0]</code>分别是’H’和72，其中72是字符’H’的ASCII码。</p>\n<p>解码函数将二进制数组转换为字符串或反之：</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>decode()</td>\n<td>将二进制数组转换为字符串</td>\n</tr>\n<tr>\n<td>encode()</td>\n<td>将字符串转换为二进制数组</td>\n</tr>\n</tbody>\n</table>\n<p>许多Python函数都需要将二进制数据转换为字符串，然后再做处理。</p>\n<h3 id=\"4-去除空白函数\"><a href=\"#4-去除空白函数\" class=\"headerlink\" title=\"4. 去除空白函数\"></a>4. 去除空白函数</h3><p>字符串处理的第一步是去除不需要的空白(包括换行符和制表符)。</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>lstrip()</td>\n<td>left strip 在字符串的开始处删除所有空格</td>\n</tr>\n<tr>\n<td>rstrip()</td>\n<td>right strip 在字符串的结束处删除所有空格</td>\n</tr>\n<tr>\n<td>strip()</td>\n<td>对整个字符串删除所有空格(不删除字符串内部空格)</td>\n</tr>\n</tbody>\n</table>\n<p>经过这些删除操作后，得到的可能会是一个空字符串！</p>\n<p>Example:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ls = <span class=\"string\">\" Hello world \"</span>.lstrip()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ls + <span class=\"string\">\",\"</span> + str(len(ls))  <span class=\"comment\"># Hello world ,12</span></span><br><span class=\"line\"></span><br><span class=\"line\">rs = <span class=\"string\">\" Hello World \"</span>.rstrip()</span><br><span class=\"line\"><span class=\"keyword\">print</span> rs + <span class=\"string\">\",\"</span> + str(len(rs))  <span class=\"comment\">#  Hello World,12</span></span><br><span class=\"line\"></span><br><span class=\"line\">ss = <span class=\"string\">\" Hello World \"</span>.strip()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ss + <span class=\"string\">\",\"</span> + str(len(ss))  <span class=\"comment\"># Hello World,1</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-分割函数\"><a href=\"#5-分割函数\" class=\"headerlink\" title=\"5. 分割函数\"></a>5. 分割函数</h3><p>字符串通常包含多个标记符，用空格、冒号和逗号这样的分隔符分割。函数split(delim=’’)使用delim作为分隔符，将字符串s分割为子字符串组成的一个列表。如果未指定分隔符，Python会使用空白字符来分割字符串，并将所有连续的空白合并：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ss = <span class=\"string\">\"Hello World\"</span>.split()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ss  <span class=\"comment\"># ['Hello', 'World']</span></span><br><span class=\"line\"></span><br><span class=\"line\">ss = <span class=\"string\">\"Hello,World\"</span>.split(<span class=\"string\">\",\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> ss  <span class=\"comment\"># ['Hello', 'World']</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-连接函数\"><a href=\"#6-连接函数\" class=\"headerlink\" title=\"6. 连接函数\"></a>6. 连接函数</h3><p>连接函数join(ls)，将字符串列表ls连接在一起，形成一个字符串，并使用特定的对象字符串作为连接符：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">s = <span class=\"string\">\",\"</span>.join(<span class=\"string\">\"b\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># b</span></span><br><span class=\"line\"></span><br><span class=\"line\">s = <span class=\"string\">\",\"</span>.join([<span class=\"string\">\"a\"</span>, <span class=\"string\">\"b\"</span>, <span class=\"string\">\"c\"</span>, <span class=\"string\">\"d\"</span>])</span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># a,b,c,d</span></span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">join()函数仅在字符串之间插入连接符，而在第一个字符串前或最后一个字符串后都不插入连接符。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-查找函数\"><a href=\"#7-查找函数\" class=\"headerlink\" title=\"7. 查找函数\"></a>7. 查找函数</h3><p>find(needle)函数返回对象字符串中子字符串needle第一次出现的索引值(下标从0开始)，当子字符串不存在时，返回-1。该函数区分大小写。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">index = &quot;Hello World&quot;.find(&quot;o&quot;)</span><br><span class=\"line\">print index  # 4</span><br></pre></td></tr></table></figure></p>\n<p>来自于<python数据科学入门></python数据科学入门></p>\n","site":{"data":{}},"excerpt":"","more":"<p>字符串是 Python 中最常用的数据类型。我们可以使用引号(‘或”)来创建字符串。<br>创建字符串很简单，只要为变量分配一个值即可。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">s = &quot;Hello World&quot;</span><br><span class=\"line\">print s  # Hello World</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-大小写转换函数\"><a href=\"#1-大小写转换函数\" class=\"headerlink\" title=\"1. 大小写转换函数\"></a>1. 大小写转换函数</h3><p>大小写转换函数返回原始字符串s的一个副本：</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>lower()</td>\n<td>将所有字符转换为小写</td>\n</tr>\n<tr>\n<td>upper()</td>\n<td>将所有字符转换为大写</td>\n</tr>\n<tr>\n<td>capitalize()</td>\n<td>将第一个字符转换为大写，同时将其他所有字符转换为小写</td>\n</tr>\n</tbody>\n</table>\n<p>这些函数不会影响非字母字符。大小写转换函数是规范化的一个重要元素。</p>\n<p>Example:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">s = <span class=\"string\">\"Hello World\"</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># Hello World</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 转为大写</span></span><br><span class=\"line\">us = s.upper()</span><br><span class=\"line\"><span class=\"keyword\">print</span> us  <span class=\"comment\"># HELLO WORLD</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 转为小写</span></span><br><span class=\"line\">ls = s.lower()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ls  <span class=\"comment\"># hello world</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 首字母大写 其余小写</span></span><br><span class=\"line\">cs = s.capitalize()</span><br><span class=\"line\"><span class=\"keyword\">print</span> cs  <span class=\"comment\"># Hello world</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-判定函数\"><a href=\"#2-判定函数\" class=\"headerlink\" title=\"2. 判定函数\"></a>2. 判定函数</h3><p>判断函数根据字符串s是否属于适当的类而返回True或False：</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>islower()</td>\n<td>检查所有字母字符是否为小写</td>\n</tr>\n<tr>\n<td>isupper()</td>\n<td>检查所有字母字符是否为大写</td>\n</tr>\n<tr>\n<td>isspace()</td>\n<td>检查所有字符是否为空格</td>\n</tr>\n<tr>\n<td>isdigit()</td>\n<td>检查所有字符是否为范围0～9中的十进制数字</td>\n</tr>\n<tr>\n<td>isalpha()</td>\n<td>检查所有字符是否为a～z或A～Z范围内的字母字符</td>\n</tr>\n</tbody>\n</table>\n<p>使用这些函数，你可以识别有效的单词、非负整数、标点符号等。</p>\n<p>Example:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 是否为大写</span></span><br><span class=\"line\">ius = <span class=\"string\">\"HELLO\"</span>.isupper()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ius  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为小写</span></span><br><span class=\"line\">ils = <span class=\"string\">\"hello\"</span>.islower()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ils  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为空格</span></span><br><span class=\"line\">iss = <span class=\"string\">\" \"</span>.isspace()</span><br><span class=\"line\"><span class=\"keyword\">print</span> iss  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为范围0～9中的十进制数字</span></span><br><span class=\"line\">ids = <span class=\"string\">\"232\"</span>.isdigit()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ids  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否为a～z或A～Z范围内的字母字符</span></span><br><span class=\"line\">ias = <span class=\"string\">\"a2\"</span>.isalpha()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ias  <span class=\"comment\"># False</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-解码函数\"><a href=\"#3-解码函数\" class=\"headerlink\" title=\"3. 解码函数\"></a>3. 解码函数</h3><p>Python有时会将字符串数据表示为原始的二进制数组，而非字符串，尤其是当数据来自外部源（外部文件、数据库或Web）时。Python使用符号b来标识二进制数组。例如:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 二进制数组</span></span><br><span class=\"line\">bin = <span class=\"string\">b\"Hello\"</span></span><br><span class=\"line\"><span class=\"comment\"># 字符串  </span></span><br><span class=\"line\">s = <span class=\"string\">\"Hello\"</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> bin[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"keyword\">print</span> s[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure></p>\n<p><code>s[0]</code>和<code>bin[0]</code>分别是’H’和72，其中72是字符’H’的ASCII码。</p>\n<p>解码函数将二进制数组转换为字符串或反之：</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>decode()</td>\n<td>将二进制数组转换为字符串</td>\n</tr>\n<tr>\n<td>encode()</td>\n<td>将字符串转换为二进制数组</td>\n</tr>\n</tbody>\n</table>\n<p>许多Python函数都需要将二进制数据转换为字符串，然后再做处理。</p>\n<h3 id=\"4-去除空白函数\"><a href=\"#4-去除空白函数\" class=\"headerlink\" title=\"4. 去除空白函数\"></a>4. 去除空白函数</h3><p>字符串处理的第一步是去除不需要的空白(包括换行符和制表符)。</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>lstrip()</td>\n<td>left strip 在字符串的开始处删除所有空格</td>\n</tr>\n<tr>\n<td>rstrip()</td>\n<td>right strip 在字符串的结束处删除所有空格</td>\n</tr>\n<tr>\n<td>strip()</td>\n<td>对整个字符串删除所有空格(不删除字符串内部空格)</td>\n</tr>\n</tbody>\n</table>\n<p>经过这些删除操作后，得到的可能会是一个空字符串！</p>\n<p>Example:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ls = <span class=\"string\">\" Hello world \"</span>.lstrip()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ls + <span class=\"string\">\",\"</span> + str(len(ls))  <span class=\"comment\"># Hello world ,12</span></span><br><span class=\"line\"></span><br><span class=\"line\">rs = <span class=\"string\">\" Hello World \"</span>.rstrip()</span><br><span class=\"line\"><span class=\"keyword\">print</span> rs + <span class=\"string\">\",\"</span> + str(len(rs))  <span class=\"comment\">#  Hello World,12</span></span><br><span class=\"line\"></span><br><span class=\"line\">ss = <span class=\"string\">\" Hello World \"</span>.strip()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ss + <span class=\"string\">\",\"</span> + str(len(ss))  <span class=\"comment\"># Hello World,1</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-分割函数\"><a href=\"#5-分割函数\" class=\"headerlink\" title=\"5. 分割函数\"></a>5. 分割函数</h3><p>字符串通常包含多个标记符，用空格、冒号和逗号这样的分隔符分割。函数split(delim=’’)使用delim作为分隔符，将字符串s分割为子字符串组成的一个列表。如果未指定分隔符，Python会使用空白字符来分割字符串，并将所有连续的空白合并：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ss = <span class=\"string\">\"Hello World\"</span>.split()</span><br><span class=\"line\"><span class=\"keyword\">print</span> ss  <span class=\"comment\"># ['Hello', 'World']</span></span><br><span class=\"line\"></span><br><span class=\"line\">ss = <span class=\"string\">\"Hello,World\"</span>.split(<span class=\"string\">\",\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> ss  <span class=\"comment\"># ['Hello', 'World']</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-连接函数\"><a href=\"#6-连接函数\" class=\"headerlink\" title=\"6. 连接函数\"></a>6. 连接函数</h3><p>连接函数join(ls)，将字符串列表ls连接在一起，形成一个字符串，并使用特定的对象字符串作为连接符：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">s = <span class=\"string\">\",\"</span>.join(<span class=\"string\">\"b\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># b</span></span><br><span class=\"line\"></span><br><span class=\"line\">s = <span class=\"string\">\",\"</span>.join([<span class=\"string\">\"a\"</span>, <span class=\"string\">\"b\"</span>, <span class=\"string\">\"c\"</span>, <span class=\"string\">\"d\"</span>])</span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># a,b,c,d</span></span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">join()函数仅在字符串之间插入连接符，而在第一个字符串前或最后一个字符串后都不插入连接符。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-查找函数\"><a href=\"#7-查找函数\" class=\"headerlink\" title=\"7. 查找函数\"></a>7. 查找函数</h3><p>find(needle)函数返回对象字符串中子字符串needle第一次出现的索引值(下标从0开始)，当子字符串不存在时，返回-1。该函数区分大小写。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">index = &quot;Hello World&quot;.find(&quot;o&quot;)</span><br><span class=\"line\">print index  # 4</span><br></pre></td></tr></table></figure></p>\n<p>来自于<python数据科学入门></python数据科学入门></p>\n"},{"layout":"post","author":"酷壳陈皓","title":"Cache 缓存更新策略","date":"2018-02-03T02:40:01.000Z","_content":"\n看到好些人在写更新缓存数据代码时，`先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中`。然而，这个是逻辑是 `错误` 的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。\n\n我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新策略。\n\n这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。\n\n更新缓存的的策略有四种：\n- `Cache aside`\n- `Read through`\n- `Write through`\n- `Write behind caching`\n\n我们下面一一来看一下这四种策略。\n\n### 1. Cache aside\n\n这是最常用最常用的策略了。其具体逻辑如下：\n- 失效：应用程序先从 `cache` 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。\n- 命中：应用程序从 `cache` 中取数据，取到后返回。\n- 更新：先把数据存到数据库中，成功后，再让缓存失效。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-1.png?raw=true)\n\n注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。\n\n一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。\n\n这是标准的策略，包括Facebook的论文[《Scaling Memcache at Facebook》](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答[《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》](https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend)，主要是怕两个并发的写操作导致脏数据。\n\n那么，是不是 `Cache Aside` 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。\n\n但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。\n\n所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。\n\n### 2. Read/Write Through\n\n我们可以看到，在上面的 `Cache Aside` 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（`Cache`），一个是数据库（`Repository`）。所以，应用程序比较啰嗦。而 `Read/Write Through` 套路是把更新数据库（`Repository`）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 `Cache`。\n\n#### 2.1 Read Through\n\n`Read Through` 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），`Cache Aside` 是由调用方负责把数据加载入缓存，而 `Read Through` 则用缓存服务自己来加载，从而对应用方是透明的。\n\n#### 2.2 Write Through\n\n`Write Through` 套路和 `Read Through` 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 `Cache` 自己更新数据库（这是一个同步操作）\n\n下图自来 Wikipedia 的 [Cache](https://en.wikipedia.org/wiki/Cache_(computing)) 词条。其中的 `Memory` 你可以理解为就是我们例子里的数据库。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-2.png?raw=true)\n\n### 3. Write Behind Caching\n\n`Write Behind` 又叫 `Write Back`。一些了解 `Linux` 操作系统内核的同学对 `write back` 应该非常熟悉，这不就是 `Linux` 文件系统的 `Page Cache` 的算法吗？ 是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。\n\n`Write Back` 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，`write back` 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。\n\n但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 `Unix/Linux` 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 `Trade-Off`。\n\n另外，`Write Back` 实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的 `write back` 会在仅当这个 `cache` 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 `lazy write`。\n\n在 wikipedia 上有一张 `write back` 的流程图，基本逻辑如下：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-3.png?raw=true)\n\n\n转载于： https://coolshell.cn/articles/17416.html\n","source":"_posts/Other/[Cache]Cache 缓存更新策略.md","raw":"---\nlayout: post\nauthor: 酷壳陈皓\ntitle: Cache 缓存更新策略\ndate: 2018-02-03 10:40:01\ntags:\n  - CaChe\n\ncategories: other\npermalink: cache-update-policy\n---\n\n看到好些人在写更新缓存数据代码时，`先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中`。然而，这个是逻辑是 `错误` 的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。\n\n我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新策略。\n\n这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。\n\n更新缓存的的策略有四种：\n- `Cache aside`\n- `Read through`\n- `Write through`\n- `Write behind caching`\n\n我们下面一一来看一下这四种策略。\n\n### 1. Cache aside\n\n这是最常用最常用的策略了。其具体逻辑如下：\n- 失效：应用程序先从 `cache` 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。\n- 命中：应用程序从 `cache` 中取数据，取到后返回。\n- 更新：先把数据存到数据库中，成功后，再让缓存失效。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-1.png?raw=true)\n\n注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。\n\n一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。\n\n这是标准的策略，包括Facebook的论文[《Scaling Memcache at Facebook》](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答[《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》](https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend)，主要是怕两个并发的写操作导致脏数据。\n\n那么，是不是 `Cache Aside` 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。\n\n但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。\n\n所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。\n\n### 2. Read/Write Through\n\n我们可以看到，在上面的 `Cache Aside` 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（`Cache`），一个是数据库（`Repository`）。所以，应用程序比较啰嗦。而 `Read/Write Through` 套路是把更新数据库（`Repository`）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 `Cache`。\n\n#### 2.1 Read Through\n\n`Read Through` 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），`Cache Aside` 是由调用方负责把数据加载入缓存，而 `Read Through` 则用缓存服务自己来加载，从而对应用方是透明的。\n\n#### 2.2 Write Through\n\n`Write Through` 套路和 `Read Through` 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 `Cache` 自己更新数据库（这是一个同步操作）\n\n下图自来 Wikipedia 的 [Cache](https://en.wikipedia.org/wiki/Cache_(computing)) 词条。其中的 `Memory` 你可以理解为就是我们例子里的数据库。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-2.png?raw=true)\n\n### 3. Write Behind Caching\n\n`Write Behind` 又叫 `Write Back`。一些了解 `Linux` 操作系统内核的同学对 `write back` 应该非常熟悉，这不就是 `Linux` 文件系统的 `Page Cache` 的算法吗？ 是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。\n\n`Write Back` 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，`write back` 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。\n\n但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 `Unix/Linux` 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 `Trade-Off`。\n\n另外，`Write Back` 实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的 `write back` 会在仅当这个 `cache` 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 `lazy write`。\n\n在 wikipedia 上有一张 `write back` 的流程图，基本逻辑如下：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-3.png?raw=true)\n\n\n转载于： https://coolshell.cn/articles/17416.html\n","slug":"cache-update-policy","published":1,"updated":"2018-02-06T02:16:09.857Z","comments":1,"photos":[],"link":"","_id":"cje58tiud004nordbi7g4ofng","content":"<p>看到好些人在写更新缓存数据代码时，<code>先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中</code>。然而，这个是逻辑是 <code>错误</code> 的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。</p>\n<p>我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新策略。</p>\n<p>这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。</p>\n<p>更新缓存的的策略有四种：</p>\n<ul>\n<li><code>Cache aside</code></li>\n<li><code>Read through</code></li>\n<li><code>Write through</code></li>\n<li><code>Write behind caching</code></li>\n</ul>\n<p>我们下面一一来看一下这四种策略。</p>\n<h3 id=\"1-Cache-aside\"><a href=\"#1-Cache-aside\" class=\"headerlink\" title=\"1. Cache aside\"></a>1. Cache aside</h3><p>这是最常用最常用的策略了。其具体逻辑如下：</p>\n<ul>\n<li>失效：应用程序先从 <code>cache</code> 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。</li>\n<li>命中：应用程序从 <code>cache</code> 中取数据，取到后返回。</li>\n<li>更新：先把数据存到数据库中，成功后，再让缓存失效。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-1.png?raw=true\" alt=\"\"></p>\n<p>注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。</p>\n<p>一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。</p>\n<p>这是标准的策略，包括Facebook的论文<a href=\"https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf\" target=\"_blank\" rel=\"noopener\">《Scaling Memcache at Facebook》</a>也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答<a href=\"https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend\" target=\"_blank\" rel=\"noopener\">《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》</a>，主要是怕两个并发的写操作导致脏数据。</p>\n<p>那么，是不是 <code>Cache Aside</code> 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。</p>\n<p>但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。</p>\n<p>所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。</p>\n<h3 id=\"2-Read-Write-Through\"><a href=\"#2-Read-Write-Through\" class=\"headerlink\" title=\"2. Read/Write Through\"></a>2. Read/Write Through</h3><p>我们可以看到，在上面的 <code>Cache Aside</code> 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（<code>Cache</code>），一个是数据库（<code>Repository</code>）。所以，应用程序比较啰嗦。而 <code>Read/Write Through</code> 套路是把更新数据库（<code>Repository</code>）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 <code>Cache</code>。</p>\n<h4 id=\"2-1-Read-Through\"><a href=\"#2-1-Read-Through\" class=\"headerlink\" title=\"2.1 Read Through\"></a>2.1 Read Through</h4><p><code>Read Through</code> 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），<code>Cache Aside</code> 是由调用方负责把数据加载入缓存，而 <code>Read Through</code> 则用缓存服务自己来加载，从而对应用方是透明的。</p>\n<h4 id=\"2-2-Write-Through\"><a href=\"#2-2-Write-Through\" class=\"headerlink\" title=\"2.2 Write Through\"></a>2.2 Write Through</h4><p><code>Write Through</code> 套路和 <code>Read Through</code> 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 <code>Cache</code> 自己更新数据库（这是一个同步操作）</p>\n<p>下图自来 Wikipedia 的 <a href=\"https://en.wikipedia.org/wiki/Cache_(computing\" target=\"_blank\" rel=\"noopener\">Cache</a>) 词条。其中的 <code>Memory</code> 你可以理解为就是我们例子里的数据库。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-Write-Behind-Caching\"><a href=\"#3-Write-Behind-Caching\" class=\"headerlink\" title=\"3. Write Behind Caching\"></a>3. Write Behind Caching</h3><p><code>Write Behind</code> 又叫 <code>Write Back</code>。一些了解 <code>Linux</code> 操作系统内核的同学对 <code>write back</code> 应该非常熟悉，这不就是 <code>Linux</code> 文件系统的 <code>Page Cache</code> 的算法吗？ 是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。</p>\n<p><code>Write Back</code> 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，<code>write back</code> 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。</p>\n<p>但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 <code>Unix/Linux</code> 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 <code>Trade-Off</code>。</p>\n<p>另外，<code>Write Back</code> 实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的 <code>write back</code> 会在仅当这个 <code>cache</code> 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 <code>lazy write</code>。</p>\n<p>在 wikipedia 上有一张 <code>write back</code> 的流程图，基本逻辑如下：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-3.png?raw=true\" alt=\"\"></p>\n<p>转载于： <a href=\"https://coolshell.cn/articles/17416.html\" target=\"_blank\" rel=\"noopener\">https://coolshell.cn/articles/17416.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>看到好些人在写更新缓存数据代码时，<code>先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中</code>。然而，这个是逻辑是 <code>错误</code> 的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。</p>\n<p>我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新策略。</p>\n<p>这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。</p>\n<p>更新缓存的的策略有四种：</p>\n<ul>\n<li><code>Cache aside</code></li>\n<li><code>Read through</code></li>\n<li><code>Write through</code></li>\n<li><code>Write behind caching</code></li>\n</ul>\n<p>我们下面一一来看一下这四种策略。</p>\n<h3 id=\"1-Cache-aside\"><a href=\"#1-Cache-aside\" class=\"headerlink\" title=\"1. Cache aside\"></a>1. Cache aside</h3><p>这是最常用最常用的策略了。其具体逻辑如下：</p>\n<ul>\n<li>失效：应用程序先从 <code>cache</code> 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。</li>\n<li>命中：应用程序从 <code>cache</code> 中取数据，取到后返回。</li>\n<li>更新：先把数据存到数据库中，成功后，再让缓存失效。</li>\n</ul>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-1.png?raw=true\" alt=\"\"></p>\n<p>注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。</p>\n<p>一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。</p>\n<p>这是标准的策略，包括Facebook的论文<a href=\"https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf\" target=\"_blank\" rel=\"noopener\">《Scaling Memcache at Facebook》</a>也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答<a href=\"https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend\" target=\"_blank\" rel=\"noopener\">《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》</a>，主要是怕两个并发的写操作导致脏数据。</p>\n<p>那么，是不是 <code>Cache Aside</code> 这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。</p>\n<p>但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。</p>\n<p>所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。</p>\n<h3 id=\"2-Read-Write-Through\"><a href=\"#2-Read-Write-Through\" class=\"headerlink\" title=\"2. Read/Write Through\"></a>2. Read/Write Through</h3><p>我们可以看到，在上面的 <code>Cache Aside</code> 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（<code>Cache</code>），一个是数据库（<code>Repository</code>）。所以，应用程序比较啰嗦。而 <code>Read/Write Through</code> 套路是把更新数据库（<code>Repository</code>）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的 <code>Cache</code>。</p>\n<h4 id=\"2-1-Read-Through\"><a href=\"#2-1-Read-Through\" class=\"headerlink\" title=\"2.1 Read Through\"></a>2.1 Read Through</h4><p><code>Read Through</code> 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），<code>Cache Aside</code> 是由调用方负责把数据加载入缓存，而 <code>Read Through</code> 则用缓存服务自己来加载，从而对应用方是透明的。</p>\n<h4 id=\"2-2-Write-Through\"><a href=\"#2-2-Write-Through\" class=\"headerlink\" title=\"2.2 Write Through\"></a>2.2 Write Through</h4><p><code>Write Through</code> 套路和 <code>Read Through</code> 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 <code>Cache</code> 自己更新数据库（这是一个同步操作）</p>\n<p>下图自来 Wikipedia 的 <a href=\"https://en.wikipedia.org/wiki/Cache_(computing\" target=\"_blank\" rel=\"noopener\">Cache</a>) 词条。其中的 <code>Memory</code> 你可以理解为就是我们例子里的数据库。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-Write-Behind-Caching\"><a href=\"#3-Write-Behind-Caching\" class=\"headerlink\" title=\"3. Write Behind Caching\"></a>3. Write Behind Caching</h3><p><code>Write Behind</code> 又叫 <code>Write Back</code>。一些了解 <code>Linux</code> 操作系统内核的同学对 <code>write back</code> 应该非常熟悉，这不就是 <code>Linux</code> 文件系统的 <code>Page Cache</code> 的算法吗？ 是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。</p>\n<p><code>Write Back</code> 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，<code>write back</code> 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。</p>\n<p>但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 <code>Unix/Linux</code> 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 <code>Trade-Off</code>。</p>\n<p>另外，<code>Write Back</code> 实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的 <code>write back</code> 会在仅当这个 <code>cache</code> 需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 <code>lazy write</code>。</p>\n<p>在 wikipedia 上有一张 <code>write back</code> 的流程图，基本逻辑如下：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Other/cache-update-policy-3.png?raw=true\" alt=\"\"></p>\n<p>转载于： <a href=\"https://coolshell.cn/articles/17416.html\" target=\"_blank\" rel=\"noopener\">https://coolshell.cn/articles/17416.html</a></p>\n"},{"title":"Python 列表,元组与集合","date":"2016-05-17T09:39:17.000Z","updated":"2017-12-01T02:20:17.000Z","_content":"\n### 1. 选择合适的数据结构\n\n列表、元组、集合和字典是Python中最常用的复合数据结构，它们都属于容器类的数据结构。\n\n(1) 列表\n\nPython用数组的方式实现列表。列表的搜索时间是线性的，因此用列表来存储大量可搜索的数据是不切实际的。\n\n(2) 元组\n\n元组是不可变的列表，创建后就无法再更改。元组的搜索时间也是线性的。\n\n(3) 集合\n\n与列表和元组不同，集合不是序列：集合项不存在索引。集合最多只能存储一个项的副本，具有次线性O(log(N))的搜索时间。集合非常适合于成员查找和消除重复项。\n\n在下面的例子中，展示了列表与集合查询速度对比，bigList是以十进制字符串表示的前1000万个整数的列表：\n\n```python\nbigList = [str(i) for i in range(10000000)]\n\"abc\" in bigList # 耗时0.2秒\nbigSet = set(bigList)\n\"abc\" in bigSet # 耗时15~30微秒——快了10 000倍！\n```\n(4) 字典\n\n字典构建了从键到值的映射。任何可哈希的数据类型（数字、布尔、字符串、元组）的对象都可以作为键，且同一字典中的不同键可以属于不同的数据类型。Python对字典值的数据类型也没有限制。字典具有次线性O(log(N))搜索时间，它非常适合用于键值的查找。\n\n你可以通过(键, 值)这样的元组列表创建字典，也可以使用内置类构造函数`enumerate(seq)`创建字典，这样得到的字典，其键为各项在seq中的序列号：\n\n```python\nseq = [\"alpha\", \"bravo\", \"charlie\", \"delta\"]\nd = dict(enumerate(seq))\nprint d  # {0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'}\n```\n\n另一种从键序列和值序列创建字典变量的巧妙方法，是使用内置类构造函数zip(kseq, vseq)(两个序列必须具有相同的长度):\n\n```python\nkseq = [\"apple\", \"bear\", \"banana\"]\nvseq = [12, 21, 3]\nd = dict(zip(kseq, vseq))\nprint d  # {'apple': 12, 'bear': 21, 'banana': 3}\n```\n\nPython将enumerate(seq)和zip(kseq, vseq)实现为列表生成器。列表生成器提供了一个迭代器接口，这使得我们可以在for循环中使用它们。与真正的列表不同的是，列表生成器只在需要时才生成下一个元素，这可以说是一种巧妙的偷懒方式。列表生成器便于处理大型列表，甚至允许“无限”的列表。你可以通过调用list()函数，将列表生成器显式强制转换为列表。\n\n总结：\n\n数据结构|查询时间复杂度|说明\n---|---|---\n列表|O(N)|数组实现，不适合大数据搜索\n元组|O(N)|不可变的列表，不可更改，不适合大数据搜索\n集合|O(log(N))|不是序列，不存在重复项，适合成员查找\n字典|O(log(N))|键到值的映射，键和值数据类型无限制，适合键值查找\n\n### 2. 列表推导式\n\n列表推导式是一个将数据集(不一定是列表)转换为列表的表达式。通过列表推导式，可以实现对所有或某些列表元素应用相同的操作，例如将所有元素转换为大写或对其中大于0的元素进行运算等。\n\n列表表达式的转换过程如下：\n\n(1) 表达式遍历数据集并访问集合中的每一项。\n\n(2) 为每一项计算布尔表达式(可选，不选默认为True)。\n\n(3) 如果布尔表达式为True，则计算当前项目的循环表达式，并将其值附加到结果列表中。\n\n(4) 如果布尔表达式为False，则跳过该项。\n\nExample：\n```python\n# 复制myList；等同于myList.copy()或者myList[:]，但二者的效率都没有列表推导式高\n[x for x in myList]\n# 提取非负项\n[x for x in myList if x >= 0]\n# 用Mylist非零项的倒数构建一个新列表\n[1/x for x in myList if x != 0]\n# 从打开的infile文件中选出所有的非空行，并删除这些行开头和结尾的空格\n[l.strip() for l in infile if l.strip()]\n```\n\n如果列表推导式被包含在圆括号中，而不是在方括号中，则程序将返回一个列表生成器对象：\n```\n(x**2 for x in myList) # 结果为：<generator object <genexpr> at 0x...>\n```\n\n### 3. 计数器\n\n计数器是一种字典式集合，用于给集合项目计数。计数器定义在collections模块中。你可以将要计数的集合传递给构造函数Counter，然后使用函数`most_common(n)`来获取n个出现频率最高的项及对应频率的`列表`(如果没有提供参数n，则函数返回的将是一个针对所有项目的列表)。\n\n```python\nphrase = \"a boy eat a banana\"\ncntr = Counter(phrase.split())\nprint cntr.most_common()  # [('a', 2), ('boy', 1), ('eat', 1), ('banana', 1)]\n```\n\n\n来源于:<Python数据科学指南>\n","source":"_posts/Python/Python 列表,元组与集合.md","raw":"\n---\ntitle: Python 列表,元组与集合\ndate: 2016-05-17 17:39:17\nupdated: 2017-12-01 10:20:17\ntags:\n- Python\n\ncategories: Python\n---\n\n### 1. 选择合适的数据结构\n\n列表、元组、集合和字典是Python中最常用的复合数据结构，它们都属于容器类的数据结构。\n\n(1) 列表\n\nPython用数组的方式实现列表。列表的搜索时间是线性的，因此用列表来存储大量可搜索的数据是不切实际的。\n\n(2) 元组\n\n元组是不可变的列表，创建后就无法再更改。元组的搜索时间也是线性的。\n\n(3) 集合\n\n与列表和元组不同，集合不是序列：集合项不存在索引。集合最多只能存储一个项的副本，具有次线性O(log(N))的搜索时间。集合非常适合于成员查找和消除重复项。\n\n在下面的例子中，展示了列表与集合查询速度对比，bigList是以十进制字符串表示的前1000万个整数的列表：\n\n```python\nbigList = [str(i) for i in range(10000000)]\n\"abc\" in bigList # 耗时0.2秒\nbigSet = set(bigList)\n\"abc\" in bigSet # 耗时15~30微秒——快了10 000倍！\n```\n(4) 字典\n\n字典构建了从键到值的映射。任何可哈希的数据类型（数字、布尔、字符串、元组）的对象都可以作为键，且同一字典中的不同键可以属于不同的数据类型。Python对字典值的数据类型也没有限制。字典具有次线性O(log(N))搜索时间，它非常适合用于键值的查找。\n\n你可以通过(键, 值)这样的元组列表创建字典，也可以使用内置类构造函数`enumerate(seq)`创建字典，这样得到的字典，其键为各项在seq中的序列号：\n\n```python\nseq = [\"alpha\", \"bravo\", \"charlie\", \"delta\"]\nd = dict(enumerate(seq))\nprint d  # {0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'}\n```\n\n另一种从键序列和值序列创建字典变量的巧妙方法，是使用内置类构造函数zip(kseq, vseq)(两个序列必须具有相同的长度):\n\n```python\nkseq = [\"apple\", \"bear\", \"banana\"]\nvseq = [12, 21, 3]\nd = dict(zip(kseq, vseq))\nprint d  # {'apple': 12, 'bear': 21, 'banana': 3}\n```\n\nPython将enumerate(seq)和zip(kseq, vseq)实现为列表生成器。列表生成器提供了一个迭代器接口，这使得我们可以在for循环中使用它们。与真正的列表不同的是，列表生成器只在需要时才生成下一个元素，这可以说是一种巧妙的偷懒方式。列表生成器便于处理大型列表，甚至允许“无限”的列表。你可以通过调用list()函数，将列表生成器显式强制转换为列表。\n\n总结：\n\n数据结构|查询时间复杂度|说明\n---|---|---\n列表|O(N)|数组实现，不适合大数据搜索\n元组|O(N)|不可变的列表，不可更改，不适合大数据搜索\n集合|O(log(N))|不是序列，不存在重复项，适合成员查找\n字典|O(log(N))|键到值的映射，键和值数据类型无限制，适合键值查找\n\n### 2. 列表推导式\n\n列表推导式是一个将数据集(不一定是列表)转换为列表的表达式。通过列表推导式，可以实现对所有或某些列表元素应用相同的操作，例如将所有元素转换为大写或对其中大于0的元素进行运算等。\n\n列表表达式的转换过程如下：\n\n(1) 表达式遍历数据集并访问集合中的每一项。\n\n(2) 为每一项计算布尔表达式(可选，不选默认为True)。\n\n(3) 如果布尔表达式为True，则计算当前项目的循环表达式，并将其值附加到结果列表中。\n\n(4) 如果布尔表达式为False，则跳过该项。\n\nExample：\n```python\n# 复制myList；等同于myList.copy()或者myList[:]，但二者的效率都没有列表推导式高\n[x for x in myList]\n# 提取非负项\n[x for x in myList if x >= 0]\n# 用Mylist非零项的倒数构建一个新列表\n[1/x for x in myList if x != 0]\n# 从打开的infile文件中选出所有的非空行，并删除这些行开头和结尾的空格\n[l.strip() for l in infile if l.strip()]\n```\n\n如果列表推导式被包含在圆括号中，而不是在方括号中，则程序将返回一个列表生成器对象：\n```\n(x**2 for x in myList) # 结果为：<generator object <genexpr> at 0x...>\n```\n\n### 3. 计数器\n\n计数器是一种字典式集合，用于给集合项目计数。计数器定义在collections模块中。你可以将要计数的集合传递给构造函数Counter，然后使用函数`most_common(n)`来获取n个出现频率最高的项及对应频率的`列表`(如果没有提供参数n，则函数返回的将是一个针对所有项目的列表)。\n\n```python\nphrase = \"a boy eat a banana\"\ncntr = Counter(phrase.split())\nprint cntr.most_common()  # [('a', 2), ('boy', 1), ('eat', 1), ('banana', 1)]\n```\n\n\n来源于:<Python数据科学指南>\n","slug":"Python/Python 列表,元组与集合","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cje58tiug004pordbf7oyjtuf","content":"<h3 id=\"1-选择合适的数据结构\"><a href=\"#1-选择合适的数据结构\" class=\"headerlink\" title=\"1. 选择合适的数据结构\"></a>1. 选择合适的数据结构</h3><p>列表、元组、集合和字典是Python中最常用的复合数据结构，它们都属于容器类的数据结构。</p>\n<p>(1) 列表</p>\n<p>Python用数组的方式实现列表。列表的搜索时间是线性的，因此用列表来存储大量可搜索的数据是不切实际的。</p>\n<p>(2) 元组</p>\n<p>元组是不可变的列表，创建后就无法再更改。元组的搜索时间也是线性的。</p>\n<p>(3) 集合</p>\n<p>与列表和元组不同，集合不是序列：集合项不存在索引。集合最多只能存储一个项的副本，具有次线性O(log(N))的搜索时间。集合非常适合于成员查找和消除重复项。</p>\n<p>在下面的例子中，展示了列表与集合查询速度对比，bigList是以十进制字符串表示的前1000万个整数的列表：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">bigList = [str(i) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10000000</span>)]</span><br><span class=\"line\"><span class=\"string\">\"abc\"</span> <span class=\"keyword\">in</span> bigList <span class=\"comment\"># 耗时0.2秒</span></span><br><span class=\"line\">bigSet = set(bigList)</span><br><span class=\"line\"><span class=\"string\">\"abc\"</span> <span class=\"keyword\">in</span> bigSet <span class=\"comment\"># 耗时15~30微秒——快了10 000倍！</span></span><br></pre></td></tr></table></figure>\n<p>(4) 字典</p>\n<p>字典构建了从键到值的映射。任何可哈希的数据类型（数字、布尔、字符串、元组）的对象都可以作为键，且同一字典中的不同键可以属于不同的数据类型。Python对字典值的数据类型也没有限制。字典具有次线性O(log(N))搜索时间，它非常适合用于键值的查找。</p>\n<p>你可以通过(键, 值)这样的元组列表创建字典，也可以使用内置类构造函数<code>enumerate(seq)</code>创建字典，这样得到的字典，其键为各项在seq中的序列号：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">seq = [<span class=\"string\">\"alpha\"</span>, <span class=\"string\">\"bravo\"</span>, <span class=\"string\">\"charlie\"</span>, <span class=\"string\">\"delta\"</span>]</span><br><span class=\"line\">d = dict(enumerate(seq))</span><br><span class=\"line\"><span class=\"keyword\">print</span> d  <span class=\"comment\"># &#123;0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'&#125;</span></span><br></pre></td></tr></table></figure>\n<p>另一种从键序列和值序列创建字典变量的巧妙方法，是使用内置类构造函数zip(kseq, vseq)(两个序列必须具有相同的长度):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">kseq = [<span class=\"string\">\"apple\"</span>, <span class=\"string\">\"bear\"</span>, <span class=\"string\">\"banana\"</span>]</span><br><span class=\"line\">vseq = [<span class=\"number\">12</span>, <span class=\"number\">21</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">d = dict(zip(kseq, vseq))</span><br><span class=\"line\"><span class=\"keyword\">print</span> d  <span class=\"comment\"># &#123;'apple': 12, 'bear': 21, 'banana': 3&#125;</span></span><br></pre></td></tr></table></figure>\n<p>Python将enumerate(seq)和zip(kseq, vseq)实现为列表生成器。列表生成器提供了一个迭代器接口，这使得我们可以在for循环中使用它们。与真正的列表不同的是，列表生成器只在需要时才生成下一个元素，这可以说是一种巧妙的偷懒方式。列表生成器便于处理大型列表，甚至允许“无限”的列表。你可以通过调用list()函数，将列表生成器显式强制转换为列表。</p>\n<p>总结：</p>\n<table>\n<thead>\n<tr>\n<th>数据结构</th>\n<th>查询时间复杂度</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>列表</td>\n<td>O(N)</td>\n<td>数组实现，不适合大数据搜索</td>\n</tr>\n<tr>\n<td>元组</td>\n<td>O(N)</td>\n<td>不可变的列表，不可更改，不适合大数据搜索</td>\n</tr>\n<tr>\n<td>集合</td>\n<td>O(log(N))</td>\n<td>不是序列，不存在重复项，适合成员查找</td>\n</tr>\n<tr>\n<td>字典</td>\n<td>O(log(N))</td>\n<td>键到值的映射，键和值数据类型无限制，适合键值查找</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-列表推导式\"><a href=\"#2-列表推导式\" class=\"headerlink\" title=\"2. 列表推导式\"></a>2. 列表推导式</h3><p>列表推导式是一个将数据集(不一定是列表)转换为列表的表达式。通过列表推导式，可以实现对所有或某些列表元素应用相同的操作，例如将所有元素转换为大写或对其中大于0的元素进行运算等。</p>\n<p>列表表达式的转换过程如下：</p>\n<p>(1) 表达式遍历数据集并访问集合中的每一项。</p>\n<p>(2) 为每一项计算布尔表达式(可选，不选默认为True)。</p>\n<p>(3) 如果布尔表达式为True，则计算当前项目的循环表达式，并将其值附加到结果列表中。</p>\n<p>(4) 如果布尔表达式为False，则跳过该项。</p>\n<p>Example：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 复制myList；等同于myList.copy()或者myList[:]，但二者的效率都没有列表推导式高</span></span><br><span class=\"line\">[x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> myList]</span><br><span class=\"line\"><span class=\"comment\"># 提取非负项</span></span><br><span class=\"line\">[x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> myList <span class=\"keyword\">if</span> x &gt;= <span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 用Mylist非零项的倒数构建一个新列表</span></span><br><span class=\"line\">[<span class=\"number\">1</span>/x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> myList <span class=\"keyword\">if</span> x != <span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 从打开的infile文件中选出所有的非空行，并删除这些行开头和结尾的空格</span></span><br><span class=\"line\">[l.strip() <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> infile <span class=\"keyword\">if</span> l.strip()]</span><br></pre></td></tr></table></figure></p>\n<p>如果列表推导式被包含在圆括号中，而不是在方括号中，则程序将返回一个列表生成器对象：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">(x**2 for x in myList) # 结果为：&lt;generator object &lt;genexpr&gt; at 0x...&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-计数器\"><a href=\"#3-计数器\" class=\"headerlink\" title=\"3. 计数器\"></a>3. 计数器</h3><p>计数器是一种字典式集合，用于给集合项目计数。计数器定义在collections模块中。你可以将要计数的集合传递给构造函数Counter，然后使用函数<code>most_common(n)</code>来获取n个出现频率最高的项及对应频率的<code>列表</code>(如果没有提供参数n，则函数返回的将是一个针对所有项目的列表)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">phrase = <span class=\"string\">\"a boy eat a banana\"</span></span><br><span class=\"line\">cntr = Counter(phrase.split())</span><br><span class=\"line\"><span class=\"keyword\">print</span> cntr.most_common()  <span class=\"comment\"># [('a', 2), ('boy', 1), ('eat', 1), ('banana', 1)]</span></span><br></pre></td></tr></table></figure>\n<p>来源于:<python数据科学指南></python数据科学指南></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-选择合适的数据结构\"><a href=\"#1-选择合适的数据结构\" class=\"headerlink\" title=\"1. 选择合适的数据结构\"></a>1. 选择合适的数据结构</h3><p>列表、元组、集合和字典是Python中最常用的复合数据结构，它们都属于容器类的数据结构。</p>\n<p>(1) 列表</p>\n<p>Python用数组的方式实现列表。列表的搜索时间是线性的，因此用列表来存储大量可搜索的数据是不切实际的。</p>\n<p>(2) 元组</p>\n<p>元组是不可变的列表，创建后就无法再更改。元组的搜索时间也是线性的。</p>\n<p>(3) 集合</p>\n<p>与列表和元组不同，集合不是序列：集合项不存在索引。集合最多只能存储一个项的副本，具有次线性O(log(N))的搜索时间。集合非常适合于成员查找和消除重复项。</p>\n<p>在下面的例子中，展示了列表与集合查询速度对比，bigList是以十进制字符串表示的前1000万个整数的列表：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">bigList = [str(i) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10000000</span>)]</span><br><span class=\"line\"><span class=\"string\">\"abc\"</span> <span class=\"keyword\">in</span> bigList <span class=\"comment\"># 耗时0.2秒</span></span><br><span class=\"line\">bigSet = set(bigList)</span><br><span class=\"line\"><span class=\"string\">\"abc\"</span> <span class=\"keyword\">in</span> bigSet <span class=\"comment\"># 耗时15~30微秒——快了10 000倍！</span></span><br></pre></td></tr></table></figure>\n<p>(4) 字典</p>\n<p>字典构建了从键到值的映射。任何可哈希的数据类型（数字、布尔、字符串、元组）的对象都可以作为键，且同一字典中的不同键可以属于不同的数据类型。Python对字典值的数据类型也没有限制。字典具有次线性O(log(N))搜索时间，它非常适合用于键值的查找。</p>\n<p>你可以通过(键, 值)这样的元组列表创建字典，也可以使用内置类构造函数<code>enumerate(seq)</code>创建字典，这样得到的字典，其键为各项在seq中的序列号：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">seq = [<span class=\"string\">\"alpha\"</span>, <span class=\"string\">\"bravo\"</span>, <span class=\"string\">\"charlie\"</span>, <span class=\"string\">\"delta\"</span>]</span><br><span class=\"line\">d = dict(enumerate(seq))</span><br><span class=\"line\"><span class=\"keyword\">print</span> d  <span class=\"comment\"># &#123;0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'&#125;</span></span><br></pre></td></tr></table></figure>\n<p>另一种从键序列和值序列创建字典变量的巧妙方法，是使用内置类构造函数zip(kseq, vseq)(两个序列必须具有相同的长度):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">kseq = [<span class=\"string\">\"apple\"</span>, <span class=\"string\">\"bear\"</span>, <span class=\"string\">\"banana\"</span>]</span><br><span class=\"line\">vseq = [<span class=\"number\">12</span>, <span class=\"number\">21</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">d = dict(zip(kseq, vseq))</span><br><span class=\"line\"><span class=\"keyword\">print</span> d  <span class=\"comment\"># &#123;'apple': 12, 'bear': 21, 'banana': 3&#125;</span></span><br></pre></td></tr></table></figure>\n<p>Python将enumerate(seq)和zip(kseq, vseq)实现为列表生成器。列表生成器提供了一个迭代器接口，这使得我们可以在for循环中使用它们。与真正的列表不同的是，列表生成器只在需要时才生成下一个元素，这可以说是一种巧妙的偷懒方式。列表生成器便于处理大型列表，甚至允许“无限”的列表。你可以通过调用list()函数，将列表生成器显式强制转换为列表。</p>\n<p>总结：</p>\n<table>\n<thead>\n<tr>\n<th>数据结构</th>\n<th>查询时间复杂度</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>列表</td>\n<td>O(N)</td>\n<td>数组实现，不适合大数据搜索</td>\n</tr>\n<tr>\n<td>元组</td>\n<td>O(N)</td>\n<td>不可变的列表，不可更改，不适合大数据搜索</td>\n</tr>\n<tr>\n<td>集合</td>\n<td>O(log(N))</td>\n<td>不是序列，不存在重复项，适合成员查找</td>\n</tr>\n<tr>\n<td>字典</td>\n<td>O(log(N))</td>\n<td>键到值的映射，键和值数据类型无限制，适合键值查找</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-列表推导式\"><a href=\"#2-列表推导式\" class=\"headerlink\" title=\"2. 列表推导式\"></a>2. 列表推导式</h3><p>列表推导式是一个将数据集(不一定是列表)转换为列表的表达式。通过列表推导式，可以实现对所有或某些列表元素应用相同的操作，例如将所有元素转换为大写或对其中大于0的元素进行运算等。</p>\n<p>列表表达式的转换过程如下：</p>\n<p>(1) 表达式遍历数据集并访问集合中的每一项。</p>\n<p>(2) 为每一项计算布尔表达式(可选，不选默认为True)。</p>\n<p>(3) 如果布尔表达式为True，则计算当前项目的循环表达式，并将其值附加到结果列表中。</p>\n<p>(4) 如果布尔表达式为False，则跳过该项。</p>\n<p>Example：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 复制myList；等同于myList.copy()或者myList[:]，但二者的效率都没有列表推导式高</span></span><br><span class=\"line\">[x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> myList]</span><br><span class=\"line\"><span class=\"comment\"># 提取非负项</span></span><br><span class=\"line\">[x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> myList <span class=\"keyword\">if</span> x &gt;= <span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 用Mylist非零项的倒数构建一个新列表</span></span><br><span class=\"line\">[<span class=\"number\">1</span>/x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> myList <span class=\"keyword\">if</span> x != <span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 从打开的infile文件中选出所有的非空行，并删除这些行开头和结尾的空格</span></span><br><span class=\"line\">[l.strip() <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> infile <span class=\"keyword\">if</span> l.strip()]</span><br></pre></td></tr></table></figure></p>\n<p>如果列表推导式被包含在圆括号中，而不是在方括号中，则程序将返回一个列表生成器对象：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">(x**2 for x in myList) # 结果为：&lt;generator object &lt;genexpr&gt; at 0x...&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-计数器\"><a href=\"#3-计数器\" class=\"headerlink\" title=\"3. 计数器\"></a>3. 计数器</h3><p>计数器是一种字典式集合，用于给集合项目计数。计数器定义在collections模块中。你可以将要计数的集合传递给构造函数Counter，然后使用函数<code>most_common(n)</code>来获取n个出现频率最高的项及对应频率的<code>列表</code>(如果没有提供参数n，则函数返回的将是一个针对所有项目的列表)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">phrase = <span class=\"string\">\"a boy eat a banana\"</span></span><br><span class=\"line\">cntr = Counter(phrase.split())</span><br><span class=\"line\"><span class=\"keyword\">print</span> cntr.most_common()  <span class=\"comment\"># [('a', 2), ('boy', 1), ('eat', 1), ('banana', 1)]</span></span><br></pre></td></tr></table></figure>\n<p>来源于:<python数据科学指南></python数据科学指南></p>\n"},{"layout":"post","author":"sjf0115","title":"Python Numpy 数组","date":"2016-05-17T09:39:17.000Z","updated":"2017-12-01T02:20:17.000Z","_content":"\nNumPy（Numeric Python，以numpy导入）是一系列高效的、可并行的、执行高性能数值运算的函数的接口。numpy模块提供了一种新的Python数据结构——数组(array)，以及特定于该结构的函数工具箱。该模块还支持随机数、数据聚合、线性代数和傅里叶变换等非常实用的数值计算工具。\n\n下面将学习如何创建不同形状的numpy数组，基于不同的源创建numpy数组，数组的重排和切片操作，添加数组索引，以及对某些或所有数组元素进行算术运算、逻辑运算和聚合运算。\n\n### 1. 创建数组\n\nnumpy数组比原生的Python列表更为`紧凑`和`高效`，尤其是在多维的情况下。但与列表不同的是，数组的语法要求更为`严格`：数组必须是同构的。这意味着数组项不能混合使用不同的`数据类型`，而且不能对不同数据类型的数组项进行匹配操作。\n\n创建numpy数组的方法很多。可以使用函数`array()`，基于类数组(array-like)数据创建数组。numpy基于数据本身推断出数组元素的类型，当然，你也可以给`array()`传递确定的`dtype`参数。numpy支持的数据类型接近二十种，例如bool_、int64、uint64、float64和<U32（针对Unicode字符串）。\n\n备注:\n```\n所谓的类数组数据可以是列表、元组或另一个数组。\n```\n\n为获得较高的效率，numpy在创建一个数组时，不会将数据从源复制到新数组，而是建立起数据间的连接。也就是说，在默认情况下，numpy数组相当于是其底层数据的视图，而不是其副本。如果底层数据对象发生改变，则相应的数组数据也会随之改变。如果你不喜欢这种方式（这是默认的处理方式，除非复制的数据量过大），可以给构造函数传递`copy=True`。\n\n备注:\n```\n创建数组，不会将数据从源复制到新数组，相当于是其底层数据的视图，而不是其副本。\n```\n\n实际上，Python的\"列表\"(list)是以数组的方式实现的，而并非列表的方式，这与\"列表\"(list)的字面含义并不一致。由于未使用前向指针，所以Python并没有给列表预留前向指针的存储空间。Python的大型列表只比\"真正的\"numpy数组多使用约13%的存储空间，但对于一些简单的内置操作，比如sum()，使用列表则要比数组快五到十倍。因此在使用numpy之前，应该问问自己是否真的需要用到某些numpy特有的功能。\n\n我们来创建第一个数组——前10个正整数组成的简单数组：\n```Python\nimport numpy as np\n\n# 简单数组\nnumbers = np.array(range(1, 11), copy=True)\nprint numbers  # [ 1  2  3  4  5  6  7  8  9 10]\n```\n\n函数`ones()`、`zeros()`和`empty()`分别构造全1数组、全零数组和尚未初始化的数组。这些函数必须有数组的形状参数，该参数用一个与数组的维度相同的列表或元组来表征:\n```Python\n# 给定数组形状shape与数据类型type 全1数组\nones = np.ones([2, 4], dtype=np.float64)\nprint ones\n'''\n[\n  [ 1.  1.  1.  1.]\n  [ 1.  1.  1.  1.]\n]\n'''\n\n# 给定数组形状shape与数据类型type 全0数组\nzeros = np.zeros([2, 4], dtype=np.float64)\nprint zeros\n'''\n[\n  [ 0.  0.  0.  0.]\n  [ 0.  0.  0.  0.]\n]\n'''\n\n# 给定数组形状shape与数据类型type 尚未初始化数组 其元素值不一定为零\nempty = np.empty([2, 4], dtype=np.float64)\nprint empty\n'''\n[\n  [ 1.  1.  1.  1.]\n  [ 1.  1.  1.  1.]\n]\n'''\n```\n\nnumpy使用数组的`ndim`、`shape`和`dtype`属性分别存储数组的维数、形状和数据类型:\n```Python\n# 只要没有经过变形(reshape) 该属性给出的就是数组的原始形状\nprint ones.shape  # (2, 4)\n\n# 等价于len(numbers.shape)\nprint ones.ndim  # 2\n\n# 数据类型\nprint ones.dtype  # float64\n```\n\n函数`eye(N, M=None, k=0, dtype=np.float)`用于构造一个N×M的眼形单位矩阵，其第k对角线上的值为1，其他地方的值为零。当k为正数时，对应的对角线位于主对角线上方的第k条。M为None（默认值）等价于M=N:\n```Python\n# N×M的眼形单位矩阵\neye = np.eye(3, k=1)\nprint eye\n'''\n[\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [ 0.  0.  0.]\n]\n'''\n```\n当需要将几个矩阵相乘时，可以使用单位矩阵作为乘法链累积器中的初始值。\n\n除了经典的内置函数`range()`外，numpy有其独有的、更高效的生成等间隔数值数组的方式：函数`arange([start,] stop [, step,], dtype=None)`：\n```Python\n# 等间隔数值数组\ndouble_numbers = np.arange(2, 5, 0.25)\nprint double_numbers  # [ 2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.    4.25  4.5   4.75]\n```\n\nnumpy在创建数组时记录每一项的数据类型，不过该数据类型并非不可变的。可在数组创建后，调用函数`astype(dtype, casting =\"unsafe\", copy=True)`来改变它。对于类型缩小的情况(将较抽象的数据类型转换为更具体的数据类型)，可能会丢失一些信息。这并非numpy特有的，任何缩小变换都可能会丢失信息:\n```Python\n# 改变数组数据类型\nint_numbers = double_numbers.astype(np.int)\nprint int_numbers  # [2 2 2 2 3 3 3 3 4 4 4 4]\n```\n\n大多数numpy操作返回的是一个视图，而非原始数组的副本。为了保留原始数据，可使用`copy()`函数创建现有数组的副本。这样一来，对原始数组的任何更改都不会影响到副本。但如果数组较为庞大，比如有十亿个数组项，那就不要轻易进行复制:\n```Python\n# 数组的副本\ndouble_numbers_copy = double_numbers.copy()\n```\n\n### 2. 转置和重排\n\n借助numpy可以很容易地改变数组的形状和方向，我们再也不用像“瞎猫踫到死耗子”那样看运气了。下面我们用几个标准普尔（S&P）股票代码组成一个一维数组，然后用所有可能的方式改变它的形状：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n来源于: Python数据科学入门\n","source":"_posts/Python/Python Numpy 数组.md","raw":"\n---\nlayout: post\nauthor: \"sjf0115\"\ntitle: Python Numpy 数组\ndate: 2016-05-17 17:39:17\nupdated: 2017-12-01 10:20:17\ntags:\n  - Python\n\ncategories: Python\n---\n\nNumPy（Numeric Python，以numpy导入）是一系列高效的、可并行的、执行高性能数值运算的函数的接口。numpy模块提供了一种新的Python数据结构——数组(array)，以及特定于该结构的函数工具箱。该模块还支持随机数、数据聚合、线性代数和傅里叶变换等非常实用的数值计算工具。\n\n下面将学习如何创建不同形状的numpy数组，基于不同的源创建numpy数组，数组的重排和切片操作，添加数组索引，以及对某些或所有数组元素进行算术运算、逻辑运算和聚合运算。\n\n### 1. 创建数组\n\nnumpy数组比原生的Python列表更为`紧凑`和`高效`，尤其是在多维的情况下。但与列表不同的是，数组的语法要求更为`严格`：数组必须是同构的。这意味着数组项不能混合使用不同的`数据类型`，而且不能对不同数据类型的数组项进行匹配操作。\n\n创建numpy数组的方法很多。可以使用函数`array()`，基于类数组(array-like)数据创建数组。numpy基于数据本身推断出数组元素的类型，当然，你也可以给`array()`传递确定的`dtype`参数。numpy支持的数据类型接近二十种，例如bool_、int64、uint64、float64和<U32（针对Unicode字符串）。\n\n备注:\n```\n所谓的类数组数据可以是列表、元组或另一个数组。\n```\n\n为获得较高的效率，numpy在创建一个数组时，不会将数据从源复制到新数组，而是建立起数据间的连接。也就是说，在默认情况下，numpy数组相当于是其底层数据的视图，而不是其副本。如果底层数据对象发生改变，则相应的数组数据也会随之改变。如果你不喜欢这种方式（这是默认的处理方式，除非复制的数据量过大），可以给构造函数传递`copy=True`。\n\n备注:\n```\n创建数组，不会将数据从源复制到新数组，相当于是其底层数据的视图，而不是其副本。\n```\n\n实际上，Python的\"列表\"(list)是以数组的方式实现的，而并非列表的方式，这与\"列表\"(list)的字面含义并不一致。由于未使用前向指针，所以Python并没有给列表预留前向指针的存储空间。Python的大型列表只比\"真正的\"numpy数组多使用约13%的存储空间，但对于一些简单的内置操作，比如sum()，使用列表则要比数组快五到十倍。因此在使用numpy之前，应该问问自己是否真的需要用到某些numpy特有的功能。\n\n我们来创建第一个数组——前10个正整数组成的简单数组：\n```Python\nimport numpy as np\n\n# 简单数组\nnumbers = np.array(range(1, 11), copy=True)\nprint numbers  # [ 1  2  3  4  5  6  7  8  9 10]\n```\n\n函数`ones()`、`zeros()`和`empty()`分别构造全1数组、全零数组和尚未初始化的数组。这些函数必须有数组的形状参数，该参数用一个与数组的维度相同的列表或元组来表征:\n```Python\n# 给定数组形状shape与数据类型type 全1数组\nones = np.ones([2, 4], dtype=np.float64)\nprint ones\n'''\n[\n  [ 1.  1.  1.  1.]\n  [ 1.  1.  1.  1.]\n]\n'''\n\n# 给定数组形状shape与数据类型type 全0数组\nzeros = np.zeros([2, 4], dtype=np.float64)\nprint zeros\n'''\n[\n  [ 0.  0.  0.  0.]\n  [ 0.  0.  0.  0.]\n]\n'''\n\n# 给定数组形状shape与数据类型type 尚未初始化数组 其元素值不一定为零\nempty = np.empty([2, 4], dtype=np.float64)\nprint empty\n'''\n[\n  [ 1.  1.  1.  1.]\n  [ 1.  1.  1.  1.]\n]\n'''\n```\n\nnumpy使用数组的`ndim`、`shape`和`dtype`属性分别存储数组的维数、形状和数据类型:\n```Python\n# 只要没有经过变形(reshape) 该属性给出的就是数组的原始形状\nprint ones.shape  # (2, 4)\n\n# 等价于len(numbers.shape)\nprint ones.ndim  # 2\n\n# 数据类型\nprint ones.dtype  # float64\n```\n\n函数`eye(N, M=None, k=0, dtype=np.float)`用于构造一个N×M的眼形单位矩阵，其第k对角线上的值为1，其他地方的值为零。当k为正数时，对应的对角线位于主对角线上方的第k条。M为None（默认值）等价于M=N:\n```Python\n# N×M的眼形单位矩阵\neye = np.eye(3, k=1)\nprint eye\n'''\n[\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [ 0.  0.  0.]\n]\n'''\n```\n当需要将几个矩阵相乘时，可以使用单位矩阵作为乘法链累积器中的初始值。\n\n除了经典的内置函数`range()`外，numpy有其独有的、更高效的生成等间隔数值数组的方式：函数`arange([start,] stop [, step,], dtype=None)`：\n```Python\n# 等间隔数值数组\ndouble_numbers = np.arange(2, 5, 0.25)\nprint double_numbers  # [ 2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.    4.25  4.5   4.75]\n```\n\nnumpy在创建数组时记录每一项的数据类型，不过该数据类型并非不可变的。可在数组创建后，调用函数`astype(dtype, casting =\"unsafe\", copy=True)`来改变它。对于类型缩小的情况(将较抽象的数据类型转换为更具体的数据类型)，可能会丢失一些信息。这并非numpy特有的，任何缩小变换都可能会丢失信息:\n```Python\n# 改变数组数据类型\nint_numbers = double_numbers.astype(np.int)\nprint int_numbers  # [2 2 2 2 3 3 3 3 4 4 4 4]\n```\n\n大多数numpy操作返回的是一个视图，而非原始数组的副本。为了保留原始数据，可使用`copy()`函数创建现有数组的副本。这样一来，对原始数组的任何更改都不会影响到副本。但如果数组较为庞大，比如有十亿个数组项，那就不要轻易进行复制:\n```Python\n# 数组的副本\ndouble_numbers_copy = double_numbers.copy()\n```\n\n### 2. 转置和重排\n\n借助numpy可以很容易地改变数组的形状和方向，我们再也不用像“瞎猫踫到死耗子”那样看运气了。下面我们用几个标准普尔（S&P）股票代码组成一个一维数组，然后用所有可能的方式改变它的形状：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n来源于: Python数据科学入门\n","slug":"Python/Python Numpy 数组","published":1,"comments":1,"photos":[],"link":"","_id":"cje58tiuj004uordbl8bwi1be","content":"<p>NumPy（Numeric Python，以numpy导入）是一系列高效的、可并行的、执行高性能数值运算的函数的接口。numpy模块提供了一种新的Python数据结构——数组(array)，以及特定于该结构的函数工具箱。该模块还支持随机数、数据聚合、线性代数和傅里叶变换等非常实用的数值计算工具。</p>\n<p>下面将学习如何创建不同形状的numpy数组，基于不同的源创建numpy数组，数组的重排和切片操作，添加数组索引，以及对某些或所有数组元素进行算术运算、逻辑运算和聚合运算。</p>\n<h3 id=\"1-创建数组\"><a href=\"#1-创建数组\" class=\"headerlink\" title=\"1. 创建数组\"></a>1. 创建数组</h3><p>numpy数组比原生的Python列表更为<code>紧凑</code>和<code>高效</code>，尤其是在多维的情况下。但与列表不同的是，数组的语法要求更为<code>严格</code>：数组必须是同构的。这意味着数组项不能混合使用不同的<code>数据类型</code>，而且不能对不同数据类型的数组项进行匹配操作。</p>\n<p>创建numpy数组的方法很多。可以使用函数<code>array()</code>，基于类数组(array-like)数据创建数组。numpy基于数据本身推断出数组元素的类型，当然，你也可以给<code>array()</code>传递确定的<code>dtype</code>参数。numpy支持的数据类型接近二十种，例如bool_、int64、uint64、float64和&lt;U32（针对Unicode字符串）。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">所谓的类数组数据可以是列表、元组或另一个数组。</span><br></pre></td></tr></table></figure></p>\n<p>为获得较高的效率，numpy在创建一个数组时，不会将数据从源复制到新数组，而是建立起数据间的连接。也就是说，在默认情况下，numpy数组相当于是其底层数据的视图，而不是其副本。如果底层数据对象发生改变，则相应的数组数据也会随之改变。如果你不喜欢这种方式（这是默认的处理方式，除非复制的数据量过大），可以给构造函数传递<code>copy=True</code>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">创建数组，不会将数据从源复制到新数组，相当于是其底层数据的视图，而不是其副本。</span><br></pre></td></tr></table></figure></p>\n<p>实际上，Python的”列表”(list)是以数组的方式实现的，而并非列表的方式，这与”列表”(list)的字面含义并不一致。由于未使用前向指针，所以Python并没有给列表预留前向指针的存储空间。Python的大型列表只比”真正的”numpy数组多使用约13%的存储空间，但对于一些简单的内置操作，比如sum()，使用列表则要比数组快五到十倍。因此在使用numpy之前，应该问问自己是否真的需要用到某些numpy特有的功能。</p>\n<p>我们来创建第一个数组——前10个正整数组成的简单数组：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 简单数组</span></span><br><span class=\"line\">numbers = np.array(range(<span class=\"number\">1</span>, <span class=\"number\">11</span>), copy=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> numbers  <span class=\"comment\"># [ 1  2  3  4  5  6  7  8  9 10]</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>ones()</code>、<code>zeros()</code>和<code>empty()</code>分别构造全1数组、全零数组和尚未初始化的数组。这些函数必须有数组的形状参数，该参数用一个与数组的维度相同的列表或元组来表征:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 给定数组形状shape与数据类型type 全1数组</span></span><br><span class=\"line\">ones = np.ones([<span class=\"number\">2</span>, <span class=\"number\">4</span>], dtype=np.float64)</span><br><span class=\"line\"><span class=\"keyword\">print</span> ones</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给定数组形状shape与数据类型type 全0数组</span></span><br><span class=\"line\">zeros = np.zeros([<span class=\"number\">2</span>, <span class=\"number\">4</span>], dtype=np.float64)</span><br><span class=\"line\"><span class=\"keyword\">print</span> zeros</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">  [ 0.  0.  0.  0.]</span></span><br><span class=\"line\"><span class=\"string\">  [ 0.  0.  0.  0.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给定数组形状shape与数据类型type 尚未初始化数组 其元素值不一定为零</span></span><br><span class=\"line\">empty = np.empty([<span class=\"number\">2</span>, <span class=\"number\">4</span>], dtype=np.float64)</span><br><span class=\"line\"><span class=\"keyword\">print</span> empty</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>numpy使用数组的<code>ndim</code>、<code>shape</code>和<code>dtype</code>属性分别存储数组的维数、形状和数据类型:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 只要没有经过变形(reshape) 该属性给出的就是数组的原始形状</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ones.shape  <span class=\"comment\"># (2, 4)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 等价于len(numbers.shape)</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ones.ndim  <span class=\"comment\"># 2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据类型</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ones.dtype  <span class=\"comment\"># float64</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>eye(N, M=None, k=0, dtype=np.float)</code>用于构造一个N×M的眼形单位矩阵，其第k对角线上的值为1，其他地方的值为零。当k为正数时，对应的对角线位于主对角线上方的第k条。M为None（默认值）等价于M=N:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># N×M的眼形单位矩阵</span></span><br><span class=\"line\">eye = np.eye(<span class=\"number\">3</span>, k=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> eye</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\"> [ 0.  1.  0.]</span></span><br><span class=\"line\"><span class=\"string\"> [ 0.  0.  1.]</span></span><br><span class=\"line\"><span class=\"string\"> [ 0.  0.  0.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>当需要将几个矩阵相乘时，可以使用单位矩阵作为乘法链累积器中的初始值。</p>\n<p>除了经典的内置函数<code>range()</code>外，numpy有其独有的、更高效的生成等间隔数值数组的方式：函数<code>arange([start,] stop [, step,], dtype=None)</code>：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 等间隔数值数组</span></span><br><span class=\"line\">double_numbers = np.arange(<span class=\"number\">2</span>, <span class=\"number\">5</span>, <span class=\"number\">0.25</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> double_numbers  <span class=\"comment\"># [ 2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.    4.25  4.5   4.75]</span></span><br></pre></td></tr></table></figure></p>\n<p>numpy在创建数组时记录每一项的数据类型，不过该数据类型并非不可变的。可在数组创建后，调用函数<code>astype(dtype, casting =&quot;unsafe&quot;, copy=True)</code>来改变它。对于类型缩小的情况(将较抽象的数据类型转换为更具体的数据类型)，可能会丢失一些信息。这并非numpy特有的，任何缩小变换都可能会丢失信息:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 改变数组数据类型</span></span><br><span class=\"line\">int_numbers = double_numbers.astype(np.int)</span><br><span class=\"line\"><span class=\"keyword\">print</span> int_numbers  <span class=\"comment\"># [2 2 2 2 3 3 3 3 4 4 4 4]</span></span><br></pre></td></tr></table></figure></p>\n<p>大多数numpy操作返回的是一个视图，而非原始数组的副本。为了保留原始数据，可使用<code>copy()</code>函数创建现有数组的副本。这样一来，对原始数组的任何更改都不会影响到副本。但如果数组较为庞大，比如有十亿个数组项，那就不要轻易进行复制:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 数组的副本</span></span><br><span class=\"line\">double_numbers_copy = double_numbers.copy()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-转置和重排\"><a href=\"#2-转置和重排\" class=\"headerlink\" title=\"2. 转置和重排\"></a>2. 转置和重排</h3><p>借助numpy可以很容易地改变数组的形状和方向，我们再也不用像“瞎猫踫到死耗子”那样看运气了。下面我们用几个标准普尔（S&amp;P）股票代码组成一个一维数组，然后用所有可能的方式改变它的形状：</p>\n<p>来源于: Python数据科学入门</p>\n","site":{"data":{}},"excerpt":"","more":"<p>NumPy（Numeric Python，以numpy导入）是一系列高效的、可并行的、执行高性能数值运算的函数的接口。numpy模块提供了一种新的Python数据结构——数组(array)，以及特定于该结构的函数工具箱。该模块还支持随机数、数据聚合、线性代数和傅里叶变换等非常实用的数值计算工具。</p>\n<p>下面将学习如何创建不同形状的numpy数组，基于不同的源创建numpy数组，数组的重排和切片操作，添加数组索引，以及对某些或所有数组元素进行算术运算、逻辑运算和聚合运算。</p>\n<h3 id=\"1-创建数组\"><a href=\"#1-创建数组\" class=\"headerlink\" title=\"1. 创建数组\"></a>1. 创建数组</h3><p>numpy数组比原生的Python列表更为<code>紧凑</code>和<code>高效</code>，尤其是在多维的情况下。但与列表不同的是，数组的语法要求更为<code>严格</code>：数组必须是同构的。这意味着数组项不能混合使用不同的<code>数据类型</code>，而且不能对不同数据类型的数组项进行匹配操作。</p>\n<p>创建numpy数组的方法很多。可以使用函数<code>array()</code>，基于类数组(array-like)数据创建数组。numpy基于数据本身推断出数组元素的类型，当然，你也可以给<code>array()</code>传递确定的<code>dtype</code>参数。numpy支持的数据类型接近二十种，例如bool_、int64、uint64、float64和&lt;U32（针对Unicode字符串）。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">所谓的类数组数据可以是列表、元组或另一个数组。</span><br></pre></td></tr></table></figure></p>\n<p>为获得较高的效率，numpy在创建一个数组时，不会将数据从源复制到新数组，而是建立起数据间的连接。也就是说，在默认情况下，numpy数组相当于是其底层数据的视图，而不是其副本。如果底层数据对象发生改变，则相应的数组数据也会随之改变。如果你不喜欢这种方式（这是默认的处理方式，除非复制的数据量过大），可以给构造函数传递<code>copy=True</code>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">创建数组，不会将数据从源复制到新数组，相当于是其底层数据的视图，而不是其副本。</span><br></pre></td></tr></table></figure></p>\n<p>实际上，Python的”列表”(list)是以数组的方式实现的，而并非列表的方式，这与”列表”(list)的字面含义并不一致。由于未使用前向指针，所以Python并没有给列表预留前向指针的存储空间。Python的大型列表只比”真正的”numpy数组多使用约13%的存储空间，但对于一些简单的内置操作，比如sum()，使用列表则要比数组快五到十倍。因此在使用numpy之前，应该问问自己是否真的需要用到某些numpy特有的功能。</p>\n<p>我们来创建第一个数组——前10个正整数组成的简单数组：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 简单数组</span></span><br><span class=\"line\">numbers = np.array(range(<span class=\"number\">1</span>, <span class=\"number\">11</span>), copy=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> numbers  <span class=\"comment\"># [ 1  2  3  4  5  6  7  8  9 10]</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>ones()</code>、<code>zeros()</code>和<code>empty()</code>分别构造全1数组、全零数组和尚未初始化的数组。这些函数必须有数组的形状参数，该参数用一个与数组的维度相同的列表或元组来表征:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 给定数组形状shape与数据类型type 全1数组</span></span><br><span class=\"line\">ones = np.ones([<span class=\"number\">2</span>, <span class=\"number\">4</span>], dtype=np.float64)</span><br><span class=\"line\"><span class=\"keyword\">print</span> ones</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给定数组形状shape与数据类型type 全0数组</span></span><br><span class=\"line\">zeros = np.zeros([<span class=\"number\">2</span>, <span class=\"number\">4</span>], dtype=np.float64)</span><br><span class=\"line\"><span class=\"keyword\">print</span> zeros</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">  [ 0.  0.  0.  0.]</span></span><br><span class=\"line\"><span class=\"string\">  [ 0.  0.  0.  0.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 给定数组形状shape与数据类型type 尚未初始化数组 其元素值不一定为零</span></span><br><span class=\"line\">empty = np.empty([<span class=\"number\">2</span>, <span class=\"number\">4</span>], dtype=np.float64)</span><br><span class=\"line\"><span class=\"keyword\">print</span> empty</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">  [ 1.  1.  1.  1.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>numpy使用数组的<code>ndim</code>、<code>shape</code>和<code>dtype</code>属性分别存储数组的维数、形状和数据类型:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 只要没有经过变形(reshape) 该属性给出的就是数组的原始形状</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ones.shape  <span class=\"comment\"># (2, 4)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 等价于len(numbers.shape)</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ones.ndim  <span class=\"comment\"># 2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据类型</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ones.dtype  <span class=\"comment\"># float64</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>eye(N, M=None, k=0, dtype=np.float)</code>用于构造一个N×M的眼形单位矩阵，其第k对角线上的值为1，其他地方的值为零。当k为正数时，对应的对角线位于主对角线上方的第k条。M为None（默认值）等价于M=N:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># N×M的眼形单位矩阵</span></span><br><span class=\"line\">eye = np.eye(<span class=\"number\">3</span>, k=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> eye</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\"> [ 0.  1.  0.]</span></span><br><span class=\"line\"><span class=\"string\"> [ 0.  0.  1.]</span></span><br><span class=\"line\"><span class=\"string\"> [ 0.  0.  0.]</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br></pre></td></tr></table></figure></p>\n<p>当需要将几个矩阵相乘时，可以使用单位矩阵作为乘法链累积器中的初始值。</p>\n<p>除了经典的内置函数<code>range()</code>外，numpy有其独有的、更高效的生成等间隔数值数组的方式：函数<code>arange([start,] stop [, step,], dtype=None)</code>：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 等间隔数值数组</span></span><br><span class=\"line\">double_numbers = np.arange(<span class=\"number\">2</span>, <span class=\"number\">5</span>, <span class=\"number\">0.25</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> double_numbers  <span class=\"comment\"># [ 2.    2.25  2.5   2.75  3.    3.25  3.5   3.75  4.    4.25  4.5   4.75]</span></span><br></pre></td></tr></table></figure></p>\n<p>numpy在创建数组时记录每一项的数据类型，不过该数据类型并非不可变的。可在数组创建后，调用函数<code>astype(dtype, casting =&quot;unsafe&quot;, copy=True)</code>来改变它。对于类型缩小的情况(将较抽象的数据类型转换为更具体的数据类型)，可能会丢失一些信息。这并非numpy特有的，任何缩小变换都可能会丢失信息:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 改变数组数据类型</span></span><br><span class=\"line\">int_numbers = double_numbers.astype(np.int)</span><br><span class=\"line\"><span class=\"keyword\">print</span> int_numbers  <span class=\"comment\"># [2 2 2 2 3 3 3 3 4 4 4 4]</span></span><br></pre></td></tr></table></figure></p>\n<p>大多数numpy操作返回的是一个视图，而非原始数组的副本。为了保留原始数据，可使用<code>copy()</code>函数创建现有数组的副本。这样一来，对原始数组的任何更改都不会影响到副本。但如果数组较为庞大，比如有十亿个数组项，那就不要轻易进行复制:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 数组的副本</span></span><br><span class=\"line\">double_numbers_copy = double_numbers.copy()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-转置和重排\"><a href=\"#2-转置和重排\" class=\"headerlink\" title=\"2. 转置和重排\"></a>2. 转置和重排</h3><p>借助numpy可以很容易地改变数组的形状和方向，我们再也不用像“瞎猫踫到死耗子”那样看运气了。下面我们用几个标准普尔（S&amp;P）股票代码组成一个一维数组，然后用所有可能的方式改变它的形状：</p>\n<p>来源于: Python数据科学入门</p>\n"},{"title":"Python 数据库操作","date":"2016-05-17T09:39:17.000Z","updated":"2017-12-01T02:20:17.000Z","_content":"\n\n### 1. MySQL\n\nPython使用数据库驱动模块与MySQL通信。诸如pymysql等许多数据库驱动都是免费的。这里我们将使用pymysql，它是Anaconda的一部分。驱动程序经过激活后与数据库服务器相连，然后将Python的函数调用转换为数据库查询，反过来，将数据库结果转换为Python数据结构。\n\n`connect()`函数需要以下信息：数据库(名称)、数据库服务器的位置(主机和端口号)和数据库用户(名称和密码)。如果数据库成功连接，则返回连接标识符。接下来，创建与数据库连接相关联的数据库游标：\n```Python\nimport pymysql\n\n# 连接数据库\nconn = pymysql.connect(host=\"localhost\", port=3306, user=\"root\", passwd=\"root\", db=\"test\")\ncur = conn.cursor()\n```\n\n备注:\n```\n使用pymysql需要导入pymysql库：import pymysql。\n```\n\n\n游标的`execute()`函数向数据服务器提交要执行的查询命令，并返回受影响的行数(如果查询是非破坏性的，则返回零)。与命令行MySQL查询不同，pymysql查询语句不需要在结尾加上分号。\n```Python\n# 查询\nquery = '''\n    SELECT first_name, last_name\n    FROM People\n    ORDER BY dob\n    LIMIT 3\n'''\nn_rows = cur.execute(query)\nprint n_rows  # 3\n```\n如果提交非破坏性查询(比如SELECT)，需要使用游标函数`fetchall()`获取所有匹配的记录。该函数返回一个生成器，可以将其转换为列字段的元组构成的列表：\n```Python\nresults = list(cur.fetchall())\nprint results  # [('gztAQV', 'aLhko'), ('ZXMtHd', 'cgwjI'), ('yHwDRF', 'NgBkY')]\n```\n如果查询是破坏性的(例如UPDATE、DELETE或INSERT)，则必须执行commit操作:\n```Python\n# 修改\nupdate_query = '''\n    UPDATE People\n    SET first_name = 'yoona'\n    WHERE last_name = 'aLhko'\n'''\nn_rows = cur.execute(update_query)\nprint n_rows  # 1\nconn.commit()\n\nselect_query = '''\n    SELECT first_name, last_name\n    FROM People\n    WHERE last_name = 'aLhko'\n'''\ncur.execute(select_query)\nresults = list(cur.fetchall())\nprint results  # [('yoona', 'aLhko')]\n```\n\n备注:\n```\n提供commit()函数的是连接本身(conn)，而不是游标(cur)。\n```\n\n### 2. MongoDB\n\n在Python中，我们用`pymongo`模块中`MongoClient`类的实例来实现MongoDB客户端。首先安装`pymongo`模块(ubuntu15.10):\n```\nsudo pip install pymongo\n```\n下面就可以创建一个无参数的客户端(适用于典型的安装了本地服务器的情况)，也可以用服务器的主机名和端口号作为参数创建客户端，或使用服务器的统一资源标识符(URI)作为参数创建客户端：\n```Python\n# 使用默认的初始化方式\nclient1 = pymongo.MongoClient()\n# 指定主机和端口号\nclient2 = pymongo.MongoClient(\"localhost\", 27017)\n# 用URI方式指定主机和端口号\nclient3 = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n```\n\n客户一旦端建立了与数据库服务器的连接，就可以选择当前激活的数据库，进而选择激活的集合。可以使用面向对象(\".\")或字典样式的符号。如果所选的数据库或集合不存在，服务器会立即创建它们：\n```Python\n# 创建并选择活动数据库的两种方法\ndb = client1.test_db\ndb = client1[\"test_db\"]\n\n# 创建并选择活动集合的两种方法\npeople = db.people\npeople = db[\"people\"]\n```\n\npymongo模块用字典变量来表示MongoDB文档。表示对象的每个字典必须具有_id这个键。如果该键不存在，服务器会自动生成它。\n\n集合对象提供用于在文档集合中插入、搜索、删除、更新、替换和聚合文档以及创建索引的功能。\n\n函数`insert_one(doc)`和`insert_many(docs)`将文档或文档列表插入集合。它们分别返回对象`InsertOneResult`或`InsertManyResult`，这两个对象分别提供`inserted_id`和`inserted_ids`属性。当文档没有提供明确的唯一键时，就需要使用这两个属性值作为文档的唯一键。如果指定了_id键，就是用该值作为唯一键：\n```Python\n# 插入\nperson1 = {\"name\": \"John\", \"dob\": \"2017-11-24\"}\nperson_id1 = people.insert_one(person1).inserted_id\nprint person_id1  # 5a1d4ba92317d71bb605f8ce\n\nperson2 = {\"_id\": \"XVT162\", \"name\": \"Jane\", \"dob\": \"2017-11-27\"}\nperson_id2 = people.insert_one(person2).inserted_id\nprint person_id2  # XVT162\n\npersons = [{\"name\": \"Lucy\", \"dob\": \"2017-11-12\"}, {\"name\": \"Tom\"}]\nresult = people.insert_many(persons)\nprint result.inserted_ids  # [ObjectId('5a1d4c832317d71c2c4e284f'), ObjectId('5a1d4c832317d71c2c4e2850')]\n```\n\n函数`find_one()`和`find()`分别给出匹配可选属性的一个或多个文档，其中`find_one()`返回文档，而`find()`返回一个游标(一个生成器)，可以使用`list()`函数将该游标转换为列表，或者在for循环中将其用作迭代器。如果将字典作为参数传递给这些函数中的任意一个，函数将给出与字典的所有键值相等的文档：\n```Python\n# 查找\n\neveryone = people.find()\nprint list(everyone)  # [{u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'}, {u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}, {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'}, {u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'}, {u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'}]\n\nprint list(people.find({\"dob\": \"2017-11-27\"}))  # [{u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}]\n\none_people = people.find_one()\nprint one_people  # {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'}\n\none_people = people.find_one({\"name\": \"Lucy\"})\nprint one_people  # {u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'}\n\none_people = people.find_one({\"_id\": \"XVT162\"})\nprint one_people  # {u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}\n```\n\n下面介绍几个实现数据聚合和排序的分组和排序函数。函数`sort()`对查询的结果进行排序。当以无参数的方式调用它时，该函数按键_id的升序进行排序。函数`count()`返回查询结果中或整个集合中的文档数量：\n```Python\n# 聚合\n\ncount = people.count()\nprint count  # 5\n\ncount = people.find({\"dob\": \"2017-11-27\"}).count()\nprint count  # 1\n\npeople_list = list(people.find().sort(\"dob\"))\nprint people_list  # [{u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'}, {u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'}, {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'}, {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'}, {u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}]\n```\n函数`delete_one(doc)`和`delete_many(docs)`从集合中删除字典doc所标识的一个或多个文档。如果要在删除所有文档的同时保留集合，需使用空字典作为参数调用函数`delete_many({})`：\n```Python\n# 删除\nresult = people.delete_many({\"dob\": \"2017-11-27\"})\nprint result.deleted_count  # 1\n```\n\n\n\n\n\n\n\n\n\n\n\n来自于:Python数据科学入门\n","source":"_posts/Python/Python 数据库操作.md","raw":"---\ntitle: Python 数据库操作\ndate: 2016-05-17 17:39:17\nupdated: 2017-12-01 10:20:17\ntags:\n- Python\n\ncategories: Python\n---\n\n\n### 1. MySQL\n\nPython使用数据库驱动模块与MySQL通信。诸如pymysql等许多数据库驱动都是免费的。这里我们将使用pymysql，它是Anaconda的一部分。驱动程序经过激活后与数据库服务器相连，然后将Python的函数调用转换为数据库查询，反过来，将数据库结果转换为Python数据结构。\n\n`connect()`函数需要以下信息：数据库(名称)、数据库服务器的位置(主机和端口号)和数据库用户(名称和密码)。如果数据库成功连接，则返回连接标识符。接下来，创建与数据库连接相关联的数据库游标：\n```Python\nimport pymysql\n\n# 连接数据库\nconn = pymysql.connect(host=\"localhost\", port=3306, user=\"root\", passwd=\"root\", db=\"test\")\ncur = conn.cursor()\n```\n\n备注:\n```\n使用pymysql需要导入pymysql库：import pymysql。\n```\n\n\n游标的`execute()`函数向数据服务器提交要执行的查询命令，并返回受影响的行数(如果查询是非破坏性的，则返回零)。与命令行MySQL查询不同，pymysql查询语句不需要在结尾加上分号。\n```Python\n# 查询\nquery = '''\n    SELECT first_name, last_name\n    FROM People\n    ORDER BY dob\n    LIMIT 3\n'''\nn_rows = cur.execute(query)\nprint n_rows  # 3\n```\n如果提交非破坏性查询(比如SELECT)，需要使用游标函数`fetchall()`获取所有匹配的记录。该函数返回一个生成器，可以将其转换为列字段的元组构成的列表：\n```Python\nresults = list(cur.fetchall())\nprint results  # [('gztAQV', 'aLhko'), ('ZXMtHd', 'cgwjI'), ('yHwDRF', 'NgBkY')]\n```\n如果查询是破坏性的(例如UPDATE、DELETE或INSERT)，则必须执行commit操作:\n```Python\n# 修改\nupdate_query = '''\n    UPDATE People\n    SET first_name = 'yoona'\n    WHERE last_name = 'aLhko'\n'''\nn_rows = cur.execute(update_query)\nprint n_rows  # 1\nconn.commit()\n\nselect_query = '''\n    SELECT first_name, last_name\n    FROM People\n    WHERE last_name = 'aLhko'\n'''\ncur.execute(select_query)\nresults = list(cur.fetchall())\nprint results  # [('yoona', 'aLhko')]\n```\n\n备注:\n```\n提供commit()函数的是连接本身(conn)，而不是游标(cur)。\n```\n\n### 2. MongoDB\n\n在Python中，我们用`pymongo`模块中`MongoClient`类的实例来实现MongoDB客户端。首先安装`pymongo`模块(ubuntu15.10):\n```\nsudo pip install pymongo\n```\n下面就可以创建一个无参数的客户端(适用于典型的安装了本地服务器的情况)，也可以用服务器的主机名和端口号作为参数创建客户端，或使用服务器的统一资源标识符(URI)作为参数创建客户端：\n```Python\n# 使用默认的初始化方式\nclient1 = pymongo.MongoClient()\n# 指定主机和端口号\nclient2 = pymongo.MongoClient(\"localhost\", 27017)\n# 用URI方式指定主机和端口号\nclient3 = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n```\n\n客户一旦端建立了与数据库服务器的连接，就可以选择当前激活的数据库，进而选择激活的集合。可以使用面向对象(\".\")或字典样式的符号。如果所选的数据库或集合不存在，服务器会立即创建它们：\n```Python\n# 创建并选择活动数据库的两种方法\ndb = client1.test_db\ndb = client1[\"test_db\"]\n\n# 创建并选择活动集合的两种方法\npeople = db.people\npeople = db[\"people\"]\n```\n\npymongo模块用字典变量来表示MongoDB文档。表示对象的每个字典必须具有_id这个键。如果该键不存在，服务器会自动生成它。\n\n集合对象提供用于在文档集合中插入、搜索、删除、更新、替换和聚合文档以及创建索引的功能。\n\n函数`insert_one(doc)`和`insert_many(docs)`将文档或文档列表插入集合。它们分别返回对象`InsertOneResult`或`InsertManyResult`，这两个对象分别提供`inserted_id`和`inserted_ids`属性。当文档没有提供明确的唯一键时，就需要使用这两个属性值作为文档的唯一键。如果指定了_id键，就是用该值作为唯一键：\n```Python\n# 插入\nperson1 = {\"name\": \"John\", \"dob\": \"2017-11-24\"}\nperson_id1 = people.insert_one(person1).inserted_id\nprint person_id1  # 5a1d4ba92317d71bb605f8ce\n\nperson2 = {\"_id\": \"XVT162\", \"name\": \"Jane\", \"dob\": \"2017-11-27\"}\nperson_id2 = people.insert_one(person2).inserted_id\nprint person_id2  # XVT162\n\npersons = [{\"name\": \"Lucy\", \"dob\": \"2017-11-12\"}, {\"name\": \"Tom\"}]\nresult = people.insert_many(persons)\nprint result.inserted_ids  # [ObjectId('5a1d4c832317d71c2c4e284f'), ObjectId('5a1d4c832317d71c2c4e2850')]\n```\n\n函数`find_one()`和`find()`分别给出匹配可选属性的一个或多个文档，其中`find_one()`返回文档，而`find()`返回一个游标(一个生成器)，可以使用`list()`函数将该游标转换为列表，或者在for循环中将其用作迭代器。如果将字典作为参数传递给这些函数中的任意一个，函数将给出与字典的所有键值相等的文档：\n```Python\n# 查找\n\neveryone = people.find()\nprint list(everyone)  # [{u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'}, {u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}, {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'}, {u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'}, {u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'}]\n\nprint list(people.find({\"dob\": \"2017-11-27\"}))  # [{u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}]\n\none_people = people.find_one()\nprint one_people  # {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'}\n\none_people = people.find_one({\"name\": \"Lucy\"})\nprint one_people  # {u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'}\n\none_people = people.find_one({\"_id\": \"XVT162\"})\nprint one_people  # {u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}\n```\n\n下面介绍几个实现数据聚合和排序的分组和排序函数。函数`sort()`对查询的结果进行排序。当以无参数的方式调用它时，该函数按键_id的升序进行排序。函数`count()`返回查询结果中或整个集合中的文档数量：\n```Python\n# 聚合\n\ncount = people.count()\nprint count  # 5\n\ncount = people.find({\"dob\": \"2017-11-27\"}).count()\nprint count  # 1\n\npeople_list = list(people.find().sort(\"dob\"))\nprint people_list  # [{u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'}, {u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'}, {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'}, {u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'}, {u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'}]\n```\n函数`delete_one(doc)`和`delete_many(docs)`从集合中删除字典doc所标识的一个或多个文档。如果要在删除所有文档的同时保留集合，需使用空字典作为参数调用函数`delete_many({})`：\n```Python\n# 删除\nresult = people.delete_many({\"dob\": \"2017-11-27\"})\nprint result.deleted_count  # 1\n```\n\n\n\n\n\n\n\n\n\n\n\n来自于:Python数据科学入门\n","slug":"Python/Python 数据库操作","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cje58tiul004wordbsd8fiua4","content":"<h3 id=\"1-MySQL\"><a href=\"#1-MySQL\" class=\"headerlink\" title=\"1. MySQL\"></a>1. MySQL</h3><p>Python使用数据库驱动模块与MySQL通信。诸如pymysql等许多数据库驱动都是免费的。这里我们将使用pymysql，它是Anaconda的一部分。驱动程序经过激活后与数据库服务器相连，然后将Python的函数调用转换为数据库查询，反过来，将数据库结果转换为Python数据结构。</p>\n<p><code>connect()</code>函数需要以下信息：数据库(名称)、数据库服务器的位置(主机和端口号)和数据库用户(名称和密码)。如果数据库成功连接，则返回连接标识符。接下来，创建与数据库连接相关联的数据库游标：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pymysql</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 连接数据库</span></span><br><span class=\"line\">conn = pymysql.connect(host=<span class=\"string\">\"localhost\"</span>, port=<span class=\"number\">3306</span>, user=<span class=\"string\">\"root\"</span>, passwd=<span class=\"string\">\"root\"</span>, db=<span class=\"string\">\"test\"</span>)</span><br><span class=\"line\">cur = conn.cursor()</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">使用pymysql需要导入pymysql库：import pymysql。</span><br></pre></td></tr></table></figure></p>\n<p>游标的<code>execute()</code>函数向数据服务器提交要执行的查询命令，并返回受影响的行数(如果查询是非破坏性的，则返回零)。与命令行MySQL查询不同，pymysql查询语句不需要在结尾加上分号。<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查询</span></span><br><span class=\"line\">query = <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    SELECT first_name, last_name</span></span><br><span class=\"line\"><span class=\"string\">    FROM People</span></span><br><span class=\"line\"><span class=\"string\">    ORDER BY dob</span></span><br><span class=\"line\"><span class=\"string\">    LIMIT 3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">n_rows = cur.execute(query)</span><br><span class=\"line\"><span class=\"keyword\">print</span> n_rows  <span class=\"comment\"># 3</span></span><br></pre></td></tr></table></figure></p>\n<p>如果提交非破坏性查询(比如SELECT)，需要使用游标函数<code>fetchall()</code>获取所有匹配的记录。该函数返回一个生成器，可以将其转换为列字段的元组构成的列表：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">results = list(cur.fetchall())</span><br><span class=\"line\"><span class=\"keyword\">print</span> results  <span class=\"comment\"># [('gztAQV', 'aLhko'), ('ZXMtHd', 'cgwjI'), ('yHwDRF', 'NgBkY')]</span></span><br></pre></td></tr></table></figure></p>\n<p>如果查询是破坏性的(例如UPDATE、DELETE或INSERT)，则必须执行commit操作:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 修改</span></span><br><span class=\"line\">update_query = <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    UPDATE People</span></span><br><span class=\"line\"><span class=\"string\">    SET first_name = 'yoona'</span></span><br><span class=\"line\"><span class=\"string\">    WHERE last_name = 'aLhko'</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">n_rows = cur.execute(update_query)</span><br><span class=\"line\"><span class=\"keyword\">print</span> n_rows  <span class=\"comment\"># 1</span></span><br><span class=\"line\">conn.commit()</span><br><span class=\"line\"></span><br><span class=\"line\">select_query = <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    SELECT first_name, last_name</span></span><br><span class=\"line\"><span class=\"string\">    FROM People</span></span><br><span class=\"line\"><span class=\"string\">    WHERE last_name = 'aLhko'</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">cur.execute(select_query)</span><br><span class=\"line\">results = list(cur.fetchall())</span><br><span class=\"line\"><span class=\"keyword\">print</span> results  <span class=\"comment\"># [('yoona', 'aLhko')]</span></span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">提供commit()函数的是连接本身(conn)，而不是游标(cur)。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-MongoDB\"><a href=\"#2-MongoDB\" class=\"headerlink\" title=\"2. MongoDB\"></a>2. MongoDB</h3><p>在Python中，我们用<code>pymongo</code>模块中<code>MongoClient</code>类的实例来实现MongoDB客户端。首先安装<code>pymongo</code>模块(ubuntu15.10):<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo pip install pymongo</span><br></pre></td></tr></table></figure></p>\n<p>下面就可以创建一个无参数的客户端(适用于典型的安装了本地服务器的情况)，也可以用服务器的主机名和端口号作为参数创建客户端，或使用服务器的统一资源标识符(URI)作为参数创建客户端：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用默认的初始化方式</span></span><br><span class=\"line\">client1 = pymongo.MongoClient()</span><br><span class=\"line\"><span class=\"comment\"># 指定主机和端口号</span></span><br><span class=\"line\">client2 = pymongo.MongoClient(<span class=\"string\">\"localhost\"</span>, <span class=\"number\">27017</span>)</span><br><span class=\"line\"><span class=\"comment\"># 用URI方式指定主机和端口号</span></span><br><span class=\"line\">client3 = pymongo.MongoClient(<span class=\"string\">\"mongodb://localhost:27017/\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>客户一旦端建立了与数据库服务器的连接，就可以选择当前激活的数据库，进而选择激活的集合。可以使用面向对象(“.”)或字典样式的符号。如果所选的数据库或集合不存在，服务器会立即创建它们：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建并选择活动数据库的两种方法</span></span><br><span class=\"line\">db = client1.test_db</span><br><span class=\"line\">db = client1[<span class=\"string\">\"test_db\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建并选择活动集合的两种方法</span></span><br><span class=\"line\">people = db.people</span><br><span class=\"line\">people = db[<span class=\"string\">\"people\"</span>]</span><br></pre></td></tr></table></figure></p>\n<p>pymongo模块用字典变量来表示MongoDB文档。表示对象的每个字典必须具有_id这个键。如果该键不存在，服务器会自动生成它。</p>\n<p>集合对象提供用于在文档集合中插入、搜索、删除、更新、替换和聚合文档以及创建索引的功能。</p>\n<p>函数<code>insert_one(doc)</code>和<code>insert_many(docs)</code>将文档或文档列表插入集合。它们分别返回对象<code>InsertOneResult</code>或<code>InsertManyResult</code>，这两个对象分别提供<code>inserted_id</code>和<code>inserted_ids</code>属性。当文档没有提供明确的唯一键时，就需要使用这两个属性值作为文档的唯一键。如果指定了_id键，就是用该值作为唯一键：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 插入</span></span><br><span class=\"line\">person1 = &#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"John\"</span>, <span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-24\"</span>&#125;</span><br><span class=\"line\">person_id1 = people.insert_one(person1).inserted_id</span><br><span class=\"line\"><span class=\"keyword\">print</span> person_id1  <span class=\"comment\"># 5a1d4ba92317d71bb605f8ce</span></span><br><span class=\"line\"></span><br><span class=\"line\">person2 = &#123;<span class=\"string\">\"_id\"</span>: <span class=\"string\">\"XVT162\"</span>, <span class=\"string\">\"name\"</span>: <span class=\"string\">\"Jane\"</span>, <span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;</span><br><span class=\"line\">person_id2 = people.insert_one(person2).inserted_id</span><br><span class=\"line\"><span class=\"keyword\">print</span> person_id2  <span class=\"comment\"># XVT162</span></span><br><span class=\"line\"></span><br><span class=\"line\">persons = [&#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"Lucy\"</span>, <span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-12\"</span>&#125;, &#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"Tom\"</span>&#125;]</span><br><span class=\"line\">result = people.insert_many(persons)</span><br><span class=\"line\"><span class=\"keyword\">print</span> result.inserted_ids  <span class=\"comment\"># [ObjectId('5a1d4c832317d71c2c4e284f'), ObjectId('5a1d4c832317d71c2c4e2850')]</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>find_one()</code>和<code>find()</code>分别给出匹配可选属性的一个或多个文档，其中<code>find_one()</code>返回文档，而<code>find()</code>返回一个游标(一个生成器)，可以使用<code>list()</code>函数将该游标转换为列表，或者在for循环中将其用作迭代器。如果将字典作为参数传递给这些函数中的任意一个，函数将给出与字典的所有键值相等的文档：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查找</span></span><br><span class=\"line\"></span><br><span class=\"line\">everyone = people.find()</span><br><span class=\"line\"><span class=\"keyword\">print</span> list(everyone)  <span class=\"comment\"># [&#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">print</span> list(people.find(&#123;<span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;))  <span class=\"comment\"># [&#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\">one_people = people.find_one()</span><br><span class=\"line\"><span class=\"keyword\">print</span> one_people  <span class=\"comment\"># &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">one_people = people.find_one(&#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"Lucy\"</span>&#125;)</span><br><span class=\"line\"><span class=\"keyword\">print</span> one_people  <span class=\"comment\"># &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">one_people = people.find_one(&#123;<span class=\"string\">\"_id\"</span>: <span class=\"string\">\"XVT162\"</span>&#125;)</span><br><span class=\"line\"><span class=\"keyword\">print</span> one_people  <span class=\"comment\"># &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>下面介绍几个实现数据聚合和排序的分组和排序函数。函数<code>sort()</code>对查询的结果进行排序。当以无参数的方式调用它时，该函数按键_id的升序进行排序。函数<code>count()</code>返回查询结果中或整个集合中的文档数量：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 聚合</span></span><br><span class=\"line\"></span><br><span class=\"line\">count = people.count()</span><br><span class=\"line\"><span class=\"keyword\">print</span> count  <span class=\"comment\"># 5</span></span><br><span class=\"line\"></span><br><span class=\"line\">count = people.find(&#123;<span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;).count()</span><br><span class=\"line\"><span class=\"keyword\">print</span> count  <span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\">people_list = list(people.find().sort(<span class=\"string\">\"dob\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">print</span> people_list  <span class=\"comment\"># [&#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;]</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>delete_one(doc)</code>和<code>delete_many(docs)</code>从集合中删除字典doc所标识的一个或多个文档。如果要在删除所有文档的同时保留集合，需使用空字典作为参数调用函数<code>delete_many({})</code>：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 删除</span></span><br><span class=\"line\">result = people.delete_many(&#123;<span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;)</span><br><span class=\"line\"><span class=\"keyword\">print</span> result.deleted_count  <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure></p>\n<p>来自于:Python数据科学入门</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-MySQL\"><a href=\"#1-MySQL\" class=\"headerlink\" title=\"1. MySQL\"></a>1. MySQL</h3><p>Python使用数据库驱动模块与MySQL通信。诸如pymysql等许多数据库驱动都是免费的。这里我们将使用pymysql，它是Anaconda的一部分。驱动程序经过激活后与数据库服务器相连，然后将Python的函数调用转换为数据库查询，反过来，将数据库结果转换为Python数据结构。</p>\n<p><code>connect()</code>函数需要以下信息：数据库(名称)、数据库服务器的位置(主机和端口号)和数据库用户(名称和密码)。如果数据库成功连接，则返回连接标识符。接下来，创建与数据库连接相关联的数据库游标：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pymysql</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 连接数据库</span></span><br><span class=\"line\">conn = pymysql.connect(host=<span class=\"string\">\"localhost\"</span>, port=<span class=\"number\">3306</span>, user=<span class=\"string\">\"root\"</span>, passwd=<span class=\"string\">\"root\"</span>, db=<span class=\"string\">\"test\"</span>)</span><br><span class=\"line\">cur = conn.cursor()</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">使用pymysql需要导入pymysql库：import pymysql。</span><br></pre></td></tr></table></figure></p>\n<p>游标的<code>execute()</code>函数向数据服务器提交要执行的查询命令，并返回受影响的行数(如果查询是非破坏性的，则返回零)。与命令行MySQL查询不同，pymysql查询语句不需要在结尾加上分号。<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查询</span></span><br><span class=\"line\">query = <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    SELECT first_name, last_name</span></span><br><span class=\"line\"><span class=\"string\">    FROM People</span></span><br><span class=\"line\"><span class=\"string\">    ORDER BY dob</span></span><br><span class=\"line\"><span class=\"string\">    LIMIT 3</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">n_rows = cur.execute(query)</span><br><span class=\"line\"><span class=\"keyword\">print</span> n_rows  <span class=\"comment\"># 3</span></span><br></pre></td></tr></table></figure></p>\n<p>如果提交非破坏性查询(比如SELECT)，需要使用游标函数<code>fetchall()</code>获取所有匹配的记录。该函数返回一个生成器，可以将其转换为列字段的元组构成的列表：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">results = list(cur.fetchall())</span><br><span class=\"line\"><span class=\"keyword\">print</span> results  <span class=\"comment\"># [('gztAQV', 'aLhko'), ('ZXMtHd', 'cgwjI'), ('yHwDRF', 'NgBkY')]</span></span><br></pre></td></tr></table></figure></p>\n<p>如果查询是破坏性的(例如UPDATE、DELETE或INSERT)，则必须执行commit操作:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 修改</span></span><br><span class=\"line\">update_query = <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    UPDATE People</span></span><br><span class=\"line\"><span class=\"string\">    SET first_name = 'yoona'</span></span><br><span class=\"line\"><span class=\"string\">    WHERE last_name = 'aLhko'</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">n_rows = cur.execute(update_query)</span><br><span class=\"line\"><span class=\"keyword\">print</span> n_rows  <span class=\"comment\"># 1</span></span><br><span class=\"line\">conn.commit()</span><br><span class=\"line\"></span><br><span class=\"line\">select_query = <span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    SELECT first_name, last_name</span></span><br><span class=\"line\"><span class=\"string\">    FROM People</span></span><br><span class=\"line\"><span class=\"string\">    WHERE last_name = 'aLhko'</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\">cur.execute(select_query)</span><br><span class=\"line\">results = list(cur.fetchall())</span><br><span class=\"line\"><span class=\"keyword\">print</span> results  <span class=\"comment\"># [('yoona', 'aLhko')]</span></span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">提供commit()函数的是连接本身(conn)，而不是游标(cur)。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-MongoDB\"><a href=\"#2-MongoDB\" class=\"headerlink\" title=\"2. MongoDB\"></a>2. MongoDB</h3><p>在Python中，我们用<code>pymongo</code>模块中<code>MongoClient</code>类的实例来实现MongoDB客户端。首先安装<code>pymongo</code>模块(ubuntu15.10):<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo pip install pymongo</span><br></pre></td></tr></table></figure></p>\n<p>下面就可以创建一个无参数的客户端(适用于典型的安装了本地服务器的情况)，也可以用服务器的主机名和端口号作为参数创建客户端，或使用服务器的统一资源标识符(URI)作为参数创建客户端：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用默认的初始化方式</span></span><br><span class=\"line\">client1 = pymongo.MongoClient()</span><br><span class=\"line\"><span class=\"comment\"># 指定主机和端口号</span></span><br><span class=\"line\">client2 = pymongo.MongoClient(<span class=\"string\">\"localhost\"</span>, <span class=\"number\">27017</span>)</span><br><span class=\"line\"><span class=\"comment\"># 用URI方式指定主机和端口号</span></span><br><span class=\"line\">client3 = pymongo.MongoClient(<span class=\"string\">\"mongodb://localhost:27017/\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>客户一旦端建立了与数据库服务器的连接，就可以选择当前激活的数据库，进而选择激活的集合。可以使用面向对象(“.”)或字典样式的符号。如果所选的数据库或集合不存在，服务器会立即创建它们：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建并选择活动数据库的两种方法</span></span><br><span class=\"line\">db = client1.test_db</span><br><span class=\"line\">db = client1[<span class=\"string\">\"test_db\"</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建并选择活动集合的两种方法</span></span><br><span class=\"line\">people = db.people</span><br><span class=\"line\">people = db[<span class=\"string\">\"people\"</span>]</span><br></pre></td></tr></table></figure></p>\n<p>pymongo模块用字典变量来表示MongoDB文档。表示对象的每个字典必须具有_id这个键。如果该键不存在，服务器会自动生成它。</p>\n<p>集合对象提供用于在文档集合中插入、搜索、删除、更新、替换和聚合文档以及创建索引的功能。</p>\n<p>函数<code>insert_one(doc)</code>和<code>insert_many(docs)</code>将文档或文档列表插入集合。它们分别返回对象<code>InsertOneResult</code>或<code>InsertManyResult</code>，这两个对象分别提供<code>inserted_id</code>和<code>inserted_ids</code>属性。当文档没有提供明确的唯一键时，就需要使用这两个属性值作为文档的唯一键。如果指定了_id键，就是用该值作为唯一键：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 插入</span></span><br><span class=\"line\">person1 = &#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"John\"</span>, <span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-24\"</span>&#125;</span><br><span class=\"line\">person_id1 = people.insert_one(person1).inserted_id</span><br><span class=\"line\"><span class=\"keyword\">print</span> person_id1  <span class=\"comment\"># 5a1d4ba92317d71bb605f8ce</span></span><br><span class=\"line\"></span><br><span class=\"line\">person2 = &#123;<span class=\"string\">\"_id\"</span>: <span class=\"string\">\"XVT162\"</span>, <span class=\"string\">\"name\"</span>: <span class=\"string\">\"Jane\"</span>, <span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;</span><br><span class=\"line\">person_id2 = people.insert_one(person2).inserted_id</span><br><span class=\"line\"><span class=\"keyword\">print</span> person_id2  <span class=\"comment\"># XVT162</span></span><br><span class=\"line\"></span><br><span class=\"line\">persons = [&#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"Lucy\"</span>, <span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-12\"</span>&#125;, &#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"Tom\"</span>&#125;]</span><br><span class=\"line\">result = people.insert_many(persons)</span><br><span class=\"line\"><span class=\"keyword\">print</span> result.inserted_ids  <span class=\"comment\"># [ObjectId('5a1d4c832317d71c2c4e284f'), ObjectId('5a1d4c832317d71c2c4e2850')]</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>find_one()</code>和<code>find()</code>分别给出匹配可选属性的一个或多个文档，其中<code>find_one()</code>返回文档，而<code>find()</code>返回一个游标(一个生成器)，可以使用<code>list()</code>函数将该游标转换为列表，或者在for循环中将其用作迭代器。如果将字典作为参数传递给这些函数中的任意一个，函数将给出与字典的所有键值相等的文档：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查找</span></span><br><span class=\"line\"></span><br><span class=\"line\">everyone = people.find()</span><br><span class=\"line\"><span class=\"keyword\">print</span> list(everyone)  <span class=\"comment\"># [&#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">print</span> list(people.find(&#123;<span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;))  <span class=\"comment\"># [&#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\">one_people = people.find_one()</span><br><span class=\"line\"><span class=\"keyword\">print</span> one_people  <span class=\"comment\"># &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">one_people = people.find_one(&#123;<span class=\"string\">\"name\"</span>: <span class=\"string\">\"Lucy\"</span>&#125;)</span><br><span class=\"line\"><span class=\"keyword\">print</span> one_people  <span class=\"comment\"># &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">one_people = people.find_one(&#123;<span class=\"string\">\"_id\"</span>: <span class=\"string\">\"XVT162\"</span>&#125;)</span><br><span class=\"line\"><span class=\"keyword\">print</span> one_people  <span class=\"comment\"># &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>下面介绍几个实现数据聚合和排序的分组和排序函数。函数<code>sort()</code>对查询的结果进行排序。当以无参数的方式调用它时，该函数按键_id的升序进行排序。函数<code>count()</code>返回查询结果中或整个集合中的文档数量：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 聚合</span></span><br><span class=\"line\"></span><br><span class=\"line\">count = people.count()</span><br><span class=\"line\"><span class=\"keyword\">print</span> count  <span class=\"comment\"># 5</span></span><br><span class=\"line\"></span><br><span class=\"line\">count = people.find(&#123;<span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;).count()</span><br><span class=\"line\"><span class=\"keyword\">print</span> count  <span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\">people_list = list(people.find().sort(<span class=\"string\">\"dob\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">print</span> people_list  <span class=\"comment\"># [&#123;u'_id': ObjectId('5a1d4c832317d71c2c4e2850'), u'name': u'Tom'&#125;, &#123;u'dob': u'2017-11-12', u'_id': ObjectId('5a1d4c832317d71c2c4e284f'), u'name': u'Lucy'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4ba92317d71bb605f8ce'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-24', u'_id': ObjectId('5a1d4c7c2317d71c1bbeac4b'), u'name': u'John'&#125;, &#123;u'dob': u'2017-11-27', u'_id': u'XVT162', u'name': u'Jane'&#125;]</span></span><br></pre></td></tr></table></figure></p>\n<p>函数<code>delete_one(doc)</code>和<code>delete_many(docs)</code>从集合中删除字典doc所标识的一个或多个文档。如果要在删除所有文档的同时保留集合，需使用空字典作为参数调用函数<code>delete_many({})</code>：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 删除</span></span><br><span class=\"line\">result = people.delete_many(&#123;<span class=\"string\">\"dob\"</span>: <span class=\"string\">\"2017-11-27\"</span>&#125;)</span><br><span class=\"line\"><span class=\"keyword\">print</span> result.deleted_count  <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure></p>\n<p>来自于:Python数据科学入门</p>\n"},{"title":"Python 文件处理","date":"2016-05-17T09:39:17.000Z","updated":"2017-12-01T02:20:17.000Z","_content":"\n### 1. csv文件处理\n\n记录中的字段通常由逗号分隔，但其他分隔符也是比较常见的，例如制表符（制表符分隔值，TSV）、冒号、分号和竖直条等。建议在自己创建的文件中坚持使用逗号作为分隔符，同时保证编写的处理程序能正确处理使用其他分隔符的CSV文件。\n\n备注:\n```\n有时看起来像分隔符的字符并不是分隔符。通过将字段包含在双引号中，可确保字段中的分隔符只是作为变量值的一部分，不参与分割字段(如...,\"Hello, world\",...)。\n```\n\nPython的csv模块提供了一个CSV读取器和一个CSV写入器。两个对象的第一个参数都是已打开的文本文件句柄(在下面的示例中，使用newline=''选项打开文件，从而避免删除行的操作)。必要时可以通过可选参数delimiter和quotechar，提供默认的分隔符和引用字符。Python还提供了控制转义字符、行终止符等定界符的可选参数。\n```Python\nwith open(\"somefile.csv\", newline='') as infile:\n  reader = csv.reader(infile, delimiter=',', quotechar='\"')\n```\nCSV文件的第一条记录通常包含列标题，可能与文件的其余部分有所不同。这只是一个常见的做法，并非CSV格式本身的特性。\n\nCSV读取器提供了一个可以在for循环中使用的迭代器接口。迭代器将下一条记录作为一个字符串字段列表返回。读取器不会将字段转换为任何数值数据类型，另外，除非传递可选参数`skipinitialspace=True`，否则不会删除前导的空白。\n\n如果事先不知道CSV文件的大小，而且文件可能很大，则不宜一次性读取所有记录，而应使用增量的、迭代的、逐行的处理方式：读出一行，处理一行，再获取另一行。\n\nCSV写入器提供`writerow()`和`writerows()`两个函数。`writerow()`将一个字符串或数字序列作为一条记录写入文件。该函数将数字转换成字符串，因此不必担心数值表示的问题。类似地，`writerows()`将字符串或数字序列的列表作为记录集写入文件。\n\n在下面的示例中，使用csv模块从CSV文件中提取Answer.Age列。假设此列肯定存在，但列的索引未知。一旦获得数值，借助statistics模块就能得到年龄的平均值和标准偏差。\n\n首先，打开文件并读取数据：\n```Python\nwith open(\"demographics.csv\", newline='') as infile:\n  data = list(csv.reader(infile))\n```\n检查文件中的第一个记录 data[0] ，它必须包含感兴趣的列标题：\n```Python\nageIndex = data[0].index(\"Answer.Age\")\n```\n最后，访问剩余记录中感兴趣的字段，并计算和显示统计数据：\n```Python\nages = [int(row[ageIndex]) for row in data[1:]]\nprint(statistics.mean(ages), statistics.stdev(ages))\n```\ncsv和statistics模块是底层的、快速而粗糙的工具。在第6章，你将了解如何在更为复杂的项目中使用pandas的数据frame，完成那些比对几列数据进行琐碎的检索要高端得多的任务。\n\n### 2. Json文件处理\n\n需要注意的一点就是某些Python数据类型和结构(比如集合和复数)无法存储在JSON文件中。因此，要在导出到JSON之前，将它们转换为JSON可表示的数据类型。例如，将复数存储为两个double类型的数字组成的数组，将集合存储为一个由集合的各项所组成的数组。\n\n将复杂数据存储到JSON文件中的操作称为JSON序列化，相应的反向操作则称为JSON反序列化。Python通过json模块中的函数，实现JSON序列化和反序列化。\n\n函数|说明\n---|---\ndump()| 将Python对象导出到文件中\ndumps()| 将Python对象编码成JSON字符串\nload()| 将文件导出为Python对象\nloads()| 将已编码的JSON字符串解码为Python对象\n\n备注:\n```\n把多个对象存储在一个JSON文件中是一种错误的做法，但如果已有的文件包含多个对象，则可将其以文本的方式读入，进而将文本转换为对象数组（在文本中各个对象之间添加方括号和逗号分隔符），并使用loads()将文本反序列化为对象列表。\n```\n\nExample:\n\n以下代码片段实现了将任意（可序列化的）对象按先序列化、后反序列化的顺序进行处理：\n```Python\n# 将Python对象编码成JSON字符串\ndata = [{'apple': 23, 'bear': 11, 'banana': 54}]\ns = json.dumps(data)\nprint type(s)  # <type 'str'>\nprint s  # [{\"apple\": 23, \"bear\": 11, \"banana\": 54}]\n\n# 将Python对象编码成JSON字符串并格式化输出\nformat_str = json.dumps(data, sort_keys=True, indent=4, separators=(',', ': '))\nprint format_str\n'''\n[\n    {\n        \"apple\": 23,\n        \"banana\": 54,\n        \"bear\": 11\n    }\n]\n'''\n\n# 将已编码的JSON字符串解码为Python对象\ndata = '[{\"apple\": 23, \"bear\": 11, \"banana\": 54}]'\no = json.loads(data)\nprint type(o)  # <type 'list'>\nprint o[0].get('apple', 0)  # 23\n\n# 将Python对象导出到文件中\ndata = [{'apple': 23, 'bear': 11, 'banana': 54}]\nwith open(\"/home/xiaosi/data.json\", \"w\") as f_dump:\n    s_dump = json.dump(data, f_dump, ensure_ascii=False)\n\n# 将文件导出为Python对象\nwith open(\"/home/xiaosi/data.json\", 'r') as f_load:\n    ob = json.load(f_load)\nprint type(ob)  # <type 'list'>\nprint ob[0].get('banana')  # 54\n```\n\n备注:\n```\n使用JSON函数需要导入json库：import json。\n```\nJSON 类型转换到 python 的类型对照表：\n\nJSON|Python\n---|---\nobject|dict\narray|list\nstring|unicode\nnumber (int)|int, long\nnumber (real)|float\ntrue|True\nfalse|False\nnull|None\n\n\n\n\n\n来自于:<Python数据科学入门>\n","source":"_posts/Python/Python 文件处理.md","raw":"\n---\ntitle: Python 文件处理\ndate: 2016-05-17 17:39:17\nupdated: 2017-12-01 10:20:17\ntags:\n- Python\n\ncategories: Python\n---\n\n### 1. csv文件处理\n\n记录中的字段通常由逗号分隔，但其他分隔符也是比较常见的，例如制表符（制表符分隔值，TSV）、冒号、分号和竖直条等。建议在自己创建的文件中坚持使用逗号作为分隔符，同时保证编写的处理程序能正确处理使用其他分隔符的CSV文件。\n\n备注:\n```\n有时看起来像分隔符的字符并不是分隔符。通过将字段包含在双引号中，可确保字段中的分隔符只是作为变量值的一部分，不参与分割字段(如...,\"Hello, world\",...)。\n```\n\nPython的csv模块提供了一个CSV读取器和一个CSV写入器。两个对象的第一个参数都是已打开的文本文件句柄(在下面的示例中，使用newline=''选项打开文件，从而避免删除行的操作)。必要时可以通过可选参数delimiter和quotechar，提供默认的分隔符和引用字符。Python还提供了控制转义字符、行终止符等定界符的可选参数。\n```Python\nwith open(\"somefile.csv\", newline='') as infile:\n  reader = csv.reader(infile, delimiter=',', quotechar='\"')\n```\nCSV文件的第一条记录通常包含列标题，可能与文件的其余部分有所不同。这只是一个常见的做法，并非CSV格式本身的特性。\n\nCSV读取器提供了一个可以在for循环中使用的迭代器接口。迭代器将下一条记录作为一个字符串字段列表返回。读取器不会将字段转换为任何数值数据类型，另外，除非传递可选参数`skipinitialspace=True`，否则不会删除前导的空白。\n\n如果事先不知道CSV文件的大小，而且文件可能很大，则不宜一次性读取所有记录，而应使用增量的、迭代的、逐行的处理方式：读出一行，处理一行，再获取另一行。\n\nCSV写入器提供`writerow()`和`writerows()`两个函数。`writerow()`将一个字符串或数字序列作为一条记录写入文件。该函数将数字转换成字符串，因此不必担心数值表示的问题。类似地，`writerows()`将字符串或数字序列的列表作为记录集写入文件。\n\n在下面的示例中，使用csv模块从CSV文件中提取Answer.Age列。假设此列肯定存在，但列的索引未知。一旦获得数值，借助statistics模块就能得到年龄的平均值和标准偏差。\n\n首先，打开文件并读取数据：\n```Python\nwith open(\"demographics.csv\", newline='') as infile:\n  data = list(csv.reader(infile))\n```\n检查文件中的第一个记录 data[0] ，它必须包含感兴趣的列标题：\n```Python\nageIndex = data[0].index(\"Answer.Age\")\n```\n最后，访问剩余记录中感兴趣的字段，并计算和显示统计数据：\n```Python\nages = [int(row[ageIndex]) for row in data[1:]]\nprint(statistics.mean(ages), statistics.stdev(ages))\n```\ncsv和statistics模块是底层的、快速而粗糙的工具。在第6章，你将了解如何在更为复杂的项目中使用pandas的数据frame，完成那些比对几列数据进行琐碎的检索要高端得多的任务。\n\n### 2. Json文件处理\n\n需要注意的一点就是某些Python数据类型和结构(比如集合和复数)无法存储在JSON文件中。因此，要在导出到JSON之前，将它们转换为JSON可表示的数据类型。例如，将复数存储为两个double类型的数字组成的数组，将集合存储为一个由集合的各项所组成的数组。\n\n将复杂数据存储到JSON文件中的操作称为JSON序列化，相应的反向操作则称为JSON反序列化。Python通过json模块中的函数，实现JSON序列化和反序列化。\n\n函数|说明\n---|---\ndump()| 将Python对象导出到文件中\ndumps()| 将Python对象编码成JSON字符串\nload()| 将文件导出为Python对象\nloads()| 将已编码的JSON字符串解码为Python对象\n\n备注:\n```\n把多个对象存储在一个JSON文件中是一种错误的做法，但如果已有的文件包含多个对象，则可将其以文本的方式读入，进而将文本转换为对象数组（在文本中各个对象之间添加方括号和逗号分隔符），并使用loads()将文本反序列化为对象列表。\n```\n\nExample:\n\n以下代码片段实现了将任意（可序列化的）对象按先序列化、后反序列化的顺序进行处理：\n```Python\n# 将Python对象编码成JSON字符串\ndata = [{'apple': 23, 'bear': 11, 'banana': 54}]\ns = json.dumps(data)\nprint type(s)  # <type 'str'>\nprint s  # [{\"apple\": 23, \"bear\": 11, \"banana\": 54}]\n\n# 将Python对象编码成JSON字符串并格式化输出\nformat_str = json.dumps(data, sort_keys=True, indent=4, separators=(',', ': '))\nprint format_str\n'''\n[\n    {\n        \"apple\": 23,\n        \"banana\": 54,\n        \"bear\": 11\n    }\n]\n'''\n\n# 将已编码的JSON字符串解码为Python对象\ndata = '[{\"apple\": 23, \"bear\": 11, \"banana\": 54}]'\no = json.loads(data)\nprint type(o)  # <type 'list'>\nprint o[0].get('apple', 0)  # 23\n\n# 将Python对象导出到文件中\ndata = [{'apple': 23, 'bear': 11, 'banana': 54}]\nwith open(\"/home/xiaosi/data.json\", \"w\") as f_dump:\n    s_dump = json.dump(data, f_dump, ensure_ascii=False)\n\n# 将文件导出为Python对象\nwith open(\"/home/xiaosi/data.json\", 'r') as f_load:\n    ob = json.load(f_load)\nprint type(ob)  # <type 'list'>\nprint ob[0].get('banana')  # 54\n```\n\n备注:\n```\n使用JSON函数需要导入json库：import json。\n```\nJSON 类型转换到 python 的类型对照表：\n\nJSON|Python\n---|---\nobject|dict\narray|list\nstring|unicode\nnumber (int)|int, long\nnumber (real)|float\ntrue|True\nfalse|False\nnull|None\n\n\n\n\n\n来自于:<Python数据科学入门>\n","slug":"Python/Python 文件处理","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cje58tiup0051ordbe8ybz85v","content":"<h3 id=\"1-csv文件处理\"><a href=\"#1-csv文件处理\" class=\"headerlink\" title=\"1. csv文件处理\"></a>1. csv文件处理</h3><p>记录中的字段通常由逗号分隔，但其他分隔符也是比较常见的，例如制表符（制表符分隔值，TSV）、冒号、分号和竖直条等。建议在自己创建的文件中坚持使用逗号作为分隔符，同时保证编写的处理程序能正确处理使用其他分隔符的CSV文件。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">有时看起来像分隔符的字符并不是分隔符。通过将字段包含在双引号中，可确保字段中的分隔符只是作为变量值的一部分，不参与分割字段(如...,&quot;Hello, world&quot;,...)。</span><br></pre></td></tr></table></figure></p>\n<p>Python的csv模块提供了一个CSV读取器和一个CSV写入器。两个对象的第一个参数都是已打开的文本文件句柄(在下面的示例中，使用newline=’’选项打开文件，从而避免删除行的操作)。必要时可以通过可选参数delimiter和quotechar，提供默认的分隔符和引用字符。Python还提供了控制转义字符、行终止符等定界符的可选参数。<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"somefile.csv\"</span>, newline=<span class=\"string\">''</span>) <span class=\"keyword\">as</span> infile:</span><br><span class=\"line\">  reader = csv.reader(infile, delimiter=<span class=\"string\">','</span>, quotechar=<span class=\"string\">'\"'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>CSV文件的第一条记录通常包含列标题，可能与文件的其余部分有所不同。这只是一个常见的做法，并非CSV格式本身的特性。</p>\n<p>CSV读取器提供了一个可以在for循环中使用的迭代器接口。迭代器将下一条记录作为一个字符串字段列表返回。读取器不会将字段转换为任何数值数据类型，另外，除非传递可选参数<code>skipinitialspace=True</code>，否则不会删除前导的空白。</p>\n<p>如果事先不知道CSV文件的大小，而且文件可能很大，则不宜一次性读取所有记录，而应使用增量的、迭代的、逐行的处理方式：读出一行，处理一行，再获取另一行。</p>\n<p>CSV写入器提供<code>writerow()</code>和<code>writerows()</code>两个函数。<code>writerow()</code>将一个字符串或数字序列作为一条记录写入文件。该函数将数字转换成字符串，因此不必担心数值表示的问题。类似地，<code>writerows()</code>将字符串或数字序列的列表作为记录集写入文件。</p>\n<p>在下面的示例中，使用csv模块从CSV文件中提取Answer.Age列。假设此列肯定存在，但列的索引未知。一旦获得数值，借助statistics模块就能得到年龄的平均值和标准偏差。</p>\n<p>首先，打开文件并读取数据：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"demographics.csv\"</span>, newline=<span class=\"string\">''</span>) <span class=\"keyword\">as</span> infile:</span><br><span class=\"line\">  data = list(csv.reader(infile))</span><br></pre></td></tr></table></figure></p>\n<p>检查文件中的第一个记录 data[0] ，它必须包含感兴趣的列标题：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ageIndex = data[<span class=\"number\">0</span>].index(<span class=\"string\">\"Answer.Age\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>最后，访问剩余记录中感兴趣的字段，并计算和显示统计数据：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ages = [int(row[ageIndex]) <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> data[<span class=\"number\">1</span>:]]</span><br><span class=\"line\">print(statistics.mean(ages), statistics.stdev(ages))</span><br></pre></td></tr></table></figure></p>\n<p>csv和statistics模块是底层的、快速而粗糙的工具。在第6章，你将了解如何在更为复杂的项目中使用pandas的数据frame，完成那些比对几列数据进行琐碎的检索要高端得多的任务。</p>\n<h3 id=\"2-Json文件处理\"><a href=\"#2-Json文件处理\" class=\"headerlink\" title=\"2. Json文件处理\"></a>2. Json文件处理</h3><p>需要注意的一点就是某些Python数据类型和结构(比如集合和复数)无法存储在JSON文件中。因此，要在导出到JSON之前，将它们转换为JSON可表示的数据类型。例如，将复数存储为两个double类型的数字组成的数组，将集合存储为一个由集合的各项所组成的数组。</p>\n<p>将复杂数据存储到JSON文件中的操作称为JSON序列化，相应的反向操作则称为JSON反序列化。Python通过json模块中的函数，实现JSON序列化和反序列化。</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>dump()</td>\n<td>将Python对象导出到文件中</td>\n</tr>\n<tr>\n<td>dumps()</td>\n<td>将Python对象编码成JSON字符串</td>\n</tr>\n<tr>\n<td>load()</td>\n<td>将文件导出为Python对象</td>\n</tr>\n<tr>\n<td>loads()</td>\n<td>将已编码的JSON字符串解码为Python对象</td>\n</tr>\n</tbody>\n</table>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">把多个对象存储在一个JSON文件中是一种错误的做法，但如果已有的文件包含多个对象，则可将其以文本的方式读入，进而将文本转换为对象数组（在文本中各个对象之间添加方括号和逗号分隔符），并使用loads()将文本反序列化为对象列表。</span><br></pre></td></tr></table></figure></p>\n<p>Example:</p>\n<p>以下代码片段实现了将任意（可序列化的）对象按先序列化、后反序列化的顺序进行处理：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将Python对象编码成JSON字符串</span></span><br><span class=\"line\">data = [&#123;<span class=\"string\">'apple'</span>: <span class=\"number\">23</span>, <span class=\"string\">'bear'</span>: <span class=\"number\">11</span>, <span class=\"string\">'banana'</span>: <span class=\"number\">54</span>&#125;]</span><br><span class=\"line\">s = json.dumps(data)</span><br><span class=\"line\"><span class=\"keyword\">print</span> type(s)  <span class=\"comment\"># &lt;type 'str'&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># [&#123;\"apple\": 23, \"bear\": 11, \"banana\": 54&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将Python对象编码成JSON字符串并格式化输出</span></span><br><span class=\"line\">format_str = json.dumps(data, sort_keys=<span class=\"keyword\">True</span>, indent=<span class=\"number\">4</span>, separators=(<span class=\"string\">','</span>, <span class=\"string\">': '</span>))</span><br><span class=\"line\"><span class=\"keyword\">print</span> format_str</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">        \"apple\": 23,</span></span><br><span class=\"line\"><span class=\"string\">        \"banana\": 54,</span></span><br><span class=\"line\"><span class=\"string\">        \"bear\": 11</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将已编码的JSON字符串解码为Python对象</span></span><br><span class=\"line\">data = <span class=\"string\">'[&#123;\"apple\": 23, \"bear\": 11, \"banana\": 54&#125;]'</span></span><br><span class=\"line\">o = json.loads(data)</span><br><span class=\"line\"><span class=\"keyword\">print</span> type(o)  <span class=\"comment\"># &lt;type 'list'&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> o[<span class=\"number\">0</span>].get(<span class=\"string\">'apple'</span>, <span class=\"number\">0</span>)  <span class=\"comment\"># 23</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将Python对象导出到文件中</span></span><br><span class=\"line\">data = [&#123;<span class=\"string\">'apple'</span>: <span class=\"number\">23</span>, <span class=\"string\">'bear'</span>: <span class=\"number\">11</span>, <span class=\"string\">'banana'</span>: <span class=\"number\">54</span>&#125;]</span><br><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"/home/xiaosi/data.json\"</span>, <span class=\"string\">\"w\"</span>) <span class=\"keyword\">as</span> f_dump:</span><br><span class=\"line\">    s_dump = json.dump(data, f_dump, ensure_ascii=<span class=\"keyword\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将文件导出为Python对象</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"/home/xiaosi/data.json\"</span>, <span class=\"string\">'r'</span>) <span class=\"keyword\">as</span> f_load:</span><br><span class=\"line\">    ob = json.load(f_load)</span><br><span class=\"line\"><span class=\"keyword\">print</span> type(ob)  <span class=\"comment\"># &lt;type 'list'&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ob[<span class=\"number\">0</span>].get(<span class=\"string\">'banana'</span>)  <span class=\"comment\"># 54</span></span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">使用JSON函数需要导入json库：import json。</span><br></pre></td></tr></table></figure></p>\n<p>JSON 类型转换到 python 的类型对照表：</p>\n<table>\n<thead>\n<tr>\n<th>JSON</th>\n<th>Python</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>object</td>\n<td>dict</td>\n</tr>\n<tr>\n<td>array</td>\n<td>list</td>\n</tr>\n<tr>\n<td>string</td>\n<td>unicode</td>\n</tr>\n<tr>\n<td>number (int)</td>\n<td>int, long</td>\n</tr>\n<tr>\n<td>number (real)</td>\n<td>float</td>\n</tr>\n<tr>\n<td>true</td>\n<td>True</td>\n</tr>\n<tr>\n<td>false</td>\n<td>False</td>\n</tr>\n<tr>\n<td>null</td>\n<td>None</td>\n</tr>\n</tbody>\n</table>\n<p>来自于:<python数据科学入门></python数据科学入门></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-csv文件处理\"><a href=\"#1-csv文件处理\" class=\"headerlink\" title=\"1. csv文件处理\"></a>1. csv文件处理</h3><p>记录中的字段通常由逗号分隔，但其他分隔符也是比较常见的，例如制表符（制表符分隔值，TSV）、冒号、分号和竖直条等。建议在自己创建的文件中坚持使用逗号作为分隔符，同时保证编写的处理程序能正确处理使用其他分隔符的CSV文件。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">有时看起来像分隔符的字符并不是分隔符。通过将字段包含在双引号中，可确保字段中的分隔符只是作为变量值的一部分，不参与分割字段(如...,&quot;Hello, world&quot;,...)。</span><br></pre></td></tr></table></figure></p>\n<p>Python的csv模块提供了一个CSV读取器和一个CSV写入器。两个对象的第一个参数都是已打开的文本文件句柄(在下面的示例中，使用newline=’’选项打开文件，从而避免删除行的操作)。必要时可以通过可选参数delimiter和quotechar，提供默认的分隔符和引用字符。Python还提供了控制转义字符、行终止符等定界符的可选参数。<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"somefile.csv\"</span>, newline=<span class=\"string\">''</span>) <span class=\"keyword\">as</span> infile:</span><br><span class=\"line\">  reader = csv.reader(infile, delimiter=<span class=\"string\">','</span>, quotechar=<span class=\"string\">'\"'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>CSV文件的第一条记录通常包含列标题，可能与文件的其余部分有所不同。这只是一个常见的做法，并非CSV格式本身的特性。</p>\n<p>CSV读取器提供了一个可以在for循环中使用的迭代器接口。迭代器将下一条记录作为一个字符串字段列表返回。读取器不会将字段转换为任何数值数据类型，另外，除非传递可选参数<code>skipinitialspace=True</code>，否则不会删除前导的空白。</p>\n<p>如果事先不知道CSV文件的大小，而且文件可能很大，则不宜一次性读取所有记录，而应使用增量的、迭代的、逐行的处理方式：读出一行，处理一行，再获取另一行。</p>\n<p>CSV写入器提供<code>writerow()</code>和<code>writerows()</code>两个函数。<code>writerow()</code>将一个字符串或数字序列作为一条记录写入文件。该函数将数字转换成字符串，因此不必担心数值表示的问题。类似地，<code>writerows()</code>将字符串或数字序列的列表作为记录集写入文件。</p>\n<p>在下面的示例中，使用csv模块从CSV文件中提取Answer.Age列。假设此列肯定存在，但列的索引未知。一旦获得数值，借助statistics模块就能得到年龄的平均值和标准偏差。</p>\n<p>首先，打开文件并读取数据：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"demographics.csv\"</span>, newline=<span class=\"string\">''</span>) <span class=\"keyword\">as</span> infile:</span><br><span class=\"line\">  data = list(csv.reader(infile))</span><br></pre></td></tr></table></figure></p>\n<p>检查文件中的第一个记录 data[0] ，它必须包含感兴趣的列标题：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ageIndex = data[<span class=\"number\">0</span>].index(<span class=\"string\">\"Answer.Age\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>最后，访问剩余记录中感兴趣的字段，并计算和显示统计数据：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ages = [int(row[ageIndex]) <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> data[<span class=\"number\">1</span>:]]</span><br><span class=\"line\">print(statistics.mean(ages), statistics.stdev(ages))</span><br></pre></td></tr></table></figure></p>\n<p>csv和statistics模块是底层的、快速而粗糙的工具。在第6章，你将了解如何在更为复杂的项目中使用pandas的数据frame，完成那些比对几列数据进行琐碎的检索要高端得多的任务。</p>\n<h3 id=\"2-Json文件处理\"><a href=\"#2-Json文件处理\" class=\"headerlink\" title=\"2. Json文件处理\"></a>2. Json文件处理</h3><p>需要注意的一点就是某些Python数据类型和结构(比如集合和复数)无法存储在JSON文件中。因此，要在导出到JSON之前，将它们转换为JSON可表示的数据类型。例如，将复数存储为两个double类型的数字组成的数组，将集合存储为一个由集合的各项所组成的数组。</p>\n<p>将复杂数据存储到JSON文件中的操作称为JSON序列化，相应的反向操作则称为JSON反序列化。Python通过json模块中的函数，实现JSON序列化和反序列化。</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>dump()</td>\n<td>将Python对象导出到文件中</td>\n</tr>\n<tr>\n<td>dumps()</td>\n<td>将Python对象编码成JSON字符串</td>\n</tr>\n<tr>\n<td>load()</td>\n<td>将文件导出为Python对象</td>\n</tr>\n<tr>\n<td>loads()</td>\n<td>将已编码的JSON字符串解码为Python对象</td>\n</tr>\n</tbody>\n</table>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">把多个对象存储在一个JSON文件中是一种错误的做法，但如果已有的文件包含多个对象，则可将其以文本的方式读入，进而将文本转换为对象数组（在文本中各个对象之间添加方括号和逗号分隔符），并使用loads()将文本反序列化为对象列表。</span><br></pre></td></tr></table></figure></p>\n<p>Example:</p>\n<p>以下代码片段实现了将任意（可序列化的）对象按先序列化、后反序列化的顺序进行处理：<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将Python对象编码成JSON字符串</span></span><br><span class=\"line\">data = [&#123;<span class=\"string\">'apple'</span>: <span class=\"number\">23</span>, <span class=\"string\">'bear'</span>: <span class=\"number\">11</span>, <span class=\"string\">'banana'</span>: <span class=\"number\">54</span>&#125;]</span><br><span class=\"line\">s = json.dumps(data)</span><br><span class=\"line\"><span class=\"keyword\">print</span> type(s)  <span class=\"comment\"># &lt;type 'str'&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> s  <span class=\"comment\"># [&#123;\"apple\": 23, \"bear\": 11, \"banana\": 54&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将Python对象编码成JSON字符串并格式化输出</span></span><br><span class=\"line\">format_str = json.dumps(data, sort_keys=<span class=\"keyword\">True</span>, indent=<span class=\"number\">4</span>, separators=(<span class=\"string\">','</span>, <span class=\"string\">': '</span>))</span><br><span class=\"line\"><span class=\"keyword\">print</span> format_str</span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">[</span></span><br><span class=\"line\"><span class=\"string\">    &#123;</span></span><br><span class=\"line\"><span class=\"string\">        \"apple\": 23,</span></span><br><span class=\"line\"><span class=\"string\">        \"banana\": 54,</span></span><br><span class=\"line\"><span class=\"string\">        \"bear\": 11</span></span><br><span class=\"line\"><span class=\"string\">    &#125;</span></span><br><span class=\"line\"><span class=\"string\">]</span></span><br><span class=\"line\"><span class=\"string\">'''</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将已编码的JSON字符串解码为Python对象</span></span><br><span class=\"line\">data = <span class=\"string\">'[&#123;\"apple\": 23, \"bear\": 11, \"banana\": 54&#125;]'</span></span><br><span class=\"line\">o = json.loads(data)</span><br><span class=\"line\"><span class=\"keyword\">print</span> type(o)  <span class=\"comment\"># &lt;type 'list'&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> o[<span class=\"number\">0</span>].get(<span class=\"string\">'apple'</span>, <span class=\"number\">0</span>)  <span class=\"comment\"># 23</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将Python对象导出到文件中</span></span><br><span class=\"line\">data = [&#123;<span class=\"string\">'apple'</span>: <span class=\"number\">23</span>, <span class=\"string\">'bear'</span>: <span class=\"number\">11</span>, <span class=\"string\">'banana'</span>: <span class=\"number\">54</span>&#125;]</span><br><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"/home/xiaosi/data.json\"</span>, <span class=\"string\">\"w\"</span>) <span class=\"keyword\">as</span> f_dump:</span><br><span class=\"line\">    s_dump = json.dump(data, f_dump, ensure_ascii=<span class=\"keyword\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将文件导出为Python对象</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">\"/home/xiaosi/data.json\"</span>, <span class=\"string\">'r'</span>) <span class=\"keyword\">as</span> f_load:</span><br><span class=\"line\">    ob = json.load(f_load)</span><br><span class=\"line\"><span class=\"keyword\">print</span> type(ob)  <span class=\"comment\"># &lt;type 'list'&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> ob[<span class=\"number\">0</span>].get(<span class=\"string\">'banana'</span>)  <span class=\"comment\"># 54</span></span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">使用JSON函数需要导入json库：import json。</span><br></pre></td></tr></table></figure></p>\n<p>JSON 类型转换到 python 的类型对照表：</p>\n<table>\n<thead>\n<tr>\n<th>JSON</th>\n<th>Python</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>object</td>\n<td>dict</td>\n</tr>\n<tr>\n<td>array</td>\n<td>list</td>\n</tr>\n<tr>\n<td>string</td>\n<td>unicode</td>\n</tr>\n<tr>\n<td>number (int)</td>\n<td>int, long</td>\n</tr>\n<tr>\n<td>number (real)</td>\n<td>float</td>\n</tr>\n<tr>\n<td>true</td>\n<td>True</td>\n</tr>\n<tr>\n<td>false</td>\n<td>False</td>\n</tr>\n<tr>\n<td>null</td>\n<td>None</td>\n</tr>\n</tbody>\n</table>\n<p>来自于:<python数据科学入门></python数据科学入门></p>\n"},{"author":"sjf0115","title":"Hadoop Shell中判断HDFS文件是否存在","date":"2018-01-25T07:49:01.000Z","_content":"\n### 1. 用法\n\n`Hadoop`提供了`-test`命令可以验证文件目录是否存在。我们首先看一下`-test`命令的使用用法:\n```\nhadoop fs -help\n-test -[defsz] <path>:  Answer various questions about <path>, with result via exit status.\n                  -d  return 0 if <path> is a directory.\n                  -e  return 0 if <path> exists.\n                  -f  return 0 if <path> is a file.\n                  -s  return 0 if file <path> is greater than zero bytes in size.\n                  -z  return 0 if file <path> is zero bytes in size.\n                else, return 1.\n```\n\n命令参数|描述\n---|---\n-d|如果指定路径是一个目录返回0否则返回1\n-e|如果指定路径存在返回0否则返回1\n-f|如果指定路径是一个文件返回0否则返回1\n-s|如果指定路径文件大小大于0返回0否则返回1\n-z|如果指定指定文件大小等于0返回0否则返回1\n\n\n### 2. Example:\n\n```\n[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -ls test/adv\nFound 1 items\ndrwxr-xr-x   - xiaosi xiaosi          0 2018-01-25 15:39 test/adv/day=20180123\n[xiaosi@ying:~$]$\n[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123\n[xiaosi@ying:~$]$ echo $?\n0\n[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180124\n[xiaosi@ying:~$]$ echo $?\n1\n```\n\n### 3. Shell中判断\n\n```\nsudo -uxiaosi hadoop fs -test -e test/adv/day=20180123\nif [ $? -eq 0 ] ;then\n    echo '[info]目录已存在不需要创建'\nelse\n    sudo -uxiaosi hadoop fs -mkdir -p test/adv/day=20180123\nfi\n```\n","source":"_posts/Hadoop/[Hadoop]Shell中判断HDFS文件是否存在.md","raw":"---\nauthor: sjf0115\ntitle: Hadoop Shell中判断HDFS文件是否存在\ndate: 2018-01-25 15:49:01\ntags:\n  - Hadoop\n\ncategories: Hadoop\n---\n\n### 1. 用法\n\n`Hadoop`提供了`-test`命令可以验证文件目录是否存在。我们首先看一下`-test`命令的使用用法:\n```\nhadoop fs -help\n-test -[defsz] <path>:  Answer various questions about <path>, with result via exit status.\n                  -d  return 0 if <path> is a directory.\n                  -e  return 0 if <path> exists.\n                  -f  return 0 if <path> is a file.\n                  -s  return 0 if file <path> is greater than zero bytes in size.\n                  -z  return 0 if file <path> is zero bytes in size.\n                else, return 1.\n```\n\n命令参数|描述\n---|---\n-d|如果指定路径是一个目录返回0否则返回1\n-e|如果指定路径存在返回0否则返回1\n-f|如果指定路径是一个文件返回0否则返回1\n-s|如果指定路径文件大小大于0返回0否则返回1\n-z|如果指定指定文件大小等于0返回0否则返回1\n\n\n### 2. Example:\n\n```\n[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -ls test/adv\nFound 1 items\ndrwxr-xr-x   - xiaosi xiaosi          0 2018-01-25 15:39 test/adv/day=20180123\n[xiaosi@ying:~$]$\n[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123\n[xiaosi@ying:~$]$ echo $?\n0\n[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180124\n[xiaosi@ying:~$]$ echo $?\n1\n```\n\n### 3. Shell中判断\n\n```\nsudo -uxiaosi hadoop fs -test -e test/adv/day=20180123\nif [ $? -eq 0 ] ;then\n    echo '[info]目录已存在不需要创建'\nelse\n    sudo -uxiaosi hadoop fs -mkdir -p test/adv/day=20180123\nfi\n```\n","slug":"Hadoop/[Hadoop]Shell中判断HDFS文件是否存在","published":1,"updated":"2018-01-29T11:56:03.444Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cje58tius0053ordbab212m3f","content":"<h3 id=\"1-用法\"><a href=\"#1-用法\" class=\"headerlink\" title=\"1. 用法\"></a>1. 用法</h3><p><code>Hadoop</code>提供了<code>-test</code>命令可以验证文件目录是否存在。我们首先看一下<code>-test</code>命令的使用用法:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hadoop fs -help</span><br><span class=\"line\">-test -[defsz] &lt;path&gt;:  Answer various questions about &lt;path&gt;, with result via exit status.</span><br><span class=\"line\">                  -d  return 0 if &lt;path&gt; is a directory.</span><br><span class=\"line\">                  -e  return 0 if &lt;path&gt; exists.</span><br><span class=\"line\">                  -f  return 0 if &lt;path&gt; is a file.</span><br><span class=\"line\">                  -s  return 0 if file &lt;path&gt; is greater than zero bytes in size.</span><br><span class=\"line\">                  -z  return 0 if file &lt;path&gt; is zero bytes in size.</span><br><span class=\"line\">                else, return 1.</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>命令参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>-d</td>\n<td>如果指定路径是一个目录返回0否则返回1</td>\n</tr>\n<tr>\n<td>-e</td>\n<td>如果指定路径存在返回0否则返回1</td>\n</tr>\n<tr>\n<td>-f</td>\n<td>如果指定路径是一个文件返回0否则返回1</td>\n</tr>\n<tr>\n<td>-s</td>\n<td>如果指定路径文件大小大于0返回0否则返回1</td>\n</tr>\n<tr>\n<td>-z</td>\n<td>如果指定指定文件大小等于0返回0否则返回1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-Example\"><a href=\"#2-Example\" class=\"headerlink\" title=\"2. Example:\"></a>2. Example:</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -ls test/adv</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">drwxr-xr-x   - xiaosi xiaosi          0 2018-01-25 15:39 test/adv/day=20180123</span><br><span class=\"line\">[xiaosi@ying:~$]$</span><br><span class=\"line\">[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123</span><br><span class=\"line\">[xiaosi@ying:~$]$ echo $?</span><br><span class=\"line\">0</span><br><span class=\"line\">[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180124</span><br><span class=\"line\">[xiaosi@ying:~$]$ echo $?</span><br><span class=\"line\">1</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Shell中判断\"><a href=\"#3-Shell中判断\" class=\"headerlink\" title=\"3. Shell中判断\"></a>3. Shell中判断</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123</span><br><span class=\"line\">if [ $? -eq 0 ] ;then</span><br><span class=\"line\">    echo &apos;[info]目录已存在不需要创建&apos;</span><br><span class=\"line\">else</span><br><span class=\"line\">    sudo -uxiaosi hadoop fs -mkdir -p test/adv/day=20180123</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-用法\"><a href=\"#1-用法\" class=\"headerlink\" title=\"1. 用法\"></a>1. 用法</h3><p><code>Hadoop</code>提供了<code>-test</code>命令可以验证文件目录是否存在。我们首先看一下<code>-test</code>命令的使用用法:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">hadoop fs -help</span><br><span class=\"line\">-test -[defsz] &lt;path&gt;:  Answer various questions about &lt;path&gt;, with result via exit status.</span><br><span class=\"line\">                  -d  return 0 if &lt;path&gt; is a directory.</span><br><span class=\"line\">                  -e  return 0 if &lt;path&gt; exists.</span><br><span class=\"line\">                  -f  return 0 if &lt;path&gt; is a file.</span><br><span class=\"line\">                  -s  return 0 if file &lt;path&gt; is greater than zero bytes in size.</span><br><span class=\"line\">                  -z  return 0 if file &lt;path&gt; is zero bytes in size.</span><br><span class=\"line\">                else, return 1.</span><br></pre></td></tr></table></figure></p>\n<table>\n<thead>\n<tr>\n<th>命令参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>-d</td>\n<td>如果指定路径是一个目录返回0否则返回1</td>\n</tr>\n<tr>\n<td>-e</td>\n<td>如果指定路径存在返回0否则返回1</td>\n</tr>\n<tr>\n<td>-f</td>\n<td>如果指定路径是一个文件返回0否则返回1</td>\n</tr>\n<tr>\n<td>-s</td>\n<td>如果指定路径文件大小大于0返回0否则返回1</td>\n</tr>\n<tr>\n<td>-z</td>\n<td>如果指定指定文件大小等于0返回0否则返回1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-Example\"><a href=\"#2-Example\" class=\"headerlink\" title=\"2. Example:\"></a>2. Example:</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -ls test/adv</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">drwxr-xr-x   - xiaosi xiaosi          0 2018-01-25 15:39 test/adv/day=20180123</span><br><span class=\"line\">[xiaosi@ying:~$]$</span><br><span class=\"line\">[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123</span><br><span class=\"line\">[xiaosi@ying:~$]$ echo $?</span><br><span class=\"line\">0</span><br><span class=\"line\">[xiaosi@ying:~$]$ sudo -uxiaosi hadoop fs -test -e test/adv/day=20180124</span><br><span class=\"line\">[xiaosi@ying:~$]$ echo $?</span><br><span class=\"line\">1</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Shell中判断\"><a href=\"#3-Shell中判断\" class=\"headerlink\" title=\"3. Shell中判断\"></a>3. Shell中判断</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo -uxiaosi hadoop fs -test -e test/adv/day=20180123</span><br><span class=\"line\">if [ $? -eq 0 ] ;then</span><br><span class=\"line\">    echo &apos;[info]目录已存在不需要创建&apos;</span><br><span class=\"line\">else</span><br><span class=\"line\">    sudo -uxiaosi hadoop fs -mkdir -p test/adv/day=20180123</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n"},{"layout":"post","author":"sjf0115","title":"Python 爬虫使用Requests获取网页文本内容中文乱码","date":"2017-12-05T01:46:20.000Z","_content":"\n### 1. 问题\n\n使用Requests去获取网页文本内容时，输出的中文出现乱码。\n\n### 2. 乱码原因\n\n爬取的网页编码与我们爬取编码方式不一致造成的。如果爬取的网页编码方式为`utf8`，而我们爬取后程序使用`ISO-8859-1`编码方式进行编码并输出，这会引起乱码。如果我们爬取后程序改用`utf8`编码方式，就不会造成乱码。\n\n### 3. 乱码解决方案\n\n#### 3.1 Content-Type\n\n我们首先确定爬取的网页编码方式，编码方式往往可以从HTTP头(header)的`Content-Type`得出。\n\n`Content-Type`，内容类型，一般是指网页中存在的`Content-Type`，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件，这就是经常看到一些Asp网页点击的结果却是下载到的一个文件或一张图片的原因。如果未指定`ContentType`，默认为TEXT/HTML。`charset`决定了网页的编码方式，一般为`gb2312`、`utf-8`等\n\nHTML语法格式:\n```\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n```\n\n```Python\nstation_request = requests.get(\"http://blog.csdn.net/sunnyyoona\")\ncontent_type = station_request.headers['content-type']\nprint content_type  # text/html; charset=utf-8\n```\n\n#### 3.2 chardet\n\n如果上述方式没有编码信息，一般可以采用`chardet`等第三方网页编码智能识别工具识别:\n```\npip install chardet\n```\n\n使用`chardet`可以很方便的实现文本内容的编码检测。虽然HTML页面有`charset`标签，但是有些时候并不准确，这时候我们可以使用`chardet`来进一步的判断:\n```Python\nraw_data = urllib.urlopen('http://blog.csdn.net/sunnyyoona').read()\nprint chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'utf-8'}\n\nraw_data = urllib.urlopen('http://www.jb51.net').read()\nprint chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'GB2312'}\n```\n函数返回值为字典，有2个元素，一个是检测的可信度，另外一个就是检测到的编码。\n\n\n#### 3.3 猜测编码\n\n当你收到一个响应时，`Requests`会猜测响应(response)的编码方式，用于在你调用`Response.text`方法时，对响应进行解码。`Requests`首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 `charadet`来尝试猜测编码方式。\n\n只有当HTTP头部不存在明确指定的字符集，并且`Content-Type`头部字段包含`text`值之时，Requests才不去猜测编码方式。在这种情况下， `RFC 2616`指定默认字符集必须是`ISO-8859-1`。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置`Response.encoding`属性，或使用原始的`Response.content`。\n\n```Python\n# 一等火车站\nurl = \"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'}\nr = requests.get(url, headers=headers)\nprint r.headers['Content-Type']  # text/html\n# 猜测的编码方式\nprint r.encoding  # ISO-8859-1\nprint r.text  # 出现乱码\n\nraw_data = urllib.urlopen(url).read()\nprint chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'utf-8'}\n```\n如上所述，只有当HTTP头部不存在明确指定的字符集，并且`Content-Type`头部字段包含`text`值之时，Requests才不去猜测编码方式。直接使用`ISO-8859-1`编码方式。而使用`chardet`检测结果来看，网页编码方式与猜测的编码方式不一致，这就造成了结果输出的乱码。\n\n#### 3.4 解决\n\n你可以使用`r.encoding = xxx`来更改编码方式，这样`Requests`将在你调用`r.text`时使用`r.encoding`的新值，使用新的编码方式。下面示例使用`chardet`检测的编码方式解码网页:\n```Python\n# 一等火车站\nurl = \"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'}\nr = requests.get(url, headers=headers)\n\n# 检测编码\nraw_data = urllib.urlopen(url).read()\ncharset = chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'utf-8'}\nencoding = charset['encoding']\n# 更改编码方式\nr.encoding = encoding\nprint r.text  # 未出现乱码\n```\n\n参考:\n```\nhttp://docs.python-requests.org/en/latest/user/quickstart/#response-content\nhttp://blog.csdn.net/a491057947/article/details/47292923\nhttps://www.cnblogs.com/GUIDAO/p/6679574.html\n```\n","source":"_posts/Python/[Python] 爬虫使用Requests获取网页文本内容中文乱码.md","raw":"\n---\nlayout: post\nauthor: sjf0115\ntitle: Python 爬虫使用Requests获取网页文本内容中文乱码\ndate: 2017-12-05 09:46:20\ntags:\n  - Python\n\ncategories: Python\n---\n\n### 1. 问题\n\n使用Requests去获取网页文本内容时，输出的中文出现乱码。\n\n### 2. 乱码原因\n\n爬取的网页编码与我们爬取编码方式不一致造成的。如果爬取的网页编码方式为`utf8`，而我们爬取后程序使用`ISO-8859-1`编码方式进行编码并输出，这会引起乱码。如果我们爬取后程序改用`utf8`编码方式，就不会造成乱码。\n\n### 3. 乱码解决方案\n\n#### 3.1 Content-Type\n\n我们首先确定爬取的网页编码方式，编码方式往往可以从HTTP头(header)的`Content-Type`得出。\n\n`Content-Type`，内容类型，一般是指网页中存在的`Content-Type`，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件，这就是经常看到一些Asp网页点击的结果却是下载到的一个文件或一张图片的原因。如果未指定`ContentType`，默认为TEXT/HTML。`charset`决定了网页的编码方式，一般为`gb2312`、`utf-8`等\n\nHTML语法格式:\n```\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n```\n\n```Python\nstation_request = requests.get(\"http://blog.csdn.net/sunnyyoona\")\ncontent_type = station_request.headers['content-type']\nprint content_type  # text/html; charset=utf-8\n```\n\n#### 3.2 chardet\n\n如果上述方式没有编码信息，一般可以采用`chardet`等第三方网页编码智能识别工具识别:\n```\npip install chardet\n```\n\n使用`chardet`可以很方便的实现文本内容的编码检测。虽然HTML页面有`charset`标签，但是有些时候并不准确，这时候我们可以使用`chardet`来进一步的判断:\n```Python\nraw_data = urllib.urlopen('http://blog.csdn.net/sunnyyoona').read()\nprint chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'utf-8'}\n\nraw_data = urllib.urlopen('http://www.jb51.net').read()\nprint chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'GB2312'}\n```\n函数返回值为字典，有2个元素，一个是检测的可信度，另外一个就是检测到的编码。\n\n\n#### 3.3 猜测编码\n\n当你收到一个响应时，`Requests`会猜测响应(response)的编码方式，用于在你调用`Response.text`方法时，对响应进行解码。`Requests`首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 `charadet`来尝试猜测编码方式。\n\n只有当HTTP头部不存在明确指定的字符集，并且`Content-Type`头部字段包含`text`值之时，Requests才不去猜测编码方式。在这种情况下， `RFC 2616`指定默认字符集必须是`ISO-8859-1`。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置`Response.encoding`属性，或使用原始的`Response.content`。\n\n```Python\n# 一等火车站\nurl = \"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'}\nr = requests.get(url, headers=headers)\nprint r.headers['Content-Type']  # text/html\n# 猜测的编码方式\nprint r.encoding  # ISO-8859-1\nprint r.text  # 出现乱码\n\nraw_data = urllib.urlopen(url).read()\nprint chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'utf-8'}\n```\n如上所述，只有当HTTP头部不存在明确指定的字符集，并且`Content-Type`头部字段包含`text`值之时，Requests才不去猜测编码方式。直接使用`ISO-8859-1`编码方式。而使用`chardet`检测结果来看，网页编码方式与猜测的编码方式不一致，这就造成了结果输出的乱码。\n\n#### 3.4 解决\n\n你可以使用`r.encoding = xxx`来更改编码方式，这样`Requests`将在你调用`r.text`时使用`r.encoding`的新值，使用新的编码方式。下面示例使用`chardet`检测的编码方式解码网页:\n```Python\n# 一等火车站\nurl = \"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'}\nr = requests.get(url, headers=headers)\n\n# 检测编码\nraw_data = urllib.urlopen(url).read()\ncharset = chardet.detect(raw_data)  # {'confidence': 0.99, 'encoding': 'utf-8'}\nencoding = charset['encoding']\n# 更改编码方式\nr.encoding = encoding\nprint r.text  # 未出现乱码\n```\n\n参考:\n```\nhttp://docs.python-requests.org/en/latest/user/quickstart/#response-content\nhttp://blog.csdn.net/a491057947/article/details/47292923\nhttps://www.cnblogs.com/GUIDAO/p/6679574.html\n```\n","slug":"Python/[Python] 爬虫使用Requests获取网页文本内容中文乱码","published":1,"updated":"2018-01-29T09:36:59.617Z","comments":1,"photos":[],"link":"","_id":"cje58tiuu0058ordbfw9540d5","content":"<h3 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h3><p>使用Requests去获取网页文本内容时，输出的中文出现乱码。</p>\n<h3 id=\"2-乱码原因\"><a href=\"#2-乱码原因\" class=\"headerlink\" title=\"2. 乱码原因\"></a>2. 乱码原因</h3><p>爬取的网页编码与我们爬取编码方式不一致造成的。如果爬取的网页编码方式为<code>utf8</code>，而我们爬取后程序使用<code>ISO-8859-1</code>编码方式进行编码并输出，这会引起乱码。如果我们爬取后程序改用<code>utf8</code>编码方式，就不会造成乱码。</p>\n<h3 id=\"3-乱码解决方案\"><a href=\"#3-乱码解决方案\" class=\"headerlink\" title=\"3. 乱码解决方案\"></a>3. 乱码解决方案</h3><h4 id=\"3-1-Content-Type\"><a href=\"#3-1-Content-Type\" class=\"headerlink\" title=\"3.1 Content-Type\"></a>3.1 Content-Type</h4><p>我们首先确定爬取的网页编码方式，编码方式往往可以从HTTP头(header)的<code>Content-Type</code>得出。</p>\n<p><code>Content-Type</code>，内容类型，一般是指网页中存在的<code>Content-Type</code>，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件，这就是经常看到一些Asp网页点击的结果却是下载到的一个文件或一张图片的原因。如果未指定<code>ContentType</code>，默认为TEXT/HTML。<code>charset</code>决定了网页的编码方式，一般为<code>gb2312</code>、<code>utf-8</code>等</p>\n<p>HTML语法格式:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;meta content=&quot;text/html; charset=utf-8&quot; http-equiv=&quot;Content-Type&quot;/&gt;</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">station_request = requests.get(<span class=\"string\">\"http://blog.csdn.net/sunnyyoona\"</span>)</span><br><span class=\"line\">content_type = station_request.headers[<span class=\"string\">'content-type'</span>]</span><br><span class=\"line\"><span class=\"keyword\">print</span> content_type  <span class=\"comment\"># text/html; charset=utf-8</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"3-2-chardet\"><a href=\"#3-2-chardet\" class=\"headerlink\" title=\"3.2 chardet\"></a>3.2 chardet</h4><p>如果上述方式没有编码信息，一般可以采用<code>chardet</code>等第三方网页编码智能识别工具识别:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install chardet</span><br></pre></td></tr></table></figure></p>\n<p>使用<code>chardet</code>可以很方便的实现文本内容的编码检测。虽然HTML页面有<code>charset</code>标签，但是有些时候并不准确，这时候我们可以使用<code>chardet</code>来进一步的判断:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">raw_data = urllib.urlopen(<span class=\"string\">'http://blog.csdn.net/sunnyyoona'</span>).read()</span><br><span class=\"line\"><span class=\"keyword\">print</span> chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">raw_data = urllib.urlopen(<span class=\"string\">'http://www.jb51.net'</span>).read()</span><br><span class=\"line\"><span class=\"keyword\">print</span> chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'GB2312'&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>函数返回值为字典，有2个元素，一个是检测的可信度，另外一个就是检测到的编码。</p>\n<h4 id=\"3-3-猜测编码\"><a href=\"#3-3-猜测编码\" class=\"headerlink\" title=\"3.3 猜测编码\"></a>3.3 猜测编码</h4><p>当你收到一个响应时，<code>Requests</code>会猜测响应(response)的编码方式，用于在你调用<code>Response.text</code>方法时，对响应进行解码。<code>Requests</code>首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 <code>charadet</code>来尝试猜测编码方式。</p>\n<p>只有当HTTP头部不存在明确指定的字符集，并且<code>Content-Type</code>头部字段包含<code>text</code>值之时，Requests才不去猜测编码方式。在这种情况下， <code>RFC 2616</code>指定默认字符集必须是<code>ISO-8859-1</code>。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置<code>Response.encoding</code>属性，或使用原始的<code>Response.content</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 一等火车站</span></span><br><span class=\"line\">url = <span class=\"string\">\"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"</span></span><br><span class=\"line\">headers = &#123;<span class=\"string\">'User-Agent'</span>: <span class=\"string\">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'</span>&#125;</span><br><span class=\"line\">r = requests.get(url, headers=headers)</span><br><span class=\"line\"><span class=\"keyword\">print</span> r.headers[<span class=\"string\">'Content-Type'</span>]  <span class=\"comment\"># text/html</span></span><br><span class=\"line\"><span class=\"comment\"># 猜测的编码方式</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> r.encoding  <span class=\"comment\"># ISO-8859-1</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> r.text  <span class=\"comment\"># 出现乱码</span></span><br><span class=\"line\"></span><br><span class=\"line\">raw_data = urllib.urlopen(url).read()</span><br><span class=\"line\"><span class=\"keyword\">print</span> chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;</span></span><br></pre></td></tr></table></figure>\n<p>如上所述，只有当HTTP头部不存在明确指定的字符集，并且<code>Content-Type</code>头部字段包含<code>text</code>值之时，Requests才不去猜测编码方式。直接使用<code>ISO-8859-1</code>编码方式。而使用<code>chardet</code>检测结果来看，网页编码方式与猜测的编码方式不一致，这就造成了结果输出的乱码。</p>\n<h4 id=\"3-4-解决\"><a href=\"#3-4-解决\" class=\"headerlink\" title=\"3.4 解决\"></a>3.4 解决</h4><p>你可以使用<code>r.encoding = xxx</code>来更改编码方式，这样<code>Requests</code>将在你调用<code>r.text</code>时使用<code>r.encoding</code>的新值，使用新的编码方式。下面示例使用<code>chardet</code>检测的编码方式解码网页:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 一等火车站</span></span><br><span class=\"line\">url = <span class=\"string\">\"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"</span></span><br><span class=\"line\">headers = &#123;<span class=\"string\">'User-Agent'</span>: <span class=\"string\">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'</span>&#125;</span><br><span class=\"line\">r = requests.get(url, headers=headers)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 检测编码</span></span><br><span class=\"line\">raw_data = urllib.urlopen(url).read()</span><br><span class=\"line\">charset = chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;</span></span><br><span class=\"line\">encoding = charset[<span class=\"string\">'encoding'</span>]</span><br><span class=\"line\"><span class=\"comment\"># 更改编码方式</span></span><br><span class=\"line\">r.encoding = encoding</span><br><span class=\"line\"><span class=\"keyword\">print</span> r.text  <span class=\"comment\"># 未出现乱码</span></span><br></pre></td></tr></table></figure></p>\n<p>参考:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">http://docs.python-requests.org/en/latest/user/quickstart/#response-content</span><br><span class=\"line\">http://blog.csdn.net/a491057947/article/details/47292923</span><br><span class=\"line\">https://www.cnblogs.com/GUIDAO/p/6679574.html</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-问题\"><a href=\"#1-问题\" class=\"headerlink\" title=\"1. 问题\"></a>1. 问题</h3><p>使用Requests去获取网页文本内容时，输出的中文出现乱码。</p>\n<h3 id=\"2-乱码原因\"><a href=\"#2-乱码原因\" class=\"headerlink\" title=\"2. 乱码原因\"></a>2. 乱码原因</h3><p>爬取的网页编码与我们爬取编码方式不一致造成的。如果爬取的网页编码方式为<code>utf8</code>，而我们爬取后程序使用<code>ISO-8859-1</code>编码方式进行编码并输出，这会引起乱码。如果我们爬取后程序改用<code>utf8</code>编码方式，就不会造成乱码。</p>\n<h3 id=\"3-乱码解决方案\"><a href=\"#3-乱码解决方案\" class=\"headerlink\" title=\"3. 乱码解决方案\"></a>3. 乱码解决方案</h3><h4 id=\"3-1-Content-Type\"><a href=\"#3-1-Content-Type\" class=\"headerlink\" title=\"3.1 Content-Type\"></a>3.1 Content-Type</h4><p>我们首先确定爬取的网页编码方式，编码方式往往可以从HTTP头(header)的<code>Content-Type</code>得出。</p>\n<p><code>Content-Type</code>，内容类型，一般是指网页中存在的<code>Content-Type</code>，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件，这就是经常看到一些Asp网页点击的结果却是下载到的一个文件或一张图片的原因。如果未指定<code>ContentType</code>，默认为TEXT/HTML。<code>charset</code>决定了网页的编码方式，一般为<code>gb2312</code>、<code>utf-8</code>等</p>\n<p>HTML语法格式:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;meta content=&quot;text/html; charset=utf-8&quot; http-equiv=&quot;Content-Type&quot;/&gt;</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">station_request = requests.get(<span class=\"string\">\"http://blog.csdn.net/sunnyyoona\"</span>)</span><br><span class=\"line\">content_type = station_request.headers[<span class=\"string\">'content-type'</span>]</span><br><span class=\"line\"><span class=\"keyword\">print</span> content_type  <span class=\"comment\"># text/html; charset=utf-8</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"3-2-chardet\"><a href=\"#3-2-chardet\" class=\"headerlink\" title=\"3.2 chardet\"></a>3.2 chardet</h4><p>如果上述方式没有编码信息，一般可以采用<code>chardet</code>等第三方网页编码智能识别工具识别:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install chardet</span><br></pre></td></tr></table></figure></p>\n<p>使用<code>chardet</code>可以很方便的实现文本内容的编码检测。虽然HTML页面有<code>charset</code>标签，但是有些时候并不准确，这时候我们可以使用<code>chardet</code>来进一步的判断:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">raw_data = urllib.urlopen(<span class=\"string\">'http://blog.csdn.net/sunnyyoona'</span>).read()</span><br><span class=\"line\"><span class=\"keyword\">print</span> chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\">raw_data = urllib.urlopen(<span class=\"string\">'http://www.jb51.net'</span>).read()</span><br><span class=\"line\"><span class=\"keyword\">print</span> chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'GB2312'&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>函数返回值为字典，有2个元素，一个是检测的可信度，另外一个就是检测到的编码。</p>\n<h4 id=\"3-3-猜测编码\"><a href=\"#3-3-猜测编码\" class=\"headerlink\" title=\"3.3 猜测编码\"></a>3.3 猜测编码</h4><p>当你收到一个响应时，<code>Requests</code>会猜测响应(response)的编码方式，用于在你调用<code>Response.text</code>方法时，对响应进行解码。<code>Requests</code>首先在HTTP头部检测是否存在指定的编码方式，如果不存在，则会使用 <code>charadet</code>来尝试猜测编码方式。</p>\n<p>只有当HTTP头部不存在明确指定的字符集，并且<code>Content-Type</code>头部字段包含<code>text</code>值之时，Requests才不去猜测编码方式。在这种情况下， <code>RFC 2616</code>指定默认字符集必须是<code>ISO-8859-1</code>。Requests遵从这一规范。如果你需要一种不同的编码方式，你可以手动设置<code>Response.encoding</code>属性，或使用原始的<code>Response.content</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 一等火车站</span></span><br><span class=\"line\">url = <span class=\"string\">\"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"</span></span><br><span class=\"line\">headers = &#123;<span class=\"string\">'User-Agent'</span>: <span class=\"string\">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'</span>&#125;</span><br><span class=\"line\">r = requests.get(url, headers=headers)</span><br><span class=\"line\"><span class=\"keyword\">print</span> r.headers[<span class=\"string\">'Content-Type'</span>]  <span class=\"comment\"># text/html</span></span><br><span class=\"line\"><span class=\"comment\"># 猜测的编码方式</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> r.encoding  <span class=\"comment\"># ISO-8859-1</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> r.text  <span class=\"comment\"># 出现乱码</span></span><br><span class=\"line\"></span><br><span class=\"line\">raw_data = urllib.urlopen(url).read()</span><br><span class=\"line\"><span class=\"keyword\">print</span> chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;</span></span><br></pre></td></tr></table></figure>\n<p>如上所述，只有当HTTP头部不存在明确指定的字符集，并且<code>Content-Type</code>头部字段包含<code>text</code>值之时，Requests才不去猜测编码方式。直接使用<code>ISO-8859-1</code>编码方式。而使用<code>chardet</code>检测结果来看，网页编码方式与猜测的编码方式不一致，这就造成了结果输出的乱码。</p>\n<h4 id=\"3-4-解决\"><a href=\"#3-4-解决\" class=\"headerlink\" title=\"3.4 解决\"></a>3.4 解决</h4><p>你可以使用<code>r.encoding = xxx</code>来更改编码方式，这样<code>Requests</code>将在你调用<code>r.text</code>时使用<code>r.encoding</code>的新值，使用新的编码方式。下面示例使用<code>chardet</code>检测的编码方式解码网页:<br><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 一等火车站</span></span><br><span class=\"line\">url = <span class=\"string\">\"https://baike.baidu.com/item/%E4%B8%80%E7%AD%89%E7%AB%99\"</span></span><br><span class=\"line\">headers = &#123;<span class=\"string\">'User-Agent'</span>: <span class=\"string\">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'</span>&#125;</span><br><span class=\"line\">r = requests.get(url, headers=headers)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 检测编码</span></span><br><span class=\"line\">raw_data = urllib.urlopen(url).read()</span><br><span class=\"line\">charset = chardet.detect(raw_data)  <span class=\"comment\"># &#123;'confidence': 0.99, 'encoding': 'utf-8'&#125;</span></span><br><span class=\"line\">encoding = charset[<span class=\"string\">'encoding'</span>]</span><br><span class=\"line\"><span class=\"comment\"># 更改编码方式</span></span><br><span class=\"line\">r.encoding = encoding</span><br><span class=\"line\"><span class=\"keyword\">print</span> r.text  <span class=\"comment\"># 未出现乱码</span></span><br></pre></td></tr></table></figure></p>\n<p>参考:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">http://docs.python-requests.org/en/latest/user/quickstart/#response-content</span><br><span class=\"line\">http://blog.csdn.net/a491057947/article/details/47292923</span><br><span class=\"line\">https://www.cnblogs.com/GUIDAO/p/6679574.html</span><br></pre></td></tr></table></figure></p>\n"},{"layout":"post","author":"sjf0115","title":"Scala 学习笔记之基础语法","date":"2018-02-27T01:35:01.000Z","_content":"\n#### 1. 变量\n\nval定义的值实际上是一个常亮，无法改变其内容\n```scala\n\nscala> val num = 0\nnum: Int = 0\n\nscala> num = 2\n<console>:12: error: reassignment to val\n       num = 2\n           ^\n\n```\n如果要声明其值可变的变量，可以使用var\n```scala\nscala> var number = 0\nnumber: Int = 0\n\nscala> number = 2\nnumber: Int = 2\n\n```\n\n在Scala中，建议使用`val`，除非你真的需要改变它的内容．\n\n> 备注\n\n不需要给出值或者变量的类型，可以从你用来初始化它的表达式推断出来．只声明值或者变量但不做初始化会报错：\n```scala\nscala> val str: String\n<console>:11: error: only classes can have declared but undefined members\n       val str: String\n           ^\n\nscala> val str: String = \"Hello\"\nstr: String = Hello\n```\n#### 2. 常用类型\n\n常用类型：\n- Byte\n- Char\n- Short\n- Int\n- Long\n- Float\n- Double\n- Boolean\n\n跟Java不同的是，这些类型是类．Scala并不刻意区分基本类型和引用类型．你可以对数字执行方法：\n```scala\nscala> 1.toString()\nres2: String = 1\n```\n#### 3. 条件表达式\n\nScala的 `if/else` 的语法结构和Java的一样．不过，在Scala中 `if/else` 表达式有值，这个值就是跟在 `if` 或 `else` 之后的表达式的值:\n```scala\nif(x > 0) 1 else -1\n```\n上述表达式的值是１或者-1，具体是哪一个取决于x的值．你可以将 `if/else` 表达式的值赋值给变量：\n```scala\nval s = if(x > 0) 1 else -1\n```\n等同于:\n```scala\nif(x > 0) s = 1 else s = -1\n```\n相对于第二种写法，第一种写法更好一些，因为它可以用来初始化一个val，而第二种写法当中，s必须是var．\n\n> 备注\n\nScala中每个表达式都有一个类型\n```scala\nscala> val s = if(x > 0) \"positive\" else -1;\ns: Any = positive\n```\n上述表达式的类型是两个分支类型的公共超类型．在这个例子中，其中一个分支是`java.lang.String`，而另一个分支是`Int`．它们的公共超类型是`Any`．\n```scala\nif(x > 0) 1\n```\n那么有可能if语句没有输出值．但是在Scala中，每个表达式都应该有某种值．这个问题的解决方案是引入一个 `Unit` 类，写作 `()`．不带 `else` 的这个 `if` 语句等同于:\n```scala\nif(x > 0) 1 else ()\n```\n#### 4. 循环\n\nScala拥有与Java和C++相同的while和do循环：\n```scala\nwhile(n > 2){\n  println(\"num->\" + n)\n  n = n -1\n}\n```\n但是Scala没有与`for(初始化变量;检查变量是否满足某条件;更新变量)`循环直接对应的结构．如果你需要这样的循环，有两个选择：一是选择while循环，二是使用如下for语句:\n```scala\nfor(i <- 1 to n){\n  println(\"num->\" + i)\n}\n```\n上述表达式的目标是让变量i遍历`<-`右边的表达式的所有值．至于如何遍历，则取决于表达式的类型．\n\n遍历字符串或者数组时，你通常需要使用从0到n-1的区间．这个时候你可以使用`util`方法而不是`to`方法．`util`方法返回一个并不包含上限的区间:\n```scala\nval s = \"Hello\"\nfor(i <- 0 until s.length){\n  println(i + \" = \" + s(i))\n}\n```\n或者\n```scala\nfor(ch <- \"Hello\"){\n  println(ch)\n}\n```\n#### 5. 函数\n\n要定义函数，需要给出函数的名称，参数和函数体:\n```scala\ndef abs (x: Double) = if (x >= 0) x else -x\n```\n必须给出所有参数的类型，只要函数不是递归的，就可以不需要指定返回类型．Scala编译器可以通过`=`符号右侧的表达式的类型推断出返回类型．\n如果函数体需要多个表达式完成，可以使用代码块．块中最后一个表达式的值就是函数的返回值:\n```scala\ndef fac(n: Int) = {\n  var r = 1\n  for(i <- 1 to n){\n    r = r * i\n  }\n  r\n}\n```\n上例中函数返回值为r的值\n\n> 备注\n\n虽然在函数中使用 `return` 并没有什么不对，我们还是最好适应没有 `return` 的日子．之后，我们会使用大量的匿名函数，这些函数中 `return` 并不返回值给调用者．它跳出到包含它的函数中．我们可以把 `return` 当做是函数版的 `break` 语句，仅在需要时使用．\n\n对于递归函数，我们必须指定返回类型：\n```scala\ndef fac(n: Int) : Int = if(n < 0) 1 else n * fac(n-1)\n```\n#### 6. 默认参数和带名参数\n\n我们在调用某些函数时并不显示的给出所有参数值，对于这些函数我们可以使用默认参数：\n```scala\ndef decorate (str : String, left : String = \"[\" , right : String = \"]\") {\n  left + str + right\n}\n```\n这个函数带有两个参数，left 和 right，带有默认值 `[` 和 `]`:\n```scala\ndecorate(\"Hello\") // [Hello]\ndecorate(\"Hello\", \"<\", \">\") // <Hello>\n```\n你可以在提供参数值的时候指定参数名(带名参数)：\n```scala\ndecorate(left = \"<<\", str = \"Hello\", right = \">>\") // <<Hello>>\n```\n你可以混用未命名参数和带名参数，只要那些未命名的参数是排在前面即可:\n```scala\ndecorate(\"Hello\", right = \"]###\") // 实际调用 decorate(\"Hello\", \"[\", \"]###\")\n```\n\n> 备注\n\n带名参数并不需要跟参数列表的顺序完全一致\n\n#### 7. 变长参数\n\n可以实现一个接受可变长度参数列表的函数:\n```scala\ndef sum(args : Int *) = {\n  var result = 0\n  for(arg <- args){\n    result += arg\n  }\n  result\n}\n```\n可以使用任意多的参数来调用该函数:\n```scala\nval result = sum(4, 5, 1) // 10\n```\n#### 8. 过程\n\nScala对于不返回值的函数有特殊的表示法．如果函数体包含在花括号当中但没有前面的`=`符号，那么返回类型就是Unit，这样的函数被称为过程:\n```scala\ndef welcome(str : String) {\n  println(\"welcome \" + str)\n}\n```\n或者显示声明Unit返回类型:\n```scala\ndef welcome(str : String) : Unit = {\n  println(\"welcome \" + str)\n}\n```\n#### 9. 懒值\n\n当val被声明为lazy时，它的初始化将被推迟，直到我们首次对它取值:\n```scala\nlazy val words = scala.io.Source.fromFile(\"/usr/share/dict/words\").mkString\n```\n如果程序从不访问words，那么文件也不会被打开．\n\n懒值对于开销较大的初始化语句而言十分有用．\n\n> 备注\n\n懒值并不是没有额外的开销．我们每次访问懒值，都会有一个方法被调用，而这个方法将会以线程安全的方式检查该值是否已被初始化．\n\n#### 10. 异常\n\nScala的异常工作机制跟Java一样．当你抛出异常时:\n```scala\nthrow new IllegalArgumentException(\"x should not be negative\")\n```\n当前的运算被终止，运行时系统查找可以接受 `IllegalArgumentException` 的异常处理器．控制权将在离抛出点最近的处理器中恢复．如果没有找到符合要求的异常处理器，则程序退出．\n\n和Java一样，抛出的对象必须是 `java.lang.Throwable` 的子类．不过，与Java不同的是，Scala没有\"受检\"异常，你不需要声明函数或者方法可能会抛出某种异常．\n\n`throw` 表达式有特殊的类型`Nothing`．这在if/else表达式中很有用．如果一个分支的类型是`Nothing`，那么 `if/else` 表达式的类型就是另一个分支的类型:\n```scala\nif (x > 0) {\n  sqrt(x)\n}\nelse{\n  throw new IllegalArgumentException(\"x should not be negative\")\n}\n```\n第一个分支的类型是`Double`，第二个分支的类型是`Nothing`，因此if/else表达式的类型是`Double`\n\n捕获异常的语法采用的是模式匹配的语法:\n```scala\ntry{\n  process(new URL(\"Http://hortsman.com/fred-tiny.gif\"))\n}\ncatch {\n  case _: MalformedURLException => println (\"Bad URL:\" + url)\n  case ex: IOException => ex.printStackTrace()\n}\n```\n与Java一样，更通用的异常应该排在更具体的异常之后．\n\n`try/finally` 语句可以释放资源，不论有没有异常发生:\n```scala\nvar in = new URL(\"\").openStream()\ntry{\n  process (in)\n}\nfinally {\n  in.close()\n}\n```\n\n\n来源于： 快学Scala\n","source":"_posts/Scala/[Scala]Scala学习笔记一 基础.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Scala 学习笔记之基础语法\ndate: 2018-02-27 09:35:01\ntags:\n  - Scala\n\ncategories: Scala\npermalink: scala-notes-basis\n---\n\n#### 1. 变量\n\nval定义的值实际上是一个常亮，无法改变其内容\n```scala\n\nscala> val num = 0\nnum: Int = 0\n\nscala> num = 2\n<console>:12: error: reassignment to val\n       num = 2\n           ^\n\n```\n如果要声明其值可变的变量，可以使用var\n```scala\nscala> var number = 0\nnumber: Int = 0\n\nscala> number = 2\nnumber: Int = 2\n\n```\n\n在Scala中，建议使用`val`，除非你真的需要改变它的内容．\n\n> 备注\n\n不需要给出值或者变量的类型，可以从你用来初始化它的表达式推断出来．只声明值或者变量但不做初始化会报错：\n```scala\nscala> val str: String\n<console>:11: error: only classes can have declared but undefined members\n       val str: String\n           ^\n\nscala> val str: String = \"Hello\"\nstr: String = Hello\n```\n#### 2. 常用类型\n\n常用类型：\n- Byte\n- Char\n- Short\n- Int\n- Long\n- Float\n- Double\n- Boolean\n\n跟Java不同的是，这些类型是类．Scala并不刻意区分基本类型和引用类型．你可以对数字执行方法：\n```scala\nscala> 1.toString()\nres2: String = 1\n```\n#### 3. 条件表达式\n\nScala的 `if/else` 的语法结构和Java的一样．不过，在Scala中 `if/else` 表达式有值，这个值就是跟在 `if` 或 `else` 之后的表达式的值:\n```scala\nif(x > 0) 1 else -1\n```\n上述表达式的值是１或者-1，具体是哪一个取决于x的值．你可以将 `if/else` 表达式的值赋值给变量：\n```scala\nval s = if(x > 0) 1 else -1\n```\n等同于:\n```scala\nif(x > 0) s = 1 else s = -1\n```\n相对于第二种写法，第一种写法更好一些，因为它可以用来初始化一个val，而第二种写法当中，s必须是var．\n\n> 备注\n\nScala中每个表达式都有一个类型\n```scala\nscala> val s = if(x > 0) \"positive\" else -1;\ns: Any = positive\n```\n上述表达式的类型是两个分支类型的公共超类型．在这个例子中，其中一个分支是`java.lang.String`，而另一个分支是`Int`．它们的公共超类型是`Any`．\n```scala\nif(x > 0) 1\n```\n那么有可能if语句没有输出值．但是在Scala中，每个表达式都应该有某种值．这个问题的解决方案是引入一个 `Unit` 类，写作 `()`．不带 `else` 的这个 `if` 语句等同于:\n```scala\nif(x > 0) 1 else ()\n```\n#### 4. 循环\n\nScala拥有与Java和C++相同的while和do循环：\n```scala\nwhile(n > 2){\n  println(\"num->\" + n)\n  n = n -1\n}\n```\n但是Scala没有与`for(初始化变量;检查变量是否满足某条件;更新变量)`循环直接对应的结构．如果你需要这样的循环，有两个选择：一是选择while循环，二是使用如下for语句:\n```scala\nfor(i <- 1 to n){\n  println(\"num->\" + i)\n}\n```\n上述表达式的目标是让变量i遍历`<-`右边的表达式的所有值．至于如何遍历，则取决于表达式的类型．\n\n遍历字符串或者数组时，你通常需要使用从0到n-1的区间．这个时候你可以使用`util`方法而不是`to`方法．`util`方法返回一个并不包含上限的区间:\n```scala\nval s = \"Hello\"\nfor(i <- 0 until s.length){\n  println(i + \" = \" + s(i))\n}\n```\n或者\n```scala\nfor(ch <- \"Hello\"){\n  println(ch)\n}\n```\n#### 5. 函数\n\n要定义函数，需要给出函数的名称，参数和函数体:\n```scala\ndef abs (x: Double) = if (x >= 0) x else -x\n```\n必须给出所有参数的类型，只要函数不是递归的，就可以不需要指定返回类型．Scala编译器可以通过`=`符号右侧的表达式的类型推断出返回类型．\n如果函数体需要多个表达式完成，可以使用代码块．块中最后一个表达式的值就是函数的返回值:\n```scala\ndef fac(n: Int) = {\n  var r = 1\n  for(i <- 1 to n){\n    r = r * i\n  }\n  r\n}\n```\n上例中函数返回值为r的值\n\n> 备注\n\n虽然在函数中使用 `return` 并没有什么不对，我们还是最好适应没有 `return` 的日子．之后，我们会使用大量的匿名函数，这些函数中 `return` 并不返回值给调用者．它跳出到包含它的函数中．我们可以把 `return` 当做是函数版的 `break` 语句，仅在需要时使用．\n\n对于递归函数，我们必须指定返回类型：\n```scala\ndef fac(n: Int) : Int = if(n < 0) 1 else n * fac(n-1)\n```\n#### 6. 默认参数和带名参数\n\n我们在调用某些函数时并不显示的给出所有参数值，对于这些函数我们可以使用默认参数：\n```scala\ndef decorate (str : String, left : String = \"[\" , right : String = \"]\") {\n  left + str + right\n}\n```\n这个函数带有两个参数，left 和 right，带有默认值 `[` 和 `]`:\n```scala\ndecorate(\"Hello\") // [Hello]\ndecorate(\"Hello\", \"<\", \">\") // <Hello>\n```\n你可以在提供参数值的时候指定参数名(带名参数)：\n```scala\ndecorate(left = \"<<\", str = \"Hello\", right = \">>\") // <<Hello>>\n```\n你可以混用未命名参数和带名参数，只要那些未命名的参数是排在前面即可:\n```scala\ndecorate(\"Hello\", right = \"]###\") // 实际调用 decorate(\"Hello\", \"[\", \"]###\")\n```\n\n> 备注\n\n带名参数并不需要跟参数列表的顺序完全一致\n\n#### 7. 变长参数\n\n可以实现一个接受可变长度参数列表的函数:\n```scala\ndef sum(args : Int *) = {\n  var result = 0\n  for(arg <- args){\n    result += arg\n  }\n  result\n}\n```\n可以使用任意多的参数来调用该函数:\n```scala\nval result = sum(4, 5, 1) // 10\n```\n#### 8. 过程\n\nScala对于不返回值的函数有特殊的表示法．如果函数体包含在花括号当中但没有前面的`=`符号，那么返回类型就是Unit，这样的函数被称为过程:\n```scala\ndef welcome(str : String) {\n  println(\"welcome \" + str)\n}\n```\n或者显示声明Unit返回类型:\n```scala\ndef welcome(str : String) : Unit = {\n  println(\"welcome \" + str)\n}\n```\n#### 9. 懒值\n\n当val被声明为lazy时，它的初始化将被推迟，直到我们首次对它取值:\n```scala\nlazy val words = scala.io.Source.fromFile(\"/usr/share/dict/words\").mkString\n```\n如果程序从不访问words，那么文件也不会被打开．\n\n懒值对于开销较大的初始化语句而言十分有用．\n\n> 备注\n\n懒值并不是没有额外的开销．我们每次访问懒值，都会有一个方法被调用，而这个方法将会以线程安全的方式检查该值是否已被初始化．\n\n#### 10. 异常\n\nScala的异常工作机制跟Java一样．当你抛出异常时:\n```scala\nthrow new IllegalArgumentException(\"x should not be negative\")\n```\n当前的运算被终止，运行时系统查找可以接受 `IllegalArgumentException` 的异常处理器．控制权将在离抛出点最近的处理器中恢复．如果没有找到符合要求的异常处理器，则程序退出．\n\n和Java一样，抛出的对象必须是 `java.lang.Throwable` 的子类．不过，与Java不同的是，Scala没有\"受检\"异常，你不需要声明函数或者方法可能会抛出某种异常．\n\n`throw` 表达式有特殊的类型`Nothing`．这在if/else表达式中很有用．如果一个分支的类型是`Nothing`，那么 `if/else` 表达式的类型就是另一个分支的类型:\n```scala\nif (x > 0) {\n  sqrt(x)\n}\nelse{\n  throw new IllegalArgumentException(\"x should not be negative\")\n}\n```\n第一个分支的类型是`Double`，第二个分支的类型是`Nothing`，因此if/else表达式的类型是`Double`\n\n捕获异常的语法采用的是模式匹配的语法:\n```scala\ntry{\n  process(new URL(\"Http://hortsman.com/fred-tiny.gif\"))\n}\ncatch {\n  case _: MalformedURLException => println (\"Bad URL:\" + url)\n  case ex: IOException => ex.printStackTrace()\n}\n```\n与Java一样，更通用的异常应该排在更具体的异常之后．\n\n`try/finally` 语句可以释放资源，不论有没有异常发生:\n```scala\nvar in = new URL(\"\").openStream()\ntry{\n  process (in)\n}\nfinally {\n  in.close()\n}\n```\n\n\n来源于： 快学Scala\n","slug":"scala-notes-basis","published":1,"updated":"2018-02-27T02:02:15.330Z","comments":1,"photos":[],"link":"","_id":"cje58tiux005aordbey3c35r3","content":"<h4 id=\"1-变量\"><a href=\"#1-变量\" class=\"headerlink\" title=\"1. 变量\"></a>1. 变量</h4><p>val定义的值实际上是一个常亮，无法改变其内容<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> num = <span class=\"number\">0</span></span><br><span class=\"line\">num: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; num = <span class=\"number\">2</span></span><br><span class=\"line\">&lt;console&gt;:<span class=\"number\">12</span>: error: reassignment to <span class=\"keyword\">val</span></span><br><span class=\"line\">       num = <span class=\"number\">2</span></span><br><span class=\"line\">           ^</span><br></pre></td></tr></table></figure></p>\n<p>如果要声明其值可变的变量，可以使用var<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">var</span> number = <span class=\"number\">0</span></span><br><span class=\"line\">number: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; number = <span class=\"number\">2</span></span><br><span class=\"line\">number: <span class=\"type\">Int</span> = <span class=\"number\">2</span></span><br></pre></td></tr></table></figure></p>\n<p>在Scala中，建议使用<code>val</code>，除非你真的需要改变它的内容．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>不需要给出值或者变量的类型，可以从你用来初始化它的表达式推断出来．只声明值或者变量但不做初始化会报错：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> str: <span class=\"type\">String</span></span><br><span class=\"line\">&lt;console&gt;:<span class=\"number\">11</span>: error: only classes can have declared but undefined members</span><br><span class=\"line\">       <span class=\"keyword\">val</span> str: <span class=\"type\">String</span></span><br><span class=\"line\">           ^</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> str: <span class=\"type\">String</span> = <span class=\"string\">\"Hello\"</span></span><br><span class=\"line\">str: <span class=\"type\">String</span> = <span class=\"type\">Hello</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-常用类型\"><a href=\"#2-常用类型\" class=\"headerlink\" title=\"2. 常用类型\"></a>2. 常用类型</h4><p>常用类型：</p>\n<ul>\n<li>Byte</li>\n<li>Char</li>\n<li>Short</li>\n<li>Int</li>\n<li>Long</li>\n<li>Float</li>\n<li>Double</li>\n<li>Boolean</li>\n</ul>\n<p>跟Java不同的是，这些类型是类．Scala并不刻意区分基本类型和引用类型．你可以对数字执行方法：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"number\">1.</span>toString()</span><br><span class=\"line\">res2: <span class=\"type\">String</span> = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-条件表达式\"><a href=\"#3-条件表达式\" class=\"headerlink\" title=\"3. 条件表达式\"></a>3. 条件表达式</h4><p>Scala的 <code>if/else</code> 的语法结构和Java的一样．不过，在Scala中 <code>if/else</code> 表达式有值，这个值就是跟在 <code>if</code> 或 <code>else</code> 之后的表达式的值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure></p>\n<p>上述表达式的值是１或者-1，具体是哪一个取决于x的值．你可以将 <code>if/else</code> 表达式的值赋值给变量：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> s = <span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure></p>\n<p>等同于:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) s = <span class=\"number\">1</span> <span class=\"keyword\">else</span> s = <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure></p>\n<p>相对于第二种写法，第一种写法更好一些，因为它可以用来初始化一个val，而第二种写法当中，s必须是var．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>Scala中每个表达式都有一个类型<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> s = <span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"string\">\"positive\"</span> <span class=\"keyword\">else</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">s: <span class=\"type\">Any</span> = positive</span><br></pre></td></tr></table></figure></p>\n<p>上述表达式的类型是两个分支类型的公共超类型．在这个例子中，其中一个分支是<code>java.lang.String</code>，而另一个分支是<code>Int</code>．它们的公共超类型是<code>Any</code>．<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span></span><br></pre></td></tr></table></figure></p>\n<p>那么有可能if语句没有输出值．但是在Scala中，每个表达式都应该有某种值．这个问题的解决方案是引入一个 <code>Unit</code> 类，写作 <code>()</code>．不带 <code>else</code> 的这个 <code>if</code> 语句等同于:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> ()</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-循环\"><a href=\"#4-循环\" class=\"headerlink\" title=\"4. 循环\"></a>4. 循环</h4><p>Scala拥有与Java和C++相同的while和do循环：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(n &gt; <span class=\"number\">2</span>)&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"num-&gt;\"</span> + n)</span><br><span class=\"line\">  n = n <span class=\"number\">-1</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>但是Scala没有与<code>for(初始化变量;检查变量是否满足某条件;更新变量)</code>循环直接对应的结构．如果你需要这样的循环，有两个选择：一是选择while循环，二是使用如下for语句:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">1</span> to n)&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"num-&gt;\"</span> + i)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上述表达式的目标是让变量i遍历<code>&lt;-</code>右边的表达式的所有值．至于如何遍历，则取决于表达式的类型．</p>\n<p>遍历字符串或者数组时，你通常需要使用从0到n-1的区间．这个时候你可以使用<code>util</code>方法而不是<code>to</code>方法．<code>util</code>方法返回一个并不包含上限的区间:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> s = <span class=\"string\">\"Hello\"</span></span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until s.length)&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\" = \"</span> + s(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>或者<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(ch &lt;- <span class=\"string\">\"Hello\"</span>)&#123;</span><br><span class=\"line\">  println(ch)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"5-函数\"><a href=\"#5-函数\" class=\"headerlink\" title=\"5. 函数\"></a>5. 函数</h4><p>要定义函数，需要给出函数的名称，参数和函数体:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">abs</span> </span>(x: <span class=\"type\">Double</span>) = <span class=\"keyword\">if</span> (x &gt;= <span class=\"number\">0</span>) x <span class=\"keyword\">else</span> -x</span><br></pre></td></tr></table></figure></p>\n<p>必须给出所有参数的类型，只要函数不是递归的，就可以不需要指定返回类型．Scala编译器可以通过<code>=</code>符号右侧的表达式的类型推断出返回类型．<br>如果函数体需要多个表达式完成，可以使用代码块．块中最后一个表达式的值就是函数的返回值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(n: <span class=\"type\">Int</span>) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> r = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">1</span> to n)&#123;</span><br><span class=\"line\">    r = r * i</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  r</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上例中函数返回值为r的值</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>虽然在函数中使用 <code>return</code> 并没有什么不对，我们还是最好适应没有 <code>return</code> 的日子．之后，我们会使用大量的匿名函数，这些函数中 <code>return</code> 并不返回值给调用者．它跳出到包含它的函数中．我们可以把 <code>return</code> 当做是函数版的 <code>break</code> 语句，仅在需要时使用．</p>\n<p>对于递归函数，我们必须指定返回类型：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(n: <span class=\"type\">Int</span>) : <span class=\"type\">Int</span> = <span class=\"keyword\">if</span>(n &lt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> n * fac(n<span class=\"number\">-1</span>)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"6-默认参数和带名参数\"><a href=\"#6-默认参数和带名参数\" class=\"headerlink\" title=\"6. 默认参数和带名参数\"></a>6. 默认参数和带名参数</h4><p>我们在调用某些函数时并不显示的给出所有参数值，对于这些函数我们可以使用默认参数：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decorate</span> </span>(str : <span class=\"type\">String</span>, left : <span class=\"type\">String</span> = <span class=\"string\">\"[\"</span> , right : <span class=\"type\">String</span> = <span class=\"string\">\"]\"</span>) &#123;</span><br><span class=\"line\">  left + str + right</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个函数带有两个参数，left 和 right，带有默认值 <code>[</code> 和 <code>]</code>:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">decorate(<span class=\"string\">\"Hello\"</span>) <span class=\"comment\">// [Hello]</span></span><br><span class=\"line\">decorate(<span class=\"string\">\"Hello\"</span>, <span class=\"string\">\"&lt;\"</span>, <span class=\"string\">\"&gt;\"</span>) <span class=\"comment\">// &lt;Hello&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>你可以在提供参数值的时候指定参数名(带名参数)：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">decorate(left = <span class=\"string\">\"&lt;&lt;\"</span>, str = <span class=\"string\">\"Hello\"</span>, right = <span class=\"string\">\"&gt;&gt;\"</span>) <span class=\"comment\">// &lt;&lt;Hello&gt;&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>你可以混用未命名参数和带名参数，只要那些未命名的参数是排在前面即可:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">decorate(<span class=\"string\">\"Hello\"</span>, right = <span class=\"string\">\"]###\"</span>) <span class=\"comment\">// 实际调用 decorate(\"Hello\", \"[\", \"]###\")</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>带名参数并不需要跟参数列表的顺序完全一致</p>\n<h4 id=\"7-变长参数\"><a href=\"#7-变长参数\" class=\"headerlink\" title=\"7. 变长参数\"></a>7. 变长参数</h4><p>可以实现一个接受可变长度参数列表的函数:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(args : <span class=\"type\">Int</span> *) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> result = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span>(arg &lt;- args)&#123;</span><br><span class=\"line\">    result += arg</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  result</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>可以使用任意多的参数来调用该函数:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> result = sum(<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>) <span class=\"comment\">// 10</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"8-过程\"><a href=\"#8-过程\" class=\"headerlink\" title=\"8. 过程\"></a>8. 过程</h4><p>Scala对于不返回值的函数有特殊的表示法．如果函数体包含在花括号当中但没有前面的<code>=</code>符号，那么返回类型就是Unit，这样的函数被称为过程:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">welcome</span></span>(str : <span class=\"type\">String</span>) &#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"welcome \"</span> + str)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>或者显示声明Unit返回类型:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">welcome</span></span>(str : <span class=\"type\">String</span>) : <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"welcome \"</span> + str)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"9-懒值\"><a href=\"#9-懒值\" class=\"headerlink\" title=\"9. 懒值\"></a>9. 懒值</h4><p>当val被声明为lazy时，它的初始化将被推迟，直到我们首次对它取值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">lazy</span> <span class=\"keyword\">val</span> words = scala.io.<span class=\"type\">Source</span>.fromFile(<span class=\"string\">\"/usr/share/dict/words\"</span>).mkString</span><br></pre></td></tr></table></figure></p>\n<p>如果程序从不访问words，那么文件也不会被打开．</p>\n<p>懒值对于开销较大的初始化语句而言十分有用．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>懒值并不是没有额外的开销．我们每次访问懒值，都会有一个方法被调用，而这个方法将会以线程安全的方式检查该值是否已被初始化．</p>\n<h4 id=\"10-异常\"><a href=\"#10-异常\" class=\"headerlink\" title=\"10. 异常\"></a>10. 异常</h4><p>Scala的异常工作机制跟Java一样．当你抛出异常时:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalArgumentException</span>(<span class=\"string\">\"x should not be negative\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>当前的运算被终止，运行时系统查找可以接受 <code>IllegalArgumentException</code> 的异常处理器．控制权将在离抛出点最近的处理器中恢复．如果没有找到符合要求的异常处理器，则程序退出．</p>\n<p>和Java一样，抛出的对象必须是 <code>java.lang.Throwable</code> 的子类．不过，与Java不同的是，Scala没有”受检”异常，你不需要声明函数或者方法可能会抛出某种异常．</p>\n<p><code>throw</code> 表达式有特殊的类型<code>Nothing</code>．这在if/else表达式中很有用．如果一个分支的类型是<code>Nothing</code>，那么 <code>if/else</code> 表达式的类型就是另一个分支的类型:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (x &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">  sqrt(x)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalArgumentException</span>(<span class=\"string\">\"x should not be negative\"</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>第一个分支的类型是<code>Double</code>，第二个分支的类型是<code>Nothing</code>，因此if/else表达式的类型是<code>Double</code></p>\n<p>捕获异常的语法采用的是模式匹配的语法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">  process(<span class=\"keyword\">new</span> <span class=\"type\">URL</span>(<span class=\"string\">\"Http://hortsman.com/fred-tiny.gif\"</span>))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> _: <span class=\"type\">MalformedURLException</span> =&gt; println (<span class=\"string\">\"Bad URL:\"</span> + url)</span><br><span class=\"line\">  <span class=\"keyword\">case</span> ex: <span class=\"type\">IOException</span> =&gt; ex.printStackTrace()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>与Java一样，更通用的异常应该排在更具体的异常之后．</p>\n<p><code>try/finally</code> 语句可以释放资源，不论有没有异常发生:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> in = <span class=\"keyword\">new</span> <span class=\"type\">URL</span>(<span class=\"string\">\"\"</span>).openStream()</span><br><span class=\"line\"><span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">  process (in)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">  in.close()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>来源于： 快学Scala</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"1-变量\"><a href=\"#1-变量\" class=\"headerlink\" title=\"1. 变量\"></a>1. 变量</h4><p>val定义的值实际上是一个常亮，无法改变其内容<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> num = <span class=\"number\">0</span></span><br><span class=\"line\">num: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; num = <span class=\"number\">2</span></span><br><span class=\"line\">&lt;console&gt;:<span class=\"number\">12</span>: error: reassignment to <span class=\"keyword\">val</span></span><br><span class=\"line\">       num = <span class=\"number\">2</span></span><br><span class=\"line\">           ^</span><br></pre></td></tr></table></figure></p>\n<p>如果要声明其值可变的变量，可以使用var<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">var</span> number = <span class=\"number\">0</span></span><br><span class=\"line\">number: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; number = <span class=\"number\">2</span></span><br><span class=\"line\">number: <span class=\"type\">Int</span> = <span class=\"number\">2</span></span><br></pre></td></tr></table></figure></p>\n<p>在Scala中，建议使用<code>val</code>，除非你真的需要改变它的内容．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>不需要给出值或者变量的类型，可以从你用来初始化它的表达式推断出来．只声明值或者变量但不做初始化会报错：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> str: <span class=\"type\">String</span></span><br><span class=\"line\">&lt;console&gt;:<span class=\"number\">11</span>: error: only classes can have declared but undefined members</span><br><span class=\"line\">       <span class=\"keyword\">val</span> str: <span class=\"type\">String</span></span><br><span class=\"line\">           ^</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> str: <span class=\"type\">String</span> = <span class=\"string\">\"Hello\"</span></span><br><span class=\"line\">str: <span class=\"type\">String</span> = <span class=\"type\">Hello</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-常用类型\"><a href=\"#2-常用类型\" class=\"headerlink\" title=\"2. 常用类型\"></a>2. 常用类型</h4><p>常用类型：</p>\n<ul>\n<li>Byte</li>\n<li>Char</li>\n<li>Short</li>\n<li>Int</li>\n<li>Long</li>\n<li>Float</li>\n<li>Double</li>\n<li>Boolean</li>\n</ul>\n<p>跟Java不同的是，这些类型是类．Scala并不刻意区分基本类型和引用类型．你可以对数字执行方法：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"number\">1.</span>toString()</span><br><span class=\"line\">res2: <span class=\"type\">String</span> = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-条件表达式\"><a href=\"#3-条件表达式\" class=\"headerlink\" title=\"3. 条件表达式\"></a>3. 条件表达式</h4><p>Scala的 <code>if/else</code> 的语法结构和Java的一样．不过，在Scala中 <code>if/else</code> 表达式有值，这个值就是跟在 <code>if</code> 或 <code>else</code> 之后的表达式的值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure></p>\n<p>上述表达式的值是１或者-1，具体是哪一个取决于x的值．你可以将 <code>if/else</code> 表达式的值赋值给变量：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> s = <span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure></p>\n<p>等同于:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) s = <span class=\"number\">1</span> <span class=\"keyword\">else</span> s = <span class=\"number\">-1</span></span><br></pre></td></tr></table></figure></p>\n<p>相对于第二种写法，第一种写法更好一些，因为它可以用来初始化一个val，而第二种写法当中，s必须是var．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>Scala中每个表达式都有一个类型<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> s = <span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"string\">\"positive\"</span> <span class=\"keyword\">else</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">s: <span class=\"type\">Any</span> = positive</span><br></pre></td></tr></table></figure></p>\n<p>上述表达式的类型是两个分支类型的公共超类型．在这个例子中，其中一个分支是<code>java.lang.String</code>，而另一个分支是<code>Int</code>．它们的公共超类型是<code>Any</code>．<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span></span><br></pre></td></tr></table></figure></p>\n<p>那么有可能if语句没有输出值．但是在Scala中，每个表达式都应该有某种值．这个问题的解决方案是引入一个 <code>Unit</code> 类，写作 <code>()</code>．不带 <code>else</code> 的这个 <code>if</code> 语句等同于:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(x &gt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> ()</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-循环\"><a href=\"#4-循环\" class=\"headerlink\" title=\"4. 循环\"></a>4. 循环</h4><p>Scala拥有与Java和C++相同的while和do循环：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(n &gt; <span class=\"number\">2</span>)&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"num-&gt;\"</span> + n)</span><br><span class=\"line\">  n = n <span class=\"number\">-1</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>但是Scala没有与<code>for(初始化变量;检查变量是否满足某条件;更新变量)</code>循环直接对应的结构．如果你需要这样的循环，有两个选择：一是选择while循环，二是使用如下for语句:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">1</span> to n)&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"num-&gt;\"</span> + i)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上述表达式的目标是让变量i遍历<code>&lt;-</code>右边的表达式的所有值．至于如何遍历，则取决于表达式的类型．</p>\n<p>遍历字符串或者数组时，你通常需要使用从0到n-1的区间．这个时候你可以使用<code>util</code>方法而不是<code>to</code>方法．<code>util</code>方法返回一个并不包含上限的区间:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> s = <span class=\"string\">\"Hello\"</span></span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until s.length)&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\" = \"</span> + s(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>或者<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(ch &lt;- <span class=\"string\">\"Hello\"</span>)&#123;</span><br><span class=\"line\">  println(ch)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"5-函数\"><a href=\"#5-函数\" class=\"headerlink\" title=\"5. 函数\"></a>5. 函数</h4><p>要定义函数，需要给出函数的名称，参数和函数体:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">abs</span> </span>(x: <span class=\"type\">Double</span>) = <span class=\"keyword\">if</span> (x &gt;= <span class=\"number\">0</span>) x <span class=\"keyword\">else</span> -x</span><br></pre></td></tr></table></figure></p>\n<p>必须给出所有参数的类型，只要函数不是递归的，就可以不需要指定返回类型．Scala编译器可以通过<code>=</code>符号右侧的表达式的类型推断出返回类型．<br>如果函数体需要多个表达式完成，可以使用代码块．块中最后一个表达式的值就是函数的返回值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(n: <span class=\"type\">Int</span>) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> r = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">1</span> to n)&#123;</span><br><span class=\"line\">    r = r * i</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  r</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>上例中函数返回值为r的值</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>虽然在函数中使用 <code>return</code> 并没有什么不对，我们还是最好适应没有 <code>return</code> 的日子．之后，我们会使用大量的匿名函数，这些函数中 <code>return</code> 并不返回值给调用者．它跳出到包含它的函数中．我们可以把 <code>return</code> 当做是函数版的 <code>break</code> 语句，仅在需要时使用．</p>\n<p>对于递归函数，我们必须指定返回类型：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(n: <span class=\"type\">Int</span>) : <span class=\"type\">Int</span> = <span class=\"keyword\">if</span>(n &lt; <span class=\"number\">0</span>) <span class=\"number\">1</span> <span class=\"keyword\">else</span> n * fac(n<span class=\"number\">-1</span>)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"6-默认参数和带名参数\"><a href=\"#6-默认参数和带名参数\" class=\"headerlink\" title=\"6. 默认参数和带名参数\"></a>6. 默认参数和带名参数</h4><p>我们在调用某些函数时并不显示的给出所有参数值，对于这些函数我们可以使用默认参数：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decorate</span> </span>(str : <span class=\"type\">String</span>, left : <span class=\"type\">String</span> = <span class=\"string\">\"[\"</span> , right : <span class=\"type\">String</span> = <span class=\"string\">\"]\"</span>) &#123;</span><br><span class=\"line\">  left + str + right</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个函数带有两个参数，left 和 right，带有默认值 <code>[</code> 和 <code>]</code>:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">decorate(<span class=\"string\">\"Hello\"</span>) <span class=\"comment\">// [Hello]</span></span><br><span class=\"line\">decorate(<span class=\"string\">\"Hello\"</span>, <span class=\"string\">\"&lt;\"</span>, <span class=\"string\">\"&gt;\"</span>) <span class=\"comment\">// &lt;Hello&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>你可以在提供参数值的时候指定参数名(带名参数)：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">decorate(left = <span class=\"string\">\"&lt;&lt;\"</span>, str = <span class=\"string\">\"Hello\"</span>, right = <span class=\"string\">\"&gt;&gt;\"</span>) <span class=\"comment\">// &lt;&lt;Hello&gt;&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>你可以混用未命名参数和带名参数，只要那些未命名的参数是排在前面即可:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">decorate(<span class=\"string\">\"Hello\"</span>, right = <span class=\"string\">\"]###\"</span>) <span class=\"comment\">// 实际调用 decorate(\"Hello\", \"[\", \"]###\")</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>带名参数并不需要跟参数列表的顺序完全一致</p>\n<h4 id=\"7-变长参数\"><a href=\"#7-变长参数\" class=\"headerlink\" title=\"7. 变长参数\"></a>7. 变长参数</h4><p>可以实现一个接受可变长度参数列表的函数:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(args : <span class=\"type\">Int</span> *) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> result = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span>(arg &lt;- args)&#123;</span><br><span class=\"line\">    result += arg</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  result</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>可以使用任意多的参数来调用该函数:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> result = sum(<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>) <span class=\"comment\">// 10</span></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"8-过程\"><a href=\"#8-过程\" class=\"headerlink\" title=\"8. 过程\"></a>8. 过程</h4><p>Scala对于不返回值的函数有特殊的表示法．如果函数体包含在花括号当中但没有前面的<code>=</code>符号，那么返回类型就是Unit，这样的函数被称为过程:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">welcome</span></span>(str : <span class=\"type\">String</span>) &#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"welcome \"</span> + str)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>或者显示声明Unit返回类型:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">welcome</span></span>(str : <span class=\"type\">String</span>) : <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"welcome \"</span> + str)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"9-懒值\"><a href=\"#9-懒值\" class=\"headerlink\" title=\"9. 懒值\"></a>9. 懒值</h4><p>当val被声明为lazy时，它的初始化将被推迟，直到我们首次对它取值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">lazy</span> <span class=\"keyword\">val</span> words = scala.io.<span class=\"type\">Source</span>.fromFile(<span class=\"string\">\"/usr/share/dict/words\"</span>).mkString</span><br></pre></td></tr></table></figure></p>\n<p>如果程序从不访问words，那么文件也不会被打开．</p>\n<p>懒值对于开销较大的初始化语句而言十分有用．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>懒值并不是没有额外的开销．我们每次访问懒值，都会有一个方法被调用，而这个方法将会以线程安全的方式检查该值是否已被初始化．</p>\n<h4 id=\"10-异常\"><a href=\"#10-异常\" class=\"headerlink\" title=\"10. 异常\"></a>10. 异常</h4><p>Scala的异常工作机制跟Java一样．当你抛出异常时:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalArgumentException</span>(<span class=\"string\">\"x should not be negative\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>当前的运算被终止，运行时系统查找可以接受 <code>IllegalArgumentException</code> 的异常处理器．控制权将在离抛出点最近的处理器中恢复．如果没有找到符合要求的异常处理器，则程序退出．</p>\n<p>和Java一样，抛出的对象必须是 <code>java.lang.Throwable</code> 的子类．不过，与Java不同的是，Scala没有”受检”异常，你不需要声明函数或者方法可能会抛出某种异常．</p>\n<p><code>throw</code> 表达式有特殊的类型<code>Nothing</code>．这在if/else表达式中很有用．如果一个分支的类型是<code>Nothing</code>，那么 <code>if/else</code> 表达式的类型就是另一个分支的类型:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (x &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">  sqrt(x)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalArgumentException</span>(<span class=\"string\">\"x should not be negative\"</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>第一个分支的类型是<code>Double</code>，第二个分支的类型是<code>Nothing</code>，因此if/else表达式的类型是<code>Double</code></p>\n<p>捕获异常的语法采用的是模式匹配的语法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">  process(<span class=\"keyword\">new</span> <span class=\"type\">URL</span>(<span class=\"string\">\"Http://hortsman.com/fred-tiny.gif\"</span>))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> _: <span class=\"type\">MalformedURLException</span> =&gt; println (<span class=\"string\">\"Bad URL:\"</span> + url)</span><br><span class=\"line\">  <span class=\"keyword\">case</span> ex: <span class=\"type\">IOException</span> =&gt; ex.printStackTrace()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>与Java一样，更通用的异常应该排在更具体的异常之后．</p>\n<p><code>try/finally</code> 语句可以释放资源，不论有没有异常发生:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> in = <span class=\"keyword\">new</span> <span class=\"type\">URL</span>(<span class=\"string\">\"\"</span>).openStream()</span><br><span class=\"line\"><span class=\"keyword\">try</span>&#123;</span><br><span class=\"line\">  process (in)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">  in.close()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>来源于： 快学Scala</p>\n"},{"layout":"post","author":"sjf0115","title":"Scala 学习笔记之Map与Tuple","date":"2018-02-27T01:40:01.000Z","_content":"\n### 1. 构造映射\n\n可以使用如下命令构造一个映射:\n```scala\nscala> val scores = Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 89, Bob -> 98)\n```\n上面构造了一个不可变的Map[String, Int]，其值也不能被改变．如果想要一个可变映射，使用如下命令创建:\n```scala\nscala> val scores = scala.collection.mutable.Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.mutable.Map[String,Int] = Map(Bob -> 98, Alice -> 90, Kim -> 89)\n```\n如果只想创建一个空的映射:\n```scala\nscala> val scores = new scala.collection.mutable.HashMap[String, Int]\nscores: scala.collection.mutable.HashMap[String,Int] = Map()\n```\n从上面我们可以知道使用`->`操作符来创建映射的键值对元素\n```\n\"Alice\" -> 90\n```\n我们也可以使用下面的方式定义映射:\n```scala\nscala> val scores = Map((\"Alice\",90), (\"Kim\",89), (\"Bob\",98))\nscores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 89, Bob -> 98)\n```\n### 2. 获取映射中的值\n\n可以使用`()`来查找某个键对应的值:\n```scala\nscala> val bobscores = scores(\"Bob\")\nbobscores: Int = 98\n```\n如果映射中并不包含对应键的值，则会抛出异常，这与Java返回`null`不同:\n```scala\nscala> val tomScores = scores(\"Tom\")\njava.util.NoSuchElementException: key not found: Tom\n  at scala.collection.MapLike$class.default(MapLike.scala:228)\n  at scala.collection.AbstractMap.default(Map.scala:59)\n  at scala.collection.MapLike$class.apply(MapLike.scala:141)\n  at scala.collection.AbstractMap.apply(Map.scala:59)\n  ... 32 elided\n```\n所以在获取某个键对应的值之前，要先检查映射中是否存在指定的键:\n```scala\nscala> val tomScores = if(scores.contains(\"Tom\")) scores(\"Tom\") else 0\ntomScores: Int = 0\n```\n以下是一个快捷写法:\n```scala\nscala> val tomScores = scores.getOrElse(\"Tom\", 0)\ntomScores: Int = 0\n```\n### 3. 更新映射中的值\n\n在可变映射中，可以更新某个映射的值，也可以添加一个新的键值对:\n```scala\nscala> val scores = scala.collection.mutable.Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.mutable.Map[String,Int] = Map(Bob -> 98, Alice -> 90, Kim -> 89)\nscala> scores(\"Alice\")=100 // 更新键值对\nscala> scores(\"Tom\")=67 // 添加键值对\nscala> println(scores)\nMap(Bob -> 98, Tom -> 67, Alice -> 100, Kim -> 89)\n```\n还可以使用`+=`操作符来添加多个关系:\n```scala\nscala> scores += (\"Bob\" -> 78, \"Fred\" -> 89)\nres3: scores.type = Map(Bob -> 78, Fred -> 89, Tom -> 67, Alice -> 100, Kim -> 89)\n```\n还可以使用`-=`操作符移除某个键对应的值:\n```scala\nscala> scores -= \"Tom\"\nres4: scores.type = Map(Bob -> 78, Fred -> 89, Alice -> 100, Kim -> 89)\n```\n虽然不可以更新一个不可变的映射，但是我们利用一些操作产生一个新的映射，并可以对原映射中的键值对进行修改或者添加新的键值对:\n```scala\nscala> val scores = Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 89, Bob -> 98)\n\nscala> val newScores = scores + (\"Kim\" -> 78, \"Tom\" -> 54)\nnewScores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 78, Bob -> 98, Tom -> 54)\n```\n上例中scores是不可变映射，我们在它基础上对\"Kim\"进行了修改，添加了\"Tom\"，产生了一个新的映射newScores\n\n### 4. 迭代映射\n\n可以使用如下命令迭代映射:\n```scala\nscala> for( key <- scores.keySet ) println(key + \"---\" + scores(key))\nAlice---90\nKim---89\nBob---98\n```\n或者\n```scala\nscala> for( value <- scores.values ) println(value)\n90\n89\n98\n```\n### 5. 排序映射\n\n在操作映射时，我们需要选定一个映射(哈希表还是平衡树)．默认情况下，scala给的是哈希表．有时候我们想对键进行一个排序，顺序访问键，这就需要一个树形映射:\n```scala\nscala> val scores = scala.collection.immutable.SortedMap(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.immutable.SortedMap[String,Int] = Map(Alice -> 90, Bob -> 98, Kim -> 89)\n```\n\n### 6. 与Java互操作\n\n如果你有一个Java映射，想要转换为Scala映射，以便便捷的使用Scala映射的方法，只需要增加如下语句:\n```scala\nimport scala.collection.JavaConversions.mapAsScalaMap\n```\n然后指定Scala映射类型来触发转换:\n```scala\nscala> val scores : scala.collection.mutable.Map[String,Int] = new java.util.TreeMap[String, Int]\nscores: scala.collection.mutable.Map[String,Int] = Map()\n```\n还可以将java.util.Properties到Map[String, String]的转换:\n```scala\nscala> import scala.collection.JavaConversions.propertiesAsScalaMap\nimport scala.collection.JavaConversions.propertiesAsScalaMap\n\nscala> val props : scala.collection.Map[String, String] = System.getProperties()\nprops: scala.collection.Map[String,String] =\nMap(env.emacs -> \"\", java.runtime.name -> Java(TM) SE Runtime Environment, sun.boot.library.path -> /home/xiaosi/opt/jdk-1.8.0/jre/lib/amd64, java.vm.version -> 25.91-b14, java.vm.vendor -> Oracle Corporation, ...\n```\n相反，如果想要把Scal映射转换为Java映射，只需要提供相反的隐式转换即可:\n```scala\nscala> import scala.collection.JavaConversions.mapAsJavaMap\nimport scala.collection.JavaConversions.mapAsJavaMap\n\nscala> import java.awt.font.TextAttribute._ // 引入下面的映射会用到的键\nimport java.awt.font.TextAttribute._\n\nscala> val attrs = Map(FAMILY -> \"Serif\", SIZE -> 12) // Scala映射\nattrs: scala.collection.immutable.Map[java.awt.font.TextAttribute,Any] = Map(java.awt.font.TextAttribute(family) -> Serif, java.awt.font.TextAttribute(size) -> 12)\n\nscala> val font = new java.awt.Font(attrs) // Java映射\nfont: java.awt.Font = java.awt.Font[family=Serif,name=Serif,style=plain,size=12]\n```\n\n### 7. 元组Tuple\n\n元组是不同类型的值的聚合，元组的值通过将单个的值包含在圆括号中构成的：\n```scala\nscala> val bobScore = (1, 98.5, \"Bob\")\nbobScore: (Int, Double, String) = (1,98.5,Bob)\n```\n可以使用方法`_1`，`_2`，`_3`访问其组员:\n```scala\nscala> val bobScore = (1, 98.5, \"Bob\")\nbobScore: (Int, Double, String) = (1,98.5,Bob)\n\nscala> bobScore._1\nres10: Int = 1\n\nscala> bobScore._3\nres11: String = Bob\n```\n通常，使用模式匹配的方式来获取元组的组元:\n```scala\nscala> val (id, score, name) = bobScore // 将变量id赋值为1，变量score赋值为98.5，变量name赋值为Bob\n   val bobScore: (Int, Double, String)\n\nscala> val (id, score, name) = bobScore\nid: Int = 1\nscore: Double = 98.5\nname: String = Bob\n\nscala> println(\"name = \" + name + \", score = \" + score + \", name = \" + name)\nname = Bob, score = 98.5, name = Bob\n```\n\n来源于： 快学Scala\n","source":"_posts/Scala/[Scala]Scala学习笔记三 Map与Tuple.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Scala 学习笔记之Map与Tuple\ndate: 2018-02-27 09:40:01\ntags:\n  - Scala\n\ncategories: Scala\npermalink: scala-notes-map-and-tuple\n---\n\n### 1. 构造映射\n\n可以使用如下命令构造一个映射:\n```scala\nscala> val scores = Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 89, Bob -> 98)\n```\n上面构造了一个不可变的Map[String, Int]，其值也不能被改变．如果想要一个可变映射，使用如下命令创建:\n```scala\nscala> val scores = scala.collection.mutable.Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.mutable.Map[String,Int] = Map(Bob -> 98, Alice -> 90, Kim -> 89)\n```\n如果只想创建一个空的映射:\n```scala\nscala> val scores = new scala.collection.mutable.HashMap[String, Int]\nscores: scala.collection.mutable.HashMap[String,Int] = Map()\n```\n从上面我们可以知道使用`->`操作符来创建映射的键值对元素\n```\n\"Alice\" -> 90\n```\n我们也可以使用下面的方式定义映射:\n```scala\nscala> val scores = Map((\"Alice\",90), (\"Kim\",89), (\"Bob\",98))\nscores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 89, Bob -> 98)\n```\n### 2. 获取映射中的值\n\n可以使用`()`来查找某个键对应的值:\n```scala\nscala> val bobscores = scores(\"Bob\")\nbobscores: Int = 98\n```\n如果映射中并不包含对应键的值，则会抛出异常，这与Java返回`null`不同:\n```scala\nscala> val tomScores = scores(\"Tom\")\njava.util.NoSuchElementException: key not found: Tom\n  at scala.collection.MapLike$class.default(MapLike.scala:228)\n  at scala.collection.AbstractMap.default(Map.scala:59)\n  at scala.collection.MapLike$class.apply(MapLike.scala:141)\n  at scala.collection.AbstractMap.apply(Map.scala:59)\n  ... 32 elided\n```\n所以在获取某个键对应的值之前，要先检查映射中是否存在指定的键:\n```scala\nscala> val tomScores = if(scores.contains(\"Tom\")) scores(\"Tom\") else 0\ntomScores: Int = 0\n```\n以下是一个快捷写法:\n```scala\nscala> val tomScores = scores.getOrElse(\"Tom\", 0)\ntomScores: Int = 0\n```\n### 3. 更新映射中的值\n\n在可变映射中，可以更新某个映射的值，也可以添加一个新的键值对:\n```scala\nscala> val scores = scala.collection.mutable.Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.mutable.Map[String,Int] = Map(Bob -> 98, Alice -> 90, Kim -> 89)\nscala> scores(\"Alice\")=100 // 更新键值对\nscala> scores(\"Tom\")=67 // 添加键值对\nscala> println(scores)\nMap(Bob -> 98, Tom -> 67, Alice -> 100, Kim -> 89)\n```\n还可以使用`+=`操作符来添加多个关系:\n```scala\nscala> scores += (\"Bob\" -> 78, \"Fred\" -> 89)\nres3: scores.type = Map(Bob -> 78, Fred -> 89, Tom -> 67, Alice -> 100, Kim -> 89)\n```\n还可以使用`-=`操作符移除某个键对应的值:\n```scala\nscala> scores -= \"Tom\"\nres4: scores.type = Map(Bob -> 78, Fred -> 89, Alice -> 100, Kim -> 89)\n```\n虽然不可以更新一个不可变的映射，但是我们利用一些操作产生一个新的映射，并可以对原映射中的键值对进行修改或者添加新的键值对:\n```scala\nscala> val scores = Map(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 89, Bob -> 98)\n\nscala> val newScores = scores + (\"Kim\" -> 78, \"Tom\" -> 54)\nnewScores: scala.collection.immutable.Map[String,Int] = Map(Alice -> 90, Kim -> 78, Bob -> 98, Tom -> 54)\n```\n上例中scores是不可变映射，我们在它基础上对\"Kim\"进行了修改，添加了\"Tom\"，产生了一个新的映射newScores\n\n### 4. 迭代映射\n\n可以使用如下命令迭代映射:\n```scala\nscala> for( key <- scores.keySet ) println(key + \"---\" + scores(key))\nAlice---90\nKim---89\nBob---98\n```\n或者\n```scala\nscala> for( value <- scores.values ) println(value)\n90\n89\n98\n```\n### 5. 排序映射\n\n在操作映射时，我们需要选定一个映射(哈希表还是平衡树)．默认情况下，scala给的是哈希表．有时候我们想对键进行一个排序，顺序访问键，这就需要一个树形映射:\n```scala\nscala> val scores = scala.collection.immutable.SortedMap(\"Alice\" -> 90, \"Kim\" -> 89, \"Bob\"-> 98)\nscores: scala.collection.immutable.SortedMap[String,Int] = Map(Alice -> 90, Bob -> 98, Kim -> 89)\n```\n\n### 6. 与Java互操作\n\n如果你有一个Java映射，想要转换为Scala映射，以便便捷的使用Scala映射的方法，只需要增加如下语句:\n```scala\nimport scala.collection.JavaConversions.mapAsScalaMap\n```\n然后指定Scala映射类型来触发转换:\n```scala\nscala> val scores : scala.collection.mutable.Map[String,Int] = new java.util.TreeMap[String, Int]\nscores: scala.collection.mutable.Map[String,Int] = Map()\n```\n还可以将java.util.Properties到Map[String, String]的转换:\n```scala\nscala> import scala.collection.JavaConversions.propertiesAsScalaMap\nimport scala.collection.JavaConversions.propertiesAsScalaMap\n\nscala> val props : scala.collection.Map[String, String] = System.getProperties()\nprops: scala.collection.Map[String,String] =\nMap(env.emacs -> \"\", java.runtime.name -> Java(TM) SE Runtime Environment, sun.boot.library.path -> /home/xiaosi/opt/jdk-1.8.0/jre/lib/amd64, java.vm.version -> 25.91-b14, java.vm.vendor -> Oracle Corporation, ...\n```\n相反，如果想要把Scal映射转换为Java映射，只需要提供相反的隐式转换即可:\n```scala\nscala> import scala.collection.JavaConversions.mapAsJavaMap\nimport scala.collection.JavaConversions.mapAsJavaMap\n\nscala> import java.awt.font.TextAttribute._ // 引入下面的映射会用到的键\nimport java.awt.font.TextAttribute._\n\nscala> val attrs = Map(FAMILY -> \"Serif\", SIZE -> 12) // Scala映射\nattrs: scala.collection.immutable.Map[java.awt.font.TextAttribute,Any] = Map(java.awt.font.TextAttribute(family) -> Serif, java.awt.font.TextAttribute(size) -> 12)\n\nscala> val font = new java.awt.Font(attrs) // Java映射\nfont: java.awt.Font = java.awt.Font[family=Serif,name=Serif,style=plain,size=12]\n```\n\n### 7. 元组Tuple\n\n元组是不同类型的值的聚合，元组的值通过将单个的值包含在圆括号中构成的：\n```scala\nscala> val bobScore = (1, 98.5, \"Bob\")\nbobScore: (Int, Double, String) = (1,98.5,Bob)\n```\n可以使用方法`_1`，`_2`，`_3`访问其组员:\n```scala\nscala> val bobScore = (1, 98.5, \"Bob\")\nbobScore: (Int, Double, String) = (1,98.5,Bob)\n\nscala> bobScore._1\nres10: Int = 1\n\nscala> bobScore._3\nres11: String = Bob\n```\n通常，使用模式匹配的方式来获取元组的组元:\n```scala\nscala> val (id, score, name) = bobScore // 将变量id赋值为1，变量score赋值为98.5，变量name赋值为Bob\n   val bobScore: (Int, Double, String)\n\nscala> val (id, score, name) = bobScore\nid: Int = 1\nscore: Double = 98.5\nname: String = Bob\n\nscala> println(\"name = \" + name + \", score = \" + score + \", name = \" + name)\nname = Bob, score = 98.5, name = Bob\n```\n\n来源于： 快学Scala\n","slug":"scala-notes-map-and-tuple","published":1,"updated":"2018-02-27T02:48:24.167Z","comments":1,"photos":[],"link":"","_id":"cje58tiv0005fordbd65kjp5z","content":"<h3 id=\"1-构造映射\"><a href=\"#1-构造映射\" class=\"headerlink\" title=\"1. 构造映射\"></a>1. 构造映射</h3><p>可以使用如下命令构造一个映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>)</span><br></pre></td></tr></table></figure></p>\n<p>上面构造了一个不可变的Map[String, Int]，其值也不能被改变．如果想要一个可变映射，使用如下命令创建:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = scala.collection.mutable.<span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>如果只想创建一个空的映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"keyword\">new</span> scala.collection.mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">Int</span>]</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>()</span><br></pre></td></tr></table></figure></p>\n<p>从上面我们可以知道使用<code>-&gt;</code>操作符来创建映射的键值对元素<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&quot;Alice&quot; -&gt; 90</span><br></pre></td></tr></table></figure></p>\n<p>我们也可以使用下面的方式定义映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"type\">Map</span>((<span class=\"string\">\"Alice\"</span>,<span class=\"number\">90</span>), (<span class=\"string\">\"Kim\"</span>,<span class=\"number\">89</span>), (<span class=\"string\">\"Bob\"</span>,<span class=\"number\">98</span>))</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-获取映射中的值\"><a href=\"#2-获取映射中的值\" class=\"headerlink\" title=\"2. 获取映射中的值\"></a>2. 获取映射中的值</h3><p>可以使用<code>()</code>来查找某个键对应的值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> bobscores = scores(<span class=\"string\">\"Bob\"</span>)</span><br><span class=\"line\">bobscores: <span class=\"type\">Int</span> = <span class=\"number\">98</span></span><br></pre></td></tr></table></figure></p>\n<p>如果映射中并不包含对应键的值，则会抛出异常，这与Java返回<code>null</code>不同:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> tomScores = scores(<span class=\"string\">\"Tom\"</span>)</span><br><span class=\"line\">java.util.<span class=\"type\">NoSuchElementException</span>: key not found: <span class=\"type\">Tom</span></span><br><span class=\"line\">  at scala.collection.<span class=\"type\">MapLike</span>$<span class=\"class\"><span class=\"keyword\">class</span>.<span class=\"title\">default</span>(<span class=\"params\"><span class=\"type\">MapLike</span>.scala:228</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"title\">at</span> <span class=\"title\">scala</span>.<span class=\"title\">collection</span>.<span class=\"title\">AbstractMap</span>.<span class=\"title\">default</span>(<span class=\"params\"><span class=\"type\">Map</span>.scala:59</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"title\">at</span> <span class=\"title\">scala</span>.<span class=\"title\">collection</span>.<span class=\"title\">MapLike$class</span>.<span class=\"title\">apply</span>(<span class=\"params\"><span class=\"type\">MapLike</span>.scala:141</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"title\">at</span> <span class=\"title\">scala</span>.<span class=\"title\">collection</span>.<span class=\"title\">AbstractMap</span>.<span class=\"title\">apply</span>(<span class=\"params\"><span class=\"type\">Map</span>.scala:59</span>)</span></span><br><span class=\"line\"><span class=\"class\">  ... 32 <span class=\"title\">elided</span></span></span><br></pre></td></tr></table></figure></p>\n<p>所以在获取某个键对应的值之前，要先检查映射中是否存在指定的键:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> tomScores = <span class=\"keyword\">if</span>(scores.contains(<span class=\"string\">\"Tom\"</span>)) scores(<span class=\"string\">\"Tom\"</span>) <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">tomScores: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p>以下是一个快捷写法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> tomScores = scores.getOrElse(<span class=\"string\">\"Tom\"</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">tomScores: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-更新映射中的值\"><a href=\"#3-更新映射中的值\" class=\"headerlink\" title=\"3. 更新映射中的值\"></a>3. 更新映射中的值</h3><p>在可变映射中，可以更新某个映射的值，也可以添加一个新的键值对:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = scala.collection.mutable.<span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br><span class=\"line\">scala&gt; scores(<span class=\"string\">\"Alice\"</span>)=<span class=\"number\">100</span> <span class=\"comment\">// 更新键值对</span></span><br><span class=\"line\">scala&gt; scores(<span class=\"string\">\"Tom\"</span>)=<span class=\"number\">67</span> <span class=\"comment\">// 添加键值对</span></span><br><span class=\"line\">scala&gt; println(scores)</span><br><span class=\"line\"><span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Tom</span> -&gt; <span class=\"number\">67</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">100</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>还可以使用<code>+=</code>操作符来添加多个关系:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; scores += (<span class=\"string\">\"Bob\"</span> -&gt; <span class=\"number\">78</span>, <span class=\"string\">\"Fred\"</span> -&gt; <span class=\"number\">89</span>)</span><br><span class=\"line\">res3: scores.<span class=\"keyword\">type</span> = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">78</span>, <span class=\"type\">Fred</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Tom</span> -&gt; <span class=\"number\">67</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">100</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>还可以使用<code>-=</code>操作符移除某个键对应的值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; scores -= <span class=\"string\">\"Tom\"</span></span><br><span class=\"line\">res4: scores.<span class=\"keyword\">type</span> = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">78</span>, <span class=\"type\">Fred</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">100</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>虽然不可以更新一个不可变的映射，但是我们利用一些操作产生一个新的映射，并可以对原映射中的键值对进行修改或者添加新的键值对:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> newScores = scores + (<span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">78</span>, <span class=\"string\">\"Tom\"</span> -&gt; <span class=\"number\">54</span>)</span><br><span class=\"line\">newScores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">78</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Tom</span> -&gt; <span class=\"number\">54</span>)</span><br></pre></td></tr></table></figure></p>\n<p>上例中scores是不可变映射，我们在它基础上对”Kim”进行了修改，添加了”Tom”，产生了一个新的映射newScores</p>\n<h3 id=\"4-迭代映射\"><a href=\"#4-迭代映射\" class=\"headerlink\" title=\"4. 迭代映射\"></a>4. 迭代映射</h3><p>可以使用如下命令迭代映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">for</span>( key &lt;- scores.keySet ) println(key + <span class=\"string\">\"---\"</span> + scores(key))</span><br><span class=\"line\"><span class=\"type\">Alice</span>--<span class=\"number\">-90</span></span><br><span class=\"line\"><span class=\"type\">Kim</span>--<span class=\"number\">-89</span></span><br><span class=\"line\"><span class=\"type\">Bob</span>--<span class=\"number\">-98</span></span><br></pre></td></tr></table></figure></p>\n<p>或者<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">for</span>( value &lt;- scores.values ) println(value)</span><br><span class=\"line\"><span class=\"number\">90</span></span><br><span class=\"line\"><span class=\"number\">89</span></span><br><span class=\"line\"><span class=\"number\">98</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-排序映射\"><a href=\"#5-排序映射\" class=\"headerlink\" title=\"5. 排序映射\"></a>5. 排序映射</h3><p>在操作映射时，我们需要选定一个映射(哈希表还是平衡树)．默认情况下，scala给的是哈希表．有时候我们想对键进行一个排序，顺序访问键，这就需要一个树形映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = scala.collection.immutable.<span class=\"type\">SortedMap</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">SortedMap</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-与Java互操作\"><a href=\"#6-与Java互操作\" class=\"headerlink\" title=\"6. 与Java互操作\"></a>6. 与Java互操作</h3><p>如果你有一个Java映射，想要转换为Scala映射，以便便捷的使用Scala映射的方法，只需要增加如下语句:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.mapAsScalaMap</span><br></pre></td></tr></table></figure></p>\n<p>然后指定Scala映射类型来触发转换:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores : scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"keyword\">new</span> java.util.<span class=\"type\">TreeMap</span>[<span class=\"type\">String</span>, <span class=\"type\">Int</span>]</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>()</span><br></pre></td></tr></table></figure></p>\n<p>还可以将java.util.Properties到Map[String, String]的转换:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.propertiesAsScalaMap</span><br><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.propertiesAsScalaMap</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> props : scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"type\">System</span>.getProperties()</span><br><span class=\"line\">props: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] =</span><br><span class=\"line\"><span class=\"type\">Map</span>(env.emacs -&gt; <span class=\"string\">\"\"</span>, java.runtime.name -&gt; <span class=\"type\">Java</span>(<span class=\"type\">TM</span>) <span class=\"type\">SE</span> <span class=\"type\">Runtime</span> <span class=\"type\">Environment</span>, sun.boot.library.path -&gt; /home/xiaosi/opt/jdk<span class=\"number\">-1.8</span><span class=\"number\">.0</span>/jre/lib/amd64, java.vm.version -&gt; <span class=\"number\">25.91</span>-b14, java.vm.vendor -&gt; <span class=\"type\">Oracle</span> <span class=\"type\">Corporation</span>, ...</span><br></pre></td></tr></table></figure></p>\n<p>相反，如果想要把Scal映射转换为Java映射，只需要提供相反的隐式转换即可:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.mapAsJavaMap</span><br><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.mapAsJavaMap</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">import</span> java.awt.font.<span class=\"type\">TextAttribute</span>._ <span class=\"comment\">// 引入下面的映射会用到的键</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.awt.font.<span class=\"type\">TextAttribute</span>._</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> attrs = <span class=\"type\">Map</span>(<span class=\"type\">FAMILY</span> -&gt; <span class=\"string\">\"Serif\"</span>, <span class=\"type\">SIZE</span> -&gt; <span class=\"number\">12</span>) <span class=\"comment\">// Scala映射</span></span><br><span class=\"line\">attrs: scala.collection.immutable.<span class=\"type\">Map</span>[java.awt.font.<span class=\"type\">TextAttribute</span>,<span class=\"type\">Any</span>] = <span class=\"type\">Map</span>(java.awt.font.<span class=\"type\">TextAttribute</span>(family) -&gt; <span class=\"type\">Serif</span>, java.awt.font.<span class=\"type\">TextAttribute</span>(size) -&gt; <span class=\"number\">12</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> font = <span class=\"keyword\">new</span> java.awt.<span class=\"type\">Font</span>(attrs) <span class=\"comment\">// Java映射</span></span><br><span class=\"line\">font: java.awt.<span class=\"type\">Font</span> = java.awt.<span class=\"type\">Font</span>[family=<span class=\"type\">Serif</span>,name=<span class=\"type\">Serif</span>,style=plain,size=<span class=\"number\">12</span>]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-元组Tuple\"><a href=\"#7-元组Tuple\" class=\"headerlink\" title=\"7. 元组Tuple\"></a>7. 元组Tuple</h3><p>元组是不同类型的值的聚合，元组的值通过将单个的值包含在圆括号中构成的：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> bobScore = (<span class=\"number\">1</span>, <span class=\"number\">98.5</span>, <span class=\"string\">\"Bob\"</span>)</span><br><span class=\"line\">bobScore: (<span class=\"type\">Int</span>, <span class=\"type\">Double</span>, <span class=\"type\">String</span>) = (<span class=\"number\">1</span>,<span class=\"number\">98.5</span>,<span class=\"type\">Bob</span>)</span><br></pre></td></tr></table></figure></p>\n<p>可以使用方法<code>_1</code>，<code>_2</code>，<code>_3</code>访问其组员:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> bobScore = (<span class=\"number\">1</span>, <span class=\"number\">98.5</span>, <span class=\"string\">\"Bob\"</span>)</span><br><span class=\"line\">bobScore: (<span class=\"type\">Int</span>, <span class=\"type\">Double</span>, <span class=\"type\">String</span>) = (<span class=\"number\">1</span>,<span class=\"number\">98.5</span>,<span class=\"type\">Bob</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; bobScore._1</span><br><span class=\"line\">res10: <span class=\"type\">Int</span> = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; bobScore._3</span><br><span class=\"line\">res11: <span class=\"type\">String</span> = <span class=\"type\">Bob</span></span><br></pre></td></tr></table></figure></p>\n<p>通常，使用模式匹配的方式来获取元组的组元:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> (id, score, name) = bobScore <span class=\"comment\">// 将变量id赋值为1，变量score赋值为98.5，变量name赋值为Bob</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> bobScore: (<span class=\"type\">Int</span>, <span class=\"type\">Double</span>, <span class=\"type\">String</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> (id, score, name) = bobScore</span><br><span class=\"line\">id: <span class=\"type\">Int</span> = <span class=\"number\">1</span></span><br><span class=\"line\">score: <span class=\"type\">Double</span> = <span class=\"number\">98.5</span></span><br><span class=\"line\">name: <span class=\"type\">String</span> = <span class=\"type\">Bob</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; println(<span class=\"string\">\"name = \"</span> + name + <span class=\"string\">\", score = \"</span> + score + <span class=\"string\">\", name = \"</span> + name)</span><br><span class=\"line\">name = <span class=\"type\">Bob</span>, score = <span class=\"number\">98.5</span>, name = <span class=\"type\">Bob</span></span><br></pre></td></tr></table></figure></p>\n<p>来源于： 快学Scala</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-构造映射\"><a href=\"#1-构造映射\" class=\"headerlink\" title=\"1. 构造映射\"></a>1. 构造映射</h3><p>可以使用如下命令构造一个映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>)</span><br></pre></td></tr></table></figure></p>\n<p>上面构造了一个不可变的Map[String, Int]，其值也不能被改变．如果想要一个可变映射，使用如下命令创建:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = scala.collection.mutable.<span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>如果只想创建一个空的映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"keyword\">new</span> scala.collection.mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">Int</span>]</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>()</span><br></pre></td></tr></table></figure></p>\n<p>从上面我们可以知道使用<code>-&gt;</code>操作符来创建映射的键值对元素<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">&quot;Alice&quot; -&gt; 90</span><br></pre></td></tr></table></figure></p>\n<p>我们也可以使用下面的方式定义映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"type\">Map</span>((<span class=\"string\">\"Alice\"</span>,<span class=\"number\">90</span>), (<span class=\"string\">\"Kim\"</span>,<span class=\"number\">89</span>), (<span class=\"string\">\"Bob\"</span>,<span class=\"number\">98</span>))</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-获取映射中的值\"><a href=\"#2-获取映射中的值\" class=\"headerlink\" title=\"2. 获取映射中的值\"></a>2. 获取映射中的值</h3><p>可以使用<code>()</code>来查找某个键对应的值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> bobscores = scores(<span class=\"string\">\"Bob\"</span>)</span><br><span class=\"line\">bobscores: <span class=\"type\">Int</span> = <span class=\"number\">98</span></span><br></pre></td></tr></table></figure></p>\n<p>如果映射中并不包含对应键的值，则会抛出异常，这与Java返回<code>null</code>不同:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> tomScores = scores(<span class=\"string\">\"Tom\"</span>)</span><br><span class=\"line\">java.util.<span class=\"type\">NoSuchElementException</span>: key not found: <span class=\"type\">Tom</span></span><br><span class=\"line\">  at scala.collection.<span class=\"type\">MapLike</span>$<span class=\"class\"><span class=\"keyword\">class</span>.<span class=\"title\">default</span>(<span class=\"params\"><span class=\"type\">MapLike</span>.scala:228</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"title\">at</span> <span class=\"title\">scala</span>.<span class=\"title\">collection</span>.<span class=\"title\">AbstractMap</span>.<span class=\"title\">default</span>(<span class=\"params\"><span class=\"type\">Map</span>.scala:59</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"title\">at</span> <span class=\"title\">scala</span>.<span class=\"title\">collection</span>.<span class=\"title\">MapLike$class</span>.<span class=\"title\">apply</span>(<span class=\"params\"><span class=\"type\">MapLike</span>.scala:141</span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"title\">at</span> <span class=\"title\">scala</span>.<span class=\"title\">collection</span>.<span class=\"title\">AbstractMap</span>.<span class=\"title\">apply</span>(<span class=\"params\"><span class=\"type\">Map</span>.scala:59</span>)</span></span><br><span class=\"line\"><span class=\"class\">  ... 32 <span class=\"title\">elided</span></span></span><br></pre></td></tr></table></figure></p>\n<p>所以在获取某个键对应的值之前，要先检查映射中是否存在指定的键:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> tomScores = <span class=\"keyword\">if</span>(scores.contains(<span class=\"string\">\"Tom\"</span>)) scores(<span class=\"string\">\"Tom\"</span>) <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">tomScores: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p>以下是一个快捷写法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> tomScores = scores.getOrElse(<span class=\"string\">\"Tom\"</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">tomScores: <span class=\"type\">Int</span> = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-更新映射中的值\"><a href=\"#3-更新映射中的值\" class=\"headerlink\" title=\"3. 更新映射中的值\"></a>3. 更新映射中的值</h3><p>在可变映射中，可以更新某个映射的值，也可以添加一个新的键值对:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = scala.collection.mutable.<span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br><span class=\"line\">scala&gt; scores(<span class=\"string\">\"Alice\"</span>)=<span class=\"number\">100</span> <span class=\"comment\">// 更新键值对</span></span><br><span class=\"line\">scala&gt; scores(<span class=\"string\">\"Tom\"</span>)=<span class=\"number\">67</span> <span class=\"comment\">// 添加键值对</span></span><br><span class=\"line\">scala&gt; println(scores)</span><br><span class=\"line\"><span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Tom</span> -&gt; <span class=\"number\">67</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">100</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>还可以使用<code>+=</code>操作符来添加多个关系:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; scores += (<span class=\"string\">\"Bob\"</span> -&gt; <span class=\"number\">78</span>, <span class=\"string\">\"Fred\"</span> -&gt; <span class=\"number\">89</span>)</span><br><span class=\"line\">res3: scores.<span class=\"keyword\">type</span> = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">78</span>, <span class=\"type\">Fred</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Tom</span> -&gt; <span class=\"number\">67</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">100</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>还可以使用<code>-=</code>操作符移除某个键对应的值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; scores -= <span class=\"string\">\"Tom\"</span></span><br><span class=\"line\">res4: scores.<span class=\"keyword\">type</span> = <span class=\"type\">Map</span>(<span class=\"type\">Bob</span> -&gt; <span class=\"number\">78</span>, <span class=\"type\">Fred</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Alice</span> -&gt; <span class=\"number\">100</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<p>虽然不可以更新一个不可变的映射，但是我们利用一些操作产生一个新的映射，并可以对原映射中的键值对进行修改或者添加新的键值对:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = <span class=\"type\">Map</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> newScores = scores + (<span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">78</span>, <span class=\"string\">\"Tom\"</span> -&gt; <span class=\"number\">54</span>)</span><br><span class=\"line\">newScores: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">78</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Tom</span> -&gt; <span class=\"number\">54</span>)</span><br></pre></td></tr></table></figure></p>\n<p>上例中scores是不可变映射，我们在它基础上对”Kim”进行了修改，添加了”Tom”，产生了一个新的映射newScores</p>\n<h3 id=\"4-迭代映射\"><a href=\"#4-迭代映射\" class=\"headerlink\" title=\"4. 迭代映射\"></a>4. 迭代映射</h3><p>可以使用如下命令迭代映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">for</span>( key &lt;- scores.keySet ) println(key + <span class=\"string\">\"---\"</span> + scores(key))</span><br><span class=\"line\"><span class=\"type\">Alice</span>--<span class=\"number\">-90</span></span><br><span class=\"line\"><span class=\"type\">Kim</span>--<span class=\"number\">-89</span></span><br><span class=\"line\"><span class=\"type\">Bob</span>--<span class=\"number\">-98</span></span><br></pre></td></tr></table></figure></p>\n<p>或者<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">for</span>( value &lt;- scores.values ) println(value)</span><br><span class=\"line\"><span class=\"number\">90</span></span><br><span class=\"line\"><span class=\"number\">89</span></span><br><span class=\"line\"><span class=\"number\">98</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-排序映射\"><a href=\"#5-排序映射\" class=\"headerlink\" title=\"5. 排序映射\"></a>5. 排序映射</h3><p>在操作映射时，我们需要选定一个映射(哈希表还是平衡树)．默认情况下，scala给的是哈希表．有时候我们想对键进行一个排序，顺序访问键，这就需要一个树形映射:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores = scala.collection.immutable.<span class=\"type\">SortedMap</span>(<span class=\"string\">\"Alice\"</span> -&gt; <span class=\"number\">90</span>, <span class=\"string\">\"Kim\"</span> -&gt; <span class=\"number\">89</span>, <span class=\"string\">\"Bob\"</span>-&gt; <span class=\"number\">98</span>)</span><br><span class=\"line\">scores: scala.collection.immutable.<span class=\"type\">SortedMap</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"type\">Alice</span> -&gt; <span class=\"number\">90</span>, <span class=\"type\">Bob</span> -&gt; <span class=\"number\">98</span>, <span class=\"type\">Kim</span> -&gt; <span class=\"number\">89</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"6-与Java互操作\"><a href=\"#6-与Java互操作\" class=\"headerlink\" title=\"6. 与Java互操作\"></a>6. 与Java互操作</h3><p>如果你有一个Java映射，想要转换为Scala映射，以便便捷的使用Scala映射的方法，只需要增加如下语句:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.mapAsScalaMap</span><br></pre></td></tr></table></figure></p>\n<p>然后指定Scala映射类型来触发转换:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> scores : scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"keyword\">new</span> java.util.<span class=\"type\">TreeMap</span>[<span class=\"type\">String</span>, <span class=\"type\">Int</span>]</span><br><span class=\"line\">scores: scala.collection.mutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>()</span><br></pre></td></tr></table></figure></p>\n<p>还可以将java.util.Properties到Map[String, String]的转换:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.propertiesAsScalaMap</span><br><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.propertiesAsScalaMap</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> props : scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"type\">System</span>.getProperties()</span><br><span class=\"line\">props: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] =</span><br><span class=\"line\"><span class=\"type\">Map</span>(env.emacs -&gt; <span class=\"string\">\"\"</span>, java.runtime.name -&gt; <span class=\"type\">Java</span>(<span class=\"type\">TM</span>) <span class=\"type\">SE</span> <span class=\"type\">Runtime</span> <span class=\"type\">Environment</span>, sun.boot.library.path -&gt; /home/xiaosi/opt/jdk<span class=\"number\">-1.8</span><span class=\"number\">.0</span>/jre/lib/amd64, java.vm.version -&gt; <span class=\"number\">25.91</span>-b14, java.vm.vendor -&gt; <span class=\"type\">Oracle</span> <span class=\"type\">Corporation</span>, ...</span><br></pre></td></tr></table></figure></p>\n<p>相反，如果想要把Scal映射转换为Java映射，只需要提供相反的隐式转换即可:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.mapAsJavaMap</span><br><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.<span class=\"type\">JavaConversions</span>.mapAsJavaMap</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">import</span> java.awt.font.<span class=\"type\">TextAttribute</span>._ <span class=\"comment\">// 引入下面的映射会用到的键</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> java.awt.font.<span class=\"type\">TextAttribute</span>._</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> attrs = <span class=\"type\">Map</span>(<span class=\"type\">FAMILY</span> -&gt; <span class=\"string\">\"Serif\"</span>, <span class=\"type\">SIZE</span> -&gt; <span class=\"number\">12</span>) <span class=\"comment\">// Scala映射</span></span><br><span class=\"line\">attrs: scala.collection.immutable.<span class=\"type\">Map</span>[java.awt.font.<span class=\"type\">TextAttribute</span>,<span class=\"type\">Any</span>] = <span class=\"type\">Map</span>(java.awt.font.<span class=\"type\">TextAttribute</span>(family) -&gt; <span class=\"type\">Serif</span>, java.awt.font.<span class=\"type\">TextAttribute</span>(size) -&gt; <span class=\"number\">12</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> font = <span class=\"keyword\">new</span> java.awt.<span class=\"type\">Font</span>(attrs) <span class=\"comment\">// Java映射</span></span><br><span class=\"line\">font: java.awt.<span class=\"type\">Font</span> = java.awt.<span class=\"type\">Font</span>[family=<span class=\"type\">Serif</span>,name=<span class=\"type\">Serif</span>,style=plain,size=<span class=\"number\">12</span>]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-元组Tuple\"><a href=\"#7-元组Tuple\" class=\"headerlink\" title=\"7. 元组Tuple\"></a>7. 元组Tuple</h3><p>元组是不同类型的值的聚合，元组的值通过将单个的值包含在圆括号中构成的：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> bobScore = (<span class=\"number\">1</span>, <span class=\"number\">98.5</span>, <span class=\"string\">\"Bob\"</span>)</span><br><span class=\"line\">bobScore: (<span class=\"type\">Int</span>, <span class=\"type\">Double</span>, <span class=\"type\">String</span>) = (<span class=\"number\">1</span>,<span class=\"number\">98.5</span>,<span class=\"type\">Bob</span>)</span><br></pre></td></tr></table></figure></p>\n<p>可以使用方法<code>_1</code>，<code>_2</code>，<code>_3</code>访问其组员:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> bobScore = (<span class=\"number\">1</span>, <span class=\"number\">98.5</span>, <span class=\"string\">\"Bob\"</span>)</span><br><span class=\"line\">bobScore: (<span class=\"type\">Int</span>, <span class=\"type\">Double</span>, <span class=\"type\">String</span>) = (<span class=\"number\">1</span>,<span class=\"number\">98.5</span>,<span class=\"type\">Bob</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; bobScore._1</span><br><span class=\"line\">res10: <span class=\"type\">Int</span> = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; bobScore._3</span><br><span class=\"line\">res11: <span class=\"type\">String</span> = <span class=\"type\">Bob</span></span><br></pre></td></tr></table></figure></p>\n<p>通常，使用模式匹配的方式来获取元组的组元:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> (id, score, name) = bobScore <span class=\"comment\">// 将变量id赋值为1，变量score赋值为98.5，变量name赋值为Bob</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> bobScore: (<span class=\"type\">Int</span>, <span class=\"type\">Double</span>, <span class=\"type\">String</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; <span class=\"keyword\">val</span> (id, score, name) = bobScore</span><br><span class=\"line\">id: <span class=\"type\">Int</span> = <span class=\"number\">1</span></span><br><span class=\"line\">score: <span class=\"type\">Double</span> = <span class=\"number\">98.5</span></span><br><span class=\"line\">name: <span class=\"type\">String</span> = <span class=\"type\">Bob</span></span><br><span class=\"line\"></span><br><span class=\"line\">scala&gt; println(<span class=\"string\">\"name = \"</span> + name + <span class=\"string\">\", score = \"</span> + score + <span class=\"string\">\", name = \"</span> + name)</span><br><span class=\"line\">name = <span class=\"type\">Bob</span>, score = <span class=\"number\">98.5</span>, name = <span class=\"type\">Bob</span></span><br></pre></td></tr></table></figure></p>\n<p>来源于： 快学Scala</p>\n"},{"layout":"post","author":"李雪蕤","title":"Spark 性能优化之基础篇","date":"2018-02-01T01:32:17.000Z","_content":"\n### 1. 前言\n\n在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。\n\n然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。\n\nSpark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。\n\n笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。\n\n本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。\n\n### 2. 开发调优\n\n#### 2.1 调优概述\n\nSpark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。\n\n#### 2.2 原则一：避免创建重复的RDD\n\n通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。\n\n我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。\n\n一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。\n\n一个简单的例子:\n```\n// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。\n\n// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。\n// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。\n// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")\nrdd1.map(...)\nval rdd2 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")\nrdd2.reduce(...)\n\n// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。\n// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。\n// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。\n// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")\nrdd1.map(...)\nrdd1.reduce(...)\n```\n\n#### 2.3 原则二：尽可能复用同一个RDD\n\n除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。\n\n一个简单的例子:\n```\n// 错误的做法。\n\n// 有一个<Long, String>格式的RDD，即rdd1。\n// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。\nJavaPairRDD<Long, String> rdd1 = ...\nJavaRDD<String> rdd2 = rdd1.map(...)\n\n// 分别对rdd1和rdd2执行了不同的算子操作。\nrdd1.reduceByKey(...)\nrdd2.map(...)\n\n// 正确的做法。\n\n// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。\n// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。\n\n// 其实在这种情况下完全可以复用同一个RDD。\n// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。\n// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。\nJavaPairRDD<Long, String> rdd1 = ...\nrdd1.reduceByKey(...)\nrdd1.map(tuple._2...)\n\n// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。\n// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。\n// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。\n```\n\n#### 2.4 原则三：对多次使用的RDD进行持久化\n\n当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。\n\nSpark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。\n\n因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。\n\n对多次使用的RDD进行持久化的代码示例:\n```\n// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。\n\n// 正确的做法。\n// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。\n// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。\n// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").cache()\nrdd1.map(...)\nrdd1.reduce(...)\n\n// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。\n// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。\n// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。\n// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").persist(StorageLevel.MEMORY_AND_DISK_SER)\nrdd1.map(...)\nrdd1.reduce(...)\n```\n对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。\n\nSpark的持久化级别\n\n持久化级别|含义解释\n---|---\nMEMORY_ONLY|使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。\nMEMORY_AND_DISK|使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。\nMEMORY_ONLY_SER|基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。\nMEMORY_AND_DISK_SER|基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。\nDISK_ONLY|使用未序列化的Java对象格式，将数据全部写入磁盘文件中。\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.|对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。\n\n如何选择一种最合适的持久化策略\n\n- 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。\n- 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。\n- 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。\n- 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。\n\n#### 2.5 原则四：尽量避免使用shuffle类算子\n\n如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。\n\nshuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。\n\n因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。\n\nBroadcast与map进行join代码示例:\n```\n// 传统的join操作会导致shuffle操作。\n// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。\nval rdd3 = rdd1.join(rdd2)\n\n// Broadcast+map的join操作，不会导致shuffle操作。\n// 使用Broadcast将一个数据量较小的RDD作为广播变量。\nval rdd2Data = rdd2.collect()\nval rdd2DataBroadcast = sc.broadcast(rdd2Data)\n\n// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。\n// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。\n// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。\nval rdd3 = rdd1.map(rdd2DataBroadcast...)\n\n// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。\n// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。\n```\n#### 2.6 原则五：使用map-side预聚合的shuffle操作\n\n如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。\n\n所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。\n\n比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-1.png?raw=true)\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-2.png?raw=true)\n\n#### 2.7 原则六：使用高性能的算子\n\n除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。\n\n(1) 使用reduceByKey/aggregateByKey替代groupByKey\n\n详情见“原则五：使用map-side预聚合的shuffle操作”。\n\n(2)使用mapPartitions替代普通map\n\nmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！\n\n(3) 使用foreachPartitions替代foreach\n\n原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。\n\n(4) 使用filter之后进行coalesce操作\n\n通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。\n\n(5) 使用repartitionAndSortWithinPartitions替代repartition与sort类操作\n\nrepartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。\n\n#### 2.8 原则七：广播大变量\n\n有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。\n\n在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。\n\n因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。\n\n广播大变量的代码示例:\n```\n// 以下代码在算子函数中，使用了外部的变量。\n// 此时没有做任何特殊操作，每个task都会有一份list1的副本。\nval list1 = ...\nrdd1.map(list1...)\n\n// 以下代码将list1封装成了Broadcast类型的广播变量。\n// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。\n// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。\n// 每个Executor内存中，就只会驻留一份广播变量副本。\nval list1 = ...\nval list1Broadcast = sc.broadcast(list1)\nrdd1.map(list1Broadcast...)\n```\n\n#### 2.9 原则八：使用Kryo优化序列化性能\n\n在Spark中，主要有三个地方涉及到了序列化：\n- 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。\n- 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。\n- 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。\n\n对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。\n\n以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：\n```\n// 创建SparkConf对象。\nval conf = new SparkConf().setMaster(...).setAppName(...)\n// 设置序列化器为KryoSerializer。\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n// 注册要序列化的自定义类型。\nconf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n```\n\n#### 2.10 原则九：优化数据结构\n\nJava中，有三种类型比较耗费内存：\n- 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。\n- 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。\n- 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。\n\n因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。\n\n但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。\n\n### 3. 资源调优\n\n#### 3.1 调优概述\n\n在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。\n\n#### 3.2 Spark作业基本运行原理\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-3.png?raw=true)\n\n详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。\n\n在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。\n\nSpark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。\n\n当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。\n\n因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。\n\ntask的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。\n\n以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。\n\n#### 3.3 资源参数调优\n\n了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。\n\n(1) num-executors\n- 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。\n- 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。\n\n(2) executor-memory\n- 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。\n- 参数调优建议：每个Executor进程的内存设置4G-8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3-1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。\n\n(3) executor-cores\n- 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。\n- 参数调优建议：Executor的CPU core数量设置为2-4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3-1/2左右比较合适，也是避免影响其他同学的作业运行。\n\n(4) driver-memory\n- 参数说明：该参数用于设置Driver进程的内存。\n- 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。\n\n(5) spark.default.parallelism\n- 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。\n- 参数调优建议：Spark作业的默认task数量为500-1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2-3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。\n\n(6) spark.storage.memoryFraction\n- 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。\n- 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。\n\n(7) spark.shuffle.memoryFraction\n- 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。\n- 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。\n\n资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。\n\n#### 3.4 资源参数参考示例\n\n以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：\n```\n./bin/spark-submit \\\n  --master yarn-cluster \\\n  --num-executors 100 \\\n  --executor-memory 6G \\\n  --executor-cores 4 \\\n  --driver-memory 1G \\\n  --conf spark.default.parallelism=1000 \\\n  --conf spark.storage.memoryFraction=0.5 \\\n  --conf spark.shuffle.memoryFraction=0.3 \\\n```\n\n### 4. 写在最后的话\n\n根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。\n\n\n原文: https://tech.meituan.com/spark-tuning-basic.html\n","source":"_posts/Spark/[Spark]Spark 性能优化之基础篇.md","raw":"---\nlayout: post\nauthor: 李雪蕤\ntitle: Spark 性能优化之基础篇\ndate: 2018-02-01 09:32:17\ntags:\n  - Spark\n  - Spark 优化\n\ncategories: Spark\npermalink: spark-performance-optimization-basic\n---\n\n### 1. 前言\n\n在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。\n\n然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。\n\nSpark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。\n\n笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。\n\n本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。\n\n### 2. 开发调优\n\n#### 2.1 调优概述\n\nSpark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。\n\n#### 2.2 原则一：避免创建重复的RDD\n\n通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。\n\n我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。\n\n一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。\n\n一个简单的例子:\n```\n// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。\n\n// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。\n// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。\n// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")\nrdd1.map(...)\nval rdd2 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")\nrdd2.reduce(...)\n\n// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。\n// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。\n// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。\n// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")\nrdd1.map(...)\nrdd1.reduce(...)\n```\n\n#### 2.3 原则二：尽可能复用同一个RDD\n\n除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。\n\n一个简单的例子:\n```\n// 错误的做法。\n\n// 有一个<Long, String>格式的RDD，即rdd1。\n// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。\nJavaPairRDD<Long, String> rdd1 = ...\nJavaRDD<String> rdd2 = rdd1.map(...)\n\n// 分别对rdd1和rdd2执行了不同的算子操作。\nrdd1.reduceByKey(...)\nrdd2.map(...)\n\n// 正确的做法。\n\n// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。\n// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。\n\n// 其实在这种情况下完全可以复用同一个RDD。\n// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。\n// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。\nJavaPairRDD<Long, String> rdd1 = ...\nrdd1.reduceByKey(...)\nrdd1.map(tuple._2...)\n\n// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。\n// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。\n// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。\n```\n\n#### 2.4 原则三：对多次使用的RDD进行持久化\n\n当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。\n\nSpark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。\n\n因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。\n\n对多次使用的RDD进行持久化的代码示例:\n```\n// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。\n\n// 正确的做法。\n// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。\n// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。\n// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").cache()\nrdd1.map(...)\nrdd1.reduce(...)\n\n// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。\n// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。\n// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。\n// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。\nval rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").persist(StorageLevel.MEMORY_AND_DISK_SER)\nrdd1.map(...)\nrdd1.reduce(...)\n```\n对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。\n\nSpark的持久化级别\n\n持久化级别|含义解释\n---|---\nMEMORY_ONLY|使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。\nMEMORY_AND_DISK|使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。\nMEMORY_ONLY_SER|基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。\nMEMORY_AND_DISK_SER|基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。\nDISK_ONLY|使用未序列化的Java对象格式，将数据全部写入磁盘文件中。\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.|对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。\n\n如何选择一种最合适的持久化策略\n\n- 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。\n- 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。\n- 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。\n- 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。\n\n#### 2.5 原则四：尽量避免使用shuffle类算子\n\n如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。\n\nshuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。\n\n因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。\n\nBroadcast与map进行join代码示例:\n```\n// 传统的join操作会导致shuffle操作。\n// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。\nval rdd3 = rdd1.join(rdd2)\n\n// Broadcast+map的join操作，不会导致shuffle操作。\n// 使用Broadcast将一个数据量较小的RDD作为广播变量。\nval rdd2Data = rdd2.collect()\nval rdd2DataBroadcast = sc.broadcast(rdd2Data)\n\n// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。\n// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。\n// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。\nval rdd3 = rdd1.map(rdd2DataBroadcast...)\n\n// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。\n// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。\n```\n#### 2.6 原则五：使用map-side预聚合的shuffle操作\n\n如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。\n\n所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。\n\n比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-1.png?raw=true)\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-2.png?raw=true)\n\n#### 2.7 原则六：使用高性能的算子\n\n除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。\n\n(1) 使用reduceByKey/aggregateByKey替代groupByKey\n\n详情见“原则五：使用map-side预聚合的shuffle操作”。\n\n(2)使用mapPartitions替代普通map\n\nmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！\n\n(3) 使用foreachPartitions替代foreach\n\n原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。\n\n(4) 使用filter之后进行coalesce操作\n\n通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。\n\n(5) 使用repartitionAndSortWithinPartitions替代repartition与sort类操作\n\nrepartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。\n\n#### 2.8 原则七：广播大变量\n\n有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。\n\n在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。\n\n因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。\n\n广播大变量的代码示例:\n```\n// 以下代码在算子函数中，使用了外部的变量。\n// 此时没有做任何特殊操作，每个task都会有一份list1的副本。\nval list1 = ...\nrdd1.map(list1...)\n\n// 以下代码将list1封装成了Broadcast类型的广播变量。\n// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。\n// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。\n// 每个Executor内存中，就只会驻留一份广播变量副本。\nval list1 = ...\nval list1Broadcast = sc.broadcast(list1)\nrdd1.map(list1Broadcast...)\n```\n\n#### 2.9 原则八：使用Kryo优化序列化性能\n\n在Spark中，主要有三个地方涉及到了序列化：\n- 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。\n- 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。\n- 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。\n\n对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。\n\n以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：\n```\n// 创建SparkConf对象。\nval conf = new SparkConf().setMaster(...).setAppName(...)\n// 设置序列化器为KryoSerializer。\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n// 注册要序列化的自定义类型。\nconf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n```\n\n#### 2.10 原则九：优化数据结构\n\nJava中，有三种类型比较耗费内存：\n- 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。\n- 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。\n- 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。\n\n因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。\n\n但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。\n\n### 3. 资源调优\n\n#### 3.1 调优概述\n\n在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。\n\n#### 3.2 Spark作业基本运行原理\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-3.png?raw=true)\n\n详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。\n\n在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。\n\nSpark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。\n\n当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。\n\n因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。\n\ntask的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。\n\n以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。\n\n#### 3.3 资源参数调优\n\n了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。\n\n(1) num-executors\n- 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。\n- 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。\n\n(2) executor-memory\n- 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。\n- 参数调优建议：每个Executor进程的内存设置4G-8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3-1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。\n\n(3) executor-cores\n- 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。\n- 参数调优建议：Executor的CPU core数量设置为2-4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3-1/2左右比较合适，也是避免影响其他同学的作业运行。\n\n(4) driver-memory\n- 参数说明：该参数用于设置Driver进程的内存。\n- 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。\n\n(5) spark.default.parallelism\n- 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。\n- 参数调优建议：Spark作业的默认task数量为500-1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2-3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。\n\n(6) spark.storage.memoryFraction\n- 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。\n- 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。\n\n(7) spark.shuffle.memoryFraction\n- 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。\n- 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。\n\n资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。\n\n#### 3.4 资源参数参考示例\n\n以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：\n```\n./bin/spark-submit \\\n  --master yarn-cluster \\\n  --num-executors 100 \\\n  --executor-memory 6G \\\n  --executor-cores 4 \\\n  --driver-memory 1G \\\n  --conf spark.default.parallelism=1000 \\\n  --conf spark.storage.memoryFraction=0.5 \\\n  --conf spark.shuffle.memoryFraction=0.3 \\\n```\n\n### 4. 写在最后的话\n\n根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。\n\n\n原文: https://tech.meituan.com/spark-tuning-basic.html\n","slug":"spark-performance-optimization-basic","published":1,"updated":"2018-02-02T01:50:25.412Z","comments":1,"photos":[],"link":"","_id":"cje58tiv3005iordbprnpkcy8","content":"<h3 id=\"1-前言\"><a href=\"#1-前言\" class=\"headerlink\" title=\"1. 前言\"></a>1. 前言</h3><p>在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p>\n<p>然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p>\n<p>Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p>\n<p>笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p>\n<p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p>\n<h3 id=\"2-开发调优\"><a href=\"#2-开发调优\" class=\"headerlink\" title=\"2. 开发调优\"></a>2. 开发调优</h3><h4 id=\"2-1-调优概述\"><a href=\"#2-1-调优概述\" class=\"headerlink\" title=\"2.1 调优概述\"></a>2.1 调优概述</h4><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p>\n<h4 id=\"2-2-原则一：避免创建重复的RDD\"><a href=\"#2-2-原则一：避免创建重复的RDD\" class=\"headerlink\" title=\"2.2 原则一：避免创建重复的RDD\"></a>2.2 原则一：避免创建重复的RDD</h4><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p>\n<p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p>\n<p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p>\n<p>一个简单的例子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span><br><span class=\"line\"></span><br><span class=\"line\">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span><br><span class=\"line\">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span><br><span class=\"line\">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">val rdd2 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class=\"line\">rdd2.reduce(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span><br><span class=\"line\">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span><br><span class=\"line\">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span><br><span class=\"line\">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">rdd1.reduce(...)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-原则二：尽可能复用同一个RDD\"><a href=\"#2-3-原则二：尽可能复用同一个RDD\" class=\"headerlink\" title=\"2.3 原则二：尽可能复用同一个RDD\"></a>2.3 原则二：尽可能复用同一个RDD</h4><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p>\n<p>一个简单的例子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 错误的做法。</span><br><span class=\"line\"></span><br><span class=\"line\">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span><br><span class=\"line\">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span><br><span class=\"line\">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class=\"line\">JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 分别对rdd1和rdd2执行了不同的算子操作。</span><br><span class=\"line\">rdd1.reduceByKey(...)</span><br><span class=\"line\">rdd2.map(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 正确的做法。</span><br><span class=\"line\"></span><br><span class=\"line\">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span><br><span class=\"line\">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span><br><span class=\"line\"></span><br><span class=\"line\">// 其实在这种情况下完全可以复用同一个RDD。</span><br><span class=\"line\">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span><br><span class=\"line\">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span><br><span class=\"line\">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class=\"line\">rdd1.reduceByKey(...)</span><br><span class=\"line\">rdd1.map(tuple._2...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span><br><span class=\"line\">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span><br><span class=\"line\">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-4-原则三：对多次使用的RDD进行持久化\"><a href=\"#2-4-原则三：对多次使用的RDD进行持久化\" class=\"headerlink\" title=\"2.4 原则三：对多次使用的RDD进行持久化\"></a>2.4 原则三：对多次使用的RDD进行持久化</h4><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p>\n<p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p>\n<p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p>\n<p>对多次使用的RDD进行持久化的代码示例:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span><br><span class=\"line\"></span><br><span class=\"line\">// 正确的做法。</span><br><span class=\"line\">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span><br><span class=\"line\">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span><br><span class=\"line\">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;).cache()</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">rdd1.reduce(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span><br><span class=\"line\">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span><br><span class=\"line\">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span><br><span class=\"line\">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;).persist(StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">rdd1.reduce(...)</span><br></pre></td></tr></table></figure></p>\n<p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p>\n<p>Spark的持久化级别</p>\n<table>\n<thead>\n<tr>\n<th>持久化级别</th>\n<th>含义解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MEMORY_ONLY</td>\n<td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td>\n</tr>\n<tr>\n<td>MEMORY_AND_DISK</td>\n<td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td>\n</tr>\n<tr>\n<td>MEMORY_ONLY_SER</td>\n<td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>\n</tr>\n<tr>\n<td>MEMORY_AND_DISK_SER</td>\n<td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>\n</tr>\n<tr>\n<td>DISK_ONLY</td>\n<td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td>\n</tr>\n<tr>\n<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td>\n<td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td>\n</tr>\n</tbody>\n</table>\n<p>如何选择一种最合适的持久化策略</p>\n<ul>\n<li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li>\n<li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li>\n<li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li>\n<li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li>\n</ul>\n<h4 id=\"2-5-原则四：尽量避免使用shuffle类算子\"><a href=\"#2-5-原则四：尽量避免使用shuffle类算子\" class=\"headerlink\" title=\"2.5 原则四：尽量避免使用shuffle类算子\"></a>2.5 原则四：尽量避免使用shuffle类算子</h4><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p>\n<p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p>\n<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>\n<p>Broadcast与map进行join代码示例:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 传统的join操作会导致shuffle操作。</span><br><span class=\"line\">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span><br><span class=\"line\">val rdd3 = rdd1.join(rdd2)</span><br><span class=\"line\"></span><br><span class=\"line\">// Broadcast+map的join操作，不会导致shuffle操作。</span><br><span class=\"line\">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span><br><span class=\"line\">val rdd2Data = rdd2.collect()</span><br><span class=\"line\">val rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class=\"line\"></span><br><span class=\"line\">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span><br><span class=\"line\">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span><br><span class=\"line\">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span><br><span class=\"line\">val rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span><br><span class=\"line\">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-6-原则五：使用map-side预聚合的shuffle操作\"><a href=\"#2-6-原则五：使用map-side预聚合的shuffle操作\" class=\"headerlink\" title=\"2.6 原则五：使用map-side预聚合的shuffle操作\"></a>2.6 原则五：使用map-side预聚合的shuffle操作</h4><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>\n<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>\n<p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-1.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-2.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-7-原则六：使用高性能的算子\"><a href=\"#2-7-原则六：使用高性能的算子\" class=\"headerlink\" title=\"2.7 原则六：使用高性能的算子\"></a>2.7 原则六：使用高性能的算子</h4><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>\n<p>(1) 使用reduceByKey/aggregateByKey替代groupByKey</p>\n<p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p>\n<p>(2)使用mapPartitions替代普通map</p>\n<p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>\n<p>(3) 使用foreachPartitions替代foreach</p>\n<p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>\n<p>(4) 使用filter之后进行coalesce操作</p>\n<p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>\n<p>(5) 使用repartitionAndSortWithinPartitions替代repartition与sort类操作</p>\n<p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p>\n<h4 id=\"2-8-原则七：广播大变量\"><a href=\"#2-8-原则七：广播大变量\" class=\"headerlink\" title=\"2.8 原则七：广播大变量\"></a>2.8 原则七：广播大变量</h4><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>\n<p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>\n<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>\n<p>广播大变量的代码示例:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 以下代码在算子函数中，使用了外部的变量。</span><br><span class=\"line\">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span><br><span class=\"line\">val list1 = ...</span><br><span class=\"line\">rdd1.map(list1...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 以下代码将list1封装成了Broadcast类型的广播变量。</span><br><span class=\"line\">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span><br><span class=\"line\">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span><br><span class=\"line\">// 每个Executor内存中，就只会驻留一份广播变量副本。</span><br><span class=\"line\">val list1 = ...</span><br><span class=\"line\">val list1Broadcast = sc.broadcast(list1)</span><br><span class=\"line\">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-9-原则八：使用Kryo优化序列化性能\"><a href=\"#2-9-原则八：使用Kryo优化序列化性能\" class=\"headerlink\" title=\"2.9 原则八：使用Kryo优化序列化性能\"></a>2.9 原则八：使用Kryo优化序列化性能</h4><p>在Spark中，主要有三个地方涉及到了序列化：</p>\n<ul>\n<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li>\n<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>\n<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li>\n</ul>\n<p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>\n<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 创建SparkConf对象。</span><br><span class=\"line\">val conf = new SparkConf().setMaster(...).setAppName(...)</span><br><span class=\"line\">// 设置序列化器为KryoSerializer。</span><br><span class=\"line\">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class=\"line\">// 注册要序列化的自定义类型。</span><br><span class=\"line\">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-10-原则九：优化数据结构\"><a href=\"#2-10-原则九：优化数据结构\" class=\"headerlink\" title=\"2.10 原则九：优化数据结构\"></a>2.10 原则九：优化数据结构</h4><p>Java中，有三种类型比较耗费内存：</p>\n<ul>\n<li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li>\n<li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li>\n<li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li>\n</ul>\n<p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p>\n<p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>\n<h3 id=\"3-资源调优\"><a href=\"#3-资源调优\" class=\"headerlink\" title=\"3. 资源调优\"></a>3. 资源调优</h3><h4 id=\"3-1-调优概述\"><a href=\"#3-1-调优概述\" class=\"headerlink\" title=\"3.1 调优概述\"></a>3.1 调优概述</h4><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p>\n<h4 id=\"3-2-Spark作业基本运行原理\"><a href=\"#3-2-Spark作业基本运行原理\" class=\"headerlink\" title=\"3.2 Spark作业基本运行原理\"></a>3.2 Spark作业基本运行原理</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-3.png?raw=true\" alt=\"\"></p>\n<p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>\n<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>\n<p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p>\n<p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p>\n<p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p>\n<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>\n<p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p>\n<h4 id=\"3-3-资源参数调优\"><a href=\"#3-3-资源参数调优\" class=\"headerlink\" title=\"3.3 资源参数调优\"></a>3.3 资源参数调优</h4><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>\n<p>(1) num-executors</p>\n<ul>\n<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>\n<li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>\n</ul>\n<p>(2) executor-memory</p>\n<ul>\n<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li>\n<li>参数调优建议：每个Executor进程的内存设置4G-8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3-1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>\n</ul>\n<p>(3) executor-cores</p>\n<ul>\n<li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>\n<li>参数调优建议：Executor的CPU core数量设置为2-4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3-1/2左右比较合适，也是避免影响其他同学的作业运行。</li>\n</ul>\n<p>(4) driver-memory</p>\n<ul>\n<li>参数说明：该参数用于设置Driver进程的内存。</li>\n<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li>\n</ul>\n<p>(5) spark.default.parallelism</p>\n<ul>\n<li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>\n<li>参数调优建议：Spark作业的默认task数量为500-1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2-3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li>\n</ul>\n<p>(6) spark.storage.memoryFraction</p>\n<ul>\n<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>\n<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>\n</ul>\n<p>(7) spark.shuffle.memoryFraction</p>\n<ul>\n<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>\n<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>\n</ul>\n<p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>\n<h4 id=\"3-4-资源参数参考示例\"><a href=\"#3-4-资源参数参考示例\" class=\"headerlink\" title=\"3.4 资源参数参考示例\"></a>3.4 资源参数参考示例</h4><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/spark-submit \\</span><br><span class=\"line\">  --master yarn-cluster \\</span><br><span class=\"line\">  --num-executors 100 \\</span><br><span class=\"line\">  --executor-memory 6G \\</span><br><span class=\"line\">  --executor-cores 4 \\</span><br><span class=\"line\">  --driver-memory 1G \\</span><br><span class=\"line\">  --conf spark.default.parallelism=1000 \\</span><br><span class=\"line\">  --conf spark.storage.memoryFraction=0.5 \\</span><br><span class=\"line\">  --conf spark.shuffle.memoryFraction=0.3 \\</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-写在最后的话\"><a href=\"#4-写在最后的话\" class=\"headerlink\" title=\"4. 写在最后的话\"></a>4. 写在最后的话</h3><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。</p>\n<p>原文: <a href=\"https://tech.meituan.com/spark-tuning-basic.html\" target=\"_blank\" rel=\"noopener\">https://tech.meituan.com/spark-tuning-basic.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-前言\"><a href=\"#1-前言\" class=\"headerlink\" title=\"1. 前言\"></a>1. 前言</h3><p>在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p>\n<p>然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p>\n<p>Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p>\n<p>笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p>\n<p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p>\n<h3 id=\"2-开发调优\"><a href=\"#2-开发调优\" class=\"headerlink\" title=\"2. 开发调优\"></a>2. 开发调优</h3><h4 id=\"2-1-调优概述\"><a href=\"#2-1-调优概述\" class=\"headerlink\" title=\"2.1 调优概述\"></a>2.1 调优概述</h4><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p>\n<h4 id=\"2-2-原则一：避免创建重复的RDD\"><a href=\"#2-2-原则一：避免创建重复的RDD\" class=\"headerlink\" title=\"2.2 原则一：避免创建重复的RDD\"></a>2.2 原则一：避免创建重复的RDD</h4><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p>\n<p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p>\n<p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p>\n<p>一个简单的例子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span><br><span class=\"line\"></span><br><span class=\"line\">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span><br><span class=\"line\">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span><br><span class=\"line\">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">val rdd2 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class=\"line\">rdd2.reduce(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span><br><span class=\"line\">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span><br><span class=\"line\">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span><br><span class=\"line\">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">rdd1.reduce(...)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-3-原则二：尽可能复用同一个RDD\"><a href=\"#2-3-原则二：尽可能复用同一个RDD\" class=\"headerlink\" title=\"2.3 原则二：尽可能复用同一个RDD\"></a>2.3 原则二：尽可能复用同一个RDD</h4><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p>\n<p>一个简单的例子:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 错误的做法。</span><br><span class=\"line\"></span><br><span class=\"line\">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span><br><span class=\"line\">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span><br><span class=\"line\">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class=\"line\">JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 分别对rdd1和rdd2执行了不同的算子操作。</span><br><span class=\"line\">rdd1.reduceByKey(...)</span><br><span class=\"line\">rdd2.map(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 正确的做法。</span><br><span class=\"line\"></span><br><span class=\"line\">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span><br><span class=\"line\">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span><br><span class=\"line\"></span><br><span class=\"line\">// 其实在这种情况下完全可以复用同一个RDD。</span><br><span class=\"line\">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span><br><span class=\"line\">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span><br><span class=\"line\">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class=\"line\">rdd1.reduceByKey(...)</span><br><span class=\"line\">rdd1.map(tuple._2...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span><br><span class=\"line\">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span><br><span class=\"line\">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-4-原则三：对多次使用的RDD进行持久化\"><a href=\"#2-4-原则三：对多次使用的RDD进行持久化\" class=\"headerlink\" title=\"2.4 原则三：对多次使用的RDD进行持久化\"></a>2.4 原则三：对多次使用的RDD进行持久化</h4><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p>\n<p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p>\n<p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p>\n<p>对多次使用的RDD进行持久化的代码示例:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span><br><span class=\"line\"></span><br><span class=\"line\">// 正确的做法。</span><br><span class=\"line\">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span><br><span class=\"line\">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span><br><span class=\"line\">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;).cache()</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">rdd1.reduce(...)</span><br><span class=\"line\"></span><br><span class=\"line\">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span><br><span class=\"line\">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span><br><span class=\"line\">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span><br><span class=\"line\">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span><br><span class=\"line\">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;).persist(StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class=\"line\">rdd1.map(...)</span><br><span class=\"line\">rdd1.reduce(...)</span><br></pre></td></tr></table></figure></p>\n<p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p>\n<p>Spark的持久化级别</p>\n<table>\n<thead>\n<tr>\n<th>持久化级别</th>\n<th>含义解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MEMORY_ONLY</td>\n<td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td>\n</tr>\n<tr>\n<td>MEMORY_AND_DISK</td>\n<td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td>\n</tr>\n<tr>\n<td>MEMORY_ONLY_SER</td>\n<td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>\n</tr>\n<tr>\n<td>MEMORY_AND_DISK_SER</td>\n<td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>\n</tr>\n<tr>\n<td>DISK_ONLY</td>\n<td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td>\n</tr>\n<tr>\n<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td>\n<td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td>\n</tr>\n</tbody>\n</table>\n<p>如何选择一种最合适的持久化策略</p>\n<ul>\n<li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li>\n<li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li>\n<li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li>\n<li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li>\n</ul>\n<h4 id=\"2-5-原则四：尽量避免使用shuffle类算子\"><a href=\"#2-5-原则四：尽量避免使用shuffle类算子\" class=\"headerlink\" title=\"2.5 原则四：尽量避免使用shuffle类算子\"></a>2.5 原则四：尽量避免使用shuffle类算子</h4><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p>\n<p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p>\n<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>\n<p>Broadcast与map进行join代码示例:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 传统的join操作会导致shuffle操作。</span><br><span class=\"line\">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span><br><span class=\"line\">val rdd3 = rdd1.join(rdd2)</span><br><span class=\"line\"></span><br><span class=\"line\">// Broadcast+map的join操作，不会导致shuffle操作。</span><br><span class=\"line\">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span><br><span class=\"line\">val rdd2Data = rdd2.collect()</span><br><span class=\"line\">val rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class=\"line\"></span><br><span class=\"line\">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span><br><span class=\"line\">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span><br><span class=\"line\">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span><br><span class=\"line\">val rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span><br><span class=\"line\">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-6-原则五：使用map-side预聚合的shuffle操作\"><a href=\"#2-6-原则五：使用map-side预聚合的shuffle操作\" class=\"headerlink\" title=\"2.6 原则五：使用map-side预聚合的shuffle操作\"></a>2.6 原则五：使用map-side预聚合的shuffle操作</h4><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>\n<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>\n<p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-1.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-2.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-7-原则六：使用高性能的算子\"><a href=\"#2-7-原则六：使用高性能的算子\" class=\"headerlink\" title=\"2.7 原则六：使用高性能的算子\"></a>2.7 原则六：使用高性能的算子</h4><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>\n<p>(1) 使用reduceByKey/aggregateByKey替代groupByKey</p>\n<p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p>\n<p>(2)使用mapPartitions替代普通map</p>\n<p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>\n<p>(3) 使用foreachPartitions替代foreach</p>\n<p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>\n<p>(4) 使用filter之后进行coalesce操作</p>\n<p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>\n<p>(5) 使用repartitionAndSortWithinPartitions替代repartition与sort类操作</p>\n<p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p>\n<h4 id=\"2-8-原则七：广播大变量\"><a href=\"#2-8-原则七：广播大变量\" class=\"headerlink\" title=\"2.8 原则七：广播大变量\"></a>2.8 原则七：广播大变量</h4><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>\n<p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>\n<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>\n<p>广播大变量的代码示例:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 以下代码在算子函数中，使用了外部的变量。</span><br><span class=\"line\">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span><br><span class=\"line\">val list1 = ...</span><br><span class=\"line\">rdd1.map(list1...)</span><br><span class=\"line\"></span><br><span class=\"line\">// 以下代码将list1封装成了Broadcast类型的广播变量。</span><br><span class=\"line\">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span><br><span class=\"line\">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span><br><span class=\"line\">// 每个Executor内存中，就只会驻留一份广播变量副本。</span><br><span class=\"line\">val list1 = ...</span><br><span class=\"line\">val list1Broadcast = sc.broadcast(list1)</span><br><span class=\"line\">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-9-原则八：使用Kryo优化序列化性能\"><a href=\"#2-9-原则八：使用Kryo优化序列化性能\" class=\"headerlink\" title=\"2.9 原则八：使用Kryo优化序列化性能\"></a>2.9 原则八：使用Kryo优化序列化性能</h4><p>在Spark中，主要有三个地方涉及到了序列化：</p>\n<ul>\n<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li>\n<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>\n<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li>\n</ul>\n<p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>\n<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 创建SparkConf对象。</span><br><span class=\"line\">val conf = new SparkConf().setMaster(...).setAppName(...)</span><br><span class=\"line\">// 设置序列化器为KryoSerializer。</span><br><span class=\"line\">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class=\"line\">// 注册要序列化的自定义类型。</span><br><span class=\"line\">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-10-原则九：优化数据结构\"><a href=\"#2-10-原则九：优化数据结构\" class=\"headerlink\" title=\"2.10 原则九：优化数据结构\"></a>2.10 原则九：优化数据结构</h4><p>Java中，有三种类型比较耗费内存：</p>\n<ul>\n<li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li>\n<li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li>\n<li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li>\n</ul>\n<p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p>\n<p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>\n<h3 id=\"3-资源调优\"><a href=\"#3-资源调优\" class=\"headerlink\" title=\"3. 资源调优\"></a>3. 资源调优</h3><h4 id=\"3-1-调优概述\"><a href=\"#3-1-调优概述\" class=\"headerlink\" title=\"3.1 调优概述\"></a>3.1 调优概述</h4><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p>\n<h4 id=\"3-2-Spark作业基本运行原理\"><a href=\"#3-2-Spark作业基本运行原理\" class=\"headerlink\" title=\"3.2 Spark作业基本运行原理\"></a>3.2 Spark作业基本运行原理</h4><p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Spark/spark-performance-optimization-basic-3.png?raw=true\" alt=\"\"></p>\n<p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>\n<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>\n<p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p>\n<p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p>\n<p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p>\n<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>\n<p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p>\n<h4 id=\"3-3-资源参数调优\"><a href=\"#3-3-资源参数调优\" class=\"headerlink\" title=\"3.3 资源参数调优\"></a>3.3 资源参数调优</h4><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>\n<p>(1) num-executors</p>\n<ul>\n<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>\n<li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>\n</ul>\n<p>(2) executor-memory</p>\n<ul>\n<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li>\n<li>参数调优建议：每个Executor进程的内存设置4G-8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3-1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>\n</ul>\n<p>(3) executor-cores</p>\n<ul>\n<li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>\n<li>参数调优建议：Executor的CPU core数量设置为2-4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3-1/2左右比较合适，也是避免影响其他同学的作业运行。</li>\n</ul>\n<p>(4) driver-memory</p>\n<ul>\n<li>参数说明：该参数用于设置Driver进程的内存。</li>\n<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li>\n</ul>\n<p>(5) spark.default.parallelism</p>\n<ul>\n<li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>\n<li>参数调优建议：Spark作业的默认task数量为500-1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2-3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li>\n</ul>\n<p>(6) spark.storage.memoryFraction</p>\n<ul>\n<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>\n<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>\n</ul>\n<p>(7) spark.shuffle.memoryFraction</p>\n<ul>\n<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>\n<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>\n</ul>\n<p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>\n<h4 id=\"3-4-资源参数参考示例\"><a href=\"#3-4-资源参数参考示例\" class=\"headerlink\" title=\"3.4 资源参数参考示例\"></a>3.4 资源参数参考示例</h4><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/spark-submit \\</span><br><span class=\"line\">  --master yarn-cluster \\</span><br><span class=\"line\">  --num-executors 100 \\</span><br><span class=\"line\">  --executor-memory 6G \\</span><br><span class=\"line\">  --executor-cores 4 \\</span><br><span class=\"line\">  --driver-memory 1G \\</span><br><span class=\"line\">  --conf spark.default.parallelism=1000 \\</span><br><span class=\"line\">  --conf spark.storage.memoryFraction=0.5 \\</span><br><span class=\"line\">  --conf spark.shuffle.memoryFraction=0.3 \\</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-写在最后的话\"><a href=\"#4-写在最后的话\" class=\"headerlink\" title=\"4. 写在最后的话\"></a>4. 写在最后的话</h3><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。</p>\n<p>原文: <a href=\"https://tech.meituan.com/spark-tuning-basic.html\" target=\"_blank\" rel=\"noopener\">https://tech.meituan.com/spark-tuning-basic.html</a></p>\n"},{"layout":"post","author":"大愚若智","title":"Stream Exactly once 未必严格一次","date":"2018-01-11T03:18:01.000Z","_content":"\n分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中\"严格一次（Exactly-once）\" 是很多引擎追求的目标之一，很多 SPE 均宣称可提供\"严格一次\"的处理语义。\n\n然而`exactly-once`具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用`exactly-once`来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在`exactly-once`处理语义方面的差异，以及为什么`exactly-once`更适合称之为`有效一次`(Effectively-once)。同时本文还将探讨在实现所谓`exactly-once`的语义过程中，各类常用技术之间需要进行的取舍。\n\n### 1. 背景\n\n流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图，大部分情况（但也并非总是如此）下也可以看作有向无环图（Directed acyclic graph，DAG）。在这种图中，每个边（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边的数据或事件的算子（Operator）。有两种特殊类型的顶点，通常称之为 `Source` 和 `Sink`，`Source` 会消耗外部数据/事件并将其注入应用程序，而`Sink`通常负责收集应用程序生成的结果。图1展示了这样的一个流应用程序范例。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-1.png?raw=true)\n\n执行流/事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：`最多一次`（At-most-once）、`最少一次`（At-least-once），以及`严格一次`（Exactly-once）。\n这些不同处理语义可粗略理解如下：\n\n### 2. 最多一次\n\n这其实是一种\"尽力而为\"的方法。数据或事件可以保证被应用程序中的所有算子最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图2列举了一个范例。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-2.png?raw=true)\n\n### 3. 最少一次\n\n数据或事件可保证被应用程序图中的所有算子最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为\"最少一次\"。图3展示了一个范例。在本例中，第一个算子最初处理事件时失败了，随后重试并成功，然后第二次重试并再次成功，然而第二次重试实际上是不必要的。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-3.png?raw=true)\n\n### 3. 严格一次\n\n事件可保证被流应用程序中的所有算子“严格一次”处理，哪怕遇到各种失败。\n为了实现`exactly-once`处理语义，通常主要会使用下列两种机制：\n- 分布式快照/状态检查点\n- 最少一次事件交付，外加消息去重\n\n通过分布式快照/状态检查点方法实现的`exactly-once`是由 [Chandy-Lamport 分布式快照算法](http://lamport.azurewebsites.net/pubs/chandy.pdf)启发而来的。在这种机制中，会定期为流应用程序中每个算子的所有状态创建检查点，一旦系统中任何位置出现失败，每个算子的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被回退到最新一致状态，并从该状态开始重新处理。图4展示了这种机制的一些基本概念。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-4.png?raw=true)\n\n在图4中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，算子在处理传入的数据时失败了。此时 `S = 4` 这个状态值已经被保存到持久存储中，而 `S = 12` 状态值正位于算子的内存中。为了解决这种差异，在 T3 时处理图将状态回退至 `S = 4`，并\"重播\"流中每个连续状态直到最新状态，并处理每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。\n\n实现`exactly-once`的另一种方法是在实现至少一次事件交付的同时在每个算子一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个算子中移除重复的事件，随后才将结果事件发送给用户在算子中定义的逻辑。这种机制要求为每个算子保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel 以及 Apache Kafka Streams 等机制。图 5 展示了这种机制的概况。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-5.png?raw=true)\n\n### 4. 严格一次真的就一次吗？\n\n接着重新考虑一下`exactly-once`处理语义实际上能为最终用户提供怎样的保证。`exactly-once`这样的标签对于描述严格一次起到了一定的误导效果。\n\n有些人可能认为`exactly-once`描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。但实际上没有一个 SPE 能完全保证只处理一次。面对各种可能的失败，根本不可能保证每个算子中由用户自定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。\n\n假设这样一个场景：有个流处理算子需要执行 `Map` 操作输出传入事件的 `ID`，随后原样返回事件。例如这个操作可能使用了如下的虚构代码：\n```\nMap (Event event) {\n    Print \"Event ID: \" + event.getId()\n    Return event\n}\n```\n每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑严格一次执行可以得到保证，那么事件 ID 只输出一次。然而这一点永远无法保证，因为用户自定义的逻辑执行过程中可能随时随地面临失败。SPE 无法自行判断用户自定义的逻辑到底执行到哪一步了。因此任何用户自定义逻辑都无法保证只执行一次。这也意味着用户自定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。\n\n那么当 SPE 宣称提供`exactly-once`的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称`exactly-once`处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。\n\n上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个算子的状态，并自动提交状态更新。对于机制1（分布式快照/状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个算子的状态检查点）；对于机制2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个算子的状态以及每个算子追踪已经被成功处理过事件的事务日志。\n\n状态提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次(The committing of state or applying updates to the durable backend that is the source of truth can be described as occurring exactly-once)。然而在计算状态的更新/改动，例如处理在事件上执行用户自定义逻辑的事件，但是如果出现失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的效果只会在持久后端状态存储中体现一次。因此在这里我们认为\"有效一次（Effectively-once）\"术语可以更精确地描述这样的处理语义。\n\n### 5. 分布式快照与至少一次事件交付外加去重机制的对比\n\n从语义的角度来看，分布式快照以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，有可能会产生明显的性能差异。\n\n基于机制1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在流应用程序正常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率会更高一点，这会导致 SPE 需要暂停应用程序并回滚所有算子的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。\n\n机制2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被算子的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个算子中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何算子遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个算子遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他算子的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格:\n\n分布式快照/状态检查点\n\n利|弊\n---|---\n性能和资源开销小|从失败中恢复时的性能影响大\n|随着拓扑规模的增大，对性能的潜在影响增高\n\n\n至少一次交付外加去重\n\n利|弊\n---|---\n失败对性能的影响更为局部|可能需要大量的存储与基础设施的支持\n失败的影响未必会随着拓扑规模一起增加|每个算子处理每个事件均会产生性能开销\n\n虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播/重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，算子实际上将具备幂等的特性。\n\n### 6. 结论\n\n希望本文可以帮助大家意识到`exactly-once`这个术语极具误导性。提供`exactly-once`的处理语义实际上意味着在对流处理引擎所管理的算子的状态进行不同更新只会影响一次。`exactly-once`完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此这里我们更愿意使用有效一次这个术语来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只影响一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格/有效一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊。\n\n原文:https://streaml.io/blog/exactly-once/\n\n译文:https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw\n","source":"_posts/Stream/Exactly once未必严格一次.md","raw":"---\nlayout: post\nauthor: 大愚若智\ntitle: Stream Exactly once 未必严格一次\ndate: 2018-01-11 11:18:01\ntags:\n  - Stream\n\ncategories: Stream\n---\n\n分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中\"严格一次（Exactly-once）\" 是很多引擎追求的目标之一，很多 SPE 均宣称可提供\"严格一次\"的处理语义。\n\n然而`exactly-once`具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用`exactly-once`来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在`exactly-once`处理语义方面的差异，以及为什么`exactly-once`更适合称之为`有效一次`(Effectively-once)。同时本文还将探讨在实现所谓`exactly-once`的语义过程中，各类常用技术之间需要进行的取舍。\n\n### 1. 背景\n\n流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图，大部分情况（但也并非总是如此）下也可以看作有向无环图（Directed acyclic graph，DAG）。在这种图中，每个边（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边的数据或事件的算子（Operator）。有两种特殊类型的顶点，通常称之为 `Source` 和 `Sink`，`Source` 会消耗外部数据/事件并将其注入应用程序，而`Sink`通常负责收集应用程序生成的结果。图1展示了这样的一个流应用程序范例。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-1.png?raw=true)\n\n执行流/事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：`最多一次`（At-most-once）、`最少一次`（At-least-once），以及`严格一次`（Exactly-once）。\n这些不同处理语义可粗略理解如下：\n\n### 2. 最多一次\n\n这其实是一种\"尽力而为\"的方法。数据或事件可以保证被应用程序中的所有算子最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图2列举了一个范例。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-2.png?raw=true)\n\n### 3. 最少一次\n\n数据或事件可保证被应用程序图中的所有算子最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为\"最少一次\"。图3展示了一个范例。在本例中，第一个算子最初处理事件时失败了，随后重试并成功，然后第二次重试并再次成功，然而第二次重试实际上是不必要的。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-3.png?raw=true)\n\n### 3. 严格一次\n\n事件可保证被流应用程序中的所有算子“严格一次”处理，哪怕遇到各种失败。\n为了实现`exactly-once`处理语义，通常主要会使用下列两种机制：\n- 分布式快照/状态检查点\n- 最少一次事件交付，外加消息去重\n\n通过分布式快照/状态检查点方法实现的`exactly-once`是由 [Chandy-Lamport 分布式快照算法](http://lamport.azurewebsites.net/pubs/chandy.pdf)启发而来的。在这种机制中，会定期为流应用程序中每个算子的所有状态创建检查点，一旦系统中任何位置出现失败，每个算子的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被回退到最新一致状态，并从该状态开始重新处理。图4展示了这种机制的一些基本概念。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-4.png?raw=true)\n\n在图4中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，算子在处理传入的数据时失败了。此时 `S = 4` 这个状态值已经被保存到持久存储中，而 `S = 12` 状态值正位于算子的内存中。为了解决这种差异，在 T3 时处理图将状态回退至 `S = 4`，并\"重播\"流中每个连续状态直到最新状态，并处理每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。\n\n实现`exactly-once`的另一种方法是在实现至少一次事件交付的同时在每个算子一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个算子中移除重复的事件，随后才将结果事件发送给用户在算子中定义的逻辑。这种机制要求为每个算子保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel 以及 Apache Kafka Streams 等机制。图 5 展示了这种机制的概况。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-5.png?raw=true)\n\n### 4. 严格一次真的就一次吗？\n\n接着重新考虑一下`exactly-once`处理语义实际上能为最终用户提供怎样的保证。`exactly-once`这样的标签对于描述严格一次起到了一定的误导效果。\n\n有些人可能认为`exactly-once`描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。但实际上没有一个 SPE 能完全保证只处理一次。面对各种可能的失败，根本不可能保证每个算子中由用户自定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。\n\n假设这样一个场景：有个流处理算子需要执行 `Map` 操作输出传入事件的 `ID`，随后原样返回事件。例如这个操作可能使用了如下的虚构代码：\n```\nMap (Event event) {\n    Print \"Event ID: \" + event.getId()\n    Return event\n}\n```\n每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑严格一次执行可以得到保证，那么事件 ID 只输出一次。然而这一点永远无法保证，因为用户自定义的逻辑执行过程中可能随时随地面临失败。SPE 无法自行判断用户自定义的逻辑到底执行到哪一步了。因此任何用户自定义逻辑都无法保证只执行一次。这也意味着用户自定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。\n\n那么当 SPE 宣称提供`exactly-once`的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称`exactly-once`处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。\n\n上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个算子的状态，并自动提交状态更新。对于机制1（分布式快照/状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个算子的状态检查点）；对于机制2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个算子的状态以及每个算子追踪已经被成功处理过事件的事务日志。\n\n状态提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次(The committing of state or applying updates to the durable backend that is the source of truth can be described as occurring exactly-once)。然而在计算状态的更新/改动，例如处理在事件上执行用户自定义逻辑的事件，但是如果出现失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的效果只会在持久后端状态存储中体现一次。因此在这里我们认为\"有效一次（Effectively-once）\"术语可以更精确地描述这样的处理语义。\n\n### 5. 分布式快照与至少一次事件交付外加去重机制的对比\n\n从语义的角度来看，分布式快照以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，有可能会产生明显的性能差异。\n\n基于机制1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在流应用程序正常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率会更高一点，这会导致 SPE 需要暂停应用程序并回滚所有算子的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。\n\n机制2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被算子的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个算子中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何算子遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个算子遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他算子的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格:\n\n分布式快照/状态检查点\n\n利|弊\n---|---\n性能和资源开销小|从失败中恢复时的性能影响大\n|随着拓扑规模的增大，对性能的潜在影响增高\n\n\n至少一次交付外加去重\n\n利|弊\n---|---\n失败对性能的影响更为局部|可能需要大量的存储与基础设施的支持\n失败的影响未必会随着拓扑规模一起增加|每个算子处理每个事件均会产生性能开销\n\n虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播/重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，算子实际上将具备幂等的特性。\n\n### 6. 结论\n\n希望本文可以帮助大家意识到`exactly-once`这个术语极具误导性。提供`exactly-once`的处理语义实际上意味着在对流处理引擎所管理的算子的状态进行不同更新只会影响一次。`exactly-once`完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此这里我们更愿意使用有效一次这个术语来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只影响一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格/有效一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊。\n\n原文:https://streaml.io/blog/exactly-once/\n\n译文:https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw\n","slug":"Stream/Exactly once未必严格一次","published":1,"updated":"2018-01-29T09:36:59.616Z","comments":1,"photos":[],"link":"","_id":"cje58tiv5005lordbrdfoccag","content":"<p>分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中”严格一次（Exactly-once）” 是很多引擎追求的目标之一，很多 SPE 均宣称可提供”严格一次”的处理语义。</p>\n<p>然而<code>exactly-once</code>具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用<code>exactly-once</code>来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在<code>exactly-once</code>处理语义方面的差异，以及为什么<code>exactly-once</code>更适合称之为<code>有效一次</code>(Effectively-once)。同时本文还将探讨在实现所谓<code>exactly-once</code>的语义过程中，各类常用技术之间需要进行的取舍。</p>\n<h3 id=\"1-背景\"><a href=\"#1-背景\" class=\"headerlink\" title=\"1. 背景\"></a>1. 背景</h3><p>流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图，大部分情况（但也并非总是如此）下也可以看作有向无环图（Directed acyclic graph，DAG）。在这种图中，每个边（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边的数据或事件的算子（Operator）。有两种特殊类型的顶点，通常称之为 <code>Source</code> 和 <code>Sink</code>，<code>Source</code> 会消耗外部数据/事件并将其注入应用程序，而<code>Sink</code>通常负责收集应用程序生成的结果。图1展示了这样的一个流应用程序范例。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-1.png?raw=true\" alt=\"\"></p>\n<p>执行流/事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：<code>最多一次</code>（At-most-once）、<code>最少一次</code>（At-least-once），以及<code>严格一次</code>（Exactly-once）。<br>这些不同处理语义可粗略理解如下：</p>\n<h3 id=\"2-最多一次\"><a href=\"#2-最多一次\" class=\"headerlink\" title=\"2. 最多一次\"></a>2. 最多一次</h3><p>这其实是一种”尽力而为”的方法。数据或事件可以保证被应用程序中的所有算子最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图2列举了一个范例。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-最少一次\"><a href=\"#3-最少一次\" class=\"headerlink\" title=\"3. 最少一次\"></a>3. 最少一次</h3><p>数据或事件可保证被应用程序图中的所有算子最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为”最少一次”。图3展示了一个范例。在本例中，第一个算子最初处理事件时失败了，随后重试并成功，然后第二次重试并再次成功，然而第二次重试实际上是不必要的。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-严格一次\"><a href=\"#3-严格一次\" class=\"headerlink\" title=\"3. 严格一次\"></a>3. 严格一次</h3><p>事件可保证被流应用程序中的所有算子“严格一次”处理，哪怕遇到各种失败。<br>为了实现<code>exactly-once</code>处理语义，通常主要会使用下列两种机制：</p>\n<ul>\n<li>分布式快照/状态检查点</li>\n<li>最少一次事件交付，外加消息去重</li>\n</ul>\n<p>通过分布式快照/状态检查点方法实现的<code>exactly-once</code>是由 <a href=\"http://lamport.azurewebsites.net/pubs/chandy.pdf\" target=\"_blank\" rel=\"noopener\">Chandy-Lamport 分布式快照算法</a>启发而来的。在这种机制中，会定期为流应用程序中每个算子的所有状态创建检查点，一旦系统中任何位置出现失败，每个算子的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被回退到最新一致状态，并从该状态开始重新处理。图4展示了这种机制的一些基本概念。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-4.png?raw=true\" alt=\"\"></p>\n<p>在图4中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，算子在处理传入的数据时失败了。此时 <code>S = 4</code> 这个状态值已经被保存到持久存储中，而 <code>S = 12</code> 状态值正位于算子的内存中。为了解决这种差异，在 T3 时处理图将状态回退至 <code>S = 4</code>，并”重播”流中每个连续状态直到最新状态，并处理每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。</p>\n<p>实现<code>exactly-once</code>的另一种方法是在实现至少一次事件交付的同时在每个算子一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个算子中移除重复的事件，随后才将结果事件发送给用户在算子中定义的逻辑。这种机制要求为每个算子保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel 以及 Apache Kafka Streams 等机制。图 5 展示了这种机制的概况。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-严格一次真的就一次吗？\"><a href=\"#4-严格一次真的就一次吗？\" class=\"headerlink\" title=\"4. 严格一次真的就一次吗？\"></a>4. 严格一次真的就一次吗？</h3><p>接着重新考虑一下<code>exactly-once</code>处理语义实际上能为最终用户提供怎样的保证。<code>exactly-once</code>这样的标签对于描述严格一次起到了一定的误导效果。</p>\n<p>有些人可能认为<code>exactly-once</code>描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。但实际上没有一个 SPE 能完全保证只处理一次。面对各种可能的失败，根本不可能保证每个算子中由用户自定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。</p>\n<p>假设这样一个场景：有个流处理算子需要执行 <code>Map</code> 操作输出传入事件的 <code>ID</code>，随后原样返回事件。例如这个操作可能使用了如下的虚构代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map (Event event) &#123;</span><br><span class=\"line\">    Print &quot;Event ID: &quot; + event.getId()</span><br><span class=\"line\">    Return event</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑严格一次执行可以得到保证，那么事件 ID 只输出一次。然而这一点永远无法保证，因为用户自定义的逻辑执行过程中可能随时随地面临失败。SPE 无法自行判断用户自定义的逻辑到底执行到哪一步了。因此任何用户自定义逻辑都无法保证只执行一次。这也意味着用户自定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。</p>\n<p>那么当 SPE 宣称提供<code>exactly-once</code>的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称<code>exactly-once</code>处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。</p>\n<p>上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个算子的状态，并自动提交状态更新。对于机制1（分布式快照/状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个算子的状态检查点）；对于机制2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个算子的状态以及每个算子追踪已经被成功处理过事件的事务日志。</p>\n<p>状态提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次(The committing of state or applying updates to the durable backend that is the source of truth can be described as occurring exactly-once)。然而在计算状态的更新/改动，例如处理在事件上执行用户自定义逻辑的事件，但是如果出现失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的效果只会在持久后端状态存储中体现一次。因此在这里我们认为”有效一次（Effectively-once）”术语可以更精确地描述这样的处理语义。</p>\n<h3 id=\"5-分布式快照与至少一次事件交付外加去重机制的对比\"><a href=\"#5-分布式快照与至少一次事件交付外加去重机制的对比\" class=\"headerlink\" title=\"5. 分布式快照与至少一次事件交付外加去重机制的对比\"></a>5. 分布式快照与至少一次事件交付外加去重机制的对比</h3><p>从语义的角度来看，分布式快照以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，有可能会产生明显的性能差异。</p>\n<p>基于机制1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在流应用程序正常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率会更高一点，这会导致 SPE 需要暂停应用程序并回滚所有算子的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。</p>\n<p>机制2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被算子的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个算子中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何算子遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个算子遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他算子的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格:</p>\n<p>分布式快照/状态检查点</p>\n<table>\n<thead>\n<tr>\n<th>利</th>\n<th>弊</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>性能和资源开销小</td>\n<td>从失败中恢复时的性能影响大</td>\n</tr>\n<tr>\n<td></td>\n<td>随着拓扑规模的增大，对性能的潜在影响增高</td>\n</tr>\n</tbody>\n</table>\n<p>至少一次交付外加去重</p>\n<table>\n<thead>\n<tr>\n<th>利</th>\n<th>弊</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>失败对性能的影响更为局部</td>\n<td>可能需要大量的存储与基础设施的支持</td>\n</tr>\n<tr>\n<td>失败的影响未必会随着拓扑规模一起增加</td>\n<td>每个算子处理每个事件均会产生性能开销</td>\n</tr>\n</tbody>\n</table>\n<p>虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播/重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，算子实际上将具备幂等的特性。</p>\n<h3 id=\"6-结论\"><a href=\"#6-结论\" class=\"headerlink\" title=\"6. 结论\"></a>6. 结论</h3><p>希望本文可以帮助大家意识到<code>exactly-once</code>这个术语极具误导性。提供<code>exactly-once</code>的处理语义实际上意味着在对流处理引擎所管理的算子的状态进行不同更新只会影响一次。<code>exactly-once</code>完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此这里我们更愿意使用有效一次这个术语来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只影响一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格/有效一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊。</p>\n<p>原文:<a href=\"https://streaml.io/blog/exactly-once/\" target=\"_blank\" rel=\"noopener\">https://streaml.io/blog/exactly-once/</a></p>\n<p>译文:<a href=\"https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw\" target=\"_blank\" rel=\"noopener\">https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中”严格一次（Exactly-once）” 是很多引擎追求的目标之一，很多 SPE 均宣称可提供”严格一次”的处理语义。</p>\n<p>然而<code>exactly-once</code>具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用<code>exactly-once</code>来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在<code>exactly-once</code>处理语义方面的差异，以及为什么<code>exactly-once</code>更适合称之为<code>有效一次</code>(Effectively-once)。同时本文还将探讨在实现所谓<code>exactly-once</code>的语义过程中，各类常用技术之间需要进行的取舍。</p>\n<h3 id=\"1-背景\"><a href=\"#1-背景\" class=\"headerlink\" title=\"1. 背景\"></a>1. 背景</h3><p>流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图，大部分情况（但也并非总是如此）下也可以看作有向无环图（Directed acyclic graph，DAG）。在这种图中，每个边（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边的数据或事件的算子（Operator）。有两种特殊类型的顶点，通常称之为 <code>Source</code> 和 <code>Sink</code>，<code>Source</code> 会消耗外部数据/事件并将其注入应用程序，而<code>Sink</code>通常负责收集应用程序生成的结果。图1展示了这样的一个流应用程序范例。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-1.png?raw=true\" alt=\"\"></p>\n<p>执行流/事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：<code>最多一次</code>（At-most-once）、<code>最少一次</code>（At-least-once），以及<code>严格一次</code>（Exactly-once）。<br>这些不同处理语义可粗略理解如下：</p>\n<h3 id=\"2-最多一次\"><a href=\"#2-最多一次\" class=\"headerlink\" title=\"2. 最多一次\"></a>2. 最多一次</h3><p>这其实是一种”尽力而为”的方法。数据或事件可以保证被应用程序中的所有算子最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图2列举了一个范例。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-2.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-最少一次\"><a href=\"#3-最少一次\" class=\"headerlink\" title=\"3. 最少一次\"></a>3. 最少一次</h3><p>数据或事件可保证被应用程序图中的所有算子最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为”最少一次”。图3展示了一个范例。在本例中，第一个算子最初处理事件时失败了，随后重试并成功，然后第二次重试并再次成功，然而第二次重试实际上是不必要的。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-严格一次\"><a href=\"#3-严格一次\" class=\"headerlink\" title=\"3. 严格一次\"></a>3. 严格一次</h3><p>事件可保证被流应用程序中的所有算子“严格一次”处理，哪怕遇到各种失败。<br>为了实现<code>exactly-once</code>处理语义，通常主要会使用下列两种机制：</p>\n<ul>\n<li>分布式快照/状态检查点</li>\n<li>最少一次事件交付，外加消息去重</li>\n</ul>\n<p>通过分布式快照/状态检查点方法实现的<code>exactly-once</code>是由 <a href=\"http://lamport.azurewebsites.net/pubs/chandy.pdf\" target=\"_blank\" rel=\"noopener\">Chandy-Lamport 分布式快照算法</a>启发而来的。在这种机制中，会定期为流应用程序中每个算子的所有状态创建检查点，一旦系统中任何位置出现失败，每个算子的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被回退到最新一致状态，并从该状态开始重新处理。图4展示了这种机制的一些基本概念。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-4.png?raw=true\" alt=\"\"></p>\n<p>在图4中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，算子在处理传入的数据时失败了。此时 <code>S = 4</code> 这个状态值已经被保存到持久存储中，而 <code>S = 12</code> 状态值正位于算子的内存中。为了解决这种差异，在 T3 时处理图将状态回退至 <code>S = 4</code>，并”重播”流中每个连续状态直到最新状态，并处理每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。</p>\n<p>实现<code>exactly-once</code>的另一种方法是在实现至少一次事件交付的同时在每个算子一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个算子中移除重复的事件，随后才将结果事件发送给用户在算子中定义的逻辑。这种机制要求为每个算子保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel 以及 Apache Kafka Streams 等机制。图 5 展示了这种机制的概况。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/Exactly%20once%E6%9C%AA%E5%BF%85%E4%B8%A5%E6%A0%BC%E4%B8%80%E8%87%B4-5.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-严格一次真的就一次吗？\"><a href=\"#4-严格一次真的就一次吗？\" class=\"headerlink\" title=\"4. 严格一次真的就一次吗？\"></a>4. 严格一次真的就一次吗？</h3><p>接着重新考虑一下<code>exactly-once</code>处理语义实际上能为最终用户提供怎样的保证。<code>exactly-once</code>这样的标签对于描述严格一次起到了一定的误导效果。</p>\n<p>有些人可能认为<code>exactly-once</code>描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。但实际上没有一个 SPE 能完全保证只处理一次。面对各种可能的失败，根本不可能保证每个算子中由用户自定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。</p>\n<p>假设这样一个场景：有个流处理算子需要执行 <code>Map</code> 操作输出传入事件的 <code>ID</code>，随后原样返回事件。例如这个操作可能使用了如下的虚构代码：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Map (Event event) &#123;</span><br><span class=\"line\">    Print &quot;Event ID: &quot; + event.getId()</span><br><span class=\"line\">    Return event</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑严格一次执行可以得到保证，那么事件 ID 只输出一次。然而这一点永远无法保证，因为用户自定义的逻辑执行过程中可能随时随地面临失败。SPE 无法自行判断用户自定义的逻辑到底执行到哪一步了。因此任何用户自定义逻辑都无法保证只执行一次。这也意味着用户自定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。</p>\n<p>那么当 SPE 宣称提供<code>exactly-once</code>的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称<code>exactly-once</code>处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。</p>\n<p>上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个算子的状态，并自动提交状态更新。对于机制1（分布式快照/状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个算子的状态检查点）；对于机制2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个算子的状态以及每个算子追踪已经被成功处理过事件的事务日志。</p>\n<p>状态提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次(The committing of state or applying updates to the durable backend that is the source of truth can be described as occurring exactly-once)。然而在计算状态的更新/改动，例如处理在事件上执行用户自定义逻辑的事件，但是如果出现失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的效果只会在持久后端状态存储中体现一次。因此在这里我们认为”有效一次（Effectively-once）”术语可以更精确地描述这样的处理语义。</p>\n<h3 id=\"5-分布式快照与至少一次事件交付外加去重机制的对比\"><a href=\"#5-分布式快照与至少一次事件交付外加去重机制的对比\" class=\"headerlink\" title=\"5. 分布式快照与至少一次事件交付外加去重机制的对比\"></a>5. 分布式快照与至少一次事件交付外加去重机制的对比</h3><p>从语义的角度来看，分布式快照以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，有可能会产生明显的性能差异。</p>\n<p>基于机制1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在流应用程序正常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率会更高一点，这会导致 SPE 需要暂停应用程序并回滚所有算子的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。</p>\n<p>机制2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被算子的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个算子中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何算子遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个算子遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他算子的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格:</p>\n<p>分布式快照/状态检查点</p>\n<table>\n<thead>\n<tr>\n<th>利</th>\n<th>弊</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>性能和资源开销小</td>\n<td>从失败中恢复时的性能影响大</td>\n</tr>\n<tr>\n<td></td>\n<td>随着拓扑规模的增大，对性能的潜在影响增高</td>\n</tr>\n</tbody>\n</table>\n<p>至少一次交付外加去重</p>\n<table>\n<thead>\n<tr>\n<th>利</th>\n<th>弊</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>失败对性能的影响更为局部</td>\n<td>可能需要大量的存储与基础设施的支持</td>\n</tr>\n<tr>\n<td>失败的影响未必会随着拓扑规模一起增加</td>\n<td>每个算子处理每个事件均会产生性能开销</td>\n</tr>\n</tbody>\n</table>\n<p>虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播/重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，算子实际上将具备幂等的特性。</p>\n<h3 id=\"6-结论\"><a href=\"#6-结论\" class=\"headerlink\" title=\"6. 结论\"></a>6. 结论</h3><p>希望本文可以帮助大家意识到<code>exactly-once</code>这个术语极具误导性。提供<code>exactly-once</code>的处理语义实际上意味着在对流处理引擎所管理的算子的状态进行不同更新只会影响一次。<code>exactly-once</code>完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此这里我们更愿意使用有效一次这个术语来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只影响一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格/有效一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊。</p>\n<p>原文:<a href=\"https://streaml.io/blog/exactly-once/\" target=\"_blank\" rel=\"noopener\">https://streaml.io/blog/exactly-once/</a></p>\n<p>译文:<a href=\"https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw\" target=\"_blank\" rel=\"noopener\">https://mp.weixin.qq.com/s/QjKQcFnbMxBBxFgIQ_f5lw</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Scala 学习笔记之Object","date":"2018-02-27T01:50:01.000Z","_content":"\n### 1. 单例对象\n\nScala没有静态方法或静态字段，可以使用 `object` 来达到这个目的，对象定义了某个类的单个实例:\n```scala\nobject Account{\n  private var lastNumber = 0\n  def newUniqueNumber () = {lastNumber += 1; lastNumber}\n}\n```\n当你在应用程序中需要一个新的唯一账号时，调用 `Account.newUniqueNumber()` 即可．对象的构造器在该对象第一次被使用时调用．在本例中，`Account` 的构造器在 `Account.newUniqueNumber` 的首次调用时执行．如果一个对象从未被使用，那么构造器也不会被执行．\n\n对象本质上可以拥有类的所有特性，但是不能提供构造器参数．\n\n在Scala中可以用对象来实现:\n- 作为存放工具函数或常量的地方\n- 高效的共享单个不可变实例\n- 需要用单个实例来协调某个服务时(参考单例模式)\n\n### 2. 伴生对象\n\n在Java中，通常会用到既有实例方法又有静态方法的类，在Scala中，可以通过类和类同名的 `伴生对象` 来达到同样的目的:\n```scala\nclass Account{\n  val id = Account.newUniqueNumber()\n  private var balance = 0.0\n  def deposit(amount : Double) { balance += amount }\n  ...\n}\n\n// 伴生对象\nobject Account{\n  private var lastNumber = 0\n  def newUniqueNumber () = {lastNumber += 1; lastNumber}  \n}\n```\n类和它的伴生对象可以相互访问私有特性．它们必须在同一个源文件中．\n\n### 3. apply方法\n\n我们通常会定义和使用对象的 `apply` 方法．当遇到如下形式的表达式时，`apply` 方法就会被调用:\n```scala\nObject(参数1，参数2，...，参数N)\n```\n通常，这样一个 `apply` 方法返回的是伴生类的对象．举例来说，Array对象定义了 `apply` 方法，让我们可以用下面这样的表达式来创建数组:\n```scala\nArray(\"Mary\", \"had\", \"a\", \"little\", \"lamb\")\n```\n不使用构造器，而是使用apply方法，对于使用嵌套表达式而言，省去new关键字会方便很多:\n```scala\nArray(Array(1,7), Array(2,9))\n```\n下面有一个定义apply方法的示例:\n```scala\nclass Account private (val id :Int, initialBalance: Double){\n  private var balance = initialBalance\n  ...\n}\n\n// 伴生对象\nobject Account{\n  def apply(initialBalance : Double){\n    new Account(newUniqueNumber(), initialBalance)\n  }\n  ...\n}\n```\n这样我们就可以使用如下方式创建账号了:\n```scala\nval acct = Account(1000.0)\n```\n\n### 4. 应用程序对象\n\n每个Scala程序都必须从一个对象的main方法开始，这个方法的类型为 `Array[String]=>Unit`:\n```scala\nobject Hello{\n  def main(args: Array[String]){\n    println(\"Hello world!\")\n  }\n}\n```\n除了每次都提供自己main方法外，你可以扩展App特质，然后将程序代码放入构造器方法体内:\n```scala\nobject Hello extends App{\n  println(\"Hello world!\")\n}\n```\n如果需要命令行参数，则可以通过args属性得到:\n```scala\nobject Hello extends App{\n  if(args.length > 0){\n    println(\"Hello, \" + args(0))\n  }\n  else{\n    println(\"Hello world!\")\n  }\n}\n```\n### 5. 枚举\n\n不同于Java，Scala中没有枚举类型，需要我们通过标准库类 `Enumeration` 来实现:\n```scala\nobject BusinessType extends Enumeration{\n  var FLIGHT, HOTEL, TRAIN, COACH = Value\n}\n```\n继承 `Enumeration` 类，实现一个 `BusinessType` 对象，并以 `Value` 方法调用初始化枚举中的所有可选值．在这里我们定义了４个业务线类型，然后用Value调用它们初始化．\n\n每次调用Value方法都返回内部类的新实例，该内部类也叫做Value．或者，可以向Value方法传入ID，名称:\n```scala\nval FLIGHT = Value(0, \"FLIGHT\")\nval HOTEL = Value(10) // 名称为\"HOTEL\"\nval TRAIN = Value(\"TRAIN\") // ID为11\n```\n如果不指定ID，ID为上一个枚举值上加一，如果不指定名称，名称默认为字段名．定义完成后，可以使用 `BusinessType.FLIGHT`，`BusinessType.HOTEL`，`BusinessType.TRAIN` 等来引用:\n```scala\ndef businessHandle(business: BusinessType.Value): Unit ={\n  if(business == BusinessType.FLIGHT){\n    println(\"this is a flight behavior\")\n  }\n  else if(business == BusinessType.HOTEL){\n    println(\"this ia a hotel behavior\")\n  }\n}\n\ndef main(args: Array[String]): Unit = {\n  val business = BusinessType.FLIGHT\n  businessHandle(business) // this is a flight behavior\n}\n```\n如果觉的BusinessType.FLIGHT比较冗长繁琐，可以使用如下方式引入枚举值:\n```scala\nimport BusinessType._\n```\n使用时直接使用枚举值名称即可:\n```scala\ndef businessHandle(business: BusinessType.Value): Unit ={\n  if(business == FLIGHT){\n    println(\"this is a flight behavior\")\n  }\n  else if(business == HOTEL){\n    println(\"this ia a hotel behavior\")\n  }\n}\n```\n记住枚举值的类型是BusinessType.Value而不是BusinessType，后者是拥有这些值的对象，可以增加一个类型别名:\n```scala\nobject BusinessType extends Enumeration{\n  type BusinessType = Value\n  var FLIGHT, HOTEL, TRAIN, COACH = Value\n}\n```\n如下使用:\n```scala\ndef businessHandle(business: BusinessType): Unit ={\n  if(business == FLIGHT){\n    println(\"this is a flight behavior\")\n  }\n  else if(business == HOTEL){\n    println(\"this ia a hotel behavior\")\n  }\n}\n```\n\n枚举值的ID可以通过id方法返回，名称通过toString方法返回:\n```scala\nval business = FLIGHT\nprintln(\"ID:\" + business.id + \"   name:\" + business.toString) // ID:0   name:FLIGHT\n```\n\n可以通过如下方式输出所有的枚举值:\n```scala\nfor(business <- BusinessType.values){\n  println(\"ID:\" + business.id + \"   name:\" + business.toString)\n}\n\nID:0   name:FLIGHT\nID:1   name:HOTEL\nID:2   name:TRAIN\nID:3   name:COACH\n```\n\n你也可以通过枚举的ID或名称来进行查找定位:\n```scala\nval FLIGHT1 = BusinessType(0)\nprintln(\"ID:\" + FLIGHT1.id + \"   name:\" + FLIGHT1.toString)\nval FLIGHT2 = BusinessType.withName(\"FLIGHT\")\nprintln(\"ID:\" + FLIGHT2.id + \"   name:\" + FLIGHT2.toString)\n\nID:0   name:FLIGHT\nID:0   name:FLIGHT    \n```\n\n来源于： 快学Scala\n","source":"_posts/Scala/[Scala]Scala学习笔记五 Object.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Scala 学习笔记之Object\ndate: 2018-02-27 09:50:01\ntags:\n  - Scala\n\ncategories: Scala\npermalink: scala-notes-object\n---\n\n### 1. 单例对象\n\nScala没有静态方法或静态字段，可以使用 `object` 来达到这个目的，对象定义了某个类的单个实例:\n```scala\nobject Account{\n  private var lastNumber = 0\n  def newUniqueNumber () = {lastNumber += 1; lastNumber}\n}\n```\n当你在应用程序中需要一个新的唯一账号时，调用 `Account.newUniqueNumber()` 即可．对象的构造器在该对象第一次被使用时调用．在本例中，`Account` 的构造器在 `Account.newUniqueNumber` 的首次调用时执行．如果一个对象从未被使用，那么构造器也不会被执行．\n\n对象本质上可以拥有类的所有特性，但是不能提供构造器参数．\n\n在Scala中可以用对象来实现:\n- 作为存放工具函数或常量的地方\n- 高效的共享单个不可变实例\n- 需要用单个实例来协调某个服务时(参考单例模式)\n\n### 2. 伴生对象\n\n在Java中，通常会用到既有实例方法又有静态方法的类，在Scala中，可以通过类和类同名的 `伴生对象` 来达到同样的目的:\n```scala\nclass Account{\n  val id = Account.newUniqueNumber()\n  private var balance = 0.0\n  def deposit(amount : Double) { balance += amount }\n  ...\n}\n\n// 伴生对象\nobject Account{\n  private var lastNumber = 0\n  def newUniqueNumber () = {lastNumber += 1; lastNumber}  \n}\n```\n类和它的伴生对象可以相互访问私有特性．它们必须在同一个源文件中．\n\n### 3. apply方法\n\n我们通常会定义和使用对象的 `apply` 方法．当遇到如下形式的表达式时，`apply` 方法就会被调用:\n```scala\nObject(参数1，参数2，...，参数N)\n```\n通常，这样一个 `apply` 方法返回的是伴生类的对象．举例来说，Array对象定义了 `apply` 方法，让我们可以用下面这样的表达式来创建数组:\n```scala\nArray(\"Mary\", \"had\", \"a\", \"little\", \"lamb\")\n```\n不使用构造器，而是使用apply方法，对于使用嵌套表达式而言，省去new关键字会方便很多:\n```scala\nArray(Array(1,7), Array(2,9))\n```\n下面有一个定义apply方法的示例:\n```scala\nclass Account private (val id :Int, initialBalance: Double){\n  private var balance = initialBalance\n  ...\n}\n\n// 伴生对象\nobject Account{\n  def apply(initialBalance : Double){\n    new Account(newUniqueNumber(), initialBalance)\n  }\n  ...\n}\n```\n这样我们就可以使用如下方式创建账号了:\n```scala\nval acct = Account(1000.0)\n```\n\n### 4. 应用程序对象\n\n每个Scala程序都必须从一个对象的main方法开始，这个方法的类型为 `Array[String]=>Unit`:\n```scala\nobject Hello{\n  def main(args: Array[String]){\n    println(\"Hello world!\")\n  }\n}\n```\n除了每次都提供自己main方法外，你可以扩展App特质，然后将程序代码放入构造器方法体内:\n```scala\nobject Hello extends App{\n  println(\"Hello world!\")\n}\n```\n如果需要命令行参数，则可以通过args属性得到:\n```scala\nobject Hello extends App{\n  if(args.length > 0){\n    println(\"Hello, \" + args(0))\n  }\n  else{\n    println(\"Hello world!\")\n  }\n}\n```\n### 5. 枚举\n\n不同于Java，Scala中没有枚举类型，需要我们通过标准库类 `Enumeration` 来实现:\n```scala\nobject BusinessType extends Enumeration{\n  var FLIGHT, HOTEL, TRAIN, COACH = Value\n}\n```\n继承 `Enumeration` 类，实现一个 `BusinessType` 对象，并以 `Value` 方法调用初始化枚举中的所有可选值．在这里我们定义了４个业务线类型，然后用Value调用它们初始化．\n\n每次调用Value方法都返回内部类的新实例，该内部类也叫做Value．或者，可以向Value方法传入ID，名称:\n```scala\nval FLIGHT = Value(0, \"FLIGHT\")\nval HOTEL = Value(10) // 名称为\"HOTEL\"\nval TRAIN = Value(\"TRAIN\") // ID为11\n```\n如果不指定ID，ID为上一个枚举值上加一，如果不指定名称，名称默认为字段名．定义完成后，可以使用 `BusinessType.FLIGHT`，`BusinessType.HOTEL`，`BusinessType.TRAIN` 等来引用:\n```scala\ndef businessHandle(business: BusinessType.Value): Unit ={\n  if(business == BusinessType.FLIGHT){\n    println(\"this is a flight behavior\")\n  }\n  else if(business == BusinessType.HOTEL){\n    println(\"this ia a hotel behavior\")\n  }\n}\n\ndef main(args: Array[String]): Unit = {\n  val business = BusinessType.FLIGHT\n  businessHandle(business) // this is a flight behavior\n}\n```\n如果觉的BusinessType.FLIGHT比较冗长繁琐，可以使用如下方式引入枚举值:\n```scala\nimport BusinessType._\n```\n使用时直接使用枚举值名称即可:\n```scala\ndef businessHandle(business: BusinessType.Value): Unit ={\n  if(business == FLIGHT){\n    println(\"this is a flight behavior\")\n  }\n  else if(business == HOTEL){\n    println(\"this ia a hotel behavior\")\n  }\n}\n```\n记住枚举值的类型是BusinessType.Value而不是BusinessType，后者是拥有这些值的对象，可以增加一个类型别名:\n```scala\nobject BusinessType extends Enumeration{\n  type BusinessType = Value\n  var FLIGHT, HOTEL, TRAIN, COACH = Value\n}\n```\n如下使用:\n```scala\ndef businessHandle(business: BusinessType): Unit ={\n  if(business == FLIGHT){\n    println(\"this is a flight behavior\")\n  }\n  else if(business == HOTEL){\n    println(\"this ia a hotel behavior\")\n  }\n}\n```\n\n枚举值的ID可以通过id方法返回，名称通过toString方法返回:\n```scala\nval business = FLIGHT\nprintln(\"ID:\" + business.id + \"   name:\" + business.toString) // ID:0   name:FLIGHT\n```\n\n可以通过如下方式输出所有的枚举值:\n```scala\nfor(business <- BusinessType.values){\n  println(\"ID:\" + business.id + \"   name:\" + business.toString)\n}\n\nID:0   name:FLIGHT\nID:1   name:HOTEL\nID:2   name:TRAIN\nID:3   name:COACH\n```\n\n你也可以通过枚举的ID或名称来进行查找定位:\n```scala\nval FLIGHT1 = BusinessType(0)\nprintln(\"ID:\" + FLIGHT1.id + \"   name:\" + FLIGHT1.toString)\nval FLIGHT2 = BusinessType.withName(\"FLIGHT\")\nprintln(\"ID:\" + FLIGHT2.id + \"   name:\" + FLIGHT2.toString)\n\nID:0   name:FLIGHT\nID:0   name:FLIGHT    \n```\n\n来源于： 快学Scala\n","slug":"scala-notes-object","published":1,"updated":"2018-02-27T05:13:27.687Z","comments":1,"photos":[],"link":"","_id":"cje58tiv9005qordbznkg4nkv","content":"<h3 id=\"1-单例对象\"><a href=\"#1-单例对象\" class=\"headerlink\" title=\"1. 单例对象\"></a>1. 单例对象</h3><p>Scala没有静态方法或静态字段，可以使用 <code>object</code> 来达到这个目的，对象定义了某个类的单个实例:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastNumber = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newUniqueNumber</span> </span>() = &#123;lastNumber += <span class=\"number\">1</span>; lastNumber&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>当你在应用程序中需要一个新的唯一账号时，调用 <code>Account.newUniqueNumber()</code> 即可．对象的构造器在该对象第一次被使用时调用．在本例中，<code>Account</code> 的构造器在 <code>Account.newUniqueNumber</code> 的首次调用时执行．如果一个对象从未被使用，那么构造器也不会被执行．</p>\n<p>对象本质上可以拥有类的所有特性，但是不能提供构造器参数．</p>\n<p>在Scala中可以用对象来实现:</p>\n<ul>\n<li>作为存放工具函数或常量的地方</li>\n<li>高效的共享单个不可变实例</li>\n<li>需要用单个实例来协调某个服务时(参考单例模式)</li>\n</ul>\n<h3 id=\"2-伴生对象\"><a href=\"#2-伴生对象\" class=\"headerlink\" title=\"2. 伴生对象\"></a>2. 伴生对象</h3><p>在Java中，通常会用到既有实例方法又有静态方法的类，在Scala中，可以通过类和类同名的 <code>伴生对象</code> 来达到同样的目的:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> id = <span class=\"type\">Account</span>.newUniqueNumber()</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> balance = <span class=\"number\">0.0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deposit</span></span>(amount : <span class=\"type\">Double</span>) &#123; balance += amount &#125;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 伴生对象</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastNumber = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newUniqueNumber</span> </span>() = &#123;lastNumber += <span class=\"number\">1</span>; lastNumber&#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>类和它的伴生对象可以相互访问私有特性．它们必须在同一个源文件中．</p>\n<h3 id=\"3-apply方法\"><a href=\"#3-apply方法\" class=\"headerlink\" title=\"3. apply方法\"></a>3. apply方法</h3><p>我们通常会定义和使用对象的 <code>apply</code> 方法．当遇到如下形式的表达式时，<code>apply</code> 方法就会被调用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Object</span>(参数<span class=\"number\">1</span>，参数<span class=\"number\">2</span>，...，参数<span class=\"type\">N</span>)</span><br></pre></td></tr></table></figure></p>\n<p>通常，这样一个 <code>apply</code> 方法返回的是伴生类的对象．举例来说，Array对象定义了 <code>apply</code> 方法，让我们可以用下面这样的表达式来创建数组:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Array</span>(<span class=\"string\">\"Mary\"</span>, <span class=\"string\">\"had\"</span>, <span class=\"string\">\"a\"</span>, <span class=\"string\">\"little\"</span>, <span class=\"string\">\"lamb\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>不使用构造器，而是使用apply方法，对于使用嵌套表达式而言，省去new关键字会方便很多:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Array</span>(<span class=\"type\">Array</span>(<span class=\"number\">1</span>,<span class=\"number\">7</span>), <span class=\"type\">Array</span>(<span class=\"number\">2</span>,<span class=\"number\">9</span>))</span><br></pre></td></tr></table></figure></p>\n<p>下面有一个定义apply方法的示例:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Account</span> <span class=\"title\">private</span> (<span class=\"params\">val id :<span class=\"type\">Int</span>, initialBalance: <span class=\"type\">Double</span></span>)</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> balance = initialBalance</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 伴生对象</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(initialBalance : <span class=\"type\">Double</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Account</span>(newUniqueNumber(), initialBalance)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这样我们就可以使用如下方式创建账号了:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> acct = <span class=\"type\">Account</span>(<span class=\"number\">1000.0</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-应用程序对象\"><a href=\"#4-应用程序对象\" class=\"headerlink\" title=\"4. 应用程序对象\"></a>4. 应用程序对象</h3><p>每个Scala程序都必须从一个对象的main方法开始，这个方法的类型为 <code>Array[String]=&gt;Unit</code>:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Hello</span></span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>])&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"Hello world!\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>除了每次都提供自己main方法外，你可以扩展App特质，然后将程序代码放入构造器方法体内:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Hello</span> <span class=\"keyword\">extends</span> <span class=\"title\">App</span></span>&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"Hello world!\"</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果需要命令行参数，则可以通过args属性得到:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Hello</span> <span class=\"keyword\">extends</span> <span class=\"title\">App</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(args.length &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"Hello, \"</span> + args(<span class=\"number\">0</span>))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"Hello world!\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-枚举\"><a href=\"#5-枚举\" class=\"headerlink\" title=\"5. 枚举\"></a>5. 枚举</h3><p>不同于Java，Scala中没有枚举类型，需要我们通过标准库类 <code>Enumeration</code> 来实现:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">BusinessType</span> <span class=\"keyword\">extends</span> <span class=\"title\">Enumeration</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> <span class=\"type\">FLIGHT</span>, <span class=\"type\">HOTEL</span>, <span class=\"type\">TRAIN</span>, <span class=\"type\">COACH</span> = <span class=\"type\">Value</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继承 <code>Enumeration</code> 类，实现一个 <code>BusinessType</code> 对象，并以 <code>Value</code> 方法调用初始化枚举中的所有可选值．在这里我们定义了４个业务线类型，然后用Value调用它们初始化．</p>\n<p>每次调用Value方法都返回内部类的新实例，该内部类也叫做Value．或者，可以向Value方法传入ID，名称:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">FLIGHT</span> = <span class=\"type\">Value</span>(<span class=\"number\">0</span>, <span class=\"string\">\"FLIGHT\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">HOTEL</span> = <span class=\"type\">Value</span>(<span class=\"number\">10</span>) <span class=\"comment\">// 名称为\"HOTEL\"</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">TRAIN</span> = <span class=\"type\">Value</span>(<span class=\"string\">\"TRAIN\"</span>) <span class=\"comment\">// ID为11</span></span><br></pre></td></tr></table></figure></p>\n<p>如果不指定ID，ID为上一个枚举值上加一，如果不指定名称，名称默认为字段名．定义完成后，可以使用 <code>BusinessType.FLIGHT</code>，<code>BusinessType.HOTEL</code>，<code>BusinessType.TRAIN</code> 等来引用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">businessHandle</span></span>(business: <span class=\"type\">BusinessType</span>.<span class=\"type\">Value</span>): <span class=\"type\">Unit</span> =&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(business == <span class=\"type\">BusinessType</span>.<span class=\"type\">FLIGHT</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this is a flight behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(business == <span class=\"type\">BusinessType</span>.<span class=\"type\">HOTEL</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this ia a hotel behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> business = <span class=\"type\">BusinessType</span>.<span class=\"type\">FLIGHT</span></span><br><span class=\"line\">  businessHandle(business) <span class=\"comment\">// this is a flight behavior</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果觉的BusinessType.FLIGHT比较冗长繁琐，可以使用如下方式引入枚举值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">BusinessType</span>._</span><br></pre></td></tr></table></figure></p>\n<p>使用时直接使用枚举值名称即可:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">businessHandle</span></span>(business: <span class=\"type\">BusinessType</span>.<span class=\"type\">Value</span>): <span class=\"type\">Unit</span> =&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(business == <span class=\"type\">FLIGHT</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this is a flight behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(business == <span class=\"type\">HOTEL</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this ia a hotel behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>记住枚举值的类型是BusinessType.Value而不是BusinessType，后者是拥有这些值的对象，可以增加一个类型别名:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">BusinessType</span> <span class=\"keyword\">extends</span> <span class=\"title\">Enumeration</span></span>&#123;</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">BusinessType</span> </span>= <span class=\"type\">Value</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> <span class=\"type\">FLIGHT</span>, <span class=\"type\">HOTEL</span>, <span class=\"type\">TRAIN</span>, <span class=\"type\">COACH</span> = <span class=\"type\">Value</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如下使用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">businessHandle</span></span>(business: <span class=\"type\">BusinessType</span>): <span class=\"type\">Unit</span> =&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(business == <span class=\"type\">FLIGHT</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this is a flight behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(business == <span class=\"type\">HOTEL</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this ia a hotel behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>枚举值的ID可以通过id方法返回，名称通过toString方法返回:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> business = <span class=\"type\">FLIGHT</span></span><br><span class=\"line\">println(<span class=\"string\">\"ID:\"</span> + business.id + <span class=\"string\">\"   name:\"</span> + business.toString) <span class=\"comment\">// ID:0   name:FLIGHT</span></span><br></pre></td></tr></table></figure></p>\n<p>可以通过如下方式输出所有的枚举值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(business &lt;- <span class=\"type\">BusinessType</span>.values)&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"ID:\"</span> + business.id + <span class=\"string\">\"   name:\"</span> + business.toString)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">0</span>   name:<span class=\"type\">FLIGHT</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">1</span>   name:<span class=\"type\">HOTEL</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">2</span>   name:<span class=\"type\">TRAIN</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">3</span>   name:<span class=\"type\">COACH</span></span><br></pre></td></tr></table></figure></p>\n<p>你也可以通过枚举的ID或名称来进行查找定位:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">FLIGHT1</span> = <span class=\"type\">BusinessType</span>(<span class=\"number\">0</span>)</span><br><span class=\"line\">println(<span class=\"string\">\"ID:\"</span> + <span class=\"type\">FLIGHT1</span>.id + <span class=\"string\">\"   name:\"</span> + <span class=\"type\">FLIGHT1</span>.toString)</span><br><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">FLIGHT2</span> = <span class=\"type\">BusinessType</span>.withName(<span class=\"string\">\"FLIGHT\"</span>)</span><br><span class=\"line\">println(<span class=\"string\">\"ID:\"</span> + <span class=\"type\">FLIGHT2</span>.id + <span class=\"string\">\"   name:\"</span> + <span class=\"type\">FLIGHT2</span>.toString)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">0</span>   name:<span class=\"type\">FLIGHT</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">0</span>   name:<span class=\"type\">FLIGHT</span></span><br></pre></td></tr></table></figure></p>\n<p>来源于： 快学Scala</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-单例对象\"><a href=\"#1-单例对象\" class=\"headerlink\" title=\"1. 单例对象\"></a>1. 单例对象</h3><p>Scala没有静态方法或静态字段，可以使用 <code>object</code> 来达到这个目的，对象定义了某个类的单个实例:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastNumber = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newUniqueNumber</span> </span>() = &#123;lastNumber += <span class=\"number\">1</span>; lastNumber&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>当你在应用程序中需要一个新的唯一账号时，调用 <code>Account.newUniqueNumber()</code> 即可．对象的构造器在该对象第一次被使用时调用．在本例中，<code>Account</code> 的构造器在 <code>Account.newUniqueNumber</code> 的首次调用时执行．如果一个对象从未被使用，那么构造器也不会被执行．</p>\n<p>对象本质上可以拥有类的所有特性，但是不能提供构造器参数．</p>\n<p>在Scala中可以用对象来实现:</p>\n<ul>\n<li>作为存放工具函数或常量的地方</li>\n<li>高效的共享单个不可变实例</li>\n<li>需要用单个实例来协调某个服务时(参考单例模式)</li>\n</ul>\n<h3 id=\"2-伴生对象\"><a href=\"#2-伴生对象\" class=\"headerlink\" title=\"2. 伴生对象\"></a>2. 伴生对象</h3><p>在Java中，通常会用到既有实例方法又有静态方法的类，在Scala中，可以通过类和类同名的 <code>伴生对象</code> 来达到同样的目的:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> id = <span class=\"type\">Account</span>.newUniqueNumber()</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> balance = <span class=\"number\">0.0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deposit</span></span>(amount : <span class=\"type\">Double</span>) &#123; balance += amount &#125;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 伴生对象</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> lastNumber = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newUniqueNumber</span> </span>() = &#123;lastNumber += <span class=\"number\">1</span>; lastNumber&#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>类和它的伴生对象可以相互访问私有特性．它们必须在同一个源文件中．</p>\n<h3 id=\"3-apply方法\"><a href=\"#3-apply方法\" class=\"headerlink\" title=\"3. apply方法\"></a>3. apply方法</h3><p>我们通常会定义和使用对象的 <code>apply</code> 方法．当遇到如下形式的表达式时，<code>apply</code> 方法就会被调用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Object</span>(参数<span class=\"number\">1</span>，参数<span class=\"number\">2</span>，...，参数<span class=\"type\">N</span>)</span><br></pre></td></tr></table></figure></p>\n<p>通常，这样一个 <code>apply</code> 方法返回的是伴生类的对象．举例来说，Array对象定义了 <code>apply</code> 方法，让我们可以用下面这样的表达式来创建数组:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Array</span>(<span class=\"string\">\"Mary\"</span>, <span class=\"string\">\"had\"</span>, <span class=\"string\">\"a\"</span>, <span class=\"string\">\"little\"</span>, <span class=\"string\">\"lamb\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>不使用构造器，而是使用apply方法，对于使用嵌套表达式而言，省去new关键字会方便很多:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Array</span>(<span class=\"type\">Array</span>(<span class=\"number\">1</span>,<span class=\"number\">7</span>), <span class=\"type\">Array</span>(<span class=\"number\">2</span>,<span class=\"number\">9</span>))</span><br></pre></td></tr></table></figure></p>\n<p>下面有一个定义apply方法的示例:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Account</span> <span class=\"title\">private</span> (<span class=\"params\">val id :<span class=\"type\">Int</span>, initialBalance: <span class=\"type\">Double</span></span>)</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> balance = initialBalance</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 伴生对象</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Account</span></span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(initialBalance : <span class=\"type\">Double</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Account</span>(newUniqueNumber(), initialBalance)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这样我们就可以使用如下方式创建账号了:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> acct = <span class=\"type\">Account</span>(<span class=\"number\">1000.0</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-应用程序对象\"><a href=\"#4-应用程序对象\" class=\"headerlink\" title=\"4. 应用程序对象\"></a>4. 应用程序对象</h3><p>每个Scala程序都必须从一个对象的main方法开始，这个方法的类型为 <code>Array[String]=&gt;Unit</code>:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Hello</span></span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>])&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"Hello world!\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>除了每次都提供自己main方法外，你可以扩展App特质，然后将程序代码放入构造器方法体内:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Hello</span> <span class=\"keyword\">extends</span> <span class=\"title\">App</span></span>&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"Hello world!\"</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果需要命令行参数，则可以通过args属性得到:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Hello</span> <span class=\"keyword\">extends</span> <span class=\"title\">App</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(args.length &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"Hello, \"</span> + args(<span class=\"number\">0</span>))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"Hello world!\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"5-枚举\"><a href=\"#5-枚举\" class=\"headerlink\" title=\"5. 枚举\"></a>5. 枚举</h3><p>不同于Java，Scala中没有枚举类型，需要我们通过标准库类 <code>Enumeration</code> 来实现:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">BusinessType</span> <span class=\"keyword\">extends</span> <span class=\"title\">Enumeration</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> <span class=\"type\">FLIGHT</span>, <span class=\"type\">HOTEL</span>, <span class=\"type\">TRAIN</span>, <span class=\"type\">COACH</span> = <span class=\"type\">Value</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继承 <code>Enumeration</code> 类，实现一个 <code>BusinessType</code> 对象，并以 <code>Value</code> 方法调用初始化枚举中的所有可选值．在这里我们定义了４个业务线类型，然后用Value调用它们初始化．</p>\n<p>每次调用Value方法都返回内部类的新实例，该内部类也叫做Value．或者，可以向Value方法传入ID，名称:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">FLIGHT</span> = <span class=\"type\">Value</span>(<span class=\"number\">0</span>, <span class=\"string\">\"FLIGHT\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">HOTEL</span> = <span class=\"type\">Value</span>(<span class=\"number\">10</span>) <span class=\"comment\">// 名称为\"HOTEL\"</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">TRAIN</span> = <span class=\"type\">Value</span>(<span class=\"string\">\"TRAIN\"</span>) <span class=\"comment\">// ID为11</span></span><br></pre></td></tr></table></figure></p>\n<p>如果不指定ID，ID为上一个枚举值上加一，如果不指定名称，名称默认为字段名．定义完成后，可以使用 <code>BusinessType.FLIGHT</code>，<code>BusinessType.HOTEL</code>，<code>BusinessType.TRAIN</code> 等来引用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">businessHandle</span></span>(business: <span class=\"type\">BusinessType</span>.<span class=\"type\">Value</span>): <span class=\"type\">Unit</span> =&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(business == <span class=\"type\">BusinessType</span>.<span class=\"type\">FLIGHT</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this is a flight behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(business == <span class=\"type\">BusinessType</span>.<span class=\"type\">HOTEL</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this ia a hotel behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> business = <span class=\"type\">BusinessType</span>.<span class=\"type\">FLIGHT</span></span><br><span class=\"line\">  businessHandle(business) <span class=\"comment\">// this is a flight behavior</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果觉的BusinessType.FLIGHT比较冗长繁琐，可以使用如下方式引入枚举值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">BusinessType</span>._</span><br></pre></td></tr></table></figure></p>\n<p>使用时直接使用枚举值名称即可:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">businessHandle</span></span>(business: <span class=\"type\">BusinessType</span>.<span class=\"type\">Value</span>): <span class=\"type\">Unit</span> =&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(business == <span class=\"type\">FLIGHT</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this is a flight behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(business == <span class=\"type\">HOTEL</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this ia a hotel behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>记住枚举值的类型是BusinessType.Value而不是BusinessType，后者是拥有这些值的对象，可以增加一个类型别名:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">BusinessType</span> <span class=\"keyword\">extends</span> <span class=\"title\">Enumeration</span></span>&#123;</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">BusinessType</span> </span>= <span class=\"type\">Value</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> <span class=\"type\">FLIGHT</span>, <span class=\"type\">HOTEL</span>, <span class=\"type\">TRAIN</span>, <span class=\"type\">COACH</span> = <span class=\"type\">Value</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如下使用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">businessHandle</span></span>(business: <span class=\"type\">BusinessType</span>): <span class=\"type\">Unit</span> =&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span>(business == <span class=\"type\">FLIGHT</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this is a flight behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(business == <span class=\"type\">HOTEL</span>)&#123;</span><br><span class=\"line\">    println(<span class=\"string\">\"this ia a hotel behavior\"</span>)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>枚举值的ID可以通过id方法返回，名称通过toString方法返回:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> business = <span class=\"type\">FLIGHT</span></span><br><span class=\"line\">println(<span class=\"string\">\"ID:\"</span> + business.id + <span class=\"string\">\"   name:\"</span> + business.toString) <span class=\"comment\">// ID:0   name:FLIGHT</span></span><br></pre></td></tr></table></figure></p>\n<p>可以通过如下方式输出所有的枚举值:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(business &lt;- <span class=\"type\">BusinessType</span>.values)&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"ID:\"</span> + business.id + <span class=\"string\">\"   name:\"</span> + business.toString)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">0</span>   name:<span class=\"type\">FLIGHT</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">1</span>   name:<span class=\"type\">HOTEL</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">2</span>   name:<span class=\"type\">TRAIN</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">3</span>   name:<span class=\"type\">COACH</span></span><br></pre></td></tr></table></figure></p>\n<p>你也可以通过枚举的ID或名称来进行查找定位:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">FLIGHT1</span> = <span class=\"type\">BusinessType</span>(<span class=\"number\">0</span>)</span><br><span class=\"line\">println(<span class=\"string\">\"ID:\"</span> + <span class=\"type\">FLIGHT1</span>.id + <span class=\"string\">\"   name:\"</span> + <span class=\"type\">FLIGHT1</span>.toString)</span><br><span class=\"line\"><span class=\"keyword\">val</span> <span class=\"type\">FLIGHT2</span> = <span class=\"type\">BusinessType</span>.withName(<span class=\"string\">\"FLIGHT\"</span>)</span><br><span class=\"line\">println(<span class=\"string\">\"ID:\"</span> + <span class=\"type\">FLIGHT2</span>.id + <span class=\"string\">\"   name:\"</span> + <span class=\"type\">FLIGHT2</span>.toString)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">0</span>   name:<span class=\"type\">FLIGHT</span></span><br><span class=\"line\"><span class=\"type\">ID</span>:<span class=\"number\">0</span>   name:<span class=\"type\">FLIGHT</span></span><br></pre></td></tr></table></figure></p>\n<p>来源于： 快学Scala</p>\n"},{"layout":"post","author":"sjf0115","title":"Scala 学习笔记之类","date":"2018-02-27T01:45:01.000Z","_content":"\n### 1. 简单类与无参方法\n\n```scala\nclass Person {\n  var age = 0 // 必须初始化字段\n  def getAge() = age // 方法默认为公有的\n}\n```\n> 备注\n\n在Scala中，类并不声明为public． Scala源文件可以包含多个类，所有这些类都具有公有可见性．属性不声明默认为public．\n\n使用类:\n```scala\nval p = new Person // 或者new Person()\np.age = 23\nprintln(p.getAge()) // 23\n```\n调用无参方法时，可以写上圆括号，也可以不写:\n```scala\np.getAge() // 23\np.getAge   // 23\n```\n### 2. 带getter和setter的属性\n\n#### 2.1 Java getter和setter\n\n在Java类中，我们并不喜欢使用公有字段:\n```java\npublic class Person{\n  public int age; // Java中不推荐使用这种方式\n}\n```\n更倾向于使用getter和setter方法:\n```java\npublic class Person{\n  private int age;\n  public int getAge() {return age;}\n  public void setAge(int age) {this.age = age;}\n}\n```\n像这样的一对getter/setter通常被称为属性．我们会说Person类有一个age属性．\n\n#### 2.２ Scala getter和setter\n\n在Scala中对每个字段都提供了getter和setter方法:\n```scala\nclass Person{\n  var age = 0\n}\n```\nscala生成面向JVM的类，会生成一个私有的age字段以及相应的getter和setter方法．这两个方法都是公有的，因为我们没有将age声明为private．(对于私有字段而言,getter和setter方法也是私有的)　\n\n在scala中getter和setter方法分别叫`age`和`age_=`．使用的时候如下:\n```scala\nval p = new Person\np.age = 21 // 调用p.age_=(21)\nprintln(p.age) // 调用p.age()方法\n```\n\n> 备注\n\n在scala中，getter和setter方法并非被命名为getXXX和setXXX，不过用意相同．\n\n任何时候我们都可以自己重新定义getter和setter方法:\n```scala\nclass Person {\n  private var privateAge = 0\n  def age = privateAge\n  def age_= (newAge : Int): Unit = {\n    if(newAge > 150){\n      privateAge = 150\n    }\n    else if(newAge < 0){\n      privateAge = 0\n    }\n  }\n}\n```\n使用:\n```\nval p = new Person\np.age = -1;\nprintln(p.age) // 0\np.age = 189\nprintln(p.age) // 150\n```\n\n> 备注\n\nScala对每个字段生成getter和setter方法听上去有些恐怖，不过你可以控制这个过程:\n- 如果字段是私有的，则getter和setter方法也是私有的\n- 如果字段是val，则只有getter方法被生成\n- 如果你不需要任何的getter和setter方法，可以将字段声明为`private[this]`\n\n#### 2.3 Example\n\n(1) 对于公有字段,getter和setter方法是公有的:\n```scala\nclass Student {\n  var age = 22\n}\n\nval stu = new Student\nstu.age = 23\nprintln(stu.age) // 23\n```\n(2) 对于私有字段,getter和setter方法是私有的:\n```scala\nclass Student {\n  private var age = 22\n}\n\nval stu = new Student\n//stu.age = 23 // symbol age is inaccessible from this place\n//println(stu.age) // symbol age is inaccessible from this place\n```\n\n(3) 如果字段是val，则只有getter方法被生成:\n```scala\nclass Student {\n  val age = 22\n}\n\nval stu = new Student\n// stu.age = 23 // reassignment to val\nprintln(stu.age) // 22\n```\n\n### 3. 只带getter的属性\n\n如果只想需要一个只读的属性，有getter但没有setter，属性的值在对象构建完成之后就不再改变了，可以使用val字段:\n```scala\nclass Student {\n  val age = 22\n}\n```\nScala会生成一个私有的final字段和一个getter方法，但没有setter方法\n\n### 4. 对象私有字段\n\n在Scala中，方法可以访问该类的`所有对象`的私有字段:\n```scala\nclass Counter {\n  private var value = 0\n  def increment(): Unit = {\n    value += 1\n  }\n  // 对象可以访问另一个对象的私有字段\n  def isLess (other : Counter) = value < other.value\n}\n```\n之所以访问 `other.value` 是合法的，是因为 `other` 也是Counter对象，这与Java的private权限不一样.\n\nScala允许我们定义更加严格的访问限制，通过private[this]这个修饰符来实现:\n```scala\nprivate[this] var value = 0\n```\n这样 `other.value` 是不被允许访问的，这样以来Counter类只能访问当前对象的value字段，而不能访问同样是Counter类型的其他对象的字段．\n\nScala允许你将访问权限赋予指定得类，private[类名]可以定义仅有指定类的方法可以访问给定的字段．这里的类名必须是当前定义的类，或者是包含该类的外部类．\n\n> 备注\n\n对于类私有的字段(private)，Scala会生成私有的getter和setter方法，但是对于对象私有的字段，不会生成getter和setter方法．\n\n### 5. Bean属性\n\nScala对于你定义的字段提供了getter和setter方法，但是并不是Java工具所期望的．JavaBeans规范把Java属性定义为一对getXXX/setXXX方法．很多Java工具都依赖这样的命令习惯．\n\nScala给我们提供了@BeanProperty注解，这样就会字段生成我们期望的getXXX和setXXX方法:\n```scala\nclass Student {\n  @BeanProperty\n  var age = 22\n}\n\nval stu = new Student\nstu.setAge(25)\nprintln(stu.getAge()) // 25\n```\n\n> 总结\n\nscala字段|生成的方法|何时使用\n---|---|---\nval/var name|公有的`name` `name_=`(仅限var)|实现一个可以被公开访问并且背后是以字段形式保存的属性\n@BeanProperty val/var name | 公有的`name` `getName()` `name_=`(仅限var) `setName()` (仅限var) | 与JavaBeans互操作\nprivate val/var name|私有的`name` `name_=`(仅限var)|用于将字段访问限制在本类的方法．尽量使用private，除非真的需要一个公有属性\nprivate[this] val/var name | 无 | 用于将字段访问限制在同一个对象上调用的方法．不经常用\nprivate[类名] val/var name | 依赖于具体实现 | 将访问权限赋予外部类．不经常使用\n\n### 6. 辅助构造器\n\nScala可以有任意多的构造器，不过，Scala有一个构造器比其他所有构造器都重要，就是主构造器，除了主构造器之外，类还有任意多的辅助构造器．其同Java中的构造器十分相似，只有两处不同:\n- 辅助构造器的名称为`this`\n- 每一个辅助构造器都必须以一个先前已定义的其他辅助构造器或主构造器的调用开始\n\n```scala\nclass Person {\n  private var name = \"\"\n  private var age = 0\n\n  def this (name : String){\n    this() // 调用主构造器\n    this.name = name\n  }\n\n  def this (name : String, age : Int){\n    this(name) // 调用前一个辅助构造器\n    this.age = age\n  }\n}\n```\n可以使用如下三种方式构造对象:\n```scala\nval p1 = new Person // 调用主构造器\nval p2 = new Person(\"Bob\") // 调用第一个辅助构造器\nval p3 = new Person(\"Bob\", 25) // 调用第二个辅助构造器\n```\n### 7. 主构造器\n\n在Scala中，每个类都有主构造器．主构造器并不以this方法定义，而是与类定义交织在一起．\n\n(1) 主构造器的参数直接放在类名之后\n```scala\nclass Person(val name:String) {\n  private var age = 0\n  def this (name : String, age : Int){\n    this(name) // 调用主构造器\n    this.age = age\n  }\n}\n```\n主构造器的参数被编译成字段，其值被初始化成构造时传入的参数．上述示例中name和age为Person类的字段．\n\n(2) 主构造器会执行类定义中的所有语句\n```scala\nclass Person(val name:String) {\n  println(\"constructed a person ...\")\n  private var age = 0\n\n  def this (name : String, age : Int){\n    this(name) // 调用主构造器\n    this.age = age\n  }\n}\n```\nprintln语句是主构造器的一部分．每当有对象被构造出来时．上述代码就会被执行\n\n(3) 通常可以在主构造器中使用默认参数来避免使用过多的辅助构造器\n```scala\nclass Person(val name:String = \"\", val age: Int = 0) {\n}\n```\n\n\n> 备注\n\n如果类名之后没有参数，则该类具备一个无参主构造器.\n\n\n来源于： 快学Scala\n","source":"_posts/Scala/[Scala]Scala学习笔记四 类.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Scala 学习笔记之类\ndate: 2018-02-27 09:45:01\ntags:\n  - Scala\n\ncategories: Scala\npermalink: scala-notes-class\n---\n\n### 1. 简单类与无参方法\n\n```scala\nclass Person {\n  var age = 0 // 必须初始化字段\n  def getAge() = age // 方法默认为公有的\n}\n```\n> 备注\n\n在Scala中，类并不声明为public． Scala源文件可以包含多个类，所有这些类都具有公有可见性．属性不声明默认为public．\n\n使用类:\n```scala\nval p = new Person // 或者new Person()\np.age = 23\nprintln(p.getAge()) // 23\n```\n调用无参方法时，可以写上圆括号，也可以不写:\n```scala\np.getAge() // 23\np.getAge   // 23\n```\n### 2. 带getter和setter的属性\n\n#### 2.1 Java getter和setter\n\n在Java类中，我们并不喜欢使用公有字段:\n```java\npublic class Person{\n  public int age; // Java中不推荐使用这种方式\n}\n```\n更倾向于使用getter和setter方法:\n```java\npublic class Person{\n  private int age;\n  public int getAge() {return age;}\n  public void setAge(int age) {this.age = age;}\n}\n```\n像这样的一对getter/setter通常被称为属性．我们会说Person类有一个age属性．\n\n#### 2.２ Scala getter和setter\n\n在Scala中对每个字段都提供了getter和setter方法:\n```scala\nclass Person{\n  var age = 0\n}\n```\nscala生成面向JVM的类，会生成一个私有的age字段以及相应的getter和setter方法．这两个方法都是公有的，因为我们没有将age声明为private．(对于私有字段而言,getter和setter方法也是私有的)　\n\n在scala中getter和setter方法分别叫`age`和`age_=`．使用的时候如下:\n```scala\nval p = new Person\np.age = 21 // 调用p.age_=(21)\nprintln(p.age) // 调用p.age()方法\n```\n\n> 备注\n\n在scala中，getter和setter方法并非被命名为getXXX和setXXX，不过用意相同．\n\n任何时候我们都可以自己重新定义getter和setter方法:\n```scala\nclass Person {\n  private var privateAge = 0\n  def age = privateAge\n  def age_= (newAge : Int): Unit = {\n    if(newAge > 150){\n      privateAge = 150\n    }\n    else if(newAge < 0){\n      privateAge = 0\n    }\n  }\n}\n```\n使用:\n```\nval p = new Person\np.age = -1;\nprintln(p.age) // 0\np.age = 189\nprintln(p.age) // 150\n```\n\n> 备注\n\nScala对每个字段生成getter和setter方法听上去有些恐怖，不过你可以控制这个过程:\n- 如果字段是私有的，则getter和setter方法也是私有的\n- 如果字段是val，则只有getter方法被生成\n- 如果你不需要任何的getter和setter方法，可以将字段声明为`private[this]`\n\n#### 2.3 Example\n\n(1) 对于公有字段,getter和setter方法是公有的:\n```scala\nclass Student {\n  var age = 22\n}\n\nval stu = new Student\nstu.age = 23\nprintln(stu.age) // 23\n```\n(2) 对于私有字段,getter和setter方法是私有的:\n```scala\nclass Student {\n  private var age = 22\n}\n\nval stu = new Student\n//stu.age = 23 // symbol age is inaccessible from this place\n//println(stu.age) // symbol age is inaccessible from this place\n```\n\n(3) 如果字段是val，则只有getter方法被生成:\n```scala\nclass Student {\n  val age = 22\n}\n\nval stu = new Student\n// stu.age = 23 // reassignment to val\nprintln(stu.age) // 22\n```\n\n### 3. 只带getter的属性\n\n如果只想需要一个只读的属性，有getter但没有setter，属性的值在对象构建完成之后就不再改变了，可以使用val字段:\n```scala\nclass Student {\n  val age = 22\n}\n```\nScala会生成一个私有的final字段和一个getter方法，但没有setter方法\n\n### 4. 对象私有字段\n\n在Scala中，方法可以访问该类的`所有对象`的私有字段:\n```scala\nclass Counter {\n  private var value = 0\n  def increment(): Unit = {\n    value += 1\n  }\n  // 对象可以访问另一个对象的私有字段\n  def isLess (other : Counter) = value < other.value\n}\n```\n之所以访问 `other.value` 是合法的，是因为 `other` 也是Counter对象，这与Java的private权限不一样.\n\nScala允许我们定义更加严格的访问限制，通过private[this]这个修饰符来实现:\n```scala\nprivate[this] var value = 0\n```\n这样 `other.value` 是不被允许访问的，这样以来Counter类只能访问当前对象的value字段，而不能访问同样是Counter类型的其他对象的字段．\n\nScala允许你将访问权限赋予指定得类，private[类名]可以定义仅有指定类的方法可以访问给定的字段．这里的类名必须是当前定义的类，或者是包含该类的外部类．\n\n> 备注\n\n对于类私有的字段(private)，Scala会生成私有的getter和setter方法，但是对于对象私有的字段，不会生成getter和setter方法．\n\n### 5. Bean属性\n\nScala对于你定义的字段提供了getter和setter方法，但是并不是Java工具所期望的．JavaBeans规范把Java属性定义为一对getXXX/setXXX方法．很多Java工具都依赖这样的命令习惯．\n\nScala给我们提供了@BeanProperty注解，这样就会字段生成我们期望的getXXX和setXXX方法:\n```scala\nclass Student {\n  @BeanProperty\n  var age = 22\n}\n\nval stu = new Student\nstu.setAge(25)\nprintln(stu.getAge()) // 25\n```\n\n> 总结\n\nscala字段|生成的方法|何时使用\n---|---|---\nval/var name|公有的`name` `name_=`(仅限var)|实现一个可以被公开访问并且背后是以字段形式保存的属性\n@BeanProperty val/var name | 公有的`name` `getName()` `name_=`(仅限var) `setName()` (仅限var) | 与JavaBeans互操作\nprivate val/var name|私有的`name` `name_=`(仅限var)|用于将字段访问限制在本类的方法．尽量使用private，除非真的需要一个公有属性\nprivate[this] val/var name | 无 | 用于将字段访问限制在同一个对象上调用的方法．不经常用\nprivate[类名] val/var name | 依赖于具体实现 | 将访问权限赋予外部类．不经常使用\n\n### 6. 辅助构造器\n\nScala可以有任意多的构造器，不过，Scala有一个构造器比其他所有构造器都重要，就是主构造器，除了主构造器之外，类还有任意多的辅助构造器．其同Java中的构造器十分相似，只有两处不同:\n- 辅助构造器的名称为`this`\n- 每一个辅助构造器都必须以一个先前已定义的其他辅助构造器或主构造器的调用开始\n\n```scala\nclass Person {\n  private var name = \"\"\n  private var age = 0\n\n  def this (name : String){\n    this() // 调用主构造器\n    this.name = name\n  }\n\n  def this (name : String, age : Int){\n    this(name) // 调用前一个辅助构造器\n    this.age = age\n  }\n}\n```\n可以使用如下三种方式构造对象:\n```scala\nval p1 = new Person // 调用主构造器\nval p2 = new Person(\"Bob\") // 调用第一个辅助构造器\nval p3 = new Person(\"Bob\", 25) // 调用第二个辅助构造器\n```\n### 7. 主构造器\n\n在Scala中，每个类都有主构造器．主构造器并不以this方法定义，而是与类定义交织在一起．\n\n(1) 主构造器的参数直接放在类名之后\n```scala\nclass Person(val name:String) {\n  private var age = 0\n  def this (name : String, age : Int){\n    this(name) // 调用主构造器\n    this.age = age\n  }\n}\n```\n主构造器的参数被编译成字段，其值被初始化成构造时传入的参数．上述示例中name和age为Person类的字段．\n\n(2) 主构造器会执行类定义中的所有语句\n```scala\nclass Person(val name:String) {\n  println(\"constructed a person ...\")\n  private var age = 0\n\n  def this (name : String, age : Int){\n    this(name) // 调用主构造器\n    this.age = age\n  }\n}\n```\nprintln语句是主构造器的一部分．每当有对象被构造出来时．上述代码就会被执行\n\n(3) 通常可以在主构造器中使用默认参数来避免使用过多的辅助构造器\n```scala\nclass Person(val name:String = \"\", val age: Int = 0) {\n}\n```\n\n\n> 备注\n\n如果类名之后没有参数，则该类具备一个无参主构造器.\n\n\n来源于： 快学Scala\n","slug":"scala-notes-class","published":1,"updated":"2018-02-27T05:59:18.557Z","comments":1,"photos":[],"link":"","_id":"cje58tivb005sordb5nefohxg","content":"<h3 id=\"1-简单类与无参方法\"><a href=\"#1-简单类与无参方法\" class=\"headerlink\" title=\"1. 简单类与无参方法\"></a>1. 简单类与无参方法</h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">0</span> <span class=\"comment\">// 必须初始化字段</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAge</span></span>() = age <span class=\"comment\">// 方法默认为公有的</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>在Scala中，类并不声明为public． Scala源文件可以包含多个类，所有这些类都具有公有可见性．属性不声明默认为public．</p>\n<p>使用类:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> p = <span class=\"keyword\">new</span> <span class=\"type\">Person</span> <span class=\"comment\">// 或者new Person()</span></span><br><span class=\"line\">p.age = <span class=\"number\">23</span></span><br><span class=\"line\">println(p.getAge()) <span class=\"comment\">// 23</span></span><br></pre></td></tr></table></figure></p>\n<p>调用无参方法时，可以写上圆括号，也可以不写:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">p.getAge() <span class=\"comment\">// 23</span></span><br><span class=\"line\">p.getAge   <span class=\"comment\">// 23</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-带getter和setter的属性\"><a href=\"#2-带getter和setter的属性\" class=\"headerlink\" title=\"2. 带getter和setter的属性\"></a>2. 带getter和setter的属性</h3><h4 id=\"2-1-Java-getter和setter\"><a href=\"#2-1-Java-getter和setter\" class=\"headerlink\" title=\"2.1 Java getter和setter\"></a>2.1 Java getter和setter</h4><p>在Java类中，我们并不喜欢使用公有字段:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> age; <span class=\"comment\">// Java中不推荐使用这种方式</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>更倾向于使用getter和setter方法:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> age;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getAge</span><span class=\"params\">()</span> </span>&#123;<span class=\"keyword\">return</span> age;&#125;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setAge</span><span class=\"params\">(<span class=\"keyword\">int</span> age)</span> </span>&#123;<span class=\"keyword\">this</span>.age = age;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>像这样的一对getter/setter通常被称为属性．我们会说Person类有一个age属性．</p>\n<h4 id=\"2-２-Scala-getter和setter\"><a href=\"#2-２-Scala-getter和setter\" class=\"headerlink\" title=\"2.２ Scala getter和setter\"></a>2.２ Scala getter和setter</h4><p>在Scala中对每个字段都提供了getter和setter方法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>scala生成面向JVM的类，会生成一个私有的age字段以及相应的getter和setter方法．这两个方法都是公有的，因为我们没有将age声明为private．(对于私有字段而言,getter和setter方法也是私有的)　</p>\n<p>在scala中getter和setter方法分别叫<code>age</code>和<code>age_=</code>．使用的时候如下:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> p = <span class=\"keyword\">new</span> <span class=\"type\">Person</span></span><br><span class=\"line\">p.age = <span class=\"number\">21</span> <span class=\"comment\">// 调用p.age_=(21)</span></span><br><span class=\"line\">println(p.age) <span class=\"comment\">// 调用p.age()方法</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>在scala中，getter和setter方法并非被命名为getXXX和setXXX，不过用意相同．</p>\n<p>任何时候我们都可以自己重新定义getter和setter方法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> privateAge = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">age</span> </span>= privateAge</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">age_=</span> </span>(newAge : <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(newAge &gt; <span class=\"number\">150</span>)&#123;</span><br><span class=\"line\">      privateAge = <span class=\"number\">150</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(newAge &lt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">      privateAge = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>使用:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val p = new Person</span><br><span class=\"line\">p.age = -1;</span><br><span class=\"line\">println(p.age) // 0</span><br><span class=\"line\">p.age = 189</span><br><span class=\"line\">println(p.age) // 150</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>Scala对每个字段生成getter和setter方法听上去有些恐怖，不过你可以控制这个过程:</p>\n<ul>\n<li>如果字段是私有的，则getter和setter方法也是私有的</li>\n<li>如果字段是val，则只有getter方法被生成</li>\n<li>如果你不需要任何的getter和setter方法，可以将字段声明为<code>private[this]</code></li>\n</ul>\n<h4 id=\"2-3-Example\"><a href=\"#2-3-Example\" class=\"headerlink\" title=\"2.3 Example\"></a>2.3 Example</h4><p>(1) 对于公有字段,getter和setter方法是公有的:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\">stu.age = <span class=\"number\">23</span></span><br><span class=\"line\">println(stu.age) <span class=\"comment\">// 23</span></span><br></pre></td></tr></table></figure></p>\n<p>(2) 对于私有字段,getter和setter方法是私有的:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\"><span class=\"comment\">//stu.age = 23 // symbol age is inaccessible from this place</span></span><br><span class=\"line\"><span class=\"comment\">//println(stu.age) // symbol age is inaccessible from this place</span></span><br></pre></td></tr></table></figure></p>\n<p>(3) 如果字段是val，则只有getter方法被生成:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\"><span class=\"comment\">// stu.age = 23 // reassignment to val</span></span><br><span class=\"line\">println(stu.age) <span class=\"comment\">// 22</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-只带getter的属性\"><a href=\"#3-只带getter的属性\" class=\"headerlink\" title=\"3. 只带getter的属性\"></a>3. 只带getter的属性</h3><p>如果只想需要一个只读的属性，有getter但没有setter，属性的值在对象构建完成之后就不再改变了，可以使用val字段:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala会生成一个私有的final字段和一个getter方法，但没有setter方法</p>\n<h3 id=\"4-对象私有字段\"><a href=\"#4-对象私有字段\" class=\"headerlink\" title=\"4. 对象私有字段\"></a>4. 对象私有字段</h3><p>在Scala中，方法可以访问该类的<code>所有对象</code>的私有字段:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Counter</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> value = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">increment</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    value += <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 对象可以访问另一个对象的私有字段</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isLess</span> </span>(other : <span class=\"type\">Counter</span>) = value &lt; other.value</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>之所以访问 <code>other.value</code> 是合法的，是因为 <code>other</code> 也是Counter对象，这与Java的private权限不一样.</p>\n<p>Scala允许我们定义更加严格的访问限制，通过private[this]这个修饰符来实现:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> value = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p>这样 <code>other.value</code> 是不被允许访问的，这样以来Counter类只能访问当前对象的value字段，而不能访问同样是Counter类型的其他对象的字段．</p>\n<p>Scala允许你将访问权限赋予指定得类，private[类名]可以定义仅有指定类的方法可以访问给定的字段．这里的类名必须是当前定义的类，或者是包含该类的外部类．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>对于类私有的字段(private)，Scala会生成私有的getter和setter方法，但是对于对象私有的字段，不会生成getter和setter方法．</p>\n<h3 id=\"5-Bean属性\"><a href=\"#5-Bean属性\" class=\"headerlink\" title=\"5. Bean属性\"></a>5. Bean属性</h3><p>Scala对于你定义的字段提供了getter和setter方法，但是并不是Java工具所期望的．JavaBeans规范把Java属性定义为一对getXXX/setXXX方法．很多Java工具都依赖这样的命令习惯．</p>\n<p>Scala给我们提供了@BeanProperty注解，这样就会字段生成我们期望的getXXX和setXXX方法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"meta\">@BeanProperty</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\">stu.setAge(<span class=\"number\">25</span>)</span><br><span class=\"line\">println(stu.getAge()) <span class=\"comment\">// 25</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>总结</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>scala字段</th>\n<th>生成的方法</th>\n<th>何时使用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>val/var name</td>\n<td>公有的<code>name</code> <code>name_=</code>(仅限var)</td>\n<td>实现一个可以被公开访问并且背后是以字段形式保存的属性</td>\n</tr>\n<tr>\n<td>@BeanProperty val/var name</td>\n<td>公有的<code>name</code> <code>getName()</code> <code>name_=</code>(仅限var) <code>setName()</code> (仅限var)</td>\n<td>与JavaBeans互操作</td>\n</tr>\n<tr>\n<td>private val/var name</td>\n<td>私有的<code>name</code> <code>name_=</code>(仅限var)</td>\n<td>用于将字段访问限制在本类的方法．尽量使用private，除非真的需要一个公有属性</td>\n</tr>\n<tr>\n<td>private[this] val/var name</td>\n<td>无</td>\n<td>用于将字段访问限制在同一个对象上调用的方法．不经常用</td>\n</tr>\n<tr>\n<td>private[类名] val/var name</td>\n<td>依赖于具体实现</td>\n<td>将访问权限赋予外部类．不经常使用</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"6-辅助构造器\"><a href=\"#6-辅助构造器\" class=\"headerlink\" title=\"6. 辅助构造器\"></a>6. 辅助构造器</h3><p>Scala可以有任意多的构造器，不过，Scala有一个构造器比其他所有构造器都重要，就是主构造器，除了主构造器之外，类还有任意多的辅助构造器．其同Java中的构造器十分相似，只有两处不同:</p>\n<ul>\n<li>辅助构造器的名称为<code>this</code></li>\n<li>每一个辅助构造器都必须以一个先前已定义的其他辅助构造器或主构造器的调用开始</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> name = <span class=\"string\">\"\"</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>() <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.name = name</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>, age : <span class=\"type\">Int</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(name) <span class=\"comment\">// 调用前一个辅助构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.age = age</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以使用如下三种方式构造对象:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> p1 = <span class=\"keyword\">new</span> <span class=\"type\">Person</span> <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> p2 = <span class=\"keyword\">new</span> <span class=\"type\">Person</span>(<span class=\"string\">\"Bob\"</span>) <span class=\"comment\">// 调用第一个辅助构造器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> p3 = <span class=\"keyword\">new</span> <span class=\"type\">Person</span>(<span class=\"string\">\"Bob\"</span>, <span class=\"number\">25</span>) <span class=\"comment\">// 调用第二个辅助构造器</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-主构造器\"><a href=\"#7-主构造器\" class=\"headerlink\" title=\"7. 主构造器\"></a>7. 主构造器</h3><p>在Scala中，每个类都有主构造器．主构造器并不以this方法定义，而是与类定义交织在一起．</p>\n<p>(1) 主构造器的参数直接放在类名之后<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val name:<span class=\"type\">String</span></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>, age : <span class=\"type\">Int</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(name) <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.age = age</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>主构造器的参数被编译成字段，其值被初始化成构造时传入的参数．上述示例中name和age为Person类的字段．</p>\n<p>(2) 主构造器会执行类定义中的所有语句<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val name:<span class=\"type\">String</span></span>) </span>&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"constructed a person ...\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>, age : <span class=\"type\">Int</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(name) <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.age = age</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>println语句是主构造器的一部分．每当有对象被构造出来时．上述代码就会被执行</p>\n<p>(3) 通常可以在主构造器中使用默认参数来避免使用过多的辅助构造器<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val name:<span class=\"type\">String</span> = \"\", val age: <span class=\"type\">Int</span> = 0</span>) </span>&#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>如果类名之后没有参数，则该类具备一个无参主构造器.</p>\n<p>来源于： 快学Scala</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-简单类与无参方法\"><a href=\"#1-简单类与无参方法\" class=\"headerlink\" title=\"1. 简单类与无参方法\"></a>1. 简单类与无参方法</h3><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">0</span> <span class=\"comment\">// 必须初始化字段</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAge</span></span>() = age <span class=\"comment\">// 方法默认为公有的</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>在Scala中，类并不声明为public． Scala源文件可以包含多个类，所有这些类都具有公有可见性．属性不声明默认为public．</p>\n<p>使用类:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> p = <span class=\"keyword\">new</span> <span class=\"type\">Person</span> <span class=\"comment\">// 或者new Person()</span></span><br><span class=\"line\">p.age = <span class=\"number\">23</span></span><br><span class=\"line\">println(p.getAge()) <span class=\"comment\">// 23</span></span><br></pre></td></tr></table></figure></p>\n<p>调用无参方法时，可以写上圆括号，也可以不写:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">p.getAge() <span class=\"comment\">// 23</span></span><br><span class=\"line\">p.getAge   <span class=\"comment\">// 23</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-带getter和setter的属性\"><a href=\"#2-带getter和setter的属性\" class=\"headerlink\" title=\"2. 带getter和setter的属性\"></a>2. 带getter和setter的属性</h3><h4 id=\"2-1-Java-getter和setter\"><a href=\"#2-1-Java-getter和setter\" class=\"headerlink\" title=\"2.1 Java getter和setter\"></a>2.1 Java getter和setter</h4><p>在Java类中，我们并不喜欢使用公有字段:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> age; <span class=\"comment\">// Java中不推荐使用这种方式</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>更倾向于使用getter和setter方法:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> age;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getAge</span><span class=\"params\">()</span> </span>&#123;<span class=\"keyword\">return</span> age;&#125;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setAge</span><span class=\"params\">(<span class=\"keyword\">int</span> age)</span> </span>&#123;<span class=\"keyword\">this</span>.age = age;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>像这样的一对getter/setter通常被称为属性．我们会说Person类有一个age属性．</p>\n<h4 id=\"2-２-Scala-getter和setter\"><a href=\"#2-２-Scala-getter和setter\" class=\"headerlink\" title=\"2.２ Scala getter和setter\"></a>2.２ Scala getter和setter</h4><p>在Scala中对每个字段都提供了getter和setter方法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>scala生成面向JVM的类，会生成一个私有的age字段以及相应的getter和setter方法．这两个方法都是公有的，因为我们没有将age声明为private．(对于私有字段而言,getter和setter方法也是私有的)　</p>\n<p>在scala中getter和setter方法分别叫<code>age</code>和<code>age_=</code>．使用的时候如下:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> p = <span class=\"keyword\">new</span> <span class=\"type\">Person</span></span><br><span class=\"line\">p.age = <span class=\"number\">21</span> <span class=\"comment\">// 调用p.age_=(21)</span></span><br><span class=\"line\">println(p.age) <span class=\"comment\">// 调用p.age()方法</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>在scala中，getter和setter方法并非被命名为getXXX和setXXX，不过用意相同．</p>\n<p>任何时候我们都可以自己重新定义getter和setter方法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> privateAge = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">age</span> </span>= privateAge</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">age_=</span> </span>(newAge : <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(newAge &gt; <span class=\"number\">150</span>)&#123;</span><br><span class=\"line\">      privateAge = <span class=\"number\">150</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(newAge &lt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">      privateAge = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>使用:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val p = new Person</span><br><span class=\"line\">p.age = -1;</span><br><span class=\"line\">println(p.age) // 0</span><br><span class=\"line\">p.age = 189</span><br><span class=\"line\">println(p.age) // 150</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>Scala对每个字段生成getter和setter方法听上去有些恐怖，不过你可以控制这个过程:</p>\n<ul>\n<li>如果字段是私有的，则getter和setter方法也是私有的</li>\n<li>如果字段是val，则只有getter方法被生成</li>\n<li>如果你不需要任何的getter和setter方法，可以将字段声明为<code>private[this]</code></li>\n</ul>\n<h4 id=\"2-3-Example\"><a href=\"#2-3-Example\" class=\"headerlink\" title=\"2.3 Example\"></a>2.3 Example</h4><p>(1) 对于公有字段,getter和setter方法是公有的:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\">stu.age = <span class=\"number\">23</span></span><br><span class=\"line\">println(stu.age) <span class=\"comment\">// 23</span></span><br></pre></td></tr></table></figure></p>\n<p>(2) 对于私有字段,getter和setter方法是私有的:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\"><span class=\"comment\">//stu.age = 23 // symbol age is inaccessible from this place</span></span><br><span class=\"line\"><span class=\"comment\">//println(stu.age) // symbol age is inaccessible from this place</span></span><br></pre></td></tr></table></figure></p>\n<p>(3) 如果字段是val，则只有getter方法被生成:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\"><span class=\"comment\">// stu.age = 23 // reassignment to val</span></span><br><span class=\"line\">println(stu.age) <span class=\"comment\">// 22</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-只带getter的属性\"><a href=\"#3-只带getter的属性\" class=\"headerlink\" title=\"3. 只带getter的属性\"></a>3. 只带getter的属性</h3><p>如果只想需要一个只读的属性，有getter但没有setter，属性的值在对象构建完成之后就不再改变了，可以使用val字段:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala会生成一个私有的final字段和一个getter方法，但没有setter方法</p>\n<h3 id=\"4-对象私有字段\"><a href=\"#4-对象私有字段\" class=\"headerlink\" title=\"4. 对象私有字段\"></a>4. 对象私有字段</h3><p>在Scala中，方法可以访问该类的<code>所有对象</code>的私有字段:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Counter</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> value = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">increment</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    value += <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 对象可以访问另一个对象的私有字段</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isLess</span> </span>(other : <span class=\"type\">Counter</span>) = value &lt; other.value</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>之所以访问 <code>other.value</code> 是合法的，是因为 <code>other</code> 也是Counter对象，这与Java的private权限不一样.</p>\n<p>Scala允许我们定义更加严格的访问限制，通过private[this]这个修饰符来实现:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> value = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p>这样 <code>other.value</code> 是不被允许访问的，这样以来Counter类只能访问当前对象的value字段，而不能访问同样是Counter类型的其他对象的字段．</p>\n<p>Scala允许你将访问权限赋予指定得类，private[类名]可以定义仅有指定类的方法可以访问给定的字段．这里的类名必须是当前定义的类，或者是包含该类的外部类．</p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>对于类私有的字段(private)，Scala会生成私有的getter和setter方法，但是对于对象私有的字段，不会生成getter和setter方法．</p>\n<h3 id=\"5-Bean属性\"><a href=\"#5-Bean属性\" class=\"headerlink\" title=\"5. Bean属性\"></a>5. Bean属性</h3><p>Scala对于你定义的字段提供了getter和setter方法，但是并不是Java工具所期望的．JavaBeans规范把Java属性定义为一对getXXX/setXXX方法．很多Java工具都依赖这样的命令习惯．</p>\n<p>Scala给我们提供了@BeanProperty注解，这样就会字段生成我们期望的getXXX和setXXX方法:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"meta\">@BeanProperty</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> age = <span class=\"number\">22</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> stu = <span class=\"keyword\">new</span> <span class=\"type\">Student</span></span><br><span class=\"line\">stu.setAge(<span class=\"number\">25</span>)</span><br><span class=\"line\">println(stu.getAge()) <span class=\"comment\">// 25</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>总结</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>scala字段</th>\n<th>生成的方法</th>\n<th>何时使用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>val/var name</td>\n<td>公有的<code>name</code> <code>name_=</code>(仅限var)</td>\n<td>实现一个可以被公开访问并且背后是以字段形式保存的属性</td>\n</tr>\n<tr>\n<td>@BeanProperty val/var name</td>\n<td>公有的<code>name</code> <code>getName()</code> <code>name_=</code>(仅限var) <code>setName()</code> (仅限var)</td>\n<td>与JavaBeans互操作</td>\n</tr>\n<tr>\n<td>private val/var name</td>\n<td>私有的<code>name</code> <code>name_=</code>(仅限var)</td>\n<td>用于将字段访问限制在本类的方法．尽量使用private，除非真的需要一个公有属性</td>\n</tr>\n<tr>\n<td>private[this] val/var name</td>\n<td>无</td>\n<td>用于将字段访问限制在同一个对象上调用的方法．不经常用</td>\n</tr>\n<tr>\n<td>private[类名] val/var name</td>\n<td>依赖于具体实现</td>\n<td>将访问权限赋予外部类．不经常使用</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"6-辅助构造器\"><a href=\"#6-辅助构造器\" class=\"headerlink\" title=\"6. 辅助构造器\"></a>6. 辅助构造器</h3><p>Scala可以有任意多的构造器，不过，Scala有一个构造器比其他所有构造器都重要，就是主构造器，除了主构造器之外，类还有任意多的辅助构造器．其同Java中的构造器十分相似，只有两处不同:</p>\n<ul>\n<li>辅助构造器的名称为<code>this</code></li>\n<li>每一个辅助构造器都必须以一个先前已定义的其他辅助构造器或主构造器的调用开始</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> name = <span class=\"string\">\"\"</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>() <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.name = name</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>, age : <span class=\"type\">Int</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(name) <span class=\"comment\">// 调用前一个辅助构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.age = age</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以使用如下三种方式构造对象:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> p1 = <span class=\"keyword\">new</span> <span class=\"type\">Person</span> <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> p2 = <span class=\"keyword\">new</span> <span class=\"type\">Person</span>(<span class=\"string\">\"Bob\"</span>) <span class=\"comment\">// 调用第一个辅助构造器</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> p3 = <span class=\"keyword\">new</span> <span class=\"type\">Person</span>(<span class=\"string\">\"Bob\"</span>, <span class=\"number\">25</span>) <span class=\"comment\">// 调用第二个辅助构造器</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"7-主构造器\"><a href=\"#7-主构造器\" class=\"headerlink\" title=\"7. 主构造器\"></a>7. 主构造器</h3><p>在Scala中，每个类都有主构造器．主构造器并不以this方法定义，而是与类定义交织在一起．</p>\n<p>(1) 主构造器的参数直接放在类名之后<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val name:<span class=\"type\">String</span></span>) </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>, age : <span class=\"type\">Int</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(name) <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.age = age</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>主构造器的参数被编译成字段，其值被初始化成构造时传入的参数．上述示例中name和age为Person类的字段．</p>\n<p>(2) 主构造器会执行类定义中的所有语句<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val name:<span class=\"type\">String</span></span>) </span>&#123;</span><br><span class=\"line\">  println(<span class=\"string\">\"constructed a person ...\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> age = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span> </span>(name : <span class=\"type\">String</span>, age : <span class=\"type\">Int</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>(name) <span class=\"comment\">// 调用主构造器</span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>.age = age</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>println语句是主构造器的一部分．每当有对象被构造出来时．上述代码就会被执行</p>\n<p>(3) 通常可以在主构造器中使用默认参数来避免使用过多的辅助构造器<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val name:<span class=\"type\">String</span> = \"\", val age: <span class=\"type\">Int</span> = 0</span>) </span>&#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p>如果类名之后没有参数，则该类具备一个无参主构造器.</p>\n<p>来源于： 快学Scala</p>\n"},{"layout":"post","author":"sjf0115","title":"Scala 学习笔记之数组","date":"2018-02-27T01:36:01.000Z","_content":"\n### 1. 定长数组\n\n如果你需要一个长度不变的数组，可以使用Scala中的 `Array`．\n```scala\nval nums = new Array[Int](10) // 10个整数的数组　所有元素初始化为0\nval strs = new Array[String](10) // 10个字符串的数组　所有元素初始化为null\nval s = Array(\"Hello\", \"World\") // 长度为2的Array[String] 类型是推断出来的　\ns(0) = \"GoodBye\" // Array(\"GoodBye\"，\"World\")\n```\n\n> 备注\n\n- 已提供初始值时不要使用`new`，例如上面的数组s\n- 使用`()`而不是`[]`来访问元素\n- 在JVM中，Scala的Array以Java数组方式实现．\n\n### 2. 变长数组\n\n对于那种长度按需要变化的数组，Java有 `ArrayList`．Scala中等效数据结构为 `ArrayBuffer`．\n```scala\nimport scala.collection.mutable.ArrayBuffer\nval b = ArrayBuffer[Int]() // 或者 new ArrayBuffer[Int]() 创建一个空的数组缓冲来存放整数\nb += 1　// ArrayBuffer(1) 用+=在尾端添加元素\nb += (1,2,3,5) // ArrayBuffer(1,1,2,3,5) 在尾端添加多个元素\nb ++= Array(8, 13, 21) // ArrayBuffer(1,1,2,3,5,8,13,21) 用++=操作追加任何集合\nb.trimEnd(5) // ArrayBuffer(1,1,2) 移除最后５个元素\n```\n可以在任意位置插入或移除元素，但这样的操作不如在尾端添加或移除元素操作那么高效:\n```scala\nb.insert(2,6) // ArrayBuffer(1,1,6,2) 在下标2之前插入\nb.insert(2,7,8,9) // ArrayBuffer(1,1,7,8,9,6,2) 插入任意多的元素\nb.remove(2) // ArrayBuffer(1,1,8,9,6,2) 删除下标２的元素\nb.remove(2,3) // ArrayBuffer(1,1,2) 第二个参数的含义是要移除多少个元素\n```\n有时需要构建一个Array，但不知道最终需要装多少元素．这种情况下可以先构建一个数组缓冲，然后调用:\n```scala\nb.toArray // Array(1,1,2)\n```\n### 3. 遍历数组和数组缓冲\n\n使用for循环遍历数组和数组缓冲:\n```scala\nval b = Array(6,5,4,3,2,1)\nfor(i <- 0 until b.length){\n  println(i + \"-\" + b(i))\n}\n```\n输出结果:\n```\n0-6\n1-5\n2-4\n3-3\n4-2\n5-1\n```\n> 备注\n\n`until` 是 `RichInt` 类的方法，返回所有小于(但不包括)上限的数字\n\n如果想要每两个元素一跳，可以让i这样来进行遍历:\n```scala\nval b = Array(6,5,4,3,2,1)\nfor(i <- 0 until (b.length, 2)){\n  println(i + \"-\" + b(i))\n}\n```\n输出结果：\n```\n0-6\n2-4\n4-2\n```\n如果要从数组的尾端开始:\n```scala\nval b = Array(6,5,4,3,2,1)\nfor(i <- (0 until b.length).reverse){\n  println(i + \"-\" + b(i))\n}\n```\n如果在循环体中不需要用到数组下标，我们也可以直接访问数组元素:\n```scala\nfor(elem <- b){\n  println(elem)\n}\n```\n### 4. 数组转换\n\n从一个数组(数组缓冲)出发，以某种方式对它进行转换是很简单的．这些转换操作不会修改原是数组，而是产生一个全新的数组:\n```scala\nval a = Array(1,2,3,4)\nval result = for(elem <- a) yield 2 * elem // result 是Array(2,4,6,8)\n```\n`for(...) yield`循环创建了一个类型与原实际和相同的新集合．新元素为yield之后的表达式的值，每次迭代对应一个．\n\n当你遍历一个集合时，如果只想处理满足特定条件的元素．可以通过for中的if来实现:\n```scala\nval a = Array(1,2,3,4)\nval result = for(elem <- a if elem % 2 == 0) yield 2 * elem\n```\n上面实例中对每个偶数元素翻倍，并丢掉奇数元素．\n\n### 5. 常用操作\n\n#### 5.1 sum\n```scala\nval a = Array(6,1,7,4)\na.sum // 18\n```\n要使用sum方法，元素类型必须是数值类型:整型，浮点数或者BigInteger/BigDecimal\n\n#### 5.2 min max\n```scala\nval a = Array(6,1,7,4)\na.min // 1\na.max // 7\n```\nmin和max输出数组或数组缓冲中最小和最大的元素\n\n#### 5.3 sorted\n```scala\nval a = Array(6,1,7,4)\nval asorted = a.sorted // Array(1, 4, 6, 7)\n\nval a = ArrayBuffer(6,1,7,4)\nval asorted = a.sortWith(_ > _) // ArrayBuffer(7, 6, 4, 1)\n```\nsorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，不会修改原始数组．可以使用sortWith方法提供一个比较函数．\n\n#### 5.4 mkString\n\n```scala\nval a = Array(6,1,7,4)\na.mkString(\" and \") // 6 and 1 and 7 and 4\n```\n如果想要显示数组或者数组缓冲的内容，可以使用`mkString`，允许指定元素之间的分隔符\n```scala\nval a = Array(6,1,7,4)\na.mkString(\"<\", \",\", \">\") // <6,1,7,4>\n```\n该方法的另一个重载版本可以让你指定前缀和后缀\n\n来源于: 快学Scala\n","source":"_posts/Scala/[Scala]Scala学习笔记二 数组.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Scala 学习笔记之数组\ndate: 2018-02-27 09:36:01\ntags:\n  - Scala\n\ncategories: Scala\npermalink: scala-notes-array\n---\n\n### 1. 定长数组\n\n如果你需要一个长度不变的数组，可以使用Scala中的 `Array`．\n```scala\nval nums = new Array[Int](10) // 10个整数的数组　所有元素初始化为0\nval strs = new Array[String](10) // 10个字符串的数组　所有元素初始化为null\nval s = Array(\"Hello\", \"World\") // 长度为2的Array[String] 类型是推断出来的　\ns(0) = \"GoodBye\" // Array(\"GoodBye\"，\"World\")\n```\n\n> 备注\n\n- 已提供初始值时不要使用`new`，例如上面的数组s\n- 使用`()`而不是`[]`来访问元素\n- 在JVM中，Scala的Array以Java数组方式实现．\n\n### 2. 变长数组\n\n对于那种长度按需要变化的数组，Java有 `ArrayList`．Scala中等效数据结构为 `ArrayBuffer`．\n```scala\nimport scala.collection.mutable.ArrayBuffer\nval b = ArrayBuffer[Int]() // 或者 new ArrayBuffer[Int]() 创建一个空的数组缓冲来存放整数\nb += 1　// ArrayBuffer(1) 用+=在尾端添加元素\nb += (1,2,3,5) // ArrayBuffer(1,1,2,3,5) 在尾端添加多个元素\nb ++= Array(8, 13, 21) // ArrayBuffer(1,1,2,3,5,8,13,21) 用++=操作追加任何集合\nb.trimEnd(5) // ArrayBuffer(1,1,2) 移除最后５个元素\n```\n可以在任意位置插入或移除元素，但这样的操作不如在尾端添加或移除元素操作那么高效:\n```scala\nb.insert(2,6) // ArrayBuffer(1,1,6,2) 在下标2之前插入\nb.insert(2,7,8,9) // ArrayBuffer(1,1,7,8,9,6,2) 插入任意多的元素\nb.remove(2) // ArrayBuffer(1,1,8,9,6,2) 删除下标２的元素\nb.remove(2,3) // ArrayBuffer(1,1,2) 第二个参数的含义是要移除多少个元素\n```\n有时需要构建一个Array，但不知道最终需要装多少元素．这种情况下可以先构建一个数组缓冲，然后调用:\n```scala\nb.toArray // Array(1,1,2)\n```\n### 3. 遍历数组和数组缓冲\n\n使用for循环遍历数组和数组缓冲:\n```scala\nval b = Array(6,5,4,3,2,1)\nfor(i <- 0 until b.length){\n  println(i + \"-\" + b(i))\n}\n```\n输出结果:\n```\n0-6\n1-5\n2-4\n3-3\n4-2\n5-1\n```\n> 备注\n\n`until` 是 `RichInt` 类的方法，返回所有小于(但不包括)上限的数字\n\n如果想要每两个元素一跳，可以让i这样来进行遍历:\n```scala\nval b = Array(6,5,4,3,2,1)\nfor(i <- 0 until (b.length, 2)){\n  println(i + \"-\" + b(i))\n}\n```\n输出结果：\n```\n0-6\n2-4\n4-2\n```\n如果要从数组的尾端开始:\n```scala\nval b = Array(6,5,4,3,2,1)\nfor(i <- (0 until b.length).reverse){\n  println(i + \"-\" + b(i))\n}\n```\n如果在循环体中不需要用到数组下标，我们也可以直接访问数组元素:\n```scala\nfor(elem <- b){\n  println(elem)\n}\n```\n### 4. 数组转换\n\n从一个数组(数组缓冲)出发，以某种方式对它进行转换是很简单的．这些转换操作不会修改原是数组，而是产生一个全新的数组:\n```scala\nval a = Array(1,2,3,4)\nval result = for(elem <- a) yield 2 * elem // result 是Array(2,4,6,8)\n```\n`for(...) yield`循环创建了一个类型与原实际和相同的新集合．新元素为yield之后的表达式的值，每次迭代对应一个．\n\n当你遍历一个集合时，如果只想处理满足特定条件的元素．可以通过for中的if来实现:\n```scala\nval a = Array(1,2,3,4)\nval result = for(elem <- a if elem % 2 == 0) yield 2 * elem\n```\n上面实例中对每个偶数元素翻倍，并丢掉奇数元素．\n\n### 5. 常用操作\n\n#### 5.1 sum\n```scala\nval a = Array(6,1,7,4)\na.sum // 18\n```\n要使用sum方法，元素类型必须是数值类型:整型，浮点数或者BigInteger/BigDecimal\n\n#### 5.2 min max\n```scala\nval a = Array(6,1,7,4)\na.min // 1\na.max // 7\n```\nmin和max输出数组或数组缓冲中最小和最大的元素\n\n#### 5.3 sorted\n```scala\nval a = Array(6,1,7,4)\nval asorted = a.sorted // Array(1, 4, 6, 7)\n\nval a = ArrayBuffer(6,1,7,4)\nval asorted = a.sortWith(_ > _) // ArrayBuffer(7, 6, 4, 1)\n```\nsorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，不会修改原始数组．可以使用sortWith方法提供一个比较函数．\n\n#### 5.4 mkString\n\n```scala\nval a = Array(6,1,7,4)\na.mkString(\" and \") // 6 and 1 and 7 and 4\n```\n如果想要显示数组或者数组缓冲的内容，可以使用`mkString`，允许指定元素之间的分隔符\n```scala\nval a = Array(6,1,7,4)\na.mkString(\"<\", \",\", \">\") // <6,1,7,4>\n```\n该方法的另一个重载版本可以让你指定前缀和后缀\n\n来源于: 快学Scala\n","slug":"scala-notes-array","published":1,"updated":"2018-02-27T02:32:03.477Z","comments":1,"photos":[],"link":"","_id":"cje58tivf005xordbkfguvodv","content":"<h3 id=\"1-定长数组\"><a href=\"#1-定长数组\" class=\"headerlink\" title=\"1. 定长数组\"></a>1. 定长数组</h3><p>如果你需要一个长度不变的数组，可以使用Scala中的 <code>Array</code>．<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> nums = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Int</span>](<span class=\"number\">10</span>) <span class=\"comment\">// 10个整数的数组　所有元素初始化为0</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> strs = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">String</span>](<span class=\"number\">10</span>) <span class=\"comment\">// 10个字符串的数组　所有元素初始化为null</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> s = <span class=\"type\">Array</span>(<span class=\"string\">\"Hello\"</span>, <span class=\"string\">\"World\"</span>) <span class=\"comment\">// 长度为2的Array[String] 类型是推断出来的　</span></span><br><span class=\"line\">s(<span class=\"number\">0</span>) = <span class=\"string\">\"GoodBye\"</span> <span class=\"comment\">// Array(\"GoodBye\"，\"World\")</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<ul>\n<li>已提供初始值时不要使用<code>new</code>，例如上面的数组s</li>\n<li>使用<code>()</code>而不是<code>[]</code>来访问元素</li>\n<li>在JVM中，Scala的Array以Java数组方式实现．</li>\n</ul>\n<h3 id=\"2-变长数组\"><a href=\"#2-变长数组\" class=\"headerlink\" title=\"2. 变长数组\"></a>2. 变长数组</h3><p>对于那种长度按需要变化的数组，Java有 <code>ArrayList</code>．Scala中等效数据结构为 <code>ArrayBuffer</code>．<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.mutable.<span class=\"type\">ArrayBuffer</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Int</span>]() <span class=\"comment\">// 或者 new ArrayBuffer[Int]() 创建一个空的数组缓冲来存放整数</span></span><br><span class=\"line\">b += <span class=\"number\">1</span>　<span class=\"comment\">// ArrayBuffer(1) 用+=在尾端添加元素</span></span><br><span class=\"line\">b += (<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>) <span class=\"comment\">// ArrayBuffer(1,1,2,3,5) 在尾端添加多个元素</span></span><br><span class=\"line\">b ++= <span class=\"type\">Array</span>(<span class=\"number\">8</span>, <span class=\"number\">13</span>, <span class=\"number\">21</span>) <span class=\"comment\">// ArrayBuffer(1,1,2,3,5,8,13,21) 用++=操作追加任何集合</span></span><br><span class=\"line\">b.trimEnd(<span class=\"number\">5</span>) <span class=\"comment\">// ArrayBuffer(1,1,2) 移除最后５个元素</span></span><br></pre></td></tr></table></figure></p>\n<p>可以在任意位置插入或移除元素，但这样的操作不如在尾端添加或移除元素操作那么高效:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">b.insert(<span class=\"number\">2</span>,<span class=\"number\">6</span>) <span class=\"comment\">// ArrayBuffer(1,1,6,2) 在下标2之前插入</span></span><br><span class=\"line\">b.insert(<span class=\"number\">2</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>) <span class=\"comment\">// ArrayBuffer(1,1,7,8,9,6,2) 插入任意多的元素</span></span><br><span class=\"line\">b.remove(<span class=\"number\">2</span>) <span class=\"comment\">// ArrayBuffer(1,1,8,9,6,2) 删除下标２的元素</span></span><br><span class=\"line\">b.remove(<span class=\"number\">2</span>,<span class=\"number\">3</span>) <span class=\"comment\">// ArrayBuffer(1,1,2) 第二个参数的含义是要移除多少个元素</span></span><br></pre></td></tr></table></figure></p>\n<p>有时需要构建一个Array，但不知道最终需要装多少元素．这种情况下可以先构建一个数组缓冲，然后调用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">b.toArray <span class=\"comment\">// Array(1,1,2)</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-遍历数组和数组缓冲\"><a href=\"#3-遍历数组和数组缓冲\" class=\"headerlink\" title=\"3. 遍历数组和数组缓冲\"></a>3. 遍历数组和数组缓冲</h3><p>使用for循环遍历数组和数组缓冲:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until b.length)&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\"-\"</span> + b(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>输出结果:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">0-6</span><br><span class=\"line\">1-5</span><br><span class=\"line\">2-4</span><br><span class=\"line\">3-3</span><br><span class=\"line\">4-2</span><br><span class=\"line\">5-1</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p><code>until</code> 是 <code>RichInt</code> 类的方法，返回所有小于(但不包括)上限的数字</p>\n<p>如果想要每两个元素一跳，可以让i这样来进行遍历:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until (b.length, <span class=\"number\">2</span>))&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\"-\"</span> + b(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">0-6</span><br><span class=\"line\">2-4</span><br><span class=\"line\">4-2</span><br></pre></td></tr></table></figure></p>\n<p>如果要从数组的尾端开始:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- (<span class=\"number\">0</span> until b.length).reverse)&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\"-\"</span> + b(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果在循环体中不需要用到数组下标，我们也可以直接访问数组元素:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(elem &lt;- b)&#123;</span><br><span class=\"line\">  println(elem)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-数组转换\"><a href=\"#4-数组转换\" class=\"headerlink\" title=\"4. 数组转换\"></a>4. 数组转换</h3><p>从一个数组(数组缓冲)出发，以某种方式对它进行转换是很简单的．这些转换操作不会修改原是数组，而是产生一个全新的数组:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> result = <span class=\"keyword\">for</span>(elem &lt;- a) <span class=\"keyword\">yield</span> <span class=\"number\">2</span> * elem <span class=\"comment\">// result 是Array(2,4,6,8)</span></span><br></pre></td></tr></table></figure></p>\n<p><code>for(...) yield</code>循环创建了一个类型与原实际和相同的新集合．新元素为yield之后的表达式的值，每次迭代对应一个．</p>\n<p>当你遍历一个集合时，如果只想处理满足特定条件的元素．可以通过for中的if来实现:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> result = <span class=\"keyword\">for</span>(elem &lt;- a <span class=\"keyword\">if</span> elem % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"keyword\">yield</span> <span class=\"number\">2</span> * elem</span><br></pre></td></tr></table></figure></p>\n<p>上面实例中对每个偶数元素翻倍，并丢掉奇数元素．</p>\n<h3 id=\"5-常用操作\"><a href=\"#5-常用操作\" class=\"headerlink\" title=\"5. 常用操作\"></a>5. 常用操作</h3><h4 id=\"5-1-sum\"><a href=\"#5-1-sum\" class=\"headerlink\" title=\"5.1 sum\"></a>5.1 sum</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.sum <span class=\"comment\">// 18</span></span><br></pre></td></tr></table></figure>\n<p>要使用sum方法，元素类型必须是数值类型:整型，浮点数或者BigInteger/BigDecimal</p>\n<h4 id=\"5-2-min-max\"><a href=\"#5-2-min-max\" class=\"headerlink\" title=\"5.2 min max\"></a>5.2 min max</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.min <span class=\"comment\">// 1</span></span><br><span class=\"line\">a.max <span class=\"comment\">// 7</span></span><br></pre></td></tr></table></figure>\n<p>min和max输出数组或数组缓冲中最小和最大的元素</p>\n<h4 id=\"5-3-sorted\"><a href=\"#5-3-sorted\" class=\"headerlink\" title=\"5.3 sorted\"></a>5.3 sorted</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> asorted = a.sorted <span class=\"comment\">// Array(1, 4, 6, 7)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">ArrayBuffer</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> asorted = a.sortWith(_ &gt; _) <span class=\"comment\">// ArrayBuffer(7, 6, 4, 1)</span></span><br></pre></td></tr></table></figure>\n<p>sorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，不会修改原始数组．可以使用sortWith方法提供一个比较函数．</p>\n<h4 id=\"5-4-mkString\"><a href=\"#5-4-mkString\" class=\"headerlink\" title=\"5.4 mkString\"></a>5.4 mkString</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.mkString(<span class=\"string\">\" and \"</span>) <span class=\"comment\">// 6 and 1 and 7 and 4</span></span><br></pre></td></tr></table></figure>\n<p>如果想要显示数组或者数组缓冲的内容，可以使用<code>mkString</code>，允许指定元素之间的分隔符<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.mkString(<span class=\"string\">\"&lt;\"</span>, <span class=\"string\">\",\"</span>, <span class=\"string\">\"&gt;\"</span>) <span class=\"comment\">// &lt;6,1,7,4&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>该方法的另一个重载版本可以让你指定前缀和后缀</p>\n<p>来源于: 快学Scala</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-定长数组\"><a href=\"#1-定长数组\" class=\"headerlink\" title=\"1. 定长数组\"></a>1. 定长数组</h3><p>如果你需要一个长度不变的数组，可以使用Scala中的 <code>Array</code>．<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> nums = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Int</span>](<span class=\"number\">10</span>) <span class=\"comment\">// 10个整数的数组　所有元素初始化为0</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> strs = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">String</span>](<span class=\"number\">10</span>) <span class=\"comment\">// 10个字符串的数组　所有元素初始化为null</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> s = <span class=\"type\">Array</span>(<span class=\"string\">\"Hello\"</span>, <span class=\"string\">\"World\"</span>) <span class=\"comment\">// 长度为2的Array[String] 类型是推断出来的　</span></span><br><span class=\"line\">s(<span class=\"number\">0</span>) = <span class=\"string\">\"GoodBye\"</span> <span class=\"comment\">// Array(\"GoodBye\"，\"World\")</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<ul>\n<li>已提供初始值时不要使用<code>new</code>，例如上面的数组s</li>\n<li>使用<code>()</code>而不是<code>[]</code>来访问元素</li>\n<li>在JVM中，Scala的Array以Java数组方式实现．</li>\n</ul>\n<h3 id=\"2-变长数组\"><a href=\"#2-变长数组\" class=\"headerlink\" title=\"2. 变长数组\"></a>2. 变长数组</h3><p>对于那种长度按需要变化的数组，Java有 <code>ArrayList</code>．Scala中等效数据结构为 <code>ArrayBuffer</code>．<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scala.collection.mutable.<span class=\"type\">ArrayBuffer</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Int</span>]() <span class=\"comment\">// 或者 new ArrayBuffer[Int]() 创建一个空的数组缓冲来存放整数</span></span><br><span class=\"line\">b += <span class=\"number\">1</span>　<span class=\"comment\">// ArrayBuffer(1) 用+=在尾端添加元素</span></span><br><span class=\"line\">b += (<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>) <span class=\"comment\">// ArrayBuffer(1,1,2,3,5) 在尾端添加多个元素</span></span><br><span class=\"line\">b ++= <span class=\"type\">Array</span>(<span class=\"number\">8</span>, <span class=\"number\">13</span>, <span class=\"number\">21</span>) <span class=\"comment\">// ArrayBuffer(1,1,2,3,5,8,13,21) 用++=操作追加任何集合</span></span><br><span class=\"line\">b.trimEnd(<span class=\"number\">5</span>) <span class=\"comment\">// ArrayBuffer(1,1,2) 移除最后５个元素</span></span><br></pre></td></tr></table></figure></p>\n<p>可以在任意位置插入或移除元素，但这样的操作不如在尾端添加或移除元素操作那么高效:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">b.insert(<span class=\"number\">2</span>,<span class=\"number\">6</span>) <span class=\"comment\">// ArrayBuffer(1,1,6,2) 在下标2之前插入</span></span><br><span class=\"line\">b.insert(<span class=\"number\">2</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>) <span class=\"comment\">// ArrayBuffer(1,1,7,8,9,6,2) 插入任意多的元素</span></span><br><span class=\"line\">b.remove(<span class=\"number\">2</span>) <span class=\"comment\">// ArrayBuffer(1,1,8,9,6,2) 删除下标２的元素</span></span><br><span class=\"line\">b.remove(<span class=\"number\">2</span>,<span class=\"number\">3</span>) <span class=\"comment\">// ArrayBuffer(1,1,2) 第二个参数的含义是要移除多少个元素</span></span><br></pre></td></tr></table></figure></p>\n<p>有时需要构建一个Array，但不知道最终需要装多少元素．这种情况下可以先构建一个数组缓冲，然后调用:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\">b.toArray <span class=\"comment\">// Array(1,1,2)</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-遍历数组和数组缓冲\"><a href=\"#3-遍历数组和数组缓冲\" class=\"headerlink\" title=\"3. 遍历数组和数组缓冲\"></a>3. 遍历数组和数组缓冲</h3><p>使用for循环遍历数组和数组缓冲:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until b.length)&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\"-\"</span> + b(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>输出结果:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">0-6</span><br><span class=\"line\">1-5</span><br><span class=\"line\">2-4</span><br><span class=\"line\">3-3</span><br><span class=\"line\">4-2</span><br><span class=\"line\">5-1</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>备注</p>\n</blockquote>\n<p><code>until</code> 是 <code>RichInt</code> 类的方法，返回所有小于(但不包括)上限的数字</p>\n<p>如果想要每两个元素一跳，可以让i这样来进行遍历:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until (b.length, <span class=\"number\">2</span>))&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\"-\"</span> + b(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">0-6</span><br><span class=\"line\">2-4</span><br><span class=\"line\">4-2</span><br></pre></td></tr></table></figure></p>\n<p>如果要从数组的尾端开始:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> b = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i &lt;- (<span class=\"number\">0</span> until b.length).reverse)&#123;</span><br><span class=\"line\">  println(i + <span class=\"string\">\"-\"</span> + b(i))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果在循环体中不需要用到数组下标，我们也可以直接访问数组元素:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(elem &lt;- b)&#123;</span><br><span class=\"line\">  println(elem)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-数组转换\"><a href=\"#4-数组转换\" class=\"headerlink\" title=\"4. 数组转换\"></a>4. 数组转换</h3><p>从一个数组(数组缓冲)出发，以某种方式对它进行转换是很简单的．这些转换操作不会修改原是数组，而是产生一个全新的数组:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> result = <span class=\"keyword\">for</span>(elem &lt;- a) <span class=\"keyword\">yield</span> <span class=\"number\">2</span> * elem <span class=\"comment\">// result 是Array(2,4,6,8)</span></span><br></pre></td></tr></table></figure></p>\n<p><code>for(...) yield</code>循环创建了一个类型与原实际和相同的新集合．新元素为yield之后的表达式的值，每次迭代对应一个．</p>\n<p>当你遍历一个集合时，如果只想处理满足特定条件的元素．可以通过for中的if来实现:<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> result = <span class=\"keyword\">for</span>(elem &lt;- a <span class=\"keyword\">if</span> elem % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"keyword\">yield</span> <span class=\"number\">2</span> * elem</span><br></pre></td></tr></table></figure></p>\n<p>上面实例中对每个偶数元素翻倍，并丢掉奇数元素．</p>\n<h3 id=\"5-常用操作\"><a href=\"#5-常用操作\" class=\"headerlink\" title=\"5. 常用操作\"></a>5. 常用操作</h3><h4 id=\"5-1-sum\"><a href=\"#5-1-sum\" class=\"headerlink\" title=\"5.1 sum\"></a>5.1 sum</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.sum <span class=\"comment\">// 18</span></span><br></pre></td></tr></table></figure>\n<p>要使用sum方法，元素类型必须是数值类型:整型，浮点数或者BigInteger/BigDecimal</p>\n<h4 id=\"5-2-min-max\"><a href=\"#5-2-min-max\" class=\"headerlink\" title=\"5.2 min max\"></a>5.2 min max</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.min <span class=\"comment\">// 1</span></span><br><span class=\"line\">a.max <span class=\"comment\">// 7</span></span><br></pre></td></tr></table></figure>\n<p>min和max输出数组或数组缓冲中最小和最大的元素</p>\n<h4 id=\"5-3-sorted\"><a href=\"#5-3-sorted\" class=\"headerlink\" title=\"5.3 sorted\"></a>5.3 sorted</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> asorted = a.sorted <span class=\"comment\">// Array(1, 4, 6, 7)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">ArrayBuffer</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> asorted = a.sortWith(_ &gt; _) <span class=\"comment\">// ArrayBuffer(7, 6, 4, 1)</span></span><br></pre></td></tr></table></figure>\n<p>sorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，不会修改原始数组．可以使用sortWith方法提供一个比较函数．</p>\n<h4 id=\"5-4-mkString\"><a href=\"#5-4-mkString\" class=\"headerlink\" title=\"5.4 mkString\"></a>5.4 mkString</h4><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.mkString(<span class=\"string\">\" and \"</span>) <span class=\"comment\">// 6 and 1 and 7 and 4</span></span><br></pre></td></tr></table></figure>\n<p>如果想要显示数组或者数组缓冲的内容，可以使用<code>mkString</code>，允许指定元素之间的分隔符<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> a = <span class=\"type\">Array</span>(<span class=\"number\">6</span>,<span class=\"number\">1</span>,<span class=\"number\">7</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">a.mkString(<span class=\"string\">\"&lt;\"</span>, <span class=\"string\">\",\"</span>, <span class=\"string\">\"&gt;\"</span>) <span class=\"comment\">// &lt;6,1,7,4&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>该方法的另一个重载版本可以让你指定前缀和后缀</p>\n<p>来源于: 快学Scala</p>\n"},{"layout":"post","author":"侠天","title":"Stream 主流流处理框架比较(1)","date":"2018-01-10T07:34:01.000Z","_content":"\n\n分布式流处理是对无边界数据集进行连续不断的处理、聚合和分析。它跟`MapReduce`一样是一种通用计算，但我们期望延迟在毫秒或者秒级别。这类系统一般采用有向无环图（`DAG`）。\n\n`DAG`是任务链的图形化表示，我们用它来描述流处理作业的拓扑。如下图，数据从`sources`流经处理任务链到`sinks`。单机可以运行`DAG`，但本篇文章主要聚焦在多台机器上运行`DAG`的情况。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-1.jpg?raw=true)\n\n### 1. 关注点\n\n当选择不同的流处理系统时，有以下几点需要注意的：\n- 运行时和编程模型：平台框架提供的编程模型决定了许多特色功能，编程模型要足够处理各种应用场景。这是一个相当重要的点，后续会继续。\n- 函数式原语：流处理平台应该能提供丰富的功能函数，比如，`map`或者`filter`这类易扩展、处理单条信息的函数；处理多条信息的函数`aggregation`；跨数据流、不易扩展的操作`join`。\n- 状态管理：大部分应用都需要保持状态处理的逻辑。流处理平台应该提供存储、访问和更新状态信息。\n- 消息传输保障：消息传输保障一般有三种：`at most once`，`at least once`和`exactly once`。`At most once`的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失；`At least once`意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失；`Exactly once`的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复。\n- 容错：流处理框架中的失败会发生在各个层次，比如，网络部分，磁盘崩溃或者节点宕机等。流处理框架应该具备从所有这种失败中恢复，并从上一个成功的状态（无脏数据）重新消费。\n- 性能：延迟时间（`Latency`），吞吐量（`Throughput`）和扩展性（`Scalability`）是流处理应用中极其重要的指标。\n- 平台的成熟度和接受度：成熟的流处理框架可以提供潜在的支持，可用的库，甚至开发问答帮助。选择正确的平台会在这方面提供很大的帮助。\n\n### 2. 运行时和编程模型\n\n运行时和编程模型是一个系统最重要的特质，因为它们定义了表达方式、可能的操作和将来的局限性。因此，运行时和编程模型决定了系统的能力和适用场景。\n\n实现流处理系统有两种完全不同的方式：\n\n(1) 一种是称作`原生流处理`，意味着所有输入的记录一旦到达即会一个接着一个进行处理。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-2.jpg?raw=true)\n\n(2) 第二种称为`微批处理`。把输入的数据按照某种预先定义的时间间隔(典型的是几秒钟)分成短小的批量数据，流经流处理系统。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-3.jpg?raw=true)\n\n两种方法都有其先天的优势和不足。首先以`原生流处理`开始，原生流处理的优势在于它的表达方式。数据一旦到达立即处理，这些系统的延迟性远比其它微批处理要好。除了延迟性外，原生流处理的状态操作也容易实现，后续将详细讲解。一般原生流处理系统为了达到低延迟和容错性会花费比较大的成本，因为它需要考虑每条记录。原生流处理的负载均衡也是个问题。比如，我们处理的数据按key分区，如果分区的某个key是资源密集型，那这个分区很容易成为作业的瓶颈。\n\n接下来看下`微批处理`。将流式计算分解成一系列短小的批处理作业，也不可避免的减弱系统的表达力。像状态管理或者`join`等操作的实现会变的困难，因为微批处理系统必须操作整个批量数据。并且，`batch interval`会连接两个不易连接的事情：基础属性和业务逻辑。相反地，微批处理系统的容错性和负载均衡实现起来非常简单，因为微批处理系统仅发送每批数据到一个`worker`节点上，如果一些数据出错那就使用其它副本。微批处理系统很容易建立在原生流处理系统之上。\n\n编程模型一般分为`组合式`和`声明式`。组合式编程提供基本的构建模块，它们必须紧密结合来创建拓扑。新的组件经常以接口的方式完成。相对应地，声明式API操作是定义的高阶函数。它允许我们用抽象类型和方法来写函数代码，并且系统创建拓扑和优化拓扑。声明式API经常也提供更多高级的操作（比如，窗口函数或者状态管理）。后面很快会给出样例代码。\n\n### 3. 主流流处理系统\n\n有一系列各种实现的流处理框架，不能一一列举，这里仅选出主流的流处理解决方案，并且支持`Scala API`。因此，我们将详细介绍`Apache Storm`，`Trident`，`Spark Streaming`，`Samza`和`Apache Flink`。前面选择讲述的虽然都是流处理系统，但它们实现的方法包含了各种不同的挑战。这里暂时不讲商业的系统，比如`Google MillWheel`或者`Amazon Kinesis`，也不会涉及很少使用的`Intel GearPump`或者`Apache Apex`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-4.jpg?raw=true)\n\n`Apache Storm`最开始是由`Nathan Marz`和他的团队于2010年在数据分析公司`BackType`开发的，后来`BackType`公司被`Twitter`收购，接着`Twitter`开源`Storm`并在2014年成为`Apache`顶级项目。毋庸置疑，`Storm`成为大规模流数据处理的先锋，并逐渐成为工业标准。`Storm`是原生的流处理系统，提供`low-level`的API。`Storm`使用`Thrift`来定义`topology`和支持多语言协议，使得我们可以使用大部分编程语言开发，`Scala`自然包括在内。\n\n`Trident`是对`Storm`的一个更高层次的抽象，`Trident`最大的特点以`batch`的形式进行流处理。`Trident`简化`topology`构建过程，增加了窗口操作、聚合操作或者状态管理等高级操作，这些在`Storm`中并不支持。相对应于`Storm`的`At most once`流传输机制，`Trident`提供了`Exactly once`传输机制。`Trident`支持`Java`，`Clojure`和`Scala`。\n\n当前`Spark`是非常受欢迎的批处理框架，包含`Spark SQL`，`MLlib`和`Spark Streaming`。`Spark`的运行时是建立在批处理之上，因此后续加入的`Spark Streaming`也依赖于批处理，实现了微批处理。接收器把输入数据流分成短小批处理，并以类似`Spark`作业的方式处理微批处理。`Spark Streaming`提供高级声明式API（支持`Scala`，`Java`和`Python`）。\n\n`Samza`最开始是专为`LinkedIn`公司开发的流处理解决方案，并和`LinkedIn`的`Kafka`一起贡献给社区，现已成为基础设施的关键部分。`Samza`的构建严重依赖于基于`log`的`Kafka`，两者紧密耦合。`Samza`提供组合式API，当然也支持`Scala`。\n\n最后来介绍`Apache Flink`。`Flink`是个相当早的项目，开始于2008年，但只在最近才得到注意。`Flink`是原生的流处理系统，提供`high level`的API。`Flink`也提供`API`来像`Spark`一样进行批处理，但两者处理的基础是完全不同的。`Flink`把批处理当作流处理中的一种特殊情况。在`Flink`中，所有的数据都看作流，是一种很好的抽象，因为这更接近于现实世界。\n\n快速的介绍流处理系统之后，让我们以下面的表格来更好清晰的展示它们之间的不同：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-5.jpg?raw=true)\n\n### 4. Word Count\n\n`Wordcount`之于流处理框架学习，就好比`hello world`之于编程语言学习。它能很好的展示各流处理框架的不同之处，让我们从`Storm`开始看看如何实现`Wordcount`：\n```Java\nTopologyBuilder builder = new TopologyBuilder();\nbuilder.setSpout(\"spout\", new RandomSentenceSpout(), 5);\nbuilder.setBolt(\"split\", new Split(), 8).shuffleGrouping(\"spout\");\nbuilder.setBolt(\"count\", new WordCount(), 12).fieldsGrouping(\"split\", new Fields(\"word\"));\n\n ...\n\nMap<String, Integer> counts = new HashMap<String, Integer>();\n\npublic void execute(Tuple tuple, BasicOutputCollector collector) {\n   String word = tuple.getString(0);\n   Integer count = counts.containsKey(word) ? counts.get(word) + 1 : 1;\n   counts.put(word, count);\n   collector.emit(new Values(word, count));\n}\n```\n首先，定义`topology`。第二行代码定义一个`spout`，作为数据源。然后是一个处理组件`bolt`，分割文本为单词。接着，定义另一个`bolt`来计算单词数（第四行代码）。也可以看到魔数5，8和12，这些是并行度，定义集群每个组件执行的独立线程数。第八行到十五行是实际的`WordCount bolt`实现。因为`Storm`不支持内建的状态管理，所有这里定义了一个局部状态。\n\n按之前描述，`Trident`是对`Storm`的一个更高层次的抽象，`Trident`最大的特点以`batch`的形式进行流处理。除了其它优势，`Trident`提供了状态管理，这对`wordcount`实现非常有用:\n```java\npublic static StormTopology buildTopology(LocalDRPC drpc) {\n FixedBatchSpout spout = ...\n\n TridentTopology topology = new TridentTopology();\n TridentState wordCounts = topology.newStream(\"spout1\", spout)\n .each(new Fields(\"sentence\"),new Split(), new Fields(\"word\"))\n .groupBy(new Fields(\"word\"))\n .persistentAggregate(new MemoryMapState.Factory(),\n new Count(), new Fields(\"count\"));\n\n ...\n\n }\n```\n如你所见，上面代码使用`higher level`操作，比如`each`（第七行代码）和`groupby`（第八行代码）。并且使用`Trident`管理状态来存储单词数（第九行代码）。\n\n下面是时候祭出提供声明式API的`Apache Spark`。记住，相对于前面的例子，这些代码相当简单，几乎没有冗余代码。下面是简单的流式计算单词数：\n```Scala\nval conf = new SparkConf().setAppName(\"wordcount\")\nval ssc = new StreamingContext(conf, Seconds(1))\n\nval text = ...\n\nval counts = text.flatMap(line => line.split(\" \"))\n .map(word => (word, 1))\n .reduceByKey(_ + _)\n\ncounts.print()\n\nssc.start()\nssc.awaitTermination()\n```\n每个`Spark Streaming`的作业都要有`StreamingContext`，它是流式函数的入口。`StreamingContext`加载第一行代码定义的配置`conf`，但更重要地，第二行代码定义`batch interval`（这里设置为1秒）。第六行到八行代码是整个单词数计算。这些是标准的函数式代码，`Spark`定义`topology`并且分布式执行。第十二行代码是每个`Spark Streaming`作业最后的部分：启动计算。记住，`Spark Streaming`作业一旦启动即不可修改。\n\n接下来看下`Apache Samza`，另外一个组合式API例子：\n```\nclass WordCountTask extends StreamTask {\n\n  override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector,\n    coordinator: TaskCoordinator) {\n\n    val text = envelope.getMessage.asInstanceOf[String]\n\n    val counts = text.split(\" \").foldLeft(Map.empty[String, Int]) {\n      (count, word) => count + (word -> (count.getOrElse(word, 0) + 1))\n    }\n\n    collector.send(new OutgoingMessageEnvelope(new SystemStream(\"kafka\", \"wordcount\"), counts))\n\n }\n```\n`Samza`的属性配置文件定义`topology`，为了简明这里并没把配置文件放上来。定义任务的输入和输出，并通过`Kafka topic`通信。在单词数计算整个`topology`是`WordCountTask`。在`Samza`中，实现特殊接口定义组件`StreamTask`，在第三行代码重写方法`process`。它的参数列表包含所有连接其它系统的需要。第八行到十行简单的`Scala`代码是计算本身。\n\n`Flink`的API跟`Spark Streaming`是惊人的相似，但注意到代码里并未设置`batch interval`：\n```\nval env = ExecutionEnvironment.getExecutionEnvironment\n\n val text = env.fromElements(...)\n val counts = text.flatMap ( _.split(\" \") )\n   .map ( (_, 1) )\n   .groupBy(0)\n   .sum(1)\n\n counts.print()\n\n env.execute(\"wordcount\")\n```\n上面的代码是相当的直白，仅仅只是几个函数式调用，Flink支持分布式计算。\n\n### 5. 结论\n\n上面给出了基本的理论和主流流处理框架介绍，下篇文章将会更深入的探讨其它关注点。希望你能对前面的文章感兴趣，如果有任何问题，请联系我讨论这些主题。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework\n","source":"_posts/Stream/[Stream]主流流处理框架比较(1).md","raw":"---\nlayout: post\nauthor: 侠天\ntitle: Stream 主流流处理框架比较(1)\ndate: 2018-01-10 15:34:01\ntags:\n  - Stream\n\ncategories: Stream\n---\n\n\n分布式流处理是对无边界数据集进行连续不断的处理、聚合和分析。它跟`MapReduce`一样是一种通用计算，但我们期望延迟在毫秒或者秒级别。这类系统一般采用有向无环图（`DAG`）。\n\n`DAG`是任务链的图形化表示，我们用它来描述流处理作业的拓扑。如下图，数据从`sources`流经处理任务链到`sinks`。单机可以运行`DAG`，但本篇文章主要聚焦在多台机器上运行`DAG`的情况。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-1.jpg?raw=true)\n\n### 1. 关注点\n\n当选择不同的流处理系统时，有以下几点需要注意的：\n- 运行时和编程模型：平台框架提供的编程模型决定了许多特色功能，编程模型要足够处理各种应用场景。这是一个相当重要的点，后续会继续。\n- 函数式原语：流处理平台应该能提供丰富的功能函数，比如，`map`或者`filter`这类易扩展、处理单条信息的函数；处理多条信息的函数`aggregation`；跨数据流、不易扩展的操作`join`。\n- 状态管理：大部分应用都需要保持状态处理的逻辑。流处理平台应该提供存储、访问和更新状态信息。\n- 消息传输保障：消息传输保障一般有三种：`at most once`，`at least once`和`exactly once`。`At most once`的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失；`At least once`意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失；`Exactly once`的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复。\n- 容错：流处理框架中的失败会发生在各个层次，比如，网络部分，磁盘崩溃或者节点宕机等。流处理框架应该具备从所有这种失败中恢复，并从上一个成功的状态（无脏数据）重新消费。\n- 性能：延迟时间（`Latency`），吞吐量（`Throughput`）和扩展性（`Scalability`）是流处理应用中极其重要的指标。\n- 平台的成熟度和接受度：成熟的流处理框架可以提供潜在的支持，可用的库，甚至开发问答帮助。选择正确的平台会在这方面提供很大的帮助。\n\n### 2. 运行时和编程模型\n\n运行时和编程模型是一个系统最重要的特质，因为它们定义了表达方式、可能的操作和将来的局限性。因此，运行时和编程模型决定了系统的能力和适用场景。\n\n实现流处理系统有两种完全不同的方式：\n\n(1) 一种是称作`原生流处理`，意味着所有输入的记录一旦到达即会一个接着一个进行处理。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-2.jpg?raw=true)\n\n(2) 第二种称为`微批处理`。把输入的数据按照某种预先定义的时间间隔(典型的是几秒钟)分成短小的批量数据，流经流处理系统。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-3.jpg?raw=true)\n\n两种方法都有其先天的优势和不足。首先以`原生流处理`开始，原生流处理的优势在于它的表达方式。数据一旦到达立即处理，这些系统的延迟性远比其它微批处理要好。除了延迟性外，原生流处理的状态操作也容易实现，后续将详细讲解。一般原生流处理系统为了达到低延迟和容错性会花费比较大的成本，因为它需要考虑每条记录。原生流处理的负载均衡也是个问题。比如，我们处理的数据按key分区，如果分区的某个key是资源密集型，那这个分区很容易成为作业的瓶颈。\n\n接下来看下`微批处理`。将流式计算分解成一系列短小的批处理作业，也不可避免的减弱系统的表达力。像状态管理或者`join`等操作的实现会变的困难，因为微批处理系统必须操作整个批量数据。并且，`batch interval`会连接两个不易连接的事情：基础属性和业务逻辑。相反地，微批处理系统的容错性和负载均衡实现起来非常简单，因为微批处理系统仅发送每批数据到一个`worker`节点上，如果一些数据出错那就使用其它副本。微批处理系统很容易建立在原生流处理系统之上。\n\n编程模型一般分为`组合式`和`声明式`。组合式编程提供基本的构建模块，它们必须紧密结合来创建拓扑。新的组件经常以接口的方式完成。相对应地，声明式API操作是定义的高阶函数。它允许我们用抽象类型和方法来写函数代码，并且系统创建拓扑和优化拓扑。声明式API经常也提供更多高级的操作（比如，窗口函数或者状态管理）。后面很快会给出样例代码。\n\n### 3. 主流流处理系统\n\n有一系列各种实现的流处理框架，不能一一列举，这里仅选出主流的流处理解决方案，并且支持`Scala API`。因此，我们将详细介绍`Apache Storm`，`Trident`，`Spark Streaming`，`Samza`和`Apache Flink`。前面选择讲述的虽然都是流处理系统，但它们实现的方法包含了各种不同的挑战。这里暂时不讲商业的系统，比如`Google MillWheel`或者`Amazon Kinesis`，也不会涉及很少使用的`Intel GearPump`或者`Apache Apex`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-4.jpg?raw=true)\n\n`Apache Storm`最开始是由`Nathan Marz`和他的团队于2010年在数据分析公司`BackType`开发的，后来`BackType`公司被`Twitter`收购，接着`Twitter`开源`Storm`并在2014年成为`Apache`顶级项目。毋庸置疑，`Storm`成为大规模流数据处理的先锋，并逐渐成为工业标准。`Storm`是原生的流处理系统，提供`low-level`的API。`Storm`使用`Thrift`来定义`topology`和支持多语言协议，使得我们可以使用大部分编程语言开发，`Scala`自然包括在内。\n\n`Trident`是对`Storm`的一个更高层次的抽象，`Trident`最大的特点以`batch`的形式进行流处理。`Trident`简化`topology`构建过程，增加了窗口操作、聚合操作或者状态管理等高级操作，这些在`Storm`中并不支持。相对应于`Storm`的`At most once`流传输机制，`Trident`提供了`Exactly once`传输机制。`Trident`支持`Java`，`Clojure`和`Scala`。\n\n当前`Spark`是非常受欢迎的批处理框架，包含`Spark SQL`，`MLlib`和`Spark Streaming`。`Spark`的运行时是建立在批处理之上，因此后续加入的`Spark Streaming`也依赖于批处理，实现了微批处理。接收器把输入数据流分成短小批处理，并以类似`Spark`作业的方式处理微批处理。`Spark Streaming`提供高级声明式API（支持`Scala`，`Java`和`Python`）。\n\n`Samza`最开始是专为`LinkedIn`公司开发的流处理解决方案，并和`LinkedIn`的`Kafka`一起贡献给社区，现已成为基础设施的关键部分。`Samza`的构建严重依赖于基于`log`的`Kafka`，两者紧密耦合。`Samza`提供组合式API，当然也支持`Scala`。\n\n最后来介绍`Apache Flink`。`Flink`是个相当早的项目，开始于2008年，但只在最近才得到注意。`Flink`是原生的流处理系统，提供`high level`的API。`Flink`也提供`API`来像`Spark`一样进行批处理，但两者处理的基础是完全不同的。`Flink`把批处理当作流处理中的一种特殊情况。在`Flink`中，所有的数据都看作流，是一种很好的抽象，因为这更接近于现实世界。\n\n快速的介绍流处理系统之后，让我们以下面的表格来更好清晰的展示它们之间的不同：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-5.jpg?raw=true)\n\n### 4. Word Count\n\n`Wordcount`之于流处理框架学习，就好比`hello world`之于编程语言学习。它能很好的展示各流处理框架的不同之处，让我们从`Storm`开始看看如何实现`Wordcount`：\n```Java\nTopologyBuilder builder = new TopologyBuilder();\nbuilder.setSpout(\"spout\", new RandomSentenceSpout(), 5);\nbuilder.setBolt(\"split\", new Split(), 8).shuffleGrouping(\"spout\");\nbuilder.setBolt(\"count\", new WordCount(), 12).fieldsGrouping(\"split\", new Fields(\"word\"));\n\n ...\n\nMap<String, Integer> counts = new HashMap<String, Integer>();\n\npublic void execute(Tuple tuple, BasicOutputCollector collector) {\n   String word = tuple.getString(0);\n   Integer count = counts.containsKey(word) ? counts.get(word) + 1 : 1;\n   counts.put(word, count);\n   collector.emit(new Values(word, count));\n}\n```\n首先，定义`topology`。第二行代码定义一个`spout`，作为数据源。然后是一个处理组件`bolt`，分割文本为单词。接着，定义另一个`bolt`来计算单词数（第四行代码）。也可以看到魔数5，8和12，这些是并行度，定义集群每个组件执行的独立线程数。第八行到十五行是实际的`WordCount bolt`实现。因为`Storm`不支持内建的状态管理，所有这里定义了一个局部状态。\n\n按之前描述，`Trident`是对`Storm`的一个更高层次的抽象，`Trident`最大的特点以`batch`的形式进行流处理。除了其它优势，`Trident`提供了状态管理，这对`wordcount`实现非常有用:\n```java\npublic static StormTopology buildTopology(LocalDRPC drpc) {\n FixedBatchSpout spout = ...\n\n TridentTopology topology = new TridentTopology();\n TridentState wordCounts = topology.newStream(\"spout1\", spout)\n .each(new Fields(\"sentence\"),new Split(), new Fields(\"word\"))\n .groupBy(new Fields(\"word\"))\n .persistentAggregate(new MemoryMapState.Factory(),\n new Count(), new Fields(\"count\"));\n\n ...\n\n }\n```\n如你所见，上面代码使用`higher level`操作，比如`each`（第七行代码）和`groupby`（第八行代码）。并且使用`Trident`管理状态来存储单词数（第九行代码）。\n\n下面是时候祭出提供声明式API的`Apache Spark`。记住，相对于前面的例子，这些代码相当简单，几乎没有冗余代码。下面是简单的流式计算单词数：\n```Scala\nval conf = new SparkConf().setAppName(\"wordcount\")\nval ssc = new StreamingContext(conf, Seconds(1))\n\nval text = ...\n\nval counts = text.flatMap(line => line.split(\" \"))\n .map(word => (word, 1))\n .reduceByKey(_ + _)\n\ncounts.print()\n\nssc.start()\nssc.awaitTermination()\n```\n每个`Spark Streaming`的作业都要有`StreamingContext`，它是流式函数的入口。`StreamingContext`加载第一行代码定义的配置`conf`，但更重要地，第二行代码定义`batch interval`（这里设置为1秒）。第六行到八行代码是整个单词数计算。这些是标准的函数式代码，`Spark`定义`topology`并且分布式执行。第十二行代码是每个`Spark Streaming`作业最后的部分：启动计算。记住，`Spark Streaming`作业一旦启动即不可修改。\n\n接下来看下`Apache Samza`，另外一个组合式API例子：\n```\nclass WordCountTask extends StreamTask {\n\n  override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector,\n    coordinator: TaskCoordinator) {\n\n    val text = envelope.getMessage.asInstanceOf[String]\n\n    val counts = text.split(\" \").foldLeft(Map.empty[String, Int]) {\n      (count, word) => count + (word -> (count.getOrElse(word, 0) + 1))\n    }\n\n    collector.send(new OutgoingMessageEnvelope(new SystemStream(\"kafka\", \"wordcount\"), counts))\n\n }\n```\n`Samza`的属性配置文件定义`topology`，为了简明这里并没把配置文件放上来。定义任务的输入和输出，并通过`Kafka topic`通信。在单词数计算整个`topology`是`WordCountTask`。在`Samza`中，实现特殊接口定义组件`StreamTask`，在第三行代码重写方法`process`。它的参数列表包含所有连接其它系统的需要。第八行到十行简单的`Scala`代码是计算本身。\n\n`Flink`的API跟`Spark Streaming`是惊人的相似，但注意到代码里并未设置`batch interval`：\n```\nval env = ExecutionEnvironment.getExecutionEnvironment\n\n val text = env.fromElements(...)\n val counts = text.flatMap ( _.split(\" \") )\n   .map ( (_, 1) )\n   .groupBy(0)\n   .sum(1)\n\n counts.print()\n\n env.execute(\"wordcount\")\n```\n上面的代码是相当的直白，仅仅只是几个函数式调用，Flink支持分布式计算。\n\n### 5. 结论\n\n上面给出了基本的理论和主流流处理框架介绍，下篇文章将会更深入的探讨其它关注点。希望你能对前面的文章感兴趣，如果有任何问题，请联系我讨论这些主题。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework\n","slug":"Stream/[Stream]主流流处理框架比较(1)","published":1,"updated":"2018-01-29T09:36:59.615Z","comments":1,"photos":[],"link":"","_id":"cje58tivh005zordbh3horot4","content":"<p>分布式流处理是对无边界数据集进行连续不断的处理、聚合和分析。它跟<code>MapReduce</code>一样是一种通用计算，但我们期望延迟在毫秒或者秒级别。这类系统一般采用有向无环图（<code>DAG</code>）。</p>\n<p><code>DAG</code>是任务链的图形化表示，我们用它来描述流处理作业的拓扑。如下图，数据从<code>sources</code>流经处理任务链到<code>sinks</code>。单机可以运行<code>DAG</code>，但本篇文章主要聚焦在多台机器上运行<code>DAG</code>的情况。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-1.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"1-关注点\"><a href=\"#1-关注点\" class=\"headerlink\" title=\"1. 关注点\"></a>1. 关注点</h3><p>当选择不同的流处理系统时，有以下几点需要注意的：</p>\n<ul>\n<li>运行时和编程模型：平台框架提供的编程模型决定了许多特色功能，编程模型要足够处理各种应用场景。这是一个相当重要的点，后续会继续。</li>\n<li>函数式原语：流处理平台应该能提供丰富的功能函数，比如，<code>map</code>或者<code>filter</code>这类易扩展、处理单条信息的函数；处理多条信息的函数<code>aggregation</code>；跨数据流、不易扩展的操作<code>join</code>。</li>\n<li>状态管理：大部分应用都需要保持状态处理的逻辑。流处理平台应该提供存储、访问和更新状态信息。</li>\n<li>消息传输保障：消息传输保障一般有三种：<code>at most once</code>，<code>at least once</code>和<code>exactly once</code>。<code>At most once</code>的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失；<code>At least once</code>意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失；<code>Exactly once</code>的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复。</li>\n<li>容错：流处理框架中的失败会发生在各个层次，比如，网络部分，磁盘崩溃或者节点宕机等。流处理框架应该具备从所有这种失败中恢复，并从上一个成功的状态（无脏数据）重新消费。</li>\n<li>性能：延迟时间（<code>Latency</code>），吞吐量（<code>Throughput</code>）和扩展性（<code>Scalability</code>）是流处理应用中极其重要的指标。</li>\n<li>平台的成熟度和接受度：成熟的流处理框架可以提供潜在的支持，可用的库，甚至开发问答帮助。选择正确的平台会在这方面提供很大的帮助。</li>\n</ul>\n<h3 id=\"2-运行时和编程模型\"><a href=\"#2-运行时和编程模型\" class=\"headerlink\" title=\"2. 运行时和编程模型\"></a>2. 运行时和编程模型</h3><p>运行时和编程模型是一个系统最重要的特质，因为它们定义了表达方式、可能的操作和将来的局限性。因此，运行时和编程模型决定了系统的能力和适用场景。</p>\n<p>实现流处理系统有两种完全不同的方式：</p>\n<p>(1) 一种是称作<code>原生流处理</code>，意味着所有输入的记录一旦到达即会一个接着一个进行处理。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-2.jpg?raw=true\" alt=\"\"></p>\n<p>(2) 第二种称为<code>微批处理</code>。把输入的数据按照某种预先定义的时间间隔(典型的是几秒钟)分成短小的批量数据，流经流处理系统。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-3.jpg?raw=true\" alt=\"\"></p>\n<p>两种方法都有其先天的优势和不足。首先以<code>原生流处理</code>开始，原生流处理的优势在于它的表达方式。数据一旦到达立即处理，这些系统的延迟性远比其它微批处理要好。除了延迟性外，原生流处理的状态操作也容易实现，后续将详细讲解。一般原生流处理系统为了达到低延迟和容错性会花费比较大的成本，因为它需要考虑每条记录。原生流处理的负载均衡也是个问题。比如，我们处理的数据按key分区，如果分区的某个key是资源密集型，那这个分区很容易成为作业的瓶颈。</p>\n<p>接下来看下<code>微批处理</code>。将流式计算分解成一系列短小的批处理作业，也不可避免的减弱系统的表达力。像状态管理或者<code>join</code>等操作的实现会变的困难，因为微批处理系统必须操作整个批量数据。并且，<code>batch interval</code>会连接两个不易连接的事情：基础属性和业务逻辑。相反地，微批处理系统的容错性和负载均衡实现起来非常简单，因为微批处理系统仅发送每批数据到一个<code>worker</code>节点上，如果一些数据出错那就使用其它副本。微批处理系统很容易建立在原生流处理系统之上。</p>\n<p>编程模型一般分为<code>组合式</code>和<code>声明式</code>。组合式编程提供基本的构建模块，它们必须紧密结合来创建拓扑。新的组件经常以接口的方式完成。相对应地，声明式API操作是定义的高阶函数。它允许我们用抽象类型和方法来写函数代码，并且系统创建拓扑和优化拓扑。声明式API经常也提供更多高级的操作（比如，窗口函数或者状态管理）。后面很快会给出样例代码。</p>\n<h3 id=\"3-主流流处理系统\"><a href=\"#3-主流流处理系统\" class=\"headerlink\" title=\"3. 主流流处理系统\"></a>3. 主流流处理系统</h3><p>有一系列各种实现的流处理框架，不能一一列举，这里仅选出主流的流处理解决方案，并且支持<code>Scala API</code>。因此，我们将详细介绍<code>Apache Storm</code>，<code>Trident</code>，<code>Spark Streaming</code>，<code>Samza</code>和<code>Apache Flink</code>。前面选择讲述的虽然都是流处理系统，但它们实现的方法包含了各种不同的挑战。这里暂时不讲商业的系统，比如<code>Google MillWheel</code>或者<code>Amazon Kinesis</code>，也不会涉及很少使用的<code>Intel GearPump</code>或者<code>Apache Apex</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-4.jpg?raw=true\" alt=\"\"></p>\n<p><code>Apache Storm</code>最开始是由<code>Nathan Marz</code>和他的团队于2010年在数据分析公司<code>BackType</code>开发的，后来<code>BackType</code>公司被<code>Twitter</code>收购，接着<code>Twitter</code>开源<code>Storm</code>并在2014年成为<code>Apache</code>顶级项目。毋庸置疑，<code>Storm</code>成为大规模流数据处理的先锋，并逐渐成为工业标准。<code>Storm</code>是原生的流处理系统，提供<code>low-level</code>的API。<code>Storm</code>使用<code>Thrift</code>来定义<code>topology</code>和支持多语言协议，使得我们可以使用大部分编程语言开发，<code>Scala</code>自然包括在内。</p>\n<p><code>Trident</code>是对<code>Storm</code>的一个更高层次的抽象，<code>Trident</code>最大的特点以<code>batch</code>的形式进行流处理。<code>Trident</code>简化<code>topology</code>构建过程，增加了窗口操作、聚合操作或者状态管理等高级操作，这些在<code>Storm</code>中并不支持。相对应于<code>Storm</code>的<code>At most once</code>流传输机制，<code>Trident</code>提供了<code>Exactly once</code>传输机制。<code>Trident</code>支持<code>Java</code>，<code>Clojure</code>和<code>Scala</code>。</p>\n<p>当前<code>Spark</code>是非常受欢迎的批处理框架，包含<code>Spark SQL</code>，<code>MLlib</code>和<code>Spark Streaming</code>。<code>Spark</code>的运行时是建立在批处理之上，因此后续加入的<code>Spark Streaming</code>也依赖于批处理，实现了微批处理。接收器把输入数据流分成短小批处理，并以类似<code>Spark</code>作业的方式处理微批处理。<code>Spark Streaming</code>提供高级声明式API（支持<code>Scala</code>，<code>Java</code>和<code>Python</code>）。</p>\n<p><code>Samza</code>最开始是专为<code>LinkedIn</code>公司开发的流处理解决方案，并和<code>LinkedIn</code>的<code>Kafka</code>一起贡献给社区，现已成为基础设施的关键部分。<code>Samza</code>的构建严重依赖于基于<code>log</code>的<code>Kafka</code>，两者紧密耦合。<code>Samza</code>提供组合式API，当然也支持<code>Scala</code>。</p>\n<p>最后来介绍<code>Apache Flink</code>。<code>Flink</code>是个相当早的项目，开始于2008年，但只在最近才得到注意。<code>Flink</code>是原生的流处理系统，提供<code>high level</code>的API。<code>Flink</code>也提供<code>API</code>来像<code>Spark</code>一样进行批处理，但两者处理的基础是完全不同的。<code>Flink</code>把批处理当作流处理中的一种特殊情况。在<code>Flink</code>中，所有的数据都看作流，是一种很好的抽象，因为这更接近于现实世界。</p>\n<p>快速的介绍流处理系统之后，让我们以下面的表格来更好清晰的展示它们之间的不同：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-5.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"4-Word-Count\"><a href=\"#4-Word-Count\" class=\"headerlink\" title=\"4. Word Count\"></a>4. Word Count</h3><p><code>Wordcount</code>之于流处理框架学习，就好比<code>hello world</code>之于编程语言学习。它能很好的展示各流处理框架的不同之处，让我们从<code>Storm</code>开始看看如何实现<code>Wordcount</code>：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">TopologyBuilder builder = <span class=\"keyword\">new</span> TopologyBuilder();</span><br><span class=\"line\">builder.setSpout(<span class=\"string\">\"spout\"</span>, <span class=\"keyword\">new</span> RandomSentenceSpout(), <span class=\"number\">5</span>);</span><br><span class=\"line\">builder.setBolt(<span class=\"string\">\"split\"</span>, <span class=\"keyword\">new</span> Split(), <span class=\"number\">8</span>).shuffleGrouping(<span class=\"string\">\"spout\"</span>);</span><br><span class=\"line\">builder.setBolt(<span class=\"string\">\"count\"</span>, <span class=\"keyword\">new</span> WordCount(), <span class=\"number\">12</span>).fieldsGrouping(<span class=\"string\">\"split\"</span>, <span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"> ...</span><br><span class=\"line\"></span><br><span class=\"line\">Map&lt;String, Integer&gt; counts = <span class=\"keyword\">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Tuple tuple, BasicOutputCollector collector)</span> </span>&#123;</span><br><span class=\"line\">   String word = tuple.getString(<span class=\"number\">0</span>);</span><br><span class=\"line\">   Integer count = counts.containsKey(word) ? counts.get(word) + <span class=\"number\">1</span> : <span class=\"number\">1</span>;</span><br><span class=\"line\">   counts.put(word, count);</span><br><span class=\"line\">   collector.emit(<span class=\"keyword\">new</span> Values(word, count));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>首先，定义<code>topology</code>。第二行代码定义一个<code>spout</code>，作为数据源。然后是一个处理组件<code>bolt</code>，分割文本为单词。接着，定义另一个<code>bolt</code>来计算单词数（第四行代码）。也可以看到魔数5，8和12，这些是并行度，定义集群每个组件执行的独立线程数。第八行到十五行是实际的<code>WordCount bolt</code>实现。因为<code>Storm</code>不支持内建的状态管理，所有这里定义了一个局部状态。</p>\n<p>按之前描述，<code>Trident</code>是对<code>Storm</code>的一个更高层次的抽象，<code>Trident</code>最大的特点以<code>batch</code>的形式进行流处理。除了其它优势，<code>Trident</code>提供了状态管理，这对<code>wordcount</code>实现非常有用:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> StormTopology <span class=\"title\">buildTopology</span><span class=\"params\">(LocalDRPC drpc)</span> </span>&#123;</span><br><span class=\"line\"> FixedBatchSpout spout = ...</span><br><span class=\"line\"></span><br><span class=\"line\"> TridentTopology topology = <span class=\"keyword\">new</span> TridentTopology();</span><br><span class=\"line\"> TridentState wordCounts = topology.newStream(<span class=\"string\">\"spout1\"</span>, spout)</span><br><span class=\"line\"> .each(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"sentence\"</span>),<span class=\"keyword\">new</span> Split(), <span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>))</span><br><span class=\"line\"> .groupBy(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>))</span><br><span class=\"line\"> .persistentAggregate(<span class=\"keyword\">new</span> MemoryMapState.Factory(),</span><br><span class=\"line\"> <span class=\"keyword\">new</span> Count(), <span class=\"keyword\">new</span> Fields(<span class=\"string\">\"count\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"> ...</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p>如你所见，上面代码使用<code>higher level</code>操作，比如<code>each</code>（第七行代码）和<code>groupby</code>（第八行代码）。并且使用<code>Trident</code>管理状态来存储单词数（第九行代码）。</p>\n<p>下面是时候祭出提供声明式API的<code>Apache Spark</code>。记住，相对于前面的例子，这些代码相当简单，几乎没有冗余代码。下面是简单的流式计算单词数：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"wordcount\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> ssc = <span class=\"keyword\">new</span> <span class=\"type\">StreamingContext</span>(conf, <span class=\"type\">Seconds</span>(<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> text = ...</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> counts = text.flatMap(line =&gt; line.split(<span class=\"string\">\" \"</span>))</span><br><span class=\"line\"> .map(word =&gt; (word, <span class=\"number\">1</span>))</span><br><span class=\"line\"> .reduceByKey(_ + _)</span><br><span class=\"line\"></span><br><span class=\"line\">counts.print()</span><br><span class=\"line\"></span><br><span class=\"line\">ssc.start()</span><br><span class=\"line\">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></p>\n<p>每个<code>Spark Streaming</code>的作业都要有<code>StreamingContext</code>，它是流式函数的入口。<code>StreamingContext</code>加载第一行代码定义的配置<code>conf</code>，但更重要地，第二行代码定义<code>batch interval</code>（这里设置为1秒）。第六行到八行代码是整个单词数计算。这些是标准的函数式代码，<code>Spark</code>定义<code>topology</code>并且分布式执行。第十二行代码是每个<code>Spark Streaming</code>作业最后的部分：启动计算。记住，<code>Spark Streaming</code>作业一旦启动即不可修改。</p>\n<p>接下来看下<code>Apache Samza</code>，另外一个组合式API例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class WordCountTask extends StreamTask &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector,</span><br><span class=\"line\">    coordinator: TaskCoordinator) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val text = envelope.getMessage.asInstanceOf[String]</span><br><span class=\"line\"></span><br><span class=\"line\">    val counts = text.split(&quot; &quot;).foldLeft(Map.empty[String, Int]) &#123;</span><br><span class=\"line\">      (count, word) =&gt; count + (word -&gt; (count.getOrElse(word, 0) + 1))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;), counts))</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>Samza</code>的属性配置文件定义<code>topology</code>，为了简明这里并没把配置文件放上来。定义任务的输入和输出，并通过<code>Kafka topic</code>通信。在单词数计算整个<code>topology</code>是<code>WordCountTask</code>。在<code>Samza</code>中，实现特殊接口定义组件<code>StreamTask</code>，在第三行代码重写方法<code>process</code>。它的参数列表包含所有连接其它系统的需要。第八行到十行简单的<code>Scala</code>代码是计算本身。</p>\n<p><code>Flink</code>的API跟<code>Spark Streaming</code>是惊人的相似，但注意到代码里并未设置<code>batch interval</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\"> val text = env.fromElements(...)</span><br><span class=\"line\"> val counts = text.flatMap ( _.split(&quot; &quot;) )</span><br><span class=\"line\">   .map ( (_, 1) )</span><br><span class=\"line\">   .groupBy(0)</span><br><span class=\"line\">   .sum(1)</span><br><span class=\"line\"></span><br><span class=\"line\"> counts.print()</span><br><span class=\"line\"></span><br><span class=\"line\"> env.execute(&quot;wordcount&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>上面的代码是相当的直白，仅仅只是几个函数式调用，Flink支持分布式计算。</p>\n<h3 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5. 结论\"></a>5. 结论</h3><p>上面给出了基本的理论和主流流处理框架介绍，下篇文章将会更深入的探讨其它关注点。希望你能对前面的文章感兴趣，如果有任何问题，请联系我讨论这些主题。</p>\n<p>原文:<a href=\"http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>分布式流处理是对无边界数据集进行连续不断的处理、聚合和分析。它跟<code>MapReduce</code>一样是一种通用计算，但我们期望延迟在毫秒或者秒级别。这类系统一般采用有向无环图（<code>DAG</code>）。</p>\n<p><code>DAG</code>是任务链的图形化表示，我们用它来描述流处理作业的拓扑。如下图，数据从<code>sources</code>流经处理任务链到<code>sinks</code>。单机可以运行<code>DAG</code>，但本篇文章主要聚焦在多台机器上运行<code>DAG</code>的情况。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-1.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"1-关注点\"><a href=\"#1-关注点\" class=\"headerlink\" title=\"1. 关注点\"></a>1. 关注点</h3><p>当选择不同的流处理系统时，有以下几点需要注意的：</p>\n<ul>\n<li>运行时和编程模型：平台框架提供的编程模型决定了许多特色功能，编程模型要足够处理各种应用场景。这是一个相当重要的点，后续会继续。</li>\n<li>函数式原语：流处理平台应该能提供丰富的功能函数，比如，<code>map</code>或者<code>filter</code>这类易扩展、处理单条信息的函数；处理多条信息的函数<code>aggregation</code>；跨数据流、不易扩展的操作<code>join</code>。</li>\n<li>状态管理：大部分应用都需要保持状态处理的逻辑。流处理平台应该提供存储、访问和更新状态信息。</li>\n<li>消息传输保障：消息传输保障一般有三种：<code>at most once</code>，<code>at least once</code>和<code>exactly once</code>。<code>At most once</code>的消息传输机制是每条消息传输零次或者一次，即消息可能会丢失；<code>At least once</code>意味着每条消息会进行多次传输尝试，至少一次成功，即消息传输可能重复但不会丢失；<code>Exactly once</code>的消息传输机制是每条消息有且只有一次，即消息传输既不会丢失也不会重复。</li>\n<li>容错：流处理框架中的失败会发生在各个层次，比如，网络部分，磁盘崩溃或者节点宕机等。流处理框架应该具备从所有这种失败中恢复，并从上一个成功的状态（无脏数据）重新消费。</li>\n<li>性能：延迟时间（<code>Latency</code>），吞吐量（<code>Throughput</code>）和扩展性（<code>Scalability</code>）是流处理应用中极其重要的指标。</li>\n<li>平台的成熟度和接受度：成熟的流处理框架可以提供潜在的支持，可用的库，甚至开发问答帮助。选择正确的平台会在这方面提供很大的帮助。</li>\n</ul>\n<h3 id=\"2-运行时和编程模型\"><a href=\"#2-运行时和编程模型\" class=\"headerlink\" title=\"2. 运行时和编程模型\"></a>2. 运行时和编程模型</h3><p>运行时和编程模型是一个系统最重要的特质，因为它们定义了表达方式、可能的操作和将来的局限性。因此，运行时和编程模型决定了系统的能力和适用场景。</p>\n<p>实现流处理系统有两种完全不同的方式：</p>\n<p>(1) 一种是称作<code>原生流处理</code>，意味着所有输入的记录一旦到达即会一个接着一个进行处理。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-2.jpg?raw=true\" alt=\"\"></p>\n<p>(2) 第二种称为<code>微批处理</code>。把输入的数据按照某种预先定义的时间间隔(典型的是几秒钟)分成短小的批量数据，流经流处理系统。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-3.jpg?raw=true\" alt=\"\"></p>\n<p>两种方法都有其先天的优势和不足。首先以<code>原生流处理</code>开始，原生流处理的优势在于它的表达方式。数据一旦到达立即处理，这些系统的延迟性远比其它微批处理要好。除了延迟性外，原生流处理的状态操作也容易实现，后续将详细讲解。一般原生流处理系统为了达到低延迟和容错性会花费比较大的成本，因为它需要考虑每条记录。原生流处理的负载均衡也是个问题。比如，我们处理的数据按key分区，如果分区的某个key是资源密集型，那这个分区很容易成为作业的瓶颈。</p>\n<p>接下来看下<code>微批处理</code>。将流式计算分解成一系列短小的批处理作业，也不可避免的减弱系统的表达力。像状态管理或者<code>join</code>等操作的实现会变的困难，因为微批处理系统必须操作整个批量数据。并且，<code>batch interval</code>会连接两个不易连接的事情：基础属性和业务逻辑。相反地，微批处理系统的容错性和负载均衡实现起来非常简单，因为微批处理系统仅发送每批数据到一个<code>worker</code>节点上，如果一些数据出错那就使用其它副本。微批处理系统很容易建立在原生流处理系统之上。</p>\n<p>编程模型一般分为<code>组合式</code>和<code>声明式</code>。组合式编程提供基本的构建模块，它们必须紧密结合来创建拓扑。新的组件经常以接口的方式完成。相对应地，声明式API操作是定义的高阶函数。它允许我们用抽象类型和方法来写函数代码，并且系统创建拓扑和优化拓扑。声明式API经常也提供更多高级的操作（比如，窗口函数或者状态管理）。后面很快会给出样例代码。</p>\n<h3 id=\"3-主流流处理系统\"><a href=\"#3-主流流处理系统\" class=\"headerlink\" title=\"3. 主流流处理系统\"></a>3. 主流流处理系统</h3><p>有一系列各种实现的流处理框架，不能一一列举，这里仅选出主流的流处理解决方案，并且支持<code>Scala API</code>。因此，我们将详细介绍<code>Apache Storm</code>，<code>Trident</code>，<code>Spark Streaming</code>，<code>Samza</code>和<code>Apache Flink</code>。前面选择讲述的虽然都是流处理系统，但它们实现的方法包含了各种不同的挑战。这里暂时不讲商业的系统，比如<code>Google MillWheel</code>或者<code>Amazon Kinesis</code>，也不会涉及很少使用的<code>Intel GearPump</code>或者<code>Apache Apex</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-4.jpg?raw=true\" alt=\"\"></p>\n<p><code>Apache Storm</code>最开始是由<code>Nathan Marz</code>和他的团队于2010年在数据分析公司<code>BackType</code>开发的，后来<code>BackType</code>公司被<code>Twitter</code>收购，接着<code>Twitter</code>开源<code>Storm</code>并在2014年成为<code>Apache</code>顶级项目。毋庸置疑，<code>Storm</code>成为大规模流数据处理的先锋，并逐渐成为工业标准。<code>Storm</code>是原生的流处理系统，提供<code>low-level</code>的API。<code>Storm</code>使用<code>Thrift</code>来定义<code>topology</code>和支持多语言协议，使得我们可以使用大部分编程语言开发，<code>Scala</code>自然包括在内。</p>\n<p><code>Trident</code>是对<code>Storm</code>的一个更高层次的抽象，<code>Trident</code>最大的特点以<code>batch</code>的形式进行流处理。<code>Trident</code>简化<code>topology</code>构建过程，增加了窗口操作、聚合操作或者状态管理等高级操作，这些在<code>Storm</code>中并不支持。相对应于<code>Storm</code>的<code>At most once</code>流传输机制，<code>Trident</code>提供了<code>Exactly once</code>传输机制。<code>Trident</code>支持<code>Java</code>，<code>Clojure</code>和<code>Scala</code>。</p>\n<p>当前<code>Spark</code>是非常受欢迎的批处理框架，包含<code>Spark SQL</code>，<code>MLlib</code>和<code>Spark Streaming</code>。<code>Spark</code>的运行时是建立在批处理之上，因此后续加入的<code>Spark Streaming</code>也依赖于批处理，实现了微批处理。接收器把输入数据流分成短小批处理，并以类似<code>Spark</code>作业的方式处理微批处理。<code>Spark Streaming</code>提供高级声明式API（支持<code>Scala</code>，<code>Java</code>和<code>Python</code>）。</p>\n<p><code>Samza</code>最开始是专为<code>LinkedIn</code>公司开发的流处理解决方案，并和<code>LinkedIn</code>的<code>Kafka</code>一起贡献给社区，现已成为基础设施的关键部分。<code>Samza</code>的构建严重依赖于基于<code>log</code>的<code>Kafka</code>，两者紧密耦合。<code>Samza</code>提供组合式API，当然也支持<code>Scala</code>。</p>\n<p>最后来介绍<code>Apache Flink</code>。<code>Flink</code>是个相当早的项目，开始于2008年，但只在最近才得到注意。<code>Flink</code>是原生的流处理系统，提供<code>high level</code>的API。<code>Flink</code>也提供<code>API</code>来像<code>Spark</code>一样进行批处理，但两者处理的基础是完全不同的。<code>Flink</code>把批处理当作流处理中的一种特殊情况。在<code>Flink</code>中，所有的数据都看作流，是一种很好的抽象，因为这更接近于现实世界。</p>\n<p>快速的介绍流处理系统之后，让我们以下面的表格来更好清晰的展示它们之间的不同：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83-5.jpg?raw=true\" alt=\"\"></p>\n<h3 id=\"4-Word-Count\"><a href=\"#4-Word-Count\" class=\"headerlink\" title=\"4. Word Count\"></a>4. Word Count</h3><p><code>Wordcount</code>之于流处理框架学习，就好比<code>hello world</code>之于编程语言学习。它能很好的展示各流处理框架的不同之处，让我们从<code>Storm</code>开始看看如何实现<code>Wordcount</code>：<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">TopologyBuilder builder = <span class=\"keyword\">new</span> TopologyBuilder();</span><br><span class=\"line\">builder.setSpout(<span class=\"string\">\"spout\"</span>, <span class=\"keyword\">new</span> RandomSentenceSpout(), <span class=\"number\">5</span>);</span><br><span class=\"line\">builder.setBolt(<span class=\"string\">\"split\"</span>, <span class=\"keyword\">new</span> Split(), <span class=\"number\">8</span>).shuffleGrouping(<span class=\"string\">\"spout\"</span>);</span><br><span class=\"line\">builder.setBolt(<span class=\"string\">\"count\"</span>, <span class=\"keyword\">new</span> WordCount(), <span class=\"number\">12</span>).fieldsGrouping(<span class=\"string\">\"split\"</span>, <span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"> ...</span><br><span class=\"line\"></span><br><span class=\"line\">Map&lt;String, Integer&gt; counts = <span class=\"keyword\">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">execute</span><span class=\"params\">(Tuple tuple, BasicOutputCollector collector)</span> </span>&#123;</span><br><span class=\"line\">   String word = tuple.getString(<span class=\"number\">0</span>);</span><br><span class=\"line\">   Integer count = counts.containsKey(word) ? counts.get(word) + <span class=\"number\">1</span> : <span class=\"number\">1</span>;</span><br><span class=\"line\">   counts.put(word, count);</span><br><span class=\"line\">   collector.emit(<span class=\"keyword\">new</span> Values(word, count));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>首先，定义<code>topology</code>。第二行代码定义一个<code>spout</code>，作为数据源。然后是一个处理组件<code>bolt</code>，分割文本为单词。接着，定义另一个<code>bolt</code>来计算单词数（第四行代码）。也可以看到魔数5，8和12，这些是并行度，定义集群每个组件执行的独立线程数。第八行到十五行是实际的<code>WordCount bolt</code>实现。因为<code>Storm</code>不支持内建的状态管理，所有这里定义了一个局部状态。</p>\n<p>按之前描述，<code>Trident</code>是对<code>Storm</code>的一个更高层次的抽象，<code>Trident</code>最大的特点以<code>batch</code>的形式进行流处理。除了其它优势，<code>Trident</code>提供了状态管理，这对<code>wordcount</code>实现非常有用:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> StormTopology <span class=\"title\">buildTopology</span><span class=\"params\">(LocalDRPC drpc)</span> </span>&#123;</span><br><span class=\"line\"> FixedBatchSpout spout = ...</span><br><span class=\"line\"></span><br><span class=\"line\"> TridentTopology topology = <span class=\"keyword\">new</span> TridentTopology();</span><br><span class=\"line\"> TridentState wordCounts = topology.newStream(<span class=\"string\">\"spout1\"</span>, spout)</span><br><span class=\"line\"> .each(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"sentence\"</span>),<span class=\"keyword\">new</span> Split(), <span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>))</span><br><span class=\"line\"> .groupBy(<span class=\"keyword\">new</span> Fields(<span class=\"string\">\"word\"</span>))</span><br><span class=\"line\"> .persistentAggregate(<span class=\"keyword\">new</span> MemoryMapState.Factory(),</span><br><span class=\"line\"> <span class=\"keyword\">new</span> Count(), <span class=\"keyword\">new</span> Fields(<span class=\"string\">\"count\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"> ...</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p>如你所见，上面代码使用<code>higher level</code>操作，比如<code>each</code>（第七行代码）和<code>groupby</code>（第八行代码）。并且使用<code>Trident</code>管理状态来存储单词数（第九行代码）。</p>\n<p>下面是时候祭出提供声明式API的<code>Apache Spark</code>。记住，相对于前面的例子，这些代码相当简单，几乎没有冗余代码。下面是简单的流式计算单词数：<br><figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"wordcount\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> ssc = <span class=\"keyword\">new</span> <span class=\"type\">StreamingContext</span>(conf, <span class=\"type\">Seconds</span>(<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> text = ...</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">val</span> counts = text.flatMap(line =&gt; line.split(<span class=\"string\">\" \"</span>))</span><br><span class=\"line\"> .map(word =&gt; (word, <span class=\"number\">1</span>))</span><br><span class=\"line\"> .reduceByKey(_ + _)</span><br><span class=\"line\"></span><br><span class=\"line\">counts.print()</span><br><span class=\"line\"></span><br><span class=\"line\">ssc.start()</span><br><span class=\"line\">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></p>\n<p>每个<code>Spark Streaming</code>的作业都要有<code>StreamingContext</code>，它是流式函数的入口。<code>StreamingContext</code>加载第一行代码定义的配置<code>conf</code>，但更重要地，第二行代码定义<code>batch interval</code>（这里设置为1秒）。第六行到八行代码是整个单词数计算。这些是标准的函数式代码，<code>Spark</code>定义<code>topology</code>并且分布式执行。第十二行代码是每个<code>Spark Streaming</code>作业最后的部分：启动计算。记住，<code>Spark Streaming</code>作业一旦启动即不可修改。</p>\n<p>接下来看下<code>Apache Samza</code>，另外一个组合式API例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class WordCountTask extends StreamTask &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def process(envelope: IncomingMessageEnvelope, collector: MessageCollector,</span><br><span class=\"line\">    coordinator: TaskCoordinator) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    val text = envelope.getMessage.asInstanceOf[String]</span><br><span class=\"line\"></span><br><span class=\"line\">    val counts = text.split(&quot; &quot;).foldLeft(Map.empty[String, Int]) &#123;</span><br><span class=\"line\">      (count, word) =&gt; count + (word -&gt; (count.getOrElse(word, 0) + 1))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;), counts))</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>Samza</code>的属性配置文件定义<code>topology</code>，为了简明这里并没把配置文件放上来。定义任务的输入和输出，并通过<code>Kafka topic</code>通信。在单词数计算整个<code>topology</code>是<code>WordCountTask</code>。在<code>Samza</code>中，实现特殊接口定义组件<code>StreamTask</code>，在第三行代码重写方法<code>process</code>。它的参数列表包含所有连接其它系统的需要。第八行到十行简单的<code>Scala</code>代码是计算本身。</p>\n<p><code>Flink</code>的API跟<code>Spark Streaming</code>是惊人的相似，但注意到代码里并未设置<code>batch interval</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\"> val text = env.fromElements(...)</span><br><span class=\"line\"> val counts = text.flatMap ( _.split(&quot; &quot;) )</span><br><span class=\"line\">   .map ( (_, 1) )</span><br><span class=\"line\">   .groupBy(0)</span><br><span class=\"line\">   .sum(1)</span><br><span class=\"line\"></span><br><span class=\"line\"> counts.print()</span><br><span class=\"line\"></span><br><span class=\"line\"> env.execute(&quot;wordcount&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>上面的代码是相当的直白，仅仅只是几个函数式调用，Flink支持分布式计算。</p>\n<h3 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5. 结论\"></a>5. 结论</h3><p>上面给出了基本的理论和主流流处理框架介绍，下篇文章将会更深入的探讨其它关注点。希望你能对前面的文章感兴趣，如果有任何问题，请联系我讨论这些主题。</p>\n<p>原文:<a href=\"http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework</a></p>\n"},{"layout":"post","author":"侠天","title":"Stream 主流流处理框架比较(2)","date":"2018-01-10T08:31:01.000Z","_content":"\n在上篇文章中，我们过了下基本的理论，也介绍了主流的流处理框架：`Storm`，`Trident`，`Spark Streaming`，`Samza`和`Flink`。今天咱们来点有深度的主题，比如，容错，状态管理或者性能。除此之外，我们也将讨论开发分布式流处理应用的指南，并给出推荐的流处理框架。\n\n### 1. 容错性\n\n流处理系统的`容错性`与生俱来的比批处理系统难实现。当批处理系统中出现错误时，我们只需要把失败的部分简单重启即可；但对于流处理系统，出现错误就很难恢复。因为线上许多作业都是`7 x 24`小时运行，不断有输入的数据。流处理系统面临的另外一个挑战是`状态一致性`，因为重启后会出现重复数据，并且不是所有的状态操作是幂等的。容错性这么难实现，那下面我们看看各大主流流处理框架是如何处理这一问题。\n\n#### 1.1 Apache Storm\n\n`Storm`使用`上游数据备份`和`消息确认`的机制来保障消息在失败之后会重新处理。消息确认原理：每个操作都会把前一次的操作处理消息的确认信息返回。`Topology`的数据源备份它生成的所有数据记录。当所有数据记录的处理确认信息收到，备份即会被安全拆除。失败后，如果不是所有的消息处理确认信息收到，那数据记录会被数据源数据替换。这保障了没有数据丢失，但数据结果会有重复，这就是`at-least once`传输机制。\n\n`Storm`采用取巧的办法完成了容错性，对每个源数据记录仅仅要求几个字节存储空间来跟踪确认消息。纯数据记录消息确认架构，尽管性能不错，但不能保证`exactly once`消息传输机制，所有应用开发者需要处理重复数据。`Storm`存在低吞吐量和流控问题，因为消息确认机制在反压下经常误认为失败。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-1.png?raw=true)\n\n#### 1.2 Spark Streaming\n\n`Spark Streaming`实现微批处理，容错机制的实现跟`Storm`不一样。微批处理的想法相当简单。`Spark`在集群各`worker`节点上处理`micro-batches`。每个`micro-batches`一旦失败，重新计算就行。因为`micro-batches`本身的不可变性，并且每个`micro-batches`也会持久化，所以`exactly once`传输机制很容易实现。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-2.png?raw=true)\n\n#### 1.3 Samza\n\n`Samza`的实现方法跟前面两种流处理框架完全不一样。`Samza`利用消息系统`Kafka`的持久化和偏移量。`Samza`监控任务的偏移量，当任务处理完消息，相应的偏移量被移除。消息的偏移量会被`checkpoint`到持久化存储中，并在失败时恢复。但是问题在于：从上次`checkpoint`中修复偏移量时并不知道上游消息已经被处理过，这就会造成重复。这就是`at least once`传输机制。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-3.png?raw=true)\n\n#### 1.4 Apache Flink\n\n`Flink`的容错机制是基于分布式快照实现的，这些快照会保存流处理作业的状态(本文对`Flink`的检查点和快照不进行区分，因为两者实际是同一个事物的两种不同叫法。`Flink`构建这些快照的机制可以被描述成[分布式数据流的轻量级异步快照](https://arxiv.org/abs/1506.08603)，它采用`Chandy-Lamport`算法实现。)。如果发生失败的情况，系统可以从这些检查点进行恢复。`Flink`发送`checkpoint`的栅栏（`barrier`）到数据流中（栅栏是`Flink`的分布式快照机制中一个核心的元素），当`checkpoint`的栅栏到达其中一个`operator`，`operator`会接所有收输入流中对应的栅栏（比如，图中`checkpoint n`对应栅栏`n`到`n-1`的所有输入流，其仅仅是整个输入流的一部分）。所以相对于`Storm`，`Flink`的容错机制更高效，因为`Flink`的操作是对小批量数据而不是每条数据记录。但也不要让自己糊涂了，`Flink`仍然是原生流处理框架，它与`Spark Streaming`在概念上就完全不同。`Flink`也提供`exactly once`消息传输机制。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-4.png?raw=true)\n\n### 2. 状态管理\n\n大部分大型流处理应用都涉及到状态。相对于无状态的操作(其只有一个输入数据，处理过程和输出结果)，有状态的应用会有一个输入数据和一个状态信息，然后处理过程，接着输出结果和修改状态信息。因此，我们不得不管理状态信息，并持久化。我们期望一旦因某种原因失败，状态能够修复。状态修复有可能会出现小问题，它并不总是保证`exactly once`，有时也会出现消费多次，但这并不是我们想要的。\n\n#### 2.1 Apache Storm\n\n我们知道，`Storm`提供`at-least once`的消息传输保障。那我们又该如何使用`Trident`做到`exactly once`的语义。概念上貌似挺简单，你只需要提交每条数据记录，但这显然不是那么高效。所以你会想到小批量的数据记录一起提交会优化。`Trident`定义了几个抽象来达到`exactly once`的语义，见下图，其中也会有些局限。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-5.png?raw=true)\n\n#### 2.2 Spark Streaming\n\n`Spark Streaming`是微批处理系统，它把状态信息也看做是一种微批量数据流。在处理每个微批量数据时，`Spark`加载当前的状态信息，接着通过函数操作获得处理后的微批量数据结果并修改加载过的状态信息。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-6.png?raw=true)\n\n#### 2.3 Samza\n\n`Samza`实现状态管理是通过`Kafka`来处理的。`Samza`有真实的状态操作，所以其任务会持有一个状态信息，并把状态改变的日志推送到`Kafka`。如果需要状态重建，可以很容易的从`Kafka`的`topic`重建。为了达到更快的状态管理，`Samza`也支持把状态信息放入本地`key-value`存储中，所以状态信息不必一直在`Kafka`中管理，见下图。不幸的是，`Samza`只提供`at-least once`语义，`exactly once`的支持也在计划中。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-7.png?raw=true)\n\n#### 2.4 Apache Flink\n\n`Flink`提供状态操作，和`Samza`类似。`Flink`提供两种类型的状态：一种是用户自定义状态；另外一种是窗口状态。如图，第一个状态是自定义状态，它和其它的的状态不相互作用。这些状态可以分区或者使用嵌入式`Key-Value`存储状态(参阅[文容错](https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/fault_tolerance.html)和[状态](https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/state.html))。当然`Flink`提供`exactly-once`语义。下图展示`Flink`长期运行的三个状态。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-8.png?raw=true)\n\n### 3. 单词计数例子中的状态管理\n\n单词计数的详细代码见上篇文章，这里仅关注状态管理部分。\n\n让我们先看`Trident`：\n```\npublic static StormTopology buildTopology(LocalDRPC drpc) {\n   FixedBatchSpout spout = ...\n\n   TridentTopology topology = new TridentTopology();\n\n   TridentState wordCounts = topology.newStream(\"spout1\", spout)\n     .each(new Fields(\"sentence\"),new Split(), new Fields(\"word\"))\n     .groupBy(new Fields(\"word\"))\n     .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(\"count\"));\n\n ...\n\n }\n```\n在第九行代码中，我们通过调用`persistentAggregate`创建一个状态。其中参数`Count`存储单词数，如果你想从状态中处理数据，你必须创建一个数据流。从代码中也可以看出实现起来不方便。\n\n`Spark Streaming`声明式的方法稍微好点：\n```\n// Initial RDD input to updateStateByKey\nval initialRDD = ssc.sparkContext.parallelize(List.empty[(String, Int)])\n\nval lines = ...\nval words = lines.flatMap(_.split(\" \"))\nval wordDstream = words.map(x => (x, 1))\n\nval trackStateFunc = (batchTime: Time, word: String, one: Option[Int],\n  state: State[Int]) => {\n    val sum = one.getOrElse(0) + state.getOption.getOrElse(0)\n    val output = (word, sum)\n    state.update(sum)\n    Some(output)\n  }\n\nval stateDstream = wordDstream.trackStateByKey(\n  StateSpec.function(trackStateFunc).initialState(initialRDD))\n```\n首先我们需要创建一个`RDD`来初始化状态（第二行代码），然后进行`transformations`（第五行和六行代码）。接着在第八行到十四行代码，我们定义函数来处理单词数状态。函数计算并更新状态，最后返回结果。第十六行和十七行代码，我们得到一个状态信息流，其中包含单词数。\n\n接着我们看下`Samza`:\n```\nclass WordCountTask extends StreamTask with InitableTask {\n\n  private var store: CountStore = _\n\n  def init(config: Config, context: TaskContext) {\n    this.store = context.getStore(\"wordcount-store\")\n      .asInstanceOf[KeyValueStore[String, Integer]]\n  }\n\n override def process(envelope: IncomingMessageEnvelope,\n   collector: MessageCollector, coordinator: TaskCoordinator) {\n\n   val words = envelope.getMessage.asInstanceOf[String].split(\" \")\n\n   words.foreach { key =>\n     val count: Integer = Option(store.get(key)).getOrElse(0)\n     store.put(key, count + 1)\n     collector.send(new OutgoingMessageEnvelope(new SystemStream(\"kafka\", \"wordcount\"),\n       (key, count)))\n   }\n }\n```\n首先在第三行代码定义状态，进行`Key-Value`存储，在第五行到八行代码初始化状态。接着在计算中使用，上面的代码已经很直白。\n\n最后，讲下`Flink`使用简洁的API实现状态管理：\n```\nval env = ExecutionEnvironment.getExecutionEnvironment\n\nval text = env.fromElements(...)\nval words = text.flatMap ( _.split(\" \") )\n\nwords.keyBy(x => x).mapWithState {\n  (word, count: Option[Int]) =>\n    {\n      val newCount = count.getOrElse(0) + 1\n      val output = (word, newCount)\n      (output, Some(newCount))\n    }\n}\n```\n我们仅仅需要在第六行代码中调用`mapwithstate`函数，它有一个函数参数（函数有两个变量，第一个是单词，第二个是状态。然后返回处理的结果和新的状态）。\n\n### 4. 流处理框架性能\n\n这里所讲的性能主要涉及到的是`延迟性`和`吞吐量`。\n\n对于延迟性来说，微批处理一般在秒级别，大部分原生流处理在百毫秒以下，调优的情况下`Storm`可以很轻松的达到十毫秒。\n\n同时也要记住，消息传输机制保障，容错性和状态恢复都会占用机器资源。例如，打开容错恢复可能会降低10％到15％的性能，`Storm`可能降低70%的吞吐量。总之，天下没有免费的午餐。对于有状态管理，`Flink`会降低25%的性能，`Spark Streaming`降低50%的性能。\n\n也要记住，各大流处理框架的所有操作都是分布式的，通过网络发送数据是相当耗时的，所以要利用数据本地性，也尽量优化你的应用的序列化。\n\n### 5. 项目成熟度\n\n当你为应用选型时一定会考虑项目的成熟度。下面来快速浏览一下：\n`Storm`是第一个主流的流处理框架，后期已经成为长期的工业级的标准，并在像`Twitter`，`Yahoo`，`Spotify`等大公司使用。`Spark Streaming`是最近最流行的`Scala`代码实现的流处理框架。现在`Spark Streaming`被公司（`Netflix`, `Cisco`, `DataStax`, `Intel`, `IBM`等）日渐接受。`Samza`主要在`LinkedIn`公司使用。`Flink`是一个新兴的项目，很有前景。\n\n你可能对项目的贡献者数量也感兴趣。`Storm`和`Trident`大概有180个代码贡献者；整个`Spark`有720多个；根据`github`显示，`Samza`有40个；`Flink`有超过130个代码贡献者。\n\n### 6. 小结\n\n在进行流处理框架推荐之前，先来整体看下总结表：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-9.png?raw=true)\n\n### 7. 流处理框架推荐\n\n应用选型是大家都会遇到的问题，一般是根据应用具体的场景来选择特定的流处理框架。下面给出几个作者认为优先考虑的点：\n- `High level API`：具有`high level API`的流处理框架会更简洁和高效；\n- 状态管理：大部分流处理应用都涉及到状态管理，因此你得把状态管理作为评价指标之一；\n- `exactly once`语义：`exactly once`会使得应用开发变得简单，但也要看具体需求，可能`at least once`或者`at most once`语义就满足你得要求；\n- 自动恢复：确保流处理系统能够快速恢复，你可以使用`Chaos Monkey`或者类似的工具进行测试。快速的恢复是流处理重要的部分。\n\n`Storm`：`Storm`非常适合任务量小但速度要求高的应用。如果你主要在意流处理框架的延迟性，`Storm`将可能是你的首先。但同时也要记住，`Storm`的容错恢复或者`Trident`的状态管理都会降低整体的性能水平。也有一个潜在的`Storm`更新项目-`Twitter`的`Heron`，`Heron`设计的初衷是为了替代`Storm`，并在每个单任务上做了优化但同时保留了API。\n\n`Spark Streaming`：如果你得基础架构中已经涉及到`Spark`，那`Spark Streaming`无疑是值得你尝试的。因为你可以很好的利用`Spark`各种`library`。如果你需要使用`Lambda`架构，`Spark Streaming`也是一个不错的选择。但你要时刻记住微批处理的局限性，以及它的延迟性问题。\n\n`Samza`：如果你想使用`Samza`，那`Kafka`应该是你基础架构中的基石，好在现在`Kafka`已经成为家喻户晓的组件。像前面提到的，`Samza`一般会搭配强大的本地存储一起，这对管理大数据量的状态非常有益。它可以轻松处理上万千兆字节的状态信息，但要记住`Samza`只支持`at least once`语义。\n\n`Flink`：`Flink`流处理系统的概念非常不错，并且满足绝大多数流处理场景，也经常提供前沿的功能函数，比如，高级窗口函数或者时间处理功能，这些在其它流处理框架中是没有的。同时`Flink`也有API提供给通用的批处理场景。但你需要足够的勇气去上线一个新兴的项目，并且你也不能忘了看下`Flink`的`roadmap`。\n\n### 8. Dataflow和开源\n\n最后，我们来聊下`Dataflow`和它的开源。`Dataflow`是`Google`云平台的一部分，`Google`云平台包含很多组件：大数据存储，`BigQuery`，`Cloud PubSub`，数据分析工具和前面提到的`Dataflow`。\n\n`Dataflow`是`Google`管理批处理和流处理的统一API。它是建立在`MapReduce`（批处理），`FlumeJava`（编程模型）和`MillWheel`（流处理）之上。`Google`最近决定开源`Dataflow SDK`，并完成`Spark`和`Flink`的`runner`。现在可以通过`Dataflow`的API来定义`Google`云平台作业、`Flink`作业或者`Spark`作业，后续会增加对其它引擎的支持。\n\n`Google`为`Dataflow`提供`Java`、`Python`的API，社区已经完成`Scalable`的`DSL`支持。除此之外，`Google`及其合作者提交`Apache Beam`到`Apache`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-10.png?raw=true)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&utm_campaign=user_page&utm_medium=link\n","source":"_posts/Stream/[Stream]主流流处理框架比较(2).md","raw":"---\nlayout: post\nauthor: 侠天\ntitle: Stream 主流流处理框架比较(2)\ndate: 2018-01-10 16:31:01\ntags:\n  - Stream\n\ncategories: Stream\n---\n\n在上篇文章中，我们过了下基本的理论，也介绍了主流的流处理框架：`Storm`，`Trident`，`Spark Streaming`，`Samza`和`Flink`。今天咱们来点有深度的主题，比如，容错，状态管理或者性能。除此之外，我们也将讨论开发分布式流处理应用的指南，并给出推荐的流处理框架。\n\n### 1. 容错性\n\n流处理系统的`容错性`与生俱来的比批处理系统难实现。当批处理系统中出现错误时，我们只需要把失败的部分简单重启即可；但对于流处理系统，出现错误就很难恢复。因为线上许多作业都是`7 x 24`小时运行，不断有输入的数据。流处理系统面临的另外一个挑战是`状态一致性`，因为重启后会出现重复数据，并且不是所有的状态操作是幂等的。容错性这么难实现，那下面我们看看各大主流流处理框架是如何处理这一问题。\n\n#### 1.1 Apache Storm\n\n`Storm`使用`上游数据备份`和`消息确认`的机制来保障消息在失败之后会重新处理。消息确认原理：每个操作都会把前一次的操作处理消息的确认信息返回。`Topology`的数据源备份它生成的所有数据记录。当所有数据记录的处理确认信息收到，备份即会被安全拆除。失败后，如果不是所有的消息处理确认信息收到，那数据记录会被数据源数据替换。这保障了没有数据丢失，但数据结果会有重复，这就是`at-least once`传输机制。\n\n`Storm`采用取巧的办法完成了容错性，对每个源数据记录仅仅要求几个字节存储空间来跟踪确认消息。纯数据记录消息确认架构，尽管性能不错，但不能保证`exactly once`消息传输机制，所有应用开发者需要处理重复数据。`Storm`存在低吞吐量和流控问题，因为消息确认机制在反压下经常误认为失败。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-1.png?raw=true)\n\n#### 1.2 Spark Streaming\n\n`Spark Streaming`实现微批处理，容错机制的实现跟`Storm`不一样。微批处理的想法相当简单。`Spark`在集群各`worker`节点上处理`micro-batches`。每个`micro-batches`一旦失败，重新计算就行。因为`micro-batches`本身的不可变性，并且每个`micro-batches`也会持久化，所以`exactly once`传输机制很容易实现。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-2.png?raw=true)\n\n#### 1.3 Samza\n\n`Samza`的实现方法跟前面两种流处理框架完全不一样。`Samza`利用消息系统`Kafka`的持久化和偏移量。`Samza`监控任务的偏移量，当任务处理完消息，相应的偏移量被移除。消息的偏移量会被`checkpoint`到持久化存储中，并在失败时恢复。但是问题在于：从上次`checkpoint`中修复偏移量时并不知道上游消息已经被处理过，这就会造成重复。这就是`at least once`传输机制。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-3.png?raw=true)\n\n#### 1.4 Apache Flink\n\n`Flink`的容错机制是基于分布式快照实现的，这些快照会保存流处理作业的状态(本文对`Flink`的检查点和快照不进行区分，因为两者实际是同一个事物的两种不同叫法。`Flink`构建这些快照的机制可以被描述成[分布式数据流的轻量级异步快照](https://arxiv.org/abs/1506.08603)，它采用`Chandy-Lamport`算法实现。)。如果发生失败的情况，系统可以从这些检查点进行恢复。`Flink`发送`checkpoint`的栅栏（`barrier`）到数据流中（栅栏是`Flink`的分布式快照机制中一个核心的元素），当`checkpoint`的栅栏到达其中一个`operator`，`operator`会接所有收输入流中对应的栅栏（比如，图中`checkpoint n`对应栅栏`n`到`n-1`的所有输入流，其仅仅是整个输入流的一部分）。所以相对于`Storm`，`Flink`的容错机制更高效，因为`Flink`的操作是对小批量数据而不是每条数据记录。但也不要让自己糊涂了，`Flink`仍然是原生流处理框架，它与`Spark Streaming`在概念上就完全不同。`Flink`也提供`exactly once`消息传输机制。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-4.png?raw=true)\n\n### 2. 状态管理\n\n大部分大型流处理应用都涉及到状态。相对于无状态的操作(其只有一个输入数据，处理过程和输出结果)，有状态的应用会有一个输入数据和一个状态信息，然后处理过程，接着输出结果和修改状态信息。因此，我们不得不管理状态信息，并持久化。我们期望一旦因某种原因失败，状态能够修复。状态修复有可能会出现小问题，它并不总是保证`exactly once`，有时也会出现消费多次，但这并不是我们想要的。\n\n#### 2.1 Apache Storm\n\n我们知道，`Storm`提供`at-least once`的消息传输保障。那我们又该如何使用`Trident`做到`exactly once`的语义。概念上貌似挺简单，你只需要提交每条数据记录，但这显然不是那么高效。所以你会想到小批量的数据记录一起提交会优化。`Trident`定义了几个抽象来达到`exactly once`的语义，见下图，其中也会有些局限。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-5.png?raw=true)\n\n#### 2.2 Spark Streaming\n\n`Spark Streaming`是微批处理系统，它把状态信息也看做是一种微批量数据流。在处理每个微批量数据时，`Spark`加载当前的状态信息，接着通过函数操作获得处理后的微批量数据结果并修改加载过的状态信息。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-6.png?raw=true)\n\n#### 2.3 Samza\n\n`Samza`实现状态管理是通过`Kafka`来处理的。`Samza`有真实的状态操作，所以其任务会持有一个状态信息，并把状态改变的日志推送到`Kafka`。如果需要状态重建，可以很容易的从`Kafka`的`topic`重建。为了达到更快的状态管理，`Samza`也支持把状态信息放入本地`key-value`存储中，所以状态信息不必一直在`Kafka`中管理，见下图。不幸的是，`Samza`只提供`at-least once`语义，`exactly once`的支持也在计划中。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-7.png?raw=true)\n\n#### 2.4 Apache Flink\n\n`Flink`提供状态操作，和`Samza`类似。`Flink`提供两种类型的状态：一种是用户自定义状态；另外一种是窗口状态。如图，第一个状态是自定义状态，它和其它的的状态不相互作用。这些状态可以分区或者使用嵌入式`Key-Value`存储状态(参阅[文容错](https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/fault_tolerance.html)和[状态](https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/state.html))。当然`Flink`提供`exactly-once`语义。下图展示`Flink`长期运行的三个状态。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-8.png?raw=true)\n\n### 3. 单词计数例子中的状态管理\n\n单词计数的详细代码见上篇文章，这里仅关注状态管理部分。\n\n让我们先看`Trident`：\n```\npublic static StormTopology buildTopology(LocalDRPC drpc) {\n   FixedBatchSpout spout = ...\n\n   TridentTopology topology = new TridentTopology();\n\n   TridentState wordCounts = topology.newStream(\"spout1\", spout)\n     .each(new Fields(\"sentence\"),new Split(), new Fields(\"word\"))\n     .groupBy(new Fields(\"word\"))\n     .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(\"count\"));\n\n ...\n\n }\n```\n在第九行代码中，我们通过调用`persistentAggregate`创建一个状态。其中参数`Count`存储单词数，如果你想从状态中处理数据，你必须创建一个数据流。从代码中也可以看出实现起来不方便。\n\n`Spark Streaming`声明式的方法稍微好点：\n```\n// Initial RDD input to updateStateByKey\nval initialRDD = ssc.sparkContext.parallelize(List.empty[(String, Int)])\n\nval lines = ...\nval words = lines.flatMap(_.split(\" \"))\nval wordDstream = words.map(x => (x, 1))\n\nval trackStateFunc = (batchTime: Time, word: String, one: Option[Int],\n  state: State[Int]) => {\n    val sum = one.getOrElse(0) + state.getOption.getOrElse(0)\n    val output = (word, sum)\n    state.update(sum)\n    Some(output)\n  }\n\nval stateDstream = wordDstream.trackStateByKey(\n  StateSpec.function(trackStateFunc).initialState(initialRDD))\n```\n首先我们需要创建一个`RDD`来初始化状态（第二行代码），然后进行`transformations`（第五行和六行代码）。接着在第八行到十四行代码，我们定义函数来处理单词数状态。函数计算并更新状态，最后返回结果。第十六行和十七行代码，我们得到一个状态信息流，其中包含单词数。\n\n接着我们看下`Samza`:\n```\nclass WordCountTask extends StreamTask with InitableTask {\n\n  private var store: CountStore = _\n\n  def init(config: Config, context: TaskContext) {\n    this.store = context.getStore(\"wordcount-store\")\n      .asInstanceOf[KeyValueStore[String, Integer]]\n  }\n\n override def process(envelope: IncomingMessageEnvelope,\n   collector: MessageCollector, coordinator: TaskCoordinator) {\n\n   val words = envelope.getMessage.asInstanceOf[String].split(\" \")\n\n   words.foreach { key =>\n     val count: Integer = Option(store.get(key)).getOrElse(0)\n     store.put(key, count + 1)\n     collector.send(new OutgoingMessageEnvelope(new SystemStream(\"kafka\", \"wordcount\"),\n       (key, count)))\n   }\n }\n```\n首先在第三行代码定义状态，进行`Key-Value`存储，在第五行到八行代码初始化状态。接着在计算中使用，上面的代码已经很直白。\n\n最后，讲下`Flink`使用简洁的API实现状态管理：\n```\nval env = ExecutionEnvironment.getExecutionEnvironment\n\nval text = env.fromElements(...)\nval words = text.flatMap ( _.split(\" \") )\n\nwords.keyBy(x => x).mapWithState {\n  (word, count: Option[Int]) =>\n    {\n      val newCount = count.getOrElse(0) + 1\n      val output = (word, newCount)\n      (output, Some(newCount))\n    }\n}\n```\n我们仅仅需要在第六行代码中调用`mapwithstate`函数，它有一个函数参数（函数有两个变量，第一个是单词，第二个是状态。然后返回处理的结果和新的状态）。\n\n### 4. 流处理框架性能\n\n这里所讲的性能主要涉及到的是`延迟性`和`吞吐量`。\n\n对于延迟性来说，微批处理一般在秒级别，大部分原生流处理在百毫秒以下，调优的情况下`Storm`可以很轻松的达到十毫秒。\n\n同时也要记住，消息传输机制保障，容错性和状态恢复都会占用机器资源。例如，打开容错恢复可能会降低10％到15％的性能，`Storm`可能降低70%的吞吐量。总之，天下没有免费的午餐。对于有状态管理，`Flink`会降低25%的性能，`Spark Streaming`降低50%的性能。\n\n也要记住，各大流处理框架的所有操作都是分布式的，通过网络发送数据是相当耗时的，所以要利用数据本地性，也尽量优化你的应用的序列化。\n\n### 5. 项目成熟度\n\n当你为应用选型时一定会考虑项目的成熟度。下面来快速浏览一下：\n`Storm`是第一个主流的流处理框架，后期已经成为长期的工业级的标准，并在像`Twitter`，`Yahoo`，`Spotify`等大公司使用。`Spark Streaming`是最近最流行的`Scala`代码实现的流处理框架。现在`Spark Streaming`被公司（`Netflix`, `Cisco`, `DataStax`, `Intel`, `IBM`等）日渐接受。`Samza`主要在`LinkedIn`公司使用。`Flink`是一个新兴的项目，很有前景。\n\n你可能对项目的贡献者数量也感兴趣。`Storm`和`Trident`大概有180个代码贡献者；整个`Spark`有720多个；根据`github`显示，`Samza`有40个；`Flink`有超过130个代码贡献者。\n\n### 6. 小结\n\n在进行流处理框架推荐之前，先来整体看下总结表：\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-9.png?raw=true)\n\n### 7. 流处理框架推荐\n\n应用选型是大家都会遇到的问题，一般是根据应用具体的场景来选择特定的流处理框架。下面给出几个作者认为优先考虑的点：\n- `High level API`：具有`high level API`的流处理框架会更简洁和高效；\n- 状态管理：大部分流处理应用都涉及到状态管理，因此你得把状态管理作为评价指标之一；\n- `exactly once`语义：`exactly once`会使得应用开发变得简单，但也要看具体需求，可能`at least once`或者`at most once`语义就满足你得要求；\n- 自动恢复：确保流处理系统能够快速恢复，你可以使用`Chaos Monkey`或者类似的工具进行测试。快速的恢复是流处理重要的部分。\n\n`Storm`：`Storm`非常适合任务量小但速度要求高的应用。如果你主要在意流处理框架的延迟性，`Storm`将可能是你的首先。但同时也要记住，`Storm`的容错恢复或者`Trident`的状态管理都会降低整体的性能水平。也有一个潜在的`Storm`更新项目-`Twitter`的`Heron`，`Heron`设计的初衷是为了替代`Storm`，并在每个单任务上做了优化但同时保留了API。\n\n`Spark Streaming`：如果你得基础架构中已经涉及到`Spark`，那`Spark Streaming`无疑是值得你尝试的。因为你可以很好的利用`Spark`各种`library`。如果你需要使用`Lambda`架构，`Spark Streaming`也是一个不错的选择。但你要时刻记住微批处理的局限性，以及它的延迟性问题。\n\n`Samza`：如果你想使用`Samza`，那`Kafka`应该是你基础架构中的基石，好在现在`Kafka`已经成为家喻户晓的组件。像前面提到的，`Samza`一般会搭配强大的本地存储一起，这对管理大数据量的状态非常有益。它可以轻松处理上万千兆字节的状态信息，但要记住`Samza`只支持`at least once`语义。\n\n`Flink`：`Flink`流处理系统的概念非常不错，并且满足绝大多数流处理场景，也经常提供前沿的功能函数，比如，高级窗口函数或者时间处理功能，这些在其它流处理框架中是没有的。同时`Flink`也有API提供给通用的批处理场景。但你需要足够的勇气去上线一个新兴的项目，并且你也不能忘了看下`Flink`的`roadmap`。\n\n### 8. Dataflow和开源\n\n最后，我们来聊下`Dataflow`和它的开源。`Dataflow`是`Google`云平台的一部分，`Google`云平台包含很多组件：大数据存储，`BigQuery`，`Cloud PubSub`，数据分析工具和前面提到的`Dataflow`。\n\n`Dataflow`是`Google`管理批处理和流处理的统一API。它是建立在`MapReduce`（批处理），`FlumeJava`（编程模型）和`MillWheel`（流处理）之上。`Google`最近决定开源`Dataflow SDK`，并完成`Spark`和`Flink`的`runner`。现在可以通过`Dataflow`的API来定义`Google`云平台作业、`Flink`作业或者`Spark`作业，后续会增加对其它引擎的支持。\n\n`Google`为`Dataflow`提供`Java`、`Python`的API，社区已经完成`Scalable`的`DSL`支持。除此之外，`Google`及其合作者提交`Apache Beam`到`Apache`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-10.png?raw=true)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原文:http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&utm_campaign=user_page&utm_medium=link\n","slug":"Stream/[Stream]主流流处理框架比较(2)","published":1,"updated":"2018-01-29T09:36:59.614Z","comments":1,"photos":[],"link":"","_id":"cje58tivn0064ordbd3iptop6","content":"<p>在上篇文章中，我们过了下基本的理论，也介绍了主流的流处理框架：<code>Storm</code>，<code>Trident</code>，<code>Spark Streaming</code>，<code>Samza</code>和<code>Flink</code>。今天咱们来点有深度的主题，比如，容错，状态管理或者性能。除此之外，我们也将讨论开发分布式流处理应用的指南，并给出推荐的流处理框架。</p>\n<h3 id=\"1-容错性\"><a href=\"#1-容错性\" class=\"headerlink\" title=\"1. 容错性\"></a>1. 容错性</h3><p>流处理系统的<code>容错性</code>与生俱来的比批处理系统难实现。当批处理系统中出现错误时，我们只需要把失败的部分简单重启即可；但对于流处理系统，出现错误就很难恢复。因为线上许多作业都是<code>7 x 24</code>小时运行，不断有输入的数据。流处理系统面临的另外一个挑战是<code>状态一致性</code>，因为重启后会出现重复数据，并且不是所有的状态操作是幂等的。容错性这么难实现，那下面我们看看各大主流流处理框架是如何处理这一问题。</p>\n<h4 id=\"1-1-Apache-Storm\"><a href=\"#1-1-Apache-Storm\" class=\"headerlink\" title=\"1.1 Apache Storm\"></a>1.1 Apache Storm</h4><p><code>Storm</code>使用<code>上游数据备份</code>和<code>消息确认</code>的机制来保障消息在失败之后会重新处理。消息确认原理：每个操作都会把前一次的操作处理消息的确认信息返回。<code>Topology</code>的数据源备份它生成的所有数据记录。当所有数据记录的处理确认信息收到，备份即会被安全拆除。失败后，如果不是所有的消息处理确认信息收到，那数据记录会被数据源数据替换。这保障了没有数据丢失，但数据结果会有重复，这就是<code>at-least once</code>传输机制。</p>\n<p><code>Storm</code>采用取巧的办法完成了容错性，对每个源数据记录仅仅要求几个字节存储空间来跟踪确认消息。纯数据记录消息确认架构，尽管性能不错，但不能保证<code>exactly once</code>消息传输机制，所有应用开发者需要处理重复数据。<code>Storm</code>存在低吞吐量和流控问题，因为消息确认机制在反压下经常误认为失败。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-1.png?raw=true\" alt=\"\"></p>\n<h4 id=\"1-2-Spark-Streaming\"><a href=\"#1-2-Spark-Streaming\" class=\"headerlink\" title=\"1.2 Spark Streaming\"></a>1.2 Spark Streaming</h4><p><code>Spark Streaming</code>实现微批处理，容错机制的实现跟<code>Storm</code>不一样。微批处理的想法相当简单。<code>Spark</code>在集群各<code>worker</code>节点上处理<code>micro-batches</code>。每个<code>micro-batches</code>一旦失败，重新计算就行。因为<code>micro-batches</code>本身的不可变性，并且每个<code>micro-batches</code>也会持久化，所以<code>exactly once</code>传输机制很容易实现。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-2.png?raw=true\" alt=\"\"></p>\n<h4 id=\"1-3-Samza\"><a href=\"#1-3-Samza\" class=\"headerlink\" title=\"1.3 Samza\"></a>1.3 Samza</h4><p><code>Samza</code>的实现方法跟前面两种流处理框架完全不一样。<code>Samza</code>利用消息系统<code>Kafka</code>的持久化和偏移量。<code>Samza</code>监控任务的偏移量，当任务处理完消息，相应的偏移量被移除。消息的偏移量会被<code>checkpoint</code>到持久化存储中，并在失败时恢复。但是问题在于：从上次<code>checkpoint</code>中修复偏移量时并不知道上游消息已经被处理过，这就会造成重复。这就是<code>at least once</code>传输机制。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"1-4-Apache-Flink\"><a href=\"#1-4-Apache-Flink\" class=\"headerlink\" title=\"1.4 Apache Flink\"></a>1.4 Apache Flink</h4><p><code>Flink</code>的容错机制是基于分布式快照实现的，这些快照会保存流处理作业的状态(本文对<code>Flink</code>的检查点和快照不进行区分，因为两者实际是同一个事物的两种不同叫法。<code>Flink</code>构建这些快照的机制可以被描述成<a href=\"https://arxiv.org/abs/1506.08603\" target=\"_blank\" rel=\"noopener\">分布式数据流的轻量级异步快照</a>，它采用<code>Chandy-Lamport</code>算法实现。)。如果发生失败的情况，系统可以从这些检查点进行恢复。<code>Flink</code>发送<code>checkpoint</code>的栅栏（<code>barrier</code>）到数据流中（栅栏是<code>Flink</code>的分布式快照机制中一个核心的元素），当<code>checkpoint</code>的栅栏到达其中一个<code>operator</code>，<code>operator</code>会接所有收输入流中对应的栅栏（比如，图中<code>checkpoint n</code>对应栅栏<code>n</code>到<code>n-1</code>的所有输入流，其仅仅是整个输入流的一部分）。所以相对于<code>Storm</code>，<code>Flink</code>的容错机制更高效，因为<code>Flink</code>的操作是对小批量数据而不是每条数据记录。但也不要让自己糊涂了，<code>Flink</code>仍然是原生流处理框架，它与<code>Spark Streaming</code>在概念上就完全不同。<code>Flink</code>也提供<code>exactly once</code>消息传输机制。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-4.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-状态管理\"><a href=\"#2-状态管理\" class=\"headerlink\" title=\"2. 状态管理\"></a>2. 状态管理</h3><p>大部分大型流处理应用都涉及到状态。相对于无状态的操作(其只有一个输入数据，处理过程和输出结果)，有状态的应用会有一个输入数据和一个状态信息，然后处理过程，接着输出结果和修改状态信息。因此，我们不得不管理状态信息，并持久化。我们期望一旦因某种原因失败，状态能够修复。状态修复有可能会出现小问题，它并不总是保证<code>exactly once</code>，有时也会出现消费多次，但这并不是我们想要的。</p>\n<h4 id=\"2-1-Apache-Storm\"><a href=\"#2-1-Apache-Storm\" class=\"headerlink\" title=\"2.1 Apache Storm\"></a>2.1 Apache Storm</h4><p>我们知道，<code>Storm</code>提供<code>at-least once</code>的消息传输保障。那我们又该如何使用<code>Trident</code>做到<code>exactly once</code>的语义。概念上貌似挺简单，你只需要提交每条数据记录，但这显然不是那么高效。所以你会想到小批量的数据记录一起提交会优化。<code>Trident</code>定义了几个抽象来达到<code>exactly once</code>的语义，见下图，其中也会有些局限。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-5.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-2-Spark-Streaming\"><a href=\"#2-2-Spark-Streaming\" class=\"headerlink\" title=\"2.2 Spark Streaming\"></a>2.2 Spark Streaming</h4><p><code>Spark Streaming</code>是微批处理系统，它把状态信息也看做是一种微批量数据流。在处理每个微批量数据时，<code>Spark</code>加载当前的状态信息，接着通过函数操作获得处理后的微批量数据结果并修改加载过的状态信息。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-6.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-Samza\"><a href=\"#2-3-Samza\" class=\"headerlink\" title=\"2.3 Samza\"></a>2.3 Samza</h4><p><code>Samza</code>实现状态管理是通过<code>Kafka</code>来处理的。<code>Samza</code>有真实的状态操作，所以其任务会持有一个状态信息，并把状态改变的日志推送到<code>Kafka</code>。如果需要状态重建，可以很容易的从<code>Kafka</code>的<code>topic</code>重建。为了达到更快的状态管理，<code>Samza</code>也支持把状态信息放入本地<code>key-value</code>存储中，所以状态信息不必一直在<code>Kafka</code>中管理，见下图。不幸的是，<code>Samza</code>只提供<code>at-least once</code>语义，<code>exactly once</code>的支持也在计划中。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-7.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-4-Apache-Flink\"><a href=\"#2-4-Apache-Flink\" class=\"headerlink\" title=\"2.4 Apache Flink\"></a>2.4 Apache Flink</h4><p><code>Flink</code>提供状态操作，和<code>Samza</code>类似。<code>Flink</code>提供两种类型的状态：一种是用户自定义状态；另外一种是窗口状态。如图，第一个状态是自定义状态，它和其它的的状态不相互作用。这些状态可以分区或者使用嵌入式<code>Key-Value</code>存储状态(参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/fault_tolerance.html\" target=\"_blank\" rel=\"noopener\">文容错</a>和<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/state.html\" target=\"_blank\" rel=\"noopener\">状态</a>)。当然<code>Flink</code>提供<code>exactly-once</code>语义。下图展示<code>Flink</code>长期运行的三个状态。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-8.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-单词计数例子中的状态管理\"><a href=\"#3-单词计数例子中的状态管理\" class=\"headerlink\" title=\"3. 单词计数例子中的状态管理\"></a>3. 单词计数例子中的状态管理</h3><p>单词计数的详细代码见上篇文章，这里仅关注状态管理部分。</p>\n<p>让我们先看<code>Trident</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">public static StormTopology buildTopology(LocalDRPC drpc) &#123;</span><br><span class=\"line\">   FixedBatchSpout spout = ...</span><br><span class=\"line\"></span><br><span class=\"line\">   TridentTopology topology = new TridentTopology();</span><br><span class=\"line\"></span><br><span class=\"line\">   TridentState wordCounts = topology.newStream(&quot;spout1&quot;, spout)</span><br><span class=\"line\">     .each(new Fields(&quot;sentence&quot;),new Split(), new Fields(&quot;word&quot;))</span><br><span class=\"line\">     .groupBy(new Fields(&quot;word&quot;))</span><br><span class=\"line\">     .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;));</span><br><span class=\"line\"></span><br><span class=\"line\"> ...</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p>在第九行代码中，我们通过调用<code>persistentAggregate</code>创建一个状态。其中参数<code>Count</code>存储单词数，如果你想从状态中处理数据，你必须创建一个数据流。从代码中也可以看出实现起来不方便。</p>\n<p><code>Spark Streaming</code>声明式的方法稍微好点：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// Initial RDD input to updateStateByKey</span><br><span class=\"line\">val initialRDD = ssc.sparkContext.parallelize(List.empty[(String, Int)])</span><br><span class=\"line\"></span><br><span class=\"line\">val lines = ...</span><br><span class=\"line\">val words = lines.flatMap(_.split(&quot; &quot;))</span><br><span class=\"line\">val wordDstream = words.map(x =&gt; (x, 1))</span><br><span class=\"line\"></span><br><span class=\"line\">val trackStateFunc = (batchTime: Time, word: String, one: Option[Int],</span><br><span class=\"line\">  state: State[Int]) =&gt; &#123;</span><br><span class=\"line\">    val sum = one.getOrElse(0) + state.getOption.getOrElse(0)</span><br><span class=\"line\">    val output = (word, sum)</span><br><span class=\"line\">    state.update(sum)</span><br><span class=\"line\">    Some(output)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">val stateDstream = wordDstream.trackStateByKey(</span><br><span class=\"line\">  StateSpec.function(trackStateFunc).initialState(initialRDD))</span><br></pre></td></tr></table></figure></p>\n<p>首先我们需要创建一个<code>RDD</code>来初始化状态（第二行代码），然后进行<code>transformations</code>（第五行和六行代码）。接着在第八行到十四行代码，我们定义函数来处理单词数状态。函数计算并更新状态，最后返回结果。第十六行和十七行代码，我们得到一个状态信息流，其中包含单词数。</p>\n<p>接着我们看下<code>Samza</code>:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class WordCountTask extends StreamTask with InitableTask &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  private var store: CountStore = _</span><br><span class=\"line\"></span><br><span class=\"line\">  def init(config: Config, context: TaskContext) &#123;</span><br><span class=\"line\">    this.store = context.getStore(&quot;wordcount-store&quot;)</span><br><span class=\"line\">      .asInstanceOf[KeyValueStore[String, Integer]]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> override def process(envelope: IncomingMessageEnvelope,</span><br><span class=\"line\">   collector: MessageCollector, coordinator: TaskCoordinator) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   val words = envelope.getMessage.asInstanceOf[String].split(&quot; &quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   words.foreach &#123; key =&gt;</span><br><span class=\"line\">     val count: Integer = Option(store.get(key)).getOrElse(0)</span><br><span class=\"line\">     store.put(key, count + 1)</span><br><span class=\"line\">     collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;),</span><br><span class=\"line\">       (key, count)))</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p>首先在第三行代码定义状态，进行<code>Key-Value</code>存储，在第五行到八行代码初始化状态。接着在计算中使用，上面的代码已经很直白。</p>\n<p>最后，讲下<code>Flink</code>使用简洁的API实现状态管理：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">val text = env.fromElements(...)</span><br><span class=\"line\">val words = text.flatMap ( _.split(&quot; &quot;) )</span><br><span class=\"line\"></span><br><span class=\"line\">words.keyBy(x =&gt; x).mapWithState &#123;</span><br><span class=\"line\">  (word, count: Option[Int]) =&gt;</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      val newCount = count.getOrElse(0) + 1</span><br><span class=\"line\">      val output = (word, newCount)</span><br><span class=\"line\">      (output, Some(newCount))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>我们仅仅需要在第六行代码中调用<code>mapwithstate</code>函数，它有一个函数参数（函数有两个变量，第一个是单词，第二个是状态。然后返回处理的结果和新的状态）。</p>\n<h3 id=\"4-流处理框架性能\"><a href=\"#4-流处理框架性能\" class=\"headerlink\" title=\"4. 流处理框架性能\"></a>4. 流处理框架性能</h3><p>这里所讲的性能主要涉及到的是<code>延迟性</code>和<code>吞吐量</code>。</p>\n<p>对于延迟性来说，微批处理一般在秒级别，大部分原生流处理在百毫秒以下，调优的情况下<code>Storm</code>可以很轻松的达到十毫秒。</p>\n<p>同时也要记住，消息传输机制保障，容错性和状态恢复都会占用机器资源。例如，打开容错恢复可能会降低10％到15％的性能，<code>Storm</code>可能降低70%的吞吐量。总之，天下没有免费的午餐。对于有状态管理，<code>Flink</code>会降低25%的性能，<code>Spark Streaming</code>降低50%的性能。</p>\n<p>也要记住，各大流处理框架的所有操作都是分布式的，通过网络发送数据是相当耗时的，所以要利用数据本地性，也尽量优化你的应用的序列化。</p>\n<h3 id=\"5-项目成熟度\"><a href=\"#5-项目成熟度\" class=\"headerlink\" title=\"5. 项目成熟度\"></a>5. 项目成熟度</h3><p>当你为应用选型时一定会考虑项目的成熟度。下面来快速浏览一下：<br><code>Storm</code>是第一个主流的流处理框架，后期已经成为长期的工业级的标准，并在像<code>Twitter</code>，<code>Yahoo</code>，<code>Spotify</code>等大公司使用。<code>Spark Streaming</code>是最近最流行的<code>Scala</code>代码实现的流处理框架。现在<code>Spark Streaming</code>被公司（<code>Netflix</code>, <code>Cisco</code>, <code>DataStax</code>, <code>Intel</code>, <code>IBM</code>等）日渐接受。<code>Samza</code>主要在<code>LinkedIn</code>公司使用。<code>Flink</code>是一个新兴的项目，很有前景。</p>\n<p>你可能对项目的贡献者数量也感兴趣。<code>Storm</code>和<code>Trident</code>大概有180个代码贡献者；整个<code>Spark</code>有720多个；根据<code>github</code>显示，<code>Samza</code>有40个；<code>Flink</code>有超过130个代码贡献者。</p>\n<h3 id=\"6-小结\"><a href=\"#6-小结\" class=\"headerlink\" title=\"6. 小结\"></a>6. 小结</h3><p>在进行流处理框架推荐之前，先来整体看下总结表：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-9.png?raw=true\" alt=\"\"></p>\n<h3 id=\"7-流处理框架推荐\"><a href=\"#7-流处理框架推荐\" class=\"headerlink\" title=\"7. 流处理框架推荐\"></a>7. 流处理框架推荐</h3><p>应用选型是大家都会遇到的问题，一般是根据应用具体的场景来选择特定的流处理框架。下面给出几个作者认为优先考虑的点：</p>\n<ul>\n<li><code>High level API</code>：具有<code>high level API</code>的流处理框架会更简洁和高效；</li>\n<li>状态管理：大部分流处理应用都涉及到状态管理，因此你得把状态管理作为评价指标之一；</li>\n<li><code>exactly once</code>语义：<code>exactly once</code>会使得应用开发变得简单，但也要看具体需求，可能<code>at least once</code>或者<code>at most once</code>语义就满足你得要求；</li>\n<li>自动恢复：确保流处理系统能够快速恢复，你可以使用<code>Chaos Monkey</code>或者类似的工具进行测试。快速的恢复是流处理重要的部分。</li>\n</ul>\n<p><code>Storm</code>：<code>Storm</code>非常适合任务量小但速度要求高的应用。如果你主要在意流处理框架的延迟性，<code>Storm</code>将可能是你的首先。但同时也要记住，<code>Storm</code>的容错恢复或者<code>Trident</code>的状态管理都会降低整体的性能水平。也有一个潜在的<code>Storm</code>更新项目-<code>Twitter</code>的<code>Heron</code>，<code>Heron</code>设计的初衷是为了替代<code>Storm</code>，并在每个单任务上做了优化但同时保留了API。</p>\n<p><code>Spark Streaming</code>：如果你得基础架构中已经涉及到<code>Spark</code>，那<code>Spark Streaming</code>无疑是值得你尝试的。因为你可以很好的利用<code>Spark</code>各种<code>library</code>。如果你需要使用<code>Lambda</code>架构，<code>Spark Streaming</code>也是一个不错的选择。但你要时刻记住微批处理的局限性，以及它的延迟性问题。</p>\n<p><code>Samza</code>：如果你想使用<code>Samza</code>，那<code>Kafka</code>应该是你基础架构中的基石，好在现在<code>Kafka</code>已经成为家喻户晓的组件。像前面提到的，<code>Samza</code>一般会搭配强大的本地存储一起，这对管理大数据量的状态非常有益。它可以轻松处理上万千兆字节的状态信息，但要记住<code>Samza</code>只支持<code>at least once</code>语义。</p>\n<p><code>Flink</code>：<code>Flink</code>流处理系统的概念非常不错，并且满足绝大多数流处理场景，也经常提供前沿的功能函数，比如，高级窗口函数或者时间处理功能，这些在其它流处理框架中是没有的。同时<code>Flink</code>也有API提供给通用的批处理场景。但你需要足够的勇气去上线一个新兴的项目，并且你也不能忘了看下<code>Flink</code>的<code>roadmap</code>。</p>\n<h3 id=\"8-Dataflow和开源\"><a href=\"#8-Dataflow和开源\" class=\"headerlink\" title=\"8. Dataflow和开源\"></a>8. Dataflow和开源</h3><p>最后，我们来聊下<code>Dataflow</code>和它的开源。<code>Dataflow</code>是<code>Google</code>云平台的一部分，<code>Google</code>云平台包含很多组件：大数据存储，<code>BigQuery</code>，<code>Cloud PubSub</code>，数据分析工具和前面提到的<code>Dataflow</code>。</p>\n<p><code>Dataflow</code>是<code>Google</code>管理批处理和流处理的统一API。它是建立在<code>MapReduce</code>（批处理），<code>FlumeJava</code>（编程模型）和<code>MillWheel</code>（流处理）之上。<code>Google</code>最近决定开源<code>Dataflow SDK</code>，并完成<code>Spark</code>和<code>Flink</code>的<code>runner</code>。现在可以通过<code>Dataflow</code>的API来定义<code>Google</code>云平台作业、<code>Flink</code>作业或者<code>Spark</code>作业，后续会增加对其它引擎的支持。</p>\n<p><code>Google</code>为<code>Dataflow</code>提供<code>Java</code>、<code>Python</code>的API，社区已经完成<code>Scalable</code>的<code>DSL</code>支持。除此之外，<code>Google</code>及其合作者提交<code>Apache Beam</code>到<code>Apache</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-10.png?raw=true\" alt=\"\"></p>\n<p>原文:<a href=\"http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>在上篇文章中，我们过了下基本的理论，也介绍了主流的流处理框架：<code>Storm</code>，<code>Trident</code>，<code>Spark Streaming</code>，<code>Samza</code>和<code>Flink</code>。今天咱们来点有深度的主题，比如，容错，状态管理或者性能。除此之外，我们也将讨论开发分布式流处理应用的指南，并给出推荐的流处理框架。</p>\n<h3 id=\"1-容错性\"><a href=\"#1-容错性\" class=\"headerlink\" title=\"1. 容错性\"></a>1. 容错性</h3><p>流处理系统的<code>容错性</code>与生俱来的比批处理系统难实现。当批处理系统中出现错误时，我们只需要把失败的部分简单重启即可；但对于流处理系统，出现错误就很难恢复。因为线上许多作业都是<code>7 x 24</code>小时运行，不断有输入的数据。流处理系统面临的另外一个挑战是<code>状态一致性</code>，因为重启后会出现重复数据，并且不是所有的状态操作是幂等的。容错性这么难实现，那下面我们看看各大主流流处理框架是如何处理这一问题。</p>\n<h4 id=\"1-1-Apache-Storm\"><a href=\"#1-1-Apache-Storm\" class=\"headerlink\" title=\"1.1 Apache Storm\"></a>1.1 Apache Storm</h4><p><code>Storm</code>使用<code>上游数据备份</code>和<code>消息确认</code>的机制来保障消息在失败之后会重新处理。消息确认原理：每个操作都会把前一次的操作处理消息的确认信息返回。<code>Topology</code>的数据源备份它生成的所有数据记录。当所有数据记录的处理确认信息收到，备份即会被安全拆除。失败后，如果不是所有的消息处理确认信息收到，那数据记录会被数据源数据替换。这保障了没有数据丢失，但数据结果会有重复，这就是<code>at-least once</code>传输机制。</p>\n<p><code>Storm</code>采用取巧的办法完成了容错性，对每个源数据记录仅仅要求几个字节存储空间来跟踪确认消息。纯数据记录消息确认架构，尽管性能不错，但不能保证<code>exactly once</code>消息传输机制，所有应用开发者需要处理重复数据。<code>Storm</code>存在低吞吐量和流控问题，因为消息确认机制在反压下经常误认为失败。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-1.png?raw=true\" alt=\"\"></p>\n<h4 id=\"1-2-Spark-Streaming\"><a href=\"#1-2-Spark-Streaming\" class=\"headerlink\" title=\"1.2 Spark Streaming\"></a>1.2 Spark Streaming</h4><p><code>Spark Streaming</code>实现微批处理，容错机制的实现跟<code>Storm</code>不一样。微批处理的想法相当简单。<code>Spark</code>在集群各<code>worker</code>节点上处理<code>micro-batches</code>。每个<code>micro-batches</code>一旦失败，重新计算就行。因为<code>micro-batches</code>本身的不可变性，并且每个<code>micro-batches</code>也会持久化，所以<code>exactly once</code>传输机制很容易实现。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-2.png?raw=true\" alt=\"\"></p>\n<h4 id=\"1-3-Samza\"><a href=\"#1-3-Samza\" class=\"headerlink\" title=\"1.3 Samza\"></a>1.3 Samza</h4><p><code>Samza</code>的实现方法跟前面两种流处理框架完全不一样。<code>Samza</code>利用消息系统<code>Kafka</code>的持久化和偏移量。<code>Samza</code>监控任务的偏移量，当任务处理完消息，相应的偏移量被移除。消息的偏移量会被<code>checkpoint</code>到持久化存储中，并在失败时恢复。但是问题在于：从上次<code>checkpoint</code>中修复偏移量时并不知道上游消息已经被处理过，这就会造成重复。这就是<code>at least once</code>传输机制。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-3.png?raw=true\" alt=\"\"></p>\n<h4 id=\"1-4-Apache-Flink\"><a href=\"#1-4-Apache-Flink\" class=\"headerlink\" title=\"1.4 Apache Flink\"></a>1.4 Apache Flink</h4><p><code>Flink</code>的容错机制是基于分布式快照实现的，这些快照会保存流处理作业的状态(本文对<code>Flink</code>的检查点和快照不进行区分，因为两者实际是同一个事物的两种不同叫法。<code>Flink</code>构建这些快照的机制可以被描述成<a href=\"https://arxiv.org/abs/1506.08603\" target=\"_blank\" rel=\"noopener\">分布式数据流的轻量级异步快照</a>，它采用<code>Chandy-Lamport</code>算法实现。)。如果发生失败的情况，系统可以从这些检查点进行恢复。<code>Flink</code>发送<code>checkpoint</code>的栅栏（<code>barrier</code>）到数据流中（栅栏是<code>Flink</code>的分布式快照机制中一个核心的元素），当<code>checkpoint</code>的栅栏到达其中一个<code>operator</code>，<code>operator</code>会接所有收输入流中对应的栅栏（比如，图中<code>checkpoint n</code>对应栅栏<code>n</code>到<code>n-1</code>的所有输入流，其仅仅是整个输入流的一部分）。所以相对于<code>Storm</code>，<code>Flink</code>的容错机制更高效，因为<code>Flink</code>的操作是对小批量数据而不是每条数据记录。但也不要让自己糊涂了，<code>Flink</code>仍然是原生流处理框架，它与<code>Spark Streaming</code>在概念上就完全不同。<code>Flink</code>也提供<code>exactly once</code>消息传输机制。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-4.png?raw=true\" alt=\"\"></p>\n<h3 id=\"2-状态管理\"><a href=\"#2-状态管理\" class=\"headerlink\" title=\"2. 状态管理\"></a>2. 状态管理</h3><p>大部分大型流处理应用都涉及到状态。相对于无状态的操作(其只有一个输入数据，处理过程和输出结果)，有状态的应用会有一个输入数据和一个状态信息，然后处理过程，接着输出结果和修改状态信息。因此，我们不得不管理状态信息，并持久化。我们期望一旦因某种原因失败，状态能够修复。状态修复有可能会出现小问题，它并不总是保证<code>exactly once</code>，有时也会出现消费多次，但这并不是我们想要的。</p>\n<h4 id=\"2-1-Apache-Storm\"><a href=\"#2-1-Apache-Storm\" class=\"headerlink\" title=\"2.1 Apache Storm\"></a>2.1 Apache Storm</h4><p>我们知道，<code>Storm</code>提供<code>at-least once</code>的消息传输保障。那我们又该如何使用<code>Trident</code>做到<code>exactly once</code>的语义。概念上貌似挺简单，你只需要提交每条数据记录，但这显然不是那么高效。所以你会想到小批量的数据记录一起提交会优化。<code>Trident</code>定义了几个抽象来达到<code>exactly once</code>的语义，见下图，其中也会有些局限。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-5.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-2-Spark-Streaming\"><a href=\"#2-2-Spark-Streaming\" class=\"headerlink\" title=\"2.2 Spark Streaming\"></a>2.2 Spark Streaming</h4><p><code>Spark Streaming</code>是微批处理系统，它把状态信息也看做是一种微批量数据流。在处理每个微批量数据时，<code>Spark</code>加载当前的状态信息，接着通过函数操作获得处理后的微批量数据结果并修改加载过的状态信息。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-6.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-3-Samza\"><a href=\"#2-3-Samza\" class=\"headerlink\" title=\"2.3 Samza\"></a>2.3 Samza</h4><p><code>Samza</code>实现状态管理是通过<code>Kafka</code>来处理的。<code>Samza</code>有真实的状态操作，所以其任务会持有一个状态信息，并把状态改变的日志推送到<code>Kafka</code>。如果需要状态重建，可以很容易的从<code>Kafka</code>的<code>topic</code>重建。为了达到更快的状态管理，<code>Samza</code>也支持把状态信息放入本地<code>key-value</code>存储中，所以状态信息不必一直在<code>Kafka</code>中管理，见下图。不幸的是，<code>Samza</code>只提供<code>at-least once</code>语义，<code>exactly once</code>的支持也在计划中。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-7.png?raw=true\" alt=\"\"></p>\n<h4 id=\"2-4-Apache-Flink\"><a href=\"#2-4-Apache-Flink\" class=\"headerlink\" title=\"2.4 Apache Flink\"></a>2.4 Apache Flink</h4><p><code>Flink</code>提供状态操作，和<code>Samza</code>类似。<code>Flink</code>提供两种类型的状态：一种是用户自定义状态；另外一种是窗口状态。如图，第一个状态是自定义状态，它和其它的的状态不相互作用。这些状态可以分区或者使用嵌入式<code>Key-Value</code>存储状态(参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/fault_tolerance.html\" target=\"_blank\" rel=\"noopener\">文容错</a>和<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/state.html\" target=\"_blank\" rel=\"noopener\">状态</a>)。当然<code>Flink</code>提供<code>exactly-once</code>语义。下图展示<code>Flink</code>长期运行的三个状态。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-8.png?raw=true\" alt=\"\"></p>\n<h3 id=\"3-单词计数例子中的状态管理\"><a href=\"#3-单词计数例子中的状态管理\" class=\"headerlink\" title=\"3. 单词计数例子中的状态管理\"></a>3. 单词计数例子中的状态管理</h3><p>单词计数的详细代码见上篇文章，这里仅关注状态管理部分。</p>\n<p>让我们先看<code>Trident</code>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">public static StormTopology buildTopology(LocalDRPC drpc) &#123;</span><br><span class=\"line\">   FixedBatchSpout spout = ...</span><br><span class=\"line\"></span><br><span class=\"line\">   TridentTopology topology = new TridentTopology();</span><br><span class=\"line\"></span><br><span class=\"line\">   TridentState wordCounts = topology.newStream(&quot;spout1&quot;, spout)</span><br><span class=\"line\">     .each(new Fields(&quot;sentence&quot;),new Split(), new Fields(&quot;word&quot;))</span><br><span class=\"line\">     .groupBy(new Fields(&quot;word&quot;))</span><br><span class=\"line\">     .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;));</span><br><span class=\"line\"></span><br><span class=\"line\"> ...</span><br><span class=\"line\"></span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p>在第九行代码中，我们通过调用<code>persistentAggregate</code>创建一个状态。其中参数<code>Count</code>存储单词数，如果你想从状态中处理数据，你必须创建一个数据流。从代码中也可以看出实现起来不方便。</p>\n<p><code>Spark Streaming</code>声明式的方法稍微好点：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">// Initial RDD input to updateStateByKey</span><br><span class=\"line\">val initialRDD = ssc.sparkContext.parallelize(List.empty[(String, Int)])</span><br><span class=\"line\"></span><br><span class=\"line\">val lines = ...</span><br><span class=\"line\">val words = lines.flatMap(_.split(&quot; &quot;))</span><br><span class=\"line\">val wordDstream = words.map(x =&gt; (x, 1))</span><br><span class=\"line\"></span><br><span class=\"line\">val trackStateFunc = (batchTime: Time, word: String, one: Option[Int],</span><br><span class=\"line\">  state: State[Int]) =&gt; &#123;</span><br><span class=\"line\">    val sum = one.getOrElse(0) + state.getOption.getOrElse(0)</span><br><span class=\"line\">    val output = (word, sum)</span><br><span class=\"line\">    state.update(sum)</span><br><span class=\"line\">    Some(output)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">val stateDstream = wordDstream.trackStateByKey(</span><br><span class=\"line\">  StateSpec.function(trackStateFunc).initialState(initialRDD))</span><br></pre></td></tr></table></figure></p>\n<p>首先我们需要创建一个<code>RDD</code>来初始化状态（第二行代码），然后进行<code>transformations</code>（第五行和六行代码）。接着在第八行到十四行代码，我们定义函数来处理单词数状态。函数计算并更新状态，最后返回结果。第十六行和十七行代码，我们得到一个状态信息流，其中包含单词数。</p>\n<p>接着我们看下<code>Samza</code>:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">class WordCountTask extends StreamTask with InitableTask &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  private var store: CountStore = _</span><br><span class=\"line\"></span><br><span class=\"line\">  def init(config: Config, context: TaskContext) &#123;</span><br><span class=\"line\">    this.store = context.getStore(&quot;wordcount-store&quot;)</span><br><span class=\"line\">      .asInstanceOf[KeyValueStore[String, Integer]]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> override def process(envelope: IncomingMessageEnvelope,</span><br><span class=\"line\">   collector: MessageCollector, coordinator: TaskCoordinator) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   val words = envelope.getMessage.asInstanceOf[String].split(&quot; &quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   words.foreach &#123; key =&gt;</span><br><span class=\"line\">     val count: Integer = Option(store.get(key)).getOrElse(0)</span><br><span class=\"line\">     store.put(key, count + 1)</span><br><span class=\"line\">     collector.send(new OutgoingMessageEnvelope(new SystemStream(&quot;kafka&quot;, &quot;wordcount&quot;),</span><br><span class=\"line\">       (key, count)))</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure></p>\n<p>首先在第三行代码定义状态，进行<code>Key-Value</code>存储，在第五行到八行代码初始化状态。接着在计算中使用，上面的代码已经很直白。</p>\n<p>最后，讲下<code>Flink</code>使用简洁的API实现状态管理：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">val text = env.fromElements(...)</span><br><span class=\"line\">val words = text.flatMap ( _.split(&quot; &quot;) )</span><br><span class=\"line\"></span><br><span class=\"line\">words.keyBy(x =&gt; x).mapWithState &#123;</span><br><span class=\"line\">  (word, count: Option[Int]) =&gt;</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      val newCount = count.getOrElse(0) + 1</span><br><span class=\"line\">      val output = (word, newCount)</span><br><span class=\"line\">      (output, Some(newCount))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>我们仅仅需要在第六行代码中调用<code>mapwithstate</code>函数，它有一个函数参数（函数有两个变量，第一个是单词，第二个是状态。然后返回处理的结果和新的状态）。</p>\n<h3 id=\"4-流处理框架性能\"><a href=\"#4-流处理框架性能\" class=\"headerlink\" title=\"4. 流处理框架性能\"></a>4. 流处理框架性能</h3><p>这里所讲的性能主要涉及到的是<code>延迟性</code>和<code>吞吐量</code>。</p>\n<p>对于延迟性来说，微批处理一般在秒级别，大部分原生流处理在百毫秒以下，调优的情况下<code>Storm</code>可以很轻松的达到十毫秒。</p>\n<p>同时也要记住，消息传输机制保障，容错性和状态恢复都会占用机器资源。例如，打开容错恢复可能会降低10％到15％的性能，<code>Storm</code>可能降低70%的吞吐量。总之，天下没有免费的午餐。对于有状态管理，<code>Flink</code>会降低25%的性能，<code>Spark Streaming</code>降低50%的性能。</p>\n<p>也要记住，各大流处理框架的所有操作都是分布式的，通过网络发送数据是相当耗时的，所以要利用数据本地性，也尽量优化你的应用的序列化。</p>\n<h3 id=\"5-项目成熟度\"><a href=\"#5-项目成熟度\" class=\"headerlink\" title=\"5. 项目成熟度\"></a>5. 项目成熟度</h3><p>当你为应用选型时一定会考虑项目的成熟度。下面来快速浏览一下：<br><code>Storm</code>是第一个主流的流处理框架，后期已经成为长期的工业级的标准，并在像<code>Twitter</code>，<code>Yahoo</code>，<code>Spotify</code>等大公司使用。<code>Spark Streaming</code>是最近最流行的<code>Scala</code>代码实现的流处理框架。现在<code>Spark Streaming</code>被公司（<code>Netflix</code>, <code>Cisco</code>, <code>DataStax</code>, <code>Intel</code>, <code>IBM</code>等）日渐接受。<code>Samza</code>主要在<code>LinkedIn</code>公司使用。<code>Flink</code>是一个新兴的项目，很有前景。</p>\n<p>你可能对项目的贡献者数量也感兴趣。<code>Storm</code>和<code>Trident</code>大概有180个代码贡献者；整个<code>Spark</code>有720多个；根据<code>github</code>显示，<code>Samza</code>有40个；<code>Flink</code>有超过130个代码贡献者。</p>\n<h3 id=\"6-小结\"><a href=\"#6-小结\" class=\"headerlink\" title=\"6. 小结\"></a>6. 小结</h3><p>在进行流处理框架推荐之前，先来整体看下总结表：</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-9.png?raw=true\" alt=\"\"></p>\n<h3 id=\"7-流处理框架推荐\"><a href=\"#7-流处理框架推荐\" class=\"headerlink\" title=\"7. 流处理框架推荐\"></a>7. 流处理框架推荐</h3><p>应用选型是大家都会遇到的问题，一般是根据应用具体的场景来选择特定的流处理框架。下面给出几个作者认为优先考虑的点：</p>\n<ul>\n<li><code>High level API</code>：具有<code>high level API</code>的流处理框架会更简洁和高效；</li>\n<li>状态管理：大部分流处理应用都涉及到状态管理，因此你得把状态管理作为评价指标之一；</li>\n<li><code>exactly once</code>语义：<code>exactly once</code>会使得应用开发变得简单，但也要看具体需求，可能<code>at least once</code>或者<code>at most once</code>语义就满足你得要求；</li>\n<li>自动恢复：确保流处理系统能够快速恢复，你可以使用<code>Chaos Monkey</code>或者类似的工具进行测试。快速的恢复是流处理重要的部分。</li>\n</ul>\n<p><code>Storm</code>：<code>Storm</code>非常适合任务量小但速度要求高的应用。如果你主要在意流处理框架的延迟性，<code>Storm</code>将可能是你的首先。但同时也要记住，<code>Storm</code>的容错恢复或者<code>Trident</code>的状态管理都会降低整体的性能水平。也有一个潜在的<code>Storm</code>更新项目-<code>Twitter</code>的<code>Heron</code>，<code>Heron</code>设计的初衷是为了替代<code>Storm</code>，并在每个单任务上做了优化但同时保留了API。</p>\n<p><code>Spark Streaming</code>：如果你得基础架构中已经涉及到<code>Spark</code>，那<code>Spark Streaming</code>无疑是值得你尝试的。因为你可以很好的利用<code>Spark</code>各种<code>library</code>。如果你需要使用<code>Lambda</code>架构，<code>Spark Streaming</code>也是一个不错的选择。但你要时刻记住微批处理的局限性，以及它的延迟性问题。</p>\n<p><code>Samza</code>：如果你想使用<code>Samza</code>，那<code>Kafka</code>应该是你基础架构中的基石，好在现在<code>Kafka</code>已经成为家喻户晓的组件。像前面提到的，<code>Samza</code>一般会搭配强大的本地存储一起，这对管理大数据量的状态非常有益。它可以轻松处理上万千兆字节的状态信息，但要记住<code>Samza</code>只支持<code>at least once</code>语义。</p>\n<p><code>Flink</code>：<code>Flink</code>流处理系统的概念非常不错，并且满足绝大多数流处理场景，也经常提供前沿的功能函数，比如，高级窗口函数或者时间处理功能，这些在其它流处理框架中是没有的。同时<code>Flink</code>也有API提供给通用的批处理场景。但你需要足够的勇气去上线一个新兴的项目，并且你也不能忘了看下<code>Flink</code>的<code>roadmap</code>。</p>\n<h3 id=\"8-Dataflow和开源\"><a href=\"#8-Dataflow和开源\" class=\"headerlink\" title=\"8. Dataflow和开源\"></a>8. Dataflow和开源</h3><p>最后，我们来聊下<code>Dataflow</code>和它的开源。<code>Dataflow</code>是<code>Google</code>云平台的一部分，<code>Google</code>云平台包含很多组件：大数据存储，<code>BigQuery</code>，<code>Cloud PubSub</code>，数据分析工具和前面提到的<code>Dataflow</code>。</p>\n<p><code>Dataflow</code>是<code>Google</code>管理批处理和流处理的统一API。它是建立在<code>MapReduce</code>（批处理），<code>FlumeJava</code>（编程模型）和<code>MillWheel</code>（流处理）之上。<code>Google</code>最近决定开源<code>Dataflow SDK</code>，并完成<code>Spark</code>和<code>Flink</code>的<code>runner</code>。现在可以通过<code>Dataflow</code>的API来定义<code>Google</code>云平台作业、<code>Flink</code>作业或者<code>Spark</code>作业，后续会增加对其它引擎的支持。</p>\n<p><code>Google</code>为<code>Dataflow</code>提供<code>Java</code>、<code>Python</code>的API，社区已经完成<code>Scalable</code>的<code>DSL</code>支持。除此之外，<code>Google</code>及其合作者提交<code>Apache Beam</code>到<code>Apache</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E4%B8%BB%E6%B5%81%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%AF%94%E8%BE%83%E4%B9%8B%E4%BA%8C-10.png?raw=true\" alt=\"\"></p>\n<p>原文:<a href=\"http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/articles/comparison-of-main-stream-processing-framework-part02?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link</a></p>\n"},{"layout":"post","author":"Kostas Tzoumas","title":"Stream 对于流处理技术的谬见","date":"2018-01-11T02:25:01.000Z","_content":"\n我们在思考流处理问题上花了很多时间，更酷的是，我们也花了很多时间帮助其他人认识流处理，以及如何在他们的组织里应用流处理来解决数据问题。\n\n我们首先要做的是纠正人们对流处理（作为一个快速变化的领域，这里有很多误见值得我们思考）的错误认识。\n\n在这篇文章里，我们选出了其中的六个作为例子。因为我们对`Apache Flink`比较熟悉，所以我们会基于`Flink`来讲解这些例子。\n- 谬见1：没有不使用批处理的流（Lambda架构）\n- 谬见2：延迟和吞吐量：只能选择一个\n- 谬见3：微批次意味着更好的吞吐量\n- 谬见4：`Exactly once`？完全不可能\n- 谬见5：流只能被应用在“实时”场景里\n- 谬见6：不管怎么样，流仍然很复杂\n\n### 1. 谬见1：没有不使用批处理的流（Lambda架构）\n\n`Lambda架构`在`Apache Storm`的早期阶段和其它流处理项目里是一个很有用的设计模式。这个架构包含了一个`快速流层`和一个`批次层`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-1.png?raw=true)\n\n之所以使用两个单独的层，是因为Lambda架构里的流处理只能计算出大致的结果（也就是说，如果中间出现了错误，那么计算结果就不可信），而且只能处理相对少量的事件。\n\n就算`Storm`的早期版本存在这样的问题，但现今的很多开源流处理框架都具有容错能力，它们可以在出现故障的前提下生成准确的计算结果，而且具有高吞吐的计算能力。所以没有必要再为了分别得到“快”和“准确”的结果而维护多层架构。现今的流处理器（比如`Flink`）可以同时帮你得到两种结果。\n\n好在人们不再更多地讨论Lambda架构，说明流处理正在走向成熟。\n\n### 2. 谬见2：延迟和吞吐量：只能选择一个\n\n早期的开源流处理框架要么是`高吞吐`的，要么是`低延迟`的，而`海量且快速`一直未能成为开源流处理框架的代名词。\n\n不过`Flink`（可能还有其它的框架）就同时提供了高吞吐和低延迟。这里有一个基准测试结果的[样例](https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink)。\n\n让我们从底层来剖析这个例子，特别是从硬件层，并结合具有网络瓶颈的流处理管道（很多使用`Flink`的管道都有这个瓶颈）。在硬件层不应该存在需要作出权衡的条件，所以网络才是影响吞吐量和延迟的主要因素。\n\n一个设计良好的软件系统应该会充分利用网络的上限而不会引入瓶颈问题。不过对`Flink`来说，总是有可优化的空间，可以让它更接近硬件所能提供的效能。使用一个包含10个节点的集群，`Flink`现在每秒可以处理千万级别的事件量，如果扩展到1000个节点，它的延迟可以降低到几十毫秒。在我们看来，这种水平已经比很多现有的方案高出很多。\n\n### 谬见3：微批次意味着更好的吞吐量\n\n我们可以从另一个角度来讨论性能，不过先让我们来澄清两个容易混淆的概念：\n- 微批次:微批次建立在传统批次之上，是处理数据的一个执行或编程模型。通过这项技术，进程或任务可以把一个流当作一系列小型的批次或数据块(参阅:[Apache Storm 微批次设计模式](https://hortonworks.com/blog/apache-storm-design-pattern-micro-batching/))。\n- 缓冲:缓冲技术用于对网络、磁盘、缓存的访问进行优化。Wikipedia完美地把它定义为`物理内存里的一块用于临时储存移动数据的区域`。\n\n那么第3个缪见就是说，使用微批次的数据处理框架能够比每次处理一个事件的框架达到更高的吞吐量，因为微批次在网络上传输的效率更高。这个缪见忽略了一个事实，流框架不会依赖任何编程模型层面的批次，它们只会在物理层面使用缓冲。`Flink`确实也会对数据进行缓冲，也就是说它会通过网络发送一组处理过的记录，而不是每次发送一条记录。从性能方面说，不对数据进行缓冲是不可取的，因为通过网络逐个发送记录不会带来任何性能上的好处。所以我们得承认在物理层面根本不存在类似一次一条记录这样的情况。\n\n不过缓冲只能作为对性能的优化，所以缓冲：\n- 对用户是不可见的\n- 不应该对系统造成任何影响\n- 不应该出现人为的边界\n- 不应该限制系统功能\n\n所以对`Flink`的用户来说，他们开发的程序能够单独地处理每个记录，那是因为`Flink`为了提升性能隐藏了使用缓冲的细节。\n\n事实上，在任务调度里使用微批次会带来额外的开销，而如果这样做是为了降低延迟，那么这种开销会只增不减！流处理器知道该如何利用缓冲的优势而不会带来任务调度方面的开销。\n\n### 4. 谬见4：Exactly once？完全不可能\n\n这个缪见包含了几个方面的内容：\n- 从根本上说，`Exactly once`是不可能的\n- 从端到端的`Exactly once`是不可能的\n- `Exactly once`从来都不是真实世界的需求\n- `Exactly once`以牺牲性能为代价\n\n我们退一步讲，我们并不介意`Exactly once`这种观点的存在。`Exactly once`原先指的是`一次性传递`，而现在这个词被随意用在流处理里，让这个词变得令人困惑，失去了它原本的意义。不过相关的概念还是很重要的，我们不打算跳过去。\n\n为了尽量准确，我们把`一次性状态`和`一次性传递`视为两种不同的概念。因为之前人们对这两个词的使用方式导致了它们的混淆。`Apache Storm`使用`at least once`来描述传递(`Storm`不支持状态)，而`Apache Samza`使用`at least once`来描述应用状态。\n\n(1) `一次性状态`是指应用程序在经历了故障以后恍如没有发生过故障一样。例如，假设我们在维护一个计数器应用程序，在发生了一次故障之后，它既不能多计数也不能少计数。在这里使用`Exactly once`这个词是因为应用程序状态认为每个消息只被处理了一次。\n\n(2) `一次性传递`是指接收端(应用程序之外的系统)在故障发生后会收到处理过的事件，恍如没有发生过故障一样。\n\n流处理框架在任何情况下都不保证一次性传递，但可以做到一次性状态。`Flink`可以做到一次性状态，而且不会对性能造成显著影响。`Flink`还能在与`Flink`检查点相关的数据槽上做到一次性传递。\n\n`Flink`检查点就是应用程序状态的快照，`Flink`会为应用程序定时异步地生成快照。这就是`Flink`在发生故障时仍然能保证一次性状态的原因：`Flink`定时记录（快照）输入流的读取位置和每个操作数的相关状态。如果发生故障，`Flink`会回滚到之前的状态，并重新开始计算。所以说，尽管记录被重新处理，但从结果来看，记录好像只被处理过一次。\n\n那么端到端的一次性处理呢？通过恰当的方式让检查点兼具事务协调机制是可能的，换句话说，就是让源操作和目标操作参与到检查点里来。在框架内部，结果是一次性的，从端到端来看，也是一次性的，或者说`接近一次性`。例如，在使用`Flink`和`Kafka`作为数据源并发生数据槽（HDFS）滚动时，从`Kafka`到`HDFS`就是端到端的一次性处理。类似地，在把`Kafka`作为`Flink`的源并且把`Cassandra`作为`Flink`的槽时，如果针对`Cassandra`的更新是幂等时，那么就可以实现端到端的一次性处理。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-2.png?raw=true)\n\n值得一提的是，利用`Flink`的保存点，检查点可以兼具状态版本机制。使用保存点，在保持状态一致性的同时还可以“随着时间移动”。这样可以让代码的更新、维护、迁移、调试和各种模拟测试变得简单。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-3.png?raw=true)\n\n### 5. 谬见5：流只能被应用在“实时”场景里\n\n这个谬见包括几点内容：\n- 我没有低延迟的应用，所以我不需要流处理器\n- 流处理只跟那些持久化之前的过渡数据有关系\n- 我们需要批处理器来完成笨重的离线计算\n\n现在是时候思考一下数据集的类型和处理模型之间的关系了。\n\n(1) 首先，有两种数据集：\n- 没有边界的：从非预定义的端点持续产生的数据\n- 有边界的：有限且完整的数据\n\n很多真实的数据集是没有边界的，不管这些数据时存储在文件里，还是在HDFS的目录里，还是在像Kafka这样的系统里。举一些例子：\n- 移动设备或网站用户的交互信息\n- 物理传感器提供的度量指标\n- 金融市场数据\n- 机器日志数据\n\n实际上，在现实世界中很难找到有边界的数据集，不过一个公司所有大楼的位置信息倒是有边界的（不过它也会随着公司业务的增长而变化）。\n\n(2) 其次，有两种处理模型：\n- 流：只要有数据生成就会一直处理\n- 批次：在有限的时间内结束处理，并释放资源\n\n让我们再深入一点，来区分两种没有边界的数据集：连续性流和间歇性流。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-4.png?raw=true)\n\n使用任意一种模型来处理任意一种数据集是完全可能的，虽然这不是最优的做法。例如，批次处理模型被长时间地应用在无边界的数据集上，特别是间歇性的无边界数据集。现实情况是，大多数`批处理`任务是通过调度来执行的，每次只处理无边界数据集的一小部分。这意味着流的无边界特质会给某些人带来麻烦（那些工作在流入管道上的人）。\n\n批处理是无状态的，输出只取决于输入。现实情况是，批处理任务会在内部保留状态（比如reducer经常会保留状态），但这些状态只限在批次的边界内，而且它们不会在批次间流窜。当有人尝试实现类似带有\"事件时间戳\"的时间窗，那么\"批次的边界内状态\"就会变得很有用，这在处理无边界数据集时是个很常用的手段。处理无边界数据集的批处理器将不可避免地遇到延迟事件(因为上游的延迟)，批次内的数据有可能因此变得不完整。要注意，这里假设我们是基于事件时间戳来移动时间窗的，因为事件时间戳是现实当中最为准确的模型。在执行批处理的时候，迟到的数据会成为问题，即使通过简单的时间窗修复(比如翻转或滑动时间窗)也解决不了这个问题，特别是如果使用会话时间窗，就更难以处理了。因为完成一个计算所需要的数据不会都在一个批次里，所以在使用批次处理无边界数据集时，很难保证结果的正确性。最起码，它需要额外的开销来处理迟到的数据，还要维护批次之间的状态(要等到所有数据达到后才开始处理，或者重新处理批次)。\n\n`Flink`内建了处理迟到数据的机制，迟到数据被视为真实世界无边界数据的正常现象，所以`Flink`设计了一个流处理器专门处理迟到数据。有状态的流处理器更适合用来处理无边界数据集，不管数据集是持续生成的还是间歇生成的。使用流处理器只是个锦上添花的事情。\n\n### 6. 缪见6：不管怎么样，流仍然很复杂\n\n这是最后一个缪见。你也许会想：\"理论虽好，但我仍然不会采用流技术，因为……\"：\n- 流框架难以掌握\n- 流难以解决时间窗、事件时间戳、触发器的问题\n- 流需要结合批次，而我已经知道如何使用批次，那为什么还要使用流？\n\n我们从来没有打算怂恿你使用流，虽然我们觉得流是个很酷的东西。我们相信，是否使用流完全取决于数据和代码的特点。在做决定之前问问自己：\"我正在跟什么样类型的数据集打交道？\"\n- 无边界的（用户活动数据、日志、传感器数据）\n- 有边界的\n\n然后再问另一个问题：\"哪部分变化最频繁？\"\n- 代码比数据变化更频繁\n- 数据比代码变化更频繁\n\n对于数据比代码变化更频繁的情况，例如在经常变化的数据集上执行一个相对固定的查询操作，这样会出现流方面的问题。所以，在认定流是一个\"复杂\"的东西之前，你可能在不知不觉中已经解决过流方面的问题！你可能使用过基于小时的批次任务调度，团队里的其他人可以创建和管理这些批次（在这种情况下，你得到的结果可能是不准确的，而你意识不到这样的结果是批次的时间问题和之前提过的状态问题造成的）。\n\n为了能够提供一组封装了这些时间和状态复杂性的API，`Flink`社区为此工作了很长时间。在`Flink`里可以很简单地处理事件时间戳，只要定义一个时间窗口和一个能够抽取时间戳和水印的函数(只在每个流上调用一次)。处理状态也很简单，类似于定义`Java`变量，再把这些变量注册到`Flink`。使用`Flink`的`StreamSQL`可以在源源不断的流上面运行`SQL`查询。\n\n最后一点：对代码比数据变化更频繁的情况该怎么办？对于这种情况，我们认为你遇到了探索性问题。使用笔记本或其它类似的工具进行迭代可能适合用来解决探索性问题。在代码稳定了之后，你仍然会碰到流方面的问题。我们建议从一开始就使用长远的方案来解决流方面的问题。\n\n### 7. 流处理的未来\n\n随着流处理的日渐成熟和这些缪见的逐步淡去，我们发现流正朝着除分析应用之外的领域发展。正如我们所讨论的那样，真实世界正连续不断地生成数据。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原文:http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate\n","source":"_posts/Stream/[Stream]对于流处理技术的谬见.md","raw":"---\nlayout: post\nauthor: Kostas Tzoumas\ntitle: Stream 对于流处理技术的谬见\ndate: 2018-01-11 10:25:01\ntags:\n  - Stream\n\ncategories: Stream\n---\n\n我们在思考流处理问题上花了很多时间，更酷的是，我们也花了很多时间帮助其他人认识流处理，以及如何在他们的组织里应用流处理来解决数据问题。\n\n我们首先要做的是纠正人们对流处理（作为一个快速变化的领域，这里有很多误见值得我们思考）的错误认识。\n\n在这篇文章里，我们选出了其中的六个作为例子。因为我们对`Apache Flink`比较熟悉，所以我们会基于`Flink`来讲解这些例子。\n- 谬见1：没有不使用批处理的流（Lambda架构）\n- 谬见2：延迟和吞吐量：只能选择一个\n- 谬见3：微批次意味着更好的吞吐量\n- 谬见4：`Exactly once`？完全不可能\n- 谬见5：流只能被应用在“实时”场景里\n- 谬见6：不管怎么样，流仍然很复杂\n\n### 1. 谬见1：没有不使用批处理的流（Lambda架构）\n\n`Lambda架构`在`Apache Storm`的早期阶段和其它流处理项目里是一个很有用的设计模式。这个架构包含了一个`快速流层`和一个`批次层`。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-1.png?raw=true)\n\n之所以使用两个单独的层，是因为Lambda架构里的流处理只能计算出大致的结果（也就是说，如果中间出现了错误，那么计算结果就不可信），而且只能处理相对少量的事件。\n\n就算`Storm`的早期版本存在这样的问题，但现今的很多开源流处理框架都具有容错能力，它们可以在出现故障的前提下生成准确的计算结果，而且具有高吞吐的计算能力。所以没有必要再为了分别得到“快”和“准确”的结果而维护多层架构。现今的流处理器（比如`Flink`）可以同时帮你得到两种结果。\n\n好在人们不再更多地讨论Lambda架构，说明流处理正在走向成熟。\n\n### 2. 谬见2：延迟和吞吐量：只能选择一个\n\n早期的开源流处理框架要么是`高吞吐`的，要么是`低延迟`的，而`海量且快速`一直未能成为开源流处理框架的代名词。\n\n不过`Flink`（可能还有其它的框架）就同时提供了高吞吐和低延迟。这里有一个基准测试结果的[样例](https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink)。\n\n让我们从底层来剖析这个例子，特别是从硬件层，并结合具有网络瓶颈的流处理管道（很多使用`Flink`的管道都有这个瓶颈）。在硬件层不应该存在需要作出权衡的条件，所以网络才是影响吞吐量和延迟的主要因素。\n\n一个设计良好的软件系统应该会充分利用网络的上限而不会引入瓶颈问题。不过对`Flink`来说，总是有可优化的空间，可以让它更接近硬件所能提供的效能。使用一个包含10个节点的集群，`Flink`现在每秒可以处理千万级别的事件量，如果扩展到1000个节点，它的延迟可以降低到几十毫秒。在我们看来，这种水平已经比很多现有的方案高出很多。\n\n### 谬见3：微批次意味着更好的吞吐量\n\n我们可以从另一个角度来讨论性能，不过先让我们来澄清两个容易混淆的概念：\n- 微批次:微批次建立在传统批次之上，是处理数据的一个执行或编程模型。通过这项技术，进程或任务可以把一个流当作一系列小型的批次或数据块(参阅:[Apache Storm 微批次设计模式](https://hortonworks.com/blog/apache-storm-design-pattern-micro-batching/))。\n- 缓冲:缓冲技术用于对网络、磁盘、缓存的访问进行优化。Wikipedia完美地把它定义为`物理内存里的一块用于临时储存移动数据的区域`。\n\n那么第3个缪见就是说，使用微批次的数据处理框架能够比每次处理一个事件的框架达到更高的吞吐量，因为微批次在网络上传输的效率更高。这个缪见忽略了一个事实，流框架不会依赖任何编程模型层面的批次，它们只会在物理层面使用缓冲。`Flink`确实也会对数据进行缓冲，也就是说它会通过网络发送一组处理过的记录，而不是每次发送一条记录。从性能方面说，不对数据进行缓冲是不可取的，因为通过网络逐个发送记录不会带来任何性能上的好处。所以我们得承认在物理层面根本不存在类似一次一条记录这样的情况。\n\n不过缓冲只能作为对性能的优化，所以缓冲：\n- 对用户是不可见的\n- 不应该对系统造成任何影响\n- 不应该出现人为的边界\n- 不应该限制系统功能\n\n所以对`Flink`的用户来说，他们开发的程序能够单独地处理每个记录，那是因为`Flink`为了提升性能隐藏了使用缓冲的细节。\n\n事实上，在任务调度里使用微批次会带来额外的开销，而如果这样做是为了降低延迟，那么这种开销会只增不减！流处理器知道该如何利用缓冲的优势而不会带来任务调度方面的开销。\n\n### 4. 谬见4：Exactly once？完全不可能\n\n这个缪见包含了几个方面的内容：\n- 从根本上说，`Exactly once`是不可能的\n- 从端到端的`Exactly once`是不可能的\n- `Exactly once`从来都不是真实世界的需求\n- `Exactly once`以牺牲性能为代价\n\n我们退一步讲，我们并不介意`Exactly once`这种观点的存在。`Exactly once`原先指的是`一次性传递`，而现在这个词被随意用在流处理里，让这个词变得令人困惑，失去了它原本的意义。不过相关的概念还是很重要的，我们不打算跳过去。\n\n为了尽量准确，我们把`一次性状态`和`一次性传递`视为两种不同的概念。因为之前人们对这两个词的使用方式导致了它们的混淆。`Apache Storm`使用`at least once`来描述传递(`Storm`不支持状态)，而`Apache Samza`使用`at least once`来描述应用状态。\n\n(1) `一次性状态`是指应用程序在经历了故障以后恍如没有发生过故障一样。例如，假设我们在维护一个计数器应用程序，在发生了一次故障之后，它既不能多计数也不能少计数。在这里使用`Exactly once`这个词是因为应用程序状态认为每个消息只被处理了一次。\n\n(2) `一次性传递`是指接收端(应用程序之外的系统)在故障发生后会收到处理过的事件，恍如没有发生过故障一样。\n\n流处理框架在任何情况下都不保证一次性传递，但可以做到一次性状态。`Flink`可以做到一次性状态，而且不会对性能造成显著影响。`Flink`还能在与`Flink`检查点相关的数据槽上做到一次性传递。\n\n`Flink`检查点就是应用程序状态的快照，`Flink`会为应用程序定时异步地生成快照。这就是`Flink`在发生故障时仍然能保证一次性状态的原因：`Flink`定时记录（快照）输入流的读取位置和每个操作数的相关状态。如果发生故障，`Flink`会回滚到之前的状态，并重新开始计算。所以说，尽管记录被重新处理，但从结果来看，记录好像只被处理过一次。\n\n那么端到端的一次性处理呢？通过恰当的方式让检查点兼具事务协调机制是可能的，换句话说，就是让源操作和目标操作参与到检查点里来。在框架内部，结果是一次性的，从端到端来看，也是一次性的，或者说`接近一次性`。例如，在使用`Flink`和`Kafka`作为数据源并发生数据槽（HDFS）滚动时，从`Kafka`到`HDFS`就是端到端的一次性处理。类似地，在把`Kafka`作为`Flink`的源并且把`Cassandra`作为`Flink`的槽时，如果针对`Cassandra`的更新是幂等时，那么就可以实现端到端的一次性处理。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-2.png?raw=true)\n\n值得一提的是，利用`Flink`的保存点，检查点可以兼具状态版本机制。使用保存点，在保持状态一致性的同时还可以“随着时间移动”。这样可以让代码的更新、维护、迁移、调试和各种模拟测试变得简单。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-3.png?raw=true)\n\n### 5. 谬见5：流只能被应用在“实时”场景里\n\n这个谬见包括几点内容：\n- 我没有低延迟的应用，所以我不需要流处理器\n- 流处理只跟那些持久化之前的过渡数据有关系\n- 我们需要批处理器来完成笨重的离线计算\n\n现在是时候思考一下数据集的类型和处理模型之间的关系了。\n\n(1) 首先，有两种数据集：\n- 没有边界的：从非预定义的端点持续产生的数据\n- 有边界的：有限且完整的数据\n\n很多真实的数据集是没有边界的，不管这些数据时存储在文件里，还是在HDFS的目录里，还是在像Kafka这样的系统里。举一些例子：\n- 移动设备或网站用户的交互信息\n- 物理传感器提供的度量指标\n- 金融市场数据\n- 机器日志数据\n\n实际上，在现实世界中很难找到有边界的数据集，不过一个公司所有大楼的位置信息倒是有边界的（不过它也会随着公司业务的增长而变化）。\n\n(2) 其次，有两种处理模型：\n- 流：只要有数据生成就会一直处理\n- 批次：在有限的时间内结束处理，并释放资源\n\n让我们再深入一点，来区分两种没有边界的数据集：连续性流和间歇性流。\n\n![](https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-4.png?raw=true)\n\n使用任意一种模型来处理任意一种数据集是完全可能的，虽然这不是最优的做法。例如，批次处理模型被长时间地应用在无边界的数据集上，特别是间歇性的无边界数据集。现实情况是，大多数`批处理`任务是通过调度来执行的，每次只处理无边界数据集的一小部分。这意味着流的无边界特质会给某些人带来麻烦（那些工作在流入管道上的人）。\n\n批处理是无状态的，输出只取决于输入。现实情况是，批处理任务会在内部保留状态（比如reducer经常会保留状态），但这些状态只限在批次的边界内，而且它们不会在批次间流窜。当有人尝试实现类似带有\"事件时间戳\"的时间窗，那么\"批次的边界内状态\"就会变得很有用，这在处理无边界数据集时是个很常用的手段。处理无边界数据集的批处理器将不可避免地遇到延迟事件(因为上游的延迟)，批次内的数据有可能因此变得不完整。要注意，这里假设我们是基于事件时间戳来移动时间窗的，因为事件时间戳是现实当中最为准确的模型。在执行批处理的时候，迟到的数据会成为问题，即使通过简单的时间窗修复(比如翻转或滑动时间窗)也解决不了这个问题，特别是如果使用会话时间窗，就更难以处理了。因为完成一个计算所需要的数据不会都在一个批次里，所以在使用批次处理无边界数据集时，很难保证结果的正确性。最起码，它需要额外的开销来处理迟到的数据，还要维护批次之间的状态(要等到所有数据达到后才开始处理，或者重新处理批次)。\n\n`Flink`内建了处理迟到数据的机制，迟到数据被视为真实世界无边界数据的正常现象，所以`Flink`设计了一个流处理器专门处理迟到数据。有状态的流处理器更适合用来处理无边界数据集，不管数据集是持续生成的还是间歇生成的。使用流处理器只是个锦上添花的事情。\n\n### 6. 缪见6：不管怎么样，流仍然很复杂\n\n这是最后一个缪见。你也许会想：\"理论虽好，但我仍然不会采用流技术，因为……\"：\n- 流框架难以掌握\n- 流难以解决时间窗、事件时间戳、触发器的问题\n- 流需要结合批次，而我已经知道如何使用批次，那为什么还要使用流？\n\n我们从来没有打算怂恿你使用流，虽然我们觉得流是个很酷的东西。我们相信，是否使用流完全取决于数据和代码的特点。在做决定之前问问自己：\"我正在跟什么样类型的数据集打交道？\"\n- 无边界的（用户活动数据、日志、传感器数据）\n- 有边界的\n\n然后再问另一个问题：\"哪部分变化最频繁？\"\n- 代码比数据变化更频繁\n- 数据比代码变化更频繁\n\n对于数据比代码变化更频繁的情况，例如在经常变化的数据集上执行一个相对固定的查询操作，这样会出现流方面的问题。所以，在认定流是一个\"复杂\"的东西之前，你可能在不知不觉中已经解决过流方面的问题！你可能使用过基于小时的批次任务调度，团队里的其他人可以创建和管理这些批次（在这种情况下，你得到的结果可能是不准确的，而你意识不到这样的结果是批次的时间问题和之前提过的状态问题造成的）。\n\n为了能够提供一组封装了这些时间和状态复杂性的API，`Flink`社区为此工作了很长时间。在`Flink`里可以很简单地处理事件时间戳，只要定义一个时间窗口和一个能够抽取时间戳和水印的函数(只在每个流上调用一次)。处理状态也很简单，类似于定义`Java`变量，再把这些变量注册到`Flink`。使用`Flink`的`StreamSQL`可以在源源不断的流上面运行`SQL`查询。\n\n最后一点：对代码比数据变化更频繁的情况该怎么办？对于这种情况，我们认为你遇到了探索性问题。使用笔记本或其它类似的工具进行迭代可能适合用来解决探索性问题。在代码稳定了之后，你仍然会碰到流方面的问题。我们建议从一开始就使用长远的方案来解决流方面的问题。\n\n### 7. 流处理的未来\n\n随着流处理的日渐成熟和这些缪见的逐步淡去，我们发现流正朝着除分析应用之外的领域发展。正如我们所讨论的那样，真实世界正连续不断地生成数据。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原文:http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate\n","slug":"Stream/[Stream]对于流处理技术的谬见","published":1,"updated":"2018-01-29T09:36:59.613Z","comments":1,"photos":[],"link":"","_id":"cje58tivq0066ordbl8vm0gjz","content":"<p>我们在思考流处理问题上花了很多时间，更酷的是，我们也花了很多时间帮助其他人认识流处理，以及如何在他们的组织里应用流处理来解决数据问题。</p>\n<p>我们首先要做的是纠正人们对流处理（作为一个快速变化的领域，这里有很多误见值得我们思考）的错误认识。</p>\n<p>在这篇文章里，我们选出了其中的六个作为例子。因为我们对<code>Apache Flink</code>比较熟悉，所以我们会基于<code>Flink</code>来讲解这些例子。</p>\n<ul>\n<li>谬见1：没有不使用批处理的流（Lambda架构）</li>\n<li>谬见2：延迟和吞吐量：只能选择一个</li>\n<li>谬见3：微批次意味着更好的吞吐量</li>\n<li>谬见4：<code>Exactly once</code>？完全不可能</li>\n<li>谬见5：流只能被应用在“实时”场景里</li>\n<li>谬见6：不管怎么样，流仍然很复杂</li>\n</ul>\n<h3 id=\"1-谬见1：没有不使用批处理的流（Lambda架构）\"><a href=\"#1-谬见1：没有不使用批处理的流（Lambda架构）\" class=\"headerlink\" title=\"1. 谬见1：没有不使用批处理的流（Lambda架构）\"></a>1. 谬见1：没有不使用批处理的流（Lambda架构）</h3><p><code>Lambda架构</code>在<code>Apache Storm</code>的早期阶段和其它流处理项目里是一个很有用的设计模式。这个架构包含了一个<code>快速流层</code>和一个<code>批次层</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-1.png?raw=true\" alt=\"\"></p>\n<p>之所以使用两个单独的层，是因为Lambda架构里的流处理只能计算出大致的结果（也就是说，如果中间出现了错误，那么计算结果就不可信），而且只能处理相对少量的事件。</p>\n<p>就算<code>Storm</code>的早期版本存在这样的问题，但现今的很多开源流处理框架都具有容错能力，它们可以在出现故障的前提下生成准确的计算结果，而且具有高吞吐的计算能力。所以没有必要再为了分别得到“快”和“准确”的结果而维护多层架构。现今的流处理器（比如<code>Flink</code>）可以同时帮你得到两种结果。</p>\n<p>好在人们不再更多地讨论Lambda架构，说明流处理正在走向成熟。</p>\n<h3 id=\"2-谬见2：延迟和吞吐量：只能选择一个\"><a href=\"#2-谬见2：延迟和吞吐量：只能选择一个\" class=\"headerlink\" title=\"2. 谬见2：延迟和吞吐量：只能选择一个\"></a>2. 谬见2：延迟和吞吐量：只能选择一个</h3><p>早期的开源流处理框架要么是<code>高吞吐</code>的，要么是<code>低延迟</code>的，而<code>海量且快速</code>一直未能成为开源流处理框架的代名词。</p>\n<p>不过<code>Flink</code>（可能还有其它的框架）就同时提供了高吞吐和低延迟。这里有一个基准测试结果的<a href=\"https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink\" target=\"_blank\" rel=\"noopener\">样例</a>。</p>\n<p>让我们从底层来剖析这个例子，特别是从硬件层，并结合具有网络瓶颈的流处理管道（很多使用<code>Flink</code>的管道都有这个瓶颈）。在硬件层不应该存在需要作出权衡的条件，所以网络才是影响吞吐量和延迟的主要因素。</p>\n<p>一个设计良好的软件系统应该会充分利用网络的上限而不会引入瓶颈问题。不过对<code>Flink</code>来说，总是有可优化的空间，可以让它更接近硬件所能提供的效能。使用一个包含10个节点的集群，<code>Flink</code>现在每秒可以处理千万级别的事件量，如果扩展到1000个节点，它的延迟可以降低到几十毫秒。在我们看来，这种水平已经比很多现有的方案高出很多。</p>\n<h3 id=\"谬见3：微批次意味着更好的吞吐量\"><a href=\"#谬见3：微批次意味着更好的吞吐量\" class=\"headerlink\" title=\"谬见3：微批次意味着更好的吞吐量\"></a>谬见3：微批次意味着更好的吞吐量</h3><p>我们可以从另一个角度来讨论性能，不过先让我们来澄清两个容易混淆的概念：</p>\n<ul>\n<li>微批次:微批次建立在传统批次之上，是处理数据的一个执行或编程模型。通过这项技术，进程或任务可以把一个流当作一系列小型的批次或数据块(参阅:<a href=\"https://hortonworks.com/blog/apache-storm-design-pattern-micro-batching/\" target=\"_blank\" rel=\"noopener\">Apache Storm 微批次设计模式</a>)。</li>\n<li>缓冲:缓冲技术用于对网络、磁盘、缓存的访问进行优化。Wikipedia完美地把它定义为<code>物理内存里的一块用于临时储存移动数据的区域</code>。</li>\n</ul>\n<p>那么第3个缪见就是说，使用微批次的数据处理框架能够比每次处理一个事件的框架达到更高的吞吐量，因为微批次在网络上传输的效率更高。这个缪见忽略了一个事实，流框架不会依赖任何编程模型层面的批次，它们只会在物理层面使用缓冲。<code>Flink</code>确实也会对数据进行缓冲，也就是说它会通过网络发送一组处理过的记录，而不是每次发送一条记录。从性能方面说，不对数据进行缓冲是不可取的，因为通过网络逐个发送记录不会带来任何性能上的好处。所以我们得承认在物理层面根本不存在类似一次一条记录这样的情况。</p>\n<p>不过缓冲只能作为对性能的优化，所以缓冲：</p>\n<ul>\n<li>对用户是不可见的</li>\n<li>不应该对系统造成任何影响</li>\n<li>不应该出现人为的边界</li>\n<li>不应该限制系统功能</li>\n</ul>\n<p>所以对<code>Flink</code>的用户来说，他们开发的程序能够单独地处理每个记录，那是因为<code>Flink</code>为了提升性能隐藏了使用缓冲的细节。</p>\n<p>事实上，在任务调度里使用微批次会带来额外的开销，而如果这样做是为了降低延迟，那么这种开销会只增不减！流处理器知道该如何利用缓冲的优势而不会带来任务调度方面的开销。</p>\n<h3 id=\"4-谬见4：Exactly-once？完全不可能\"><a href=\"#4-谬见4：Exactly-once？完全不可能\" class=\"headerlink\" title=\"4. 谬见4：Exactly once？完全不可能\"></a>4. 谬见4：Exactly once？完全不可能</h3><p>这个缪见包含了几个方面的内容：</p>\n<ul>\n<li>从根本上说，<code>Exactly once</code>是不可能的</li>\n<li>从端到端的<code>Exactly once</code>是不可能的</li>\n<li><code>Exactly once</code>从来都不是真实世界的需求</li>\n<li><code>Exactly once</code>以牺牲性能为代价</li>\n</ul>\n<p>我们退一步讲，我们并不介意<code>Exactly once</code>这种观点的存在。<code>Exactly once</code>原先指的是<code>一次性传递</code>，而现在这个词被随意用在流处理里，让这个词变得令人困惑，失去了它原本的意义。不过相关的概念还是很重要的，我们不打算跳过去。</p>\n<p>为了尽量准确，我们把<code>一次性状态</code>和<code>一次性传递</code>视为两种不同的概念。因为之前人们对这两个词的使用方式导致了它们的混淆。<code>Apache Storm</code>使用<code>at least once</code>来描述传递(<code>Storm</code>不支持状态)，而<code>Apache Samza</code>使用<code>at least once</code>来描述应用状态。</p>\n<p>(1) <code>一次性状态</code>是指应用程序在经历了故障以后恍如没有发生过故障一样。例如，假设我们在维护一个计数器应用程序，在发生了一次故障之后，它既不能多计数也不能少计数。在这里使用<code>Exactly once</code>这个词是因为应用程序状态认为每个消息只被处理了一次。</p>\n<p>(2) <code>一次性传递</code>是指接收端(应用程序之外的系统)在故障发生后会收到处理过的事件，恍如没有发生过故障一样。</p>\n<p>流处理框架在任何情况下都不保证一次性传递，但可以做到一次性状态。<code>Flink</code>可以做到一次性状态，而且不会对性能造成显著影响。<code>Flink</code>还能在与<code>Flink</code>检查点相关的数据槽上做到一次性传递。</p>\n<p><code>Flink</code>检查点就是应用程序状态的快照，<code>Flink</code>会为应用程序定时异步地生成快照。这就是<code>Flink</code>在发生故障时仍然能保证一次性状态的原因：<code>Flink</code>定时记录（快照）输入流的读取位置和每个操作数的相关状态。如果发生故障，<code>Flink</code>会回滚到之前的状态，并重新开始计算。所以说，尽管记录被重新处理，但从结果来看，记录好像只被处理过一次。</p>\n<p>那么端到端的一次性处理呢？通过恰当的方式让检查点兼具事务协调机制是可能的，换句话说，就是让源操作和目标操作参与到检查点里来。在框架内部，结果是一次性的，从端到端来看，也是一次性的，或者说<code>接近一次性</code>。例如，在使用<code>Flink</code>和<code>Kafka</code>作为数据源并发生数据槽（HDFS）滚动时，从<code>Kafka</code>到<code>HDFS</code>就是端到端的一次性处理。类似地，在把<code>Kafka</code>作为<code>Flink</code>的源并且把<code>Cassandra</code>作为<code>Flink</code>的槽时，如果针对<code>Cassandra</code>的更新是幂等时，那么就可以实现端到端的一次性处理。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-2.png?raw=true\" alt=\"\"></p>\n<p>值得一提的是，利用<code>Flink</code>的保存点，检查点可以兼具状态版本机制。使用保存点，在保持状态一致性的同时还可以“随着时间移动”。这样可以让代码的更新、维护、迁移、调试和各种模拟测试变得简单。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"5-谬见5：流只能被应用在“实时”场景里\"><a href=\"#5-谬见5：流只能被应用在“实时”场景里\" class=\"headerlink\" title=\"5. 谬见5：流只能被应用在“实时”场景里\"></a>5. 谬见5：流只能被应用在“实时”场景里</h3><p>这个谬见包括几点内容：</p>\n<ul>\n<li>我没有低延迟的应用，所以我不需要流处理器</li>\n<li>流处理只跟那些持久化之前的过渡数据有关系</li>\n<li>我们需要批处理器来完成笨重的离线计算</li>\n</ul>\n<p>现在是时候思考一下数据集的类型和处理模型之间的关系了。</p>\n<p>(1) 首先，有两种数据集：</p>\n<ul>\n<li>没有边界的：从非预定义的端点持续产生的数据</li>\n<li>有边界的：有限且完整的数据</li>\n</ul>\n<p>很多真实的数据集是没有边界的，不管这些数据时存储在文件里，还是在HDFS的目录里，还是在像Kafka这样的系统里。举一些例子：</p>\n<ul>\n<li>移动设备或网站用户的交互信息</li>\n<li>物理传感器提供的度量指标</li>\n<li>金融市场数据</li>\n<li>机器日志数据</li>\n</ul>\n<p>实际上，在现实世界中很难找到有边界的数据集，不过一个公司所有大楼的位置信息倒是有边界的（不过它也会随着公司业务的增长而变化）。</p>\n<p>(2) 其次，有两种处理模型：</p>\n<ul>\n<li>流：只要有数据生成就会一直处理</li>\n<li>批次：在有限的时间内结束处理，并释放资源</li>\n</ul>\n<p>让我们再深入一点，来区分两种没有边界的数据集：连续性流和间歇性流。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-4.png?raw=true\" alt=\"\"></p>\n<p>使用任意一种模型来处理任意一种数据集是完全可能的，虽然这不是最优的做法。例如，批次处理模型被长时间地应用在无边界的数据集上，特别是间歇性的无边界数据集。现实情况是，大多数<code>批处理</code>任务是通过调度来执行的，每次只处理无边界数据集的一小部分。这意味着流的无边界特质会给某些人带来麻烦（那些工作在流入管道上的人）。</p>\n<p>批处理是无状态的，输出只取决于输入。现实情况是，批处理任务会在内部保留状态（比如reducer经常会保留状态），但这些状态只限在批次的边界内，而且它们不会在批次间流窜。当有人尝试实现类似带有”事件时间戳”的时间窗，那么”批次的边界内状态”就会变得很有用，这在处理无边界数据集时是个很常用的手段。处理无边界数据集的批处理器将不可避免地遇到延迟事件(因为上游的延迟)，批次内的数据有可能因此变得不完整。要注意，这里假设我们是基于事件时间戳来移动时间窗的，因为事件时间戳是现实当中最为准确的模型。在执行批处理的时候，迟到的数据会成为问题，即使通过简单的时间窗修复(比如翻转或滑动时间窗)也解决不了这个问题，特别是如果使用会话时间窗，就更难以处理了。因为完成一个计算所需要的数据不会都在一个批次里，所以在使用批次处理无边界数据集时，很难保证结果的正确性。最起码，它需要额外的开销来处理迟到的数据，还要维护批次之间的状态(要等到所有数据达到后才开始处理，或者重新处理批次)。</p>\n<p><code>Flink</code>内建了处理迟到数据的机制，迟到数据被视为真实世界无边界数据的正常现象，所以<code>Flink</code>设计了一个流处理器专门处理迟到数据。有状态的流处理器更适合用来处理无边界数据集，不管数据集是持续生成的还是间歇生成的。使用流处理器只是个锦上添花的事情。</p>\n<h3 id=\"6-缪见6：不管怎么样，流仍然很复杂\"><a href=\"#6-缪见6：不管怎么样，流仍然很复杂\" class=\"headerlink\" title=\"6. 缪见6：不管怎么样，流仍然很复杂\"></a>6. 缪见6：不管怎么样，流仍然很复杂</h3><p>这是最后一个缪见。你也许会想：”理论虽好，但我仍然不会采用流技术，因为……”：</p>\n<ul>\n<li>流框架难以掌握</li>\n<li>流难以解决时间窗、事件时间戳、触发器的问题</li>\n<li>流需要结合批次，而我已经知道如何使用批次，那为什么还要使用流？</li>\n</ul>\n<p>我们从来没有打算怂恿你使用流，虽然我们觉得流是个很酷的东西。我们相信，是否使用流完全取决于数据和代码的特点。在做决定之前问问自己：”我正在跟什么样类型的数据集打交道？”</p>\n<ul>\n<li>无边界的（用户活动数据、日志、传感器数据）</li>\n<li>有边界的</li>\n</ul>\n<p>然后再问另一个问题：”哪部分变化最频繁？”</p>\n<ul>\n<li>代码比数据变化更频繁</li>\n<li>数据比代码变化更频繁</li>\n</ul>\n<p>对于数据比代码变化更频繁的情况，例如在经常变化的数据集上执行一个相对固定的查询操作，这样会出现流方面的问题。所以，在认定流是一个”复杂”的东西之前，你可能在不知不觉中已经解决过流方面的问题！你可能使用过基于小时的批次任务调度，团队里的其他人可以创建和管理这些批次（在这种情况下，你得到的结果可能是不准确的，而你意识不到这样的结果是批次的时间问题和之前提过的状态问题造成的）。</p>\n<p>为了能够提供一组封装了这些时间和状态复杂性的API，<code>Flink</code>社区为此工作了很长时间。在<code>Flink</code>里可以很简单地处理事件时间戳，只要定义一个时间窗口和一个能够抽取时间戳和水印的函数(只在每个流上调用一次)。处理状态也很简单，类似于定义<code>Java</code>变量，再把这些变量注册到<code>Flink</code>。使用<code>Flink</code>的<code>StreamSQL</code>可以在源源不断的流上面运行<code>SQL</code>查询。</p>\n<p>最后一点：对代码比数据变化更频繁的情况该怎么办？对于这种情况，我们认为你遇到了探索性问题。使用笔记本或其它类似的工具进行迭代可能适合用来解决探索性问题。在代码稳定了之后，你仍然会碰到流方面的问题。我们建议从一开始就使用长远的方案来解决流方面的问题。</p>\n<h3 id=\"7-流处理的未来\"><a href=\"#7-流处理的未来\" class=\"headerlink\" title=\"7. 流处理的未来\"></a>7. 流处理的未来</h3><p>随着流处理的日渐成熟和这些缪见的逐步淡去，我们发现流正朝着除分析应用之外的领域发展。正如我们所讨论的那样，真实世界正连续不断地生成数据。</p>\n<p>原文:<a href=\"http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>我们在思考流处理问题上花了很多时间，更酷的是，我们也花了很多时间帮助其他人认识流处理，以及如何在他们的组织里应用流处理来解决数据问题。</p>\n<p>我们首先要做的是纠正人们对流处理（作为一个快速变化的领域，这里有很多误见值得我们思考）的错误认识。</p>\n<p>在这篇文章里，我们选出了其中的六个作为例子。因为我们对<code>Apache Flink</code>比较熟悉，所以我们会基于<code>Flink</code>来讲解这些例子。</p>\n<ul>\n<li>谬见1：没有不使用批处理的流（Lambda架构）</li>\n<li>谬见2：延迟和吞吐量：只能选择一个</li>\n<li>谬见3：微批次意味着更好的吞吐量</li>\n<li>谬见4：<code>Exactly once</code>？完全不可能</li>\n<li>谬见5：流只能被应用在“实时”场景里</li>\n<li>谬见6：不管怎么样，流仍然很复杂</li>\n</ul>\n<h3 id=\"1-谬见1：没有不使用批处理的流（Lambda架构）\"><a href=\"#1-谬见1：没有不使用批处理的流（Lambda架构）\" class=\"headerlink\" title=\"1. 谬见1：没有不使用批处理的流（Lambda架构）\"></a>1. 谬见1：没有不使用批处理的流（Lambda架构）</h3><p><code>Lambda架构</code>在<code>Apache Storm</code>的早期阶段和其它流处理项目里是一个很有用的设计模式。这个架构包含了一个<code>快速流层</code>和一个<code>批次层</code>。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-1.png?raw=true\" alt=\"\"></p>\n<p>之所以使用两个单独的层，是因为Lambda架构里的流处理只能计算出大致的结果（也就是说，如果中间出现了错误，那么计算结果就不可信），而且只能处理相对少量的事件。</p>\n<p>就算<code>Storm</code>的早期版本存在这样的问题，但现今的很多开源流处理框架都具有容错能力，它们可以在出现故障的前提下生成准确的计算结果，而且具有高吞吐的计算能力。所以没有必要再为了分别得到“快”和“准确”的结果而维护多层架构。现今的流处理器（比如<code>Flink</code>）可以同时帮你得到两种结果。</p>\n<p>好在人们不再更多地讨论Lambda架构，说明流处理正在走向成熟。</p>\n<h3 id=\"2-谬见2：延迟和吞吐量：只能选择一个\"><a href=\"#2-谬见2：延迟和吞吐量：只能选择一个\" class=\"headerlink\" title=\"2. 谬见2：延迟和吞吐量：只能选择一个\"></a>2. 谬见2：延迟和吞吐量：只能选择一个</h3><p>早期的开源流处理框架要么是<code>高吞吐</code>的，要么是<code>低延迟</code>的，而<code>海量且快速</code>一直未能成为开源流处理框架的代名词。</p>\n<p>不过<code>Flink</code>（可能还有其它的框架）就同时提供了高吞吐和低延迟。这里有一个基准测试结果的<a href=\"https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink\" target=\"_blank\" rel=\"noopener\">样例</a>。</p>\n<p>让我们从底层来剖析这个例子，特别是从硬件层，并结合具有网络瓶颈的流处理管道（很多使用<code>Flink</code>的管道都有这个瓶颈）。在硬件层不应该存在需要作出权衡的条件，所以网络才是影响吞吐量和延迟的主要因素。</p>\n<p>一个设计良好的软件系统应该会充分利用网络的上限而不会引入瓶颈问题。不过对<code>Flink</code>来说，总是有可优化的空间，可以让它更接近硬件所能提供的效能。使用一个包含10个节点的集群，<code>Flink</code>现在每秒可以处理千万级别的事件量，如果扩展到1000个节点，它的延迟可以降低到几十毫秒。在我们看来，这种水平已经比很多现有的方案高出很多。</p>\n<h3 id=\"谬见3：微批次意味着更好的吞吐量\"><a href=\"#谬见3：微批次意味着更好的吞吐量\" class=\"headerlink\" title=\"谬见3：微批次意味着更好的吞吐量\"></a>谬见3：微批次意味着更好的吞吐量</h3><p>我们可以从另一个角度来讨论性能，不过先让我们来澄清两个容易混淆的概念：</p>\n<ul>\n<li>微批次:微批次建立在传统批次之上，是处理数据的一个执行或编程模型。通过这项技术，进程或任务可以把一个流当作一系列小型的批次或数据块(参阅:<a href=\"https://hortonworks.com/blog/apache-storm-design-pattern-micro-batching/\" target=\"_blank\" rel=\"noopener\">Apache Storm 微批次设计模式</a>)。</li>\n<li>缓冲:缓冲技术用于对网络、磁盘、缓存的访问进行优化。Wikipedia完美地把它定义为<code>物理内存里的一块用于临时储存移动数据的区域</code>。</li>\n</ul>\n<p>那么第3个缪见就是说，使用微批次的数据处理框架能够比每次处理一个事件的框架达到更高的吞吐量，因为微批次在网络上传输的效率更高。这个缪见忽略了一个事实，流框架不会依赖任何编程模型层面的批次，它们只会在物理层面使用缓冲。<code>Flink</code>确实也会对数据进行缓冲，也就是说它会通过网络发送一组处理过的记录，而不是每次发送一条记录。从性能方面说，不对数据进行缓冲是不可取的，因为通过网络逐个发送记录不会带来任何性能上的好处。所以我们得承认在物理层面根本不存在类似一次一条记录这样的情况。</p>\n<p>不过缓冲只能作为对性能的优化，所以缓冲：</p>\n<ul>\n<li>对用户是不可见的</li>\n<li>不应该对系统造成任何影响</li>\n<li>不应该出现人为的边界</li>\n<li>不应该限制系统功能</li>\n</ul>\n<p>所以对<code>Flink</code>的用户来说，他们开发的程序能够单独地处理每个记录，那是因为<code>Flink</code>为了提升性能隐藏了使用缓冲的细节。</p>\n<p>事实上，在任务调度里使用微批次会带来额外的开销，而如果这样做是为了降低延迟，那么这种开销会只增不减！流处理器知道该如何利用缓冲的优势而不会带来任务调度方面的开销。</p>\n<h3 id=\"4-谬见4：Exactly-once？完全不可能\"><a href=\"#4-谬见4：Exactly-once？完全不可能\" class=\"headerlink\" title=\"4. 谬见4：Exactly once？完全不可能\"></a>4. 谬见4：Exactly once？完全不可能</h3><p>这个缪见包含了几个方面的内容：</p>\n<ul>\n<li>从根本上说，<code>Exactly once</code>是不可能的</li>\n<li>从端到端的<code>Exactly once</code>是不可能的</li>\n<li><code>Exactly once</code>从来都不是真实世界的需求</li>\n<li><code>Exactly once</code>以牺牲性能为代价</li>\n</ul>\n<p>我们退一步讲，我们并不介意<code>Exactly once</code>这种观点的存在。<code>Exactly once</code>原先指的是<code>一次性传递</code>，而现在这个词被随意用在流处理里，让这个词变得令人困惑，失去了它原本的意义。不过相关的概念还是很重要的，我们不打算跳过去。</p>\n<p>为了尽量准确，我们把<code>一次性状态</code>和<code>一次性传递</code>视为两种不同的概念。因为之前人们对这两个词的使用方式导致了它们的混淆。<code>Apache Storm</code>使用<code>at least once</code>来描述传递(<code>Storm</code>不支持状态)，而<code>Apache Samza</code>使用<code>at least once</code>来描述应用状态。</p>\n<p>(1) <code>一次性状态</code>是指应用程序在经历了故障以后恍如没有发生过故障一样。例如，假设我们在维护一个计数器应用程序，在发生了一次故障之后，它既不能多计数也不能少计数。在这里使用<code>Exactly once</code>这个词是因为应用程序状态认为每个消息只被处理了一次。</p>\n<p>(2) <code>一次性传递</code>是指接收端(应用程序之外的系统)在故障发生后会收到处理过的事件，恍如没有发生过故障一样。</p>\n<p>流处理框架在任何情况下都不保证一次性传递，但可以做到一次性状态。<code>Flink</code>可以做到一次性状态，而且不会对性能造成显著影响。<code>Flink</code>还能在与<code>Flink</code>检查点相关的数据槽上做到一次性传递。</p>\n<p><code>Flink</code>检查点就是应用程序状态的快照，<code>Flink</code>会为应用程序定时异步地生成快照。这就是<code>Flink</code>在发生故障时仍然能保证一次性状态的原因：<code>Flink</code>定时记录（快照）输入流的读取位置和每个操作数的相关状态。如果发生故障，<code>Flink</code>会回滚到之前的状态，并重新开始计算。所以说，尽管记录被重新处理，但从结果来看，记录好像只被处理过一次。</p>\n<p>那么端到端的一次性处理呢？通过恰当的方式让检查点兼具事务协调机制是可能的，换句话说，就是让源操作和目标操作参与到检查点里来。在框架内部，结果是一次性的，从端到端来看，也是一次性的，或者说<code>接近一次性</code>。例如，在使用<code>Flink</code>和<code>Kafka</code>作为数据源并发生数据槽（HDFS）滚动时，从<code>Kafka</code>到<code>HDFS</code>就是端到端的一次性处理。类似地，在把<code>Kafka</code>作为<code>Flink</code>的源并且把<code>Cassandra</code>作为<code>Flink</code>的槽时，如果针对<code>Cassandra</code>的更新是幂等时，那么就可以实现端到端的一次性处理。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-2.png?raw=true\" alt=\"\"></p>\n<p>值得一提的是，利用<code>Flink</code>的保存点，检查点可以兼具状态版本机制。使用保存点，在保持状态一致性的同时还可以“随着时间移动”。这样可以让代码的更新、维护、迁移、调试和各种模拟测试变得简单。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-3.png?raw=true\" alt=\"\"></p>\n<h3 id=\"5-谬见5：流只能被应用在“实时”场景里\"><a href=\"#5-谬见5：流只能被应用在“实时”场景里\" class=\"headerlink\" title=\"5. 谬见5：流只能被应用在“实时”场景里\"></a>5. 谬见5：流只能被应用在“实时”场景里</h3><p>这个谬见包括几点内容：</p>\n<ul>\n<li>我没有低延迟的应用，所以我不需要流处理器</li>\n<li>流处理只跟那些持久化之前的过渡数据有关系</li>\n<li>我们需要批处理器来完成笨重的离线计算</li>\n</ul>\n<p>现在是时候思考一下数据集的类型和处理模型之间的关系了。</p>\n<p>(1) 首先，有两种数据集：</p>\n<ul>\n<li>没有边界的：从非预定义的端点持续产生的数据</li>\n<li>有边界的：有限且完整的数据</li>\n</ul>\n<p>很多真实的数据集是没有边界的，不管这些数据时存储在文件里，还是在HDFS的目录里，还是在像Kafka这样的系统里。举一些例子：</p>\n<ul>\n<li>移动设备或网站用户的交互信息</li>\n<li>物理传感器提供的度量指标</li>\n<li>金融市场数据</li>\n<li>机器日志数据</li>\n</ul>\n<p>实际上，在现实世界中很难找到有边界的数据集，不过一个公司所有大楼的位置信息倒是有边界的（不过它也会随着公司业务的增长而变化）。</p>\n<p>(2) 其次，有两种处理模型：</p>\n<ul>\n<li>流：只要有数据生成就会一直处理</li>\n<li>批次：在有限的时间内结束处理，并释放资源</li>\n</ul>\n<p>让我们再深入一点，来区分两种没有边界的数据集：连续性流和间歇性流。</p>\n<p><img src=\"https://github.com/sjf0115/PubLearnNotes/blob/master/image/Stream/%E5%AF%B9%E4%BA%8E%E6%B5%81%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E8%B0%AC%E8%A7%81-4.png?raw=true\" alt=\"\"></p>\n<p>使用任意一种模型来处理任意一种数据集是完全可能的，虽然这不是最优的做法。例如，批次处理模型被长时间地应用在无边界的数据集上，特别是间歇性的无边界数据集。现实情况是，大多数<code>批处理</code>任务是通过调度来执行的，每次只处理无边界数据集的一小部分。这意味着流的无边界特质会给某些人带来麻烦（那些工作在流入管道上的人）。</p>\n<p>批处理是无状态的，输出只取决于输入。现实情况是，批处理任务会在内部保留状态（比如reducer经常会保留状态），但这些状态只限在批次的边界内，而且它们不会在批次间流窜。当有人尝试实现类似带有”事件时间戳”的时间窗，那么”批次的边界内状态”就会变得很有用，这在处理无边界数据集时是个很常用的手段。处理无边界数据集的批处理器将不可避免地遇到延迟事件(因为上游的延迟)，批次内的数据有可能因此变得不完整。要注意，这里假设我们是基于事件时间戳来移动时间窗的，因为事件时间戳是现实当中最为准确的模型。在执行批处理的时候，迟到的数据会成为问题，即使通过简单的时间窗修复(比如翻转或滑动时间窗)也解决不了这个问题，特别是如果使用会话时间窗，就更难以处理了。因为完成一个计算所需要的数据不会都在一个批次里，所以在使用批次处理无边界数据集时，很难保证结果的正确性。最起码，它需要额外的开销来处理迟到的数据，还要维护批次之间的状态(要等到所有数据达到后才开始处理，或者重新处理批次)。</p>\n<p><code>Flink</code>内建了处理迟到数据的机制，迟到数据被视为真实世界无边界数据的正常现象，所以<code>Flink</code>设计了一个流处理器专门处理迟到数据。有状态的流处理器更适合用来处理无边界数据集，不管数据集是持续生成的还是间歇生成的。使用流处理器只是个锦上添花的事情。</p>\n<h3 id=\"6-缪见6：不管怎么样，流仍然很复杂\"><a href=\"#6-缪见6：不管怎么样，流仍然很复杂\" class=\"headerlink\" title=\"6. 缪见6：不管怎么样，流仍然很复杂\"></a>6. 缪见6：不管怎么样，流仍然很复杂</h3><p>这是最后一个缪见。你也许会想：”理论虽好，但我仍然不会采用流技术，因为……”：</p>\n<ul>\n<li>流框架难以掌握</li>\n<li>流难以解决时间窗、事件时间戳、触发器的问题</li>\n<li>流需要结合批次，而我已经知道如何使用批次，那为什么还要使用流？</li>\n</ul>\n<p>我们从来没有打算怂恿你使用流，虽然我们觉得流是个很酷的东西。我们相信，是否使用流完全取决于数据和代码的特点。在做决定之前问问自己：”我正在跟什么样类型的数据集打交道？”</p>\n<ul>\n<li>无边界的（用户活动数据、日志、传感器数据）</li>\n<li>有边界的</li>\n</ul>\n<p>然后再问另一个问题：”哪部分变化最频繁？”</p>\n<ul>\n<li>代码比数据变化更频繁</li>\n<li>数据比代码变化更频繁</li>\n</ul>\n<p>对于数据比代码变化更频繁的情况，例如在经常变化的数据集上执行一个相对固定的查询操作，这样会出现流方面的问题。所以，在认定流是一个”复杂”的东西之前，你可能在不知不觉中已经解决过流方面的问题！你可能使用过基于小时的批次任务调度，团队里的其他人可以创建和管理这些批次（在这种情况下，你得到的结果可能是不准确的，而你意识不到这样的结果是批次的时间问题和之前提过的状态问题造成的）。</p>\n<p>为了能够提供一组封装了这些时间和状态复杂性的API，<code>Flink</code>社区为此工作了很长时间。在<code>Flink</code>里可以很简单地处理事件时间戳，只要定义一个时间窗口和一个能够抽取时间戳和水印的函数(只在每个流上调用一次)。处理状态也很简单，类似于定义<code>Java</code>变量，再把这些变量注册到<code>Flink</code>。使用<code>Flink</code>的<code>StreamSQL</code>可以在源源不断的流上面运行<code>SQL</code>查询。</p>\n<p>最后一点：对代码比数据变化更频繁的情况该怎么办？对于这种情况，我们认为你遇到了探索性问题。使用笔记本或其它类似的工具进行迭代可能适合用来解决探索性问题。在代码稳定了之后，你仍然会碰到流方面的问题。我们建议从一开始就使用长远的方案来解决流方面的问题。</p>\n<h3 id=\"7-流处理的未来\"><a href=\"#7-流处理的未来\" class=\"headerlink\" title=\"7. 流处理的未来\"></a>7. 流处理的未来</h3><p>随着流处理的日渐成熟和这些缪见的逐步淡去，我们发现流正朝着除分析应用之外的领域发展。正如我们所讨论的那样，真实世界正连续不断地生成数据。</p>\n<p>原文:<a href=\"http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate\" target=\"_blank\" rel=\"noopener\">http://www.infoq.com/cn/news/2016/12/error-stream-proce-eliminate</a></p>\n"},{"layout":"post","author":"sjf0115","title":"Flink1.4 并发执行","date":"2018-01-04T06:07:01.000Z","_content":"\n本节介绍如何在`Flink`中配置程序的并行执行。一个`Flink`程序由多个任务(`transformations`/`operators`，`data sources`和`sinks`)组成。一个任务被分成多个并发实例来执行，每个并发实例只处理任务输入数据的一个子集。一个任务的并发实例的个数称为并发度(`parallelism`)。\n\n如果你想使用[保存点](https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html)，也应该考虑设置最大并发度。从保存点恢复时，可以更改特定算子或整个程序的并发度，并且此配置指定了并发的上限。\n\n### 1. 设置并发度\n\n一个任务的并发度可以在`Flink`中指定不同级别。\n\n#### 1.1 算子级别\n\n单个算子，数据源，sink可以通过调用`setParallelism()`方法来定义并发度。例如，像这样：\n\nJava版本:\n```java\nDataStream<String> text = [...]\nDataStream<Tuple2<String, Integer>> wordCounts = text\n    .flatMap(new LineSplitter())\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1).setParallelism(5);\n\nwordCounts.print();\n\nenv.execute(\"Word Count Example\");\n```\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\n\nval text = [...]\nval wordCounts = text\n    .flatMap{ _.split(\" \") map { (_, 1) } }\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1).setParallelism(5)\nwordCounts.print()\n\nenv.execute(\"Word Count Example\")\n```\n\n#### 1.2 执行环境级别\n\n如[这](http://smartsi.club/2018/01/04/Flink/[Flink]Flink%20Flink%E7%A8%8B%E5%BA%8F%E5%89%96%E6%9E%90/)所述，`Flink`程序是在执行环境的上下文中执行的。执行环境为它执行的所有算子，数据源和数据`sink`提供了默认的并发度。执行环境的并发度可以通过显式配置一个算子的并发度来覆盖。\n\n执行环境的默认并发度可以通过调用`setParallelism()`方法来指定。要为执行的所有算子，数据源和`sink`设置并发度为3，请按如下方式设置执行环境的默认并发度：\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setParallelism(3);\n\nDataStream<String> text = [...]\nDataStream<Tuple2<String, Integer>> wordCounts = [...]\nwordCounts.print();\n\nenv.execute(\"Word Count Example\");\n```\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setParallelism(3)\n\nval text = [...]\nval wordCounts = text\n    .flatMap{ _.split(\" \") map { (_, 1) } }\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1)\nwordCounts.print()\n\nenv.execute(\"Word Count Example\")\n```\n\n#### 1.3 客户端级别\n\n在向`Flink`提交作业时，可以在客户端设置并发度。客户端可以是`Java`或`Scala`程序。`Flink`的命令行接口(`CLI`)就是一种客户端。\n\n对于`CLI`客户端，可以使用`-p`指定并发度参数。 例如：\n```\n./bin/flink run -p 10 ../examples/*WordCount-java*.jar\n```\n在`Java`/`Scala`程序中，并发度设置如下：\n\nJava版本:\n```java\ntry {\n    PackagedProgram program = new PackagedProgram(file, args);\n    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\"localhost:6123\");\n    Configuration config = new Configuration();\n\n    Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader());\n\n    // set the parallelism to 10 here\n    client.run(program, 10, true);\n\n} catch (ProgramInvocationException e) {\n    e.printStackTrace();\n}\n```\n\nScala版本:\n```\ntry {\n    PackagedProgram program = new PackagedProgram(file, args)\n    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\"localhost:6123\")\n    Configuration config = new Configuration()\n\n    Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader())\n\n    // set the parallelism to 10 here\n    client.run(program, 10, true)\n\n} catch {\n    case e: Exception => e.printStackTrace\n}\n```\n\n#### 1.4 系统级别\n\n可以通过在`./conf/flink-conf.yaml`中设置`parallelism.default`属性来为所有执行环境定义全系统默认并发度。详细信息请参阅[配置文档](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/config.html)。\n\n\n### 2. 设置最大并发度\n\n最大并发度可以在可以设置并发度的地方设置(客户端级别和系统级别除外)。你可以调用`setMaxParallelism()`取代`setParallelism()`方法来设置最大并发度。\n\n最大并发度的默认设置大致为`operatorParallelism +（operatorParallelism / 2）`，下限为`127`，上限为`32768`。\n\n备注:\n```\n将最大并发度设置为非常大的数值可能会对性能造成不利影响，因为一些后端状态必须保持在内部数据结构，而这些内部数据结构随key-groups(这是可扩展状态的内部实现机制)的数量进行扩展。(some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state).)\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html\n","source":"_posts/Flink/[Flink]Flink1.4 并发执行.md","raw":"---\nlayout: post\nauthor: sjf0115\ntitle: Flink1.4 并发执行\ndate: 2018-01-04 14:07:01\ntags:\n  - Flink\n\ncategories: Flink\n---\n\n本节介绍如何在`Flink`中配置程序的并行执行。一个`Flink`程序由多个任务(`transformations`/`operators`，`data sources`和`sinks`)组成。一个任务被分成多个并发实例来执行，每个并发实例只处理任务输入数据的一个子集。一个任务的并发实例的个数称为并发度(`parallelism`)。\n\n如果你想使用[保存点](https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html)，也应该考虑设置最大并发度。从保存点恢复时，可以更改特定算子或整个程序的并发度，并且此配置指定了并发的上限。\n\n### 1. 设置并发度\n\n一个任务的并发度可以在`Flink`中指定不同级别。\n\n#### 1.1 算子级别\n\n单个算子，数据源，sink可以通过调用`setParallelism()`方法来定义并发度。例如，像这样：\n\nJava版本:\n```java\nDataStream<String> text = [...]\nDataStream<Tuple2<String, Integer>> wordCounts = text\n    .flatMap(new LineSplitter())\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1).setParallelism(5);\n\nwordCounts.print();\n\nenv.execute(\"Word Count Example\");\n```\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\n\nval text = [...]\nval wordCounts = text\n    .flatMap{ _.split(\" \") map { (_, 1) } }\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1).setParallelism(5)\nwordCounts.print()\n\nenv.execute(\"Word Count Example\")\n```\n\n#### 1.2 执行环境级别\n\n如[这](http://smartsi.club/2018/01/04/Flink/[Flink]Flink%20Flink%E7%A8%8B%E5%BA%8F%E5%89%96%E6%9E%90/)所述，`Flink`程序是在执行环境的上下文中执行的。执行环境为它执行的所有算子，数据源和数据`sink`提供了默认的并发度。执行环境的并发度可以通过显式配置一个算子的并发度来覆盖。\n\n执行环境的默认并发度可以通过调用`setParallelism()`方法来指定。要为执行的所有算子，数据源和`sink`设置并发度为3，请按如下方式设置执行环境的默认并发度：\n\nJava版本:\n```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setParallelism(3);\n\nDataStream<String> text = [...]\nDataStream<Tuple2<String, Integer>> wordCounts = [...]\nwordCounts.print();\n\nenv.execute(\"Word Count Example\");\n```\nScala版本:\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment\nenv.setParallelism(3)\n\nval text = [...]\nval wordCounts = text\n    .flatMap{ _.split(\" \") map { (_, 1) } }\n    .keyBy(0)\n    .timeWindow(Time.seconds(5))\n    .sum(1)\nwordCounts.print()\n\nenv.execute(\"Word Count Example\")\n```\n\n#### 1.3 客户端级别\n\n在向`Flink`提交作业时，可以在客户端设置并发度。客户端可以是`Java`或`Scala`程序。`Flink`的命令行接口(`CLI`)就是一种客户端。\n\n对于`CLI`客户端，可以使用`-p`指定并发度参数。 例如：\n```\n./bin/flink run -p 10 ../examples/*WordCount-java*.jar\n```\n在`Java`/`Scala`程序中，并发度设置如下：\n\nJava版本:\n```java\ntry {\n    PackagedProgram program = new PackagedProgram(file, args);\n    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\"localhost:6123\");\n    Configuration config = new Configuration();\n\n    Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader());\n\n    // set the parallelism to 10 here\n    client.run(program, 10, true);\n\n} catch (ProgramInvocationException e) {\n    e.printStackTrace();\n}\n```\n\nScala版本:\n```\ntry {\n    PackagedProgram program = new PackagedProgram(file, args)\n    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\"localhost:6123\")\n    Configuration config = new Configuration()\n\n    Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader())\n\n    // set the parallelism to 10 here\n    client.run(program, 10, true)\n\n} catch {\n    case e: Exception => e.printStackTrace\n}\n```\n\n#### 1.4 系统级别\n\n可以通过在`./conf/flink-conf.yaml`中设置`parallelism.default`属性来为所有执行环境定义全系统默认并发度。详细信息请参阅[配置文档](https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/config.html)。\n\n\n### 2. 设置最大并发度\n\n最大并发度可以在可以设置并发度的地方设置(客户端级别和系统级别除外)。你可以调用`setMaxParallelism()`取代`setParallelism()`方法来设置最大并发度。\n\n最大并发度的默认设置大致为`operatorParallelism +（operatorParallelism / 2）`，下限为`127`，上限为`32768`。\n\n备注:\n```\n将最大并发度设置为非常大的数值可能会对性能造成不利影响，因为一些后端状态必须保持在内部数据结构，而这些内部数据结构随key-groups(这是可扩展状态的内部实现机制)的数量进行扩展。(some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state).)\n```\n\n备注:\n```\nFlink版本:1.4\n```\n\n原文:https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html\n","slug":"Flink/[Flink]Flink1.4 并发执行","published":1,"updated":"2018-01-29T09:36:59.649Z","comments":1,"photos":[],"link":"","_id":"cje58tivt006aordbtlun2efp","content":"<p>本节介绍如何在<code>Flink</code>中配置程序的并行执行。一个<code>Flink</code>程序由多个任务(<code>transformations</code>/<code>operators</code>，<code>data sources</code>和<code>sinks</code>)组成。一个任务被分成多个并发实例来执行，每个并发实例只处理任务输入数据的一个子集。一个任务的并发实例的个数称为并发度(<code>parallelism</code>)。</p>\n<p>如果你想使用<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html\" target=\"_blank\" rel=\"noopener\">保存点</a>，也应该考虑设置最大并发度。从保存点恢复时，可以更改特定算子或整个程序的并发度，并且此配置指定了并发的上限。</p>\n<h3 id=\"1-设置并发度\"><a href=\"#1-设置并发度\" class=\"headerlink\" title=\"1. 设置并发度\"></a>1. 设置并发度</h3><p>一个任务的并发度可以在<code>Flink</code>中指定不同级别。</p>\n<h4 id=\"1-1-算子级别\"><a href=\"#1-1-算子级别\" class=\"headerlink\" title=\"1.1 算子级别\"></a>1.1 算子级别</h4><p>单个算子，数据源，sink可以通过调用<code>setParallelism()</code>方法来定义并发度。例如，像这样：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;String&gt; text = [...]</span><br><span class=\"line\">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text</span><br><span class=\"line\">    .flatMap(<span class=\"keyword\">new</span> LineSplitter())</span><br><span class=\"line\">    .keyBy(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .timeWindow(Time.seconds(<span class=\"number\">5</span>))</span><br><span class=\"line\">    .sum(<span class=\"number\">1</span>).setParallelism(<span class=\"number\">5</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">wordCounts.print();</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(<span class=\"string\">\"Word Count Example\"</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">val text = [...]</span><br><span class=\"line\">val wordCounts = text</span><br><span class=\"line\">    .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125;</span><br><span class=\"line\">    .keyBy(0)</span><br><span class=\"line\">    .timeWindow(Time.seconds(5))</span><br><span class=\"line\">    .sum(1).setParallelism(5)</span><br><span class=\"line\">wordCounts.print()</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(&quot;Word Count Example&quot;)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-执行环境级别\"><a href=\"#1-2-执行环境级别\" class=\"headerlink\" title=\"1.2 执行环境级别\"></a>1.2 执行环境级别</h4><p>如<a href=\"http://smartsi.club/2018/01/04/Flink/[Flink]Flink%20Flink%E7%A8%8B%E5%BA%8F%E5%89%96%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">这</a>所述，<code>Flink</code>程序是在执行环境的上下文中执行的。执行环境为它执行的所有算子，数据源和数据<code>sink</code>提供了默认的并发度。执行环境的并发度可以通过显式配置一个算子的并发度来覆盖。</p>\n<p>执行环境的默认并发度可以通过调用<code>setParallelism()</code>方法来指定。要为执行的所有算子，数据源和<code>sink</code>设置并发度为3，请按如下方式设置执行环境的默认并发度：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setParallelism(<span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;String&gt; text = [...]</span><br><span class=\"line\">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = [...]</span><br><span class=\"line\">wordCounts.print();</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(<span class=\"string\">\"Word Count Example\"</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">env.setParallelism(3)</span><br><span class=\"line\"></span><br><span class=\"line\">val text = [...]</span><br><span class=\"line\">val wordCounts = text</span><br><span class=\"line\">    .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125;</span><br><span class=\"line\">    .keyBy(0)</span><br><span class=\"line\">    .timeWindow(Time.seconds(5))</span><br><span class=\"line\">    .sum(1)</span><br><span class=\"line\">wordCounts.print()</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(&quot;Word Count Example&quot;)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-3-客户端级别\"><a href=\"#1-3-客户端级别\" class=\"headerlink\" title=\"1.3 客户端级别\"></a>1.3 客户端级别</h4><p>在向<code>Flink</code>提交作业时，可以在客户端设置并发度。客户端可以是<code>Java</code>或<code>Scala</code>程序。<code>Flink</code>的命令行接口(<code>CLI</code>)就是一种客户端。</p>\n<p>对于<code>CLI</code>客户端，可以使用<code>-p</code>指定并发度参数。 例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -p 10 ../examples/*WordCount-java*.jar</span><br></pre></td></tr></table></figure></p>\n<p>在<code>Java</code>/<code>Scala</code>程序中，并发度设置如下：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    PackagedProgram program = <span class=\"keyword\">new</span> PackagedProgram(file, args);</span><br><span class=\"line\">    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(<span class=\"string\">\"localhost:6123\"</span>);</span><br><span class=\"line\">    Configuration config = <span class=\"keyword\">new</span> Configuration();</span><br><span class=\"line\"></span><br><span class=\"line\">    Client client = <span class=\"keyword\">new</span> Client(jobManagerAddress, config, program.getUserCodeClassLoader());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// set the parallelism to 10 here</span></span><br><span class=\"line\">    client.run(program, <span class=\"number\">10</span>, <span class=\"keyword\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> (ProgramInvocationException e) &#123;</span><br><span class=\"line\">    e.printStackTrace();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">try &#123;</span><br><span class=\"line\">    PackagedProgram program = new PackagedProgram(file, args)</span><br><span class=\"line\">    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(&quot;localhost:6123&quot;)</span><br><span class=\"line\">    Configuration config = new Configuration()</span><br><span class=\"line\"></span><br><span class=\"line\">    Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader())</span><br><span class=\"line\"></span><br><span class=\"line\">    // set the parallelism to 10 here</span><br><span class=\"line\">    client.run(program, 10, true)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125; catch &#123;</span><br><span class=\"line\">    case e: Exception =&gt; e.printStackTrace</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-4-系统级别\"><a href=\"#1-4-系统级别\" class=\"headerlink\" title=\"1.4 系统级别\"></a>1.4 系统级别</h4><p>可以通过在<code>./conf/flink-conf.yaml</code>中设置<code>parallelism.default</code>属性来为所有执行环境定义全系统默认并发度。详细信息请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/config.html\" target=\"_blank\" rel=\"noopener\">配置文档</a>。</p>\n<h3 id=\"2-设置最大并发度\"><a href=\"#2-设置最大并发度\" class=\"headerlink\" title=\"2. 设置最大并发度\"></a>2. 设置最大并发度</h3><p>最大并发度可以在可以设置并发度的地方设置(客户端级别和系统级别除外)。你可以调用<code>setMaxParallelism()</code>取代<code>setParallelism()</code>方法来设置最大并发度。</p>\n<p>最大并发度的默认设置大致为<code>operatorParallelism +（operatorParallelism / 2）</code>，下限为<code>127</code>，上限为<code>32768</code>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">将最大并发度设置为非常大的数值可能会对性能造成不利影响，因为一些后端状态必须保持在内部数据结构，而这些内部数据结构随key-groups(这是可扩展状态的内部实现机制)的数量进行扩展。(some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state).)</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>本节介绍如何在<code>Flink</code>中配置程序的并行执行。一个<code>Flink</code>程序由多个任务(<code>transformations</code>/<code>operators</code>，<code>data sources</code>和<code>sinks</code>)组成。一个任务被分成多个并发实例来执行，每个并发实例只处理任务输入数据的一个子集。一个任务的并发实例的个数称为并发度(<code>parallelism</code>)。</p>\n<p>如果你想使用<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.3/setup/savepoints.html\" target=\"_blank\" rel=\"noopener\">保存点</a>，也应该考虑设置最大并发度。从保存点恢复时，可以更改特定算子或整个程序的并发度，并且此配置指定了并发的上限。</p>\n<h3 id=\"1-设置并发度\"><a href=\"#1-设置并发度\" class=\"headerlink\" title=\"1. 设置并发度\"></a>1. 设置并发度</h3><p>一个任务的并发度可以在<code>Flink</code>中指定不同级别。</p>\n<h4 id=\"1-1-算子级别\"><a href=\"#1-1-算子级别\" class=\"headerlink\" title=\"1.1 算子级别\"></a>1.1 算子级别</h4><p>单个算子，数据源，sink可以通过调用<code>setParallelism()</code>方法来定义并发度。例如，像这样：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">DataStream&lt;String&gt; text = [...]</span><br><span class=\"line\">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text</span><br><span class=\"line\">    .flatMap(<span class=\"keyword\">new</span> LineSplitter())</span><br><span class=\"line\">    .keyBy(<span class=\"number\">0</span>)</span><br><span class=\"line\">    .timeWindow(Time.seconds(<span class=\"number\">5</span>))</span><br><span class=\"line\">    .sum(<span class=\"number\">1</span>).setParallelism(<span class=\"number\">5</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">wordCounts.print();</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(<span class=\"string\">\"Word Count Example\"</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\"></span><br><span class=\"line\">val text = [...]</span><br><span class=\"line\">val wordCounts = text</span><br><span class=\"line\">    .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125;</span><br><span class=\"line\">    .keyBy(0)</span><br><span class=\"line\">    .timeWindow(Time.seconds(5))</span><br><span class=\"line\">    .sum(1).setParallelism(5)</span><br><span class=\"line\">wordCounts.print()</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(&quot;Word Count Example&quot;)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-执行环境级别\"><a href=\"#1-2-执行环境级别\" class=\"headerlink\" title=\"1.2 执行环境级别\"></a>1.2 执行环境级别</h4><p>如<a href=\"http://smartsi.club/2018/01/04/Flink/[Flink]Flink%20Flink%E7%A8%8B%E5%BA%8F%E5%89%96%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">这</a>所述，<code>Flink</code>程序是在执行环境的上下文中执行的。执行环境为它执行的所有算子，数据源和数据<code>sink</code>提供了默认的并发度。执行环境的并发度可以通过显式配置一个算子的并发度来覆盖。</p>\n<p>执行环境的默认并发度可以通过调用<code>setParallelism()</code>方法来指定。要为执行的所有算子，数据源和<code>sink</code>设置并发度为3，请按如下方式设置执行环境的默认并发度：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class=\"line\">env.setParallelism(<span class=\"number\">3</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">DataStream&lt;String&gt; text = [...]</span><br><span class=\"line\">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = [...]</span><br><span class=\"line\">wordCounts.print();</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(<span class=\"string\">\"Word Count Example\"</span>);</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">val env = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class=\"line\">env.setParallelism(3)</span><br><span class=\"line\"></span><br><span class=\"line\">val text = [...]</span><br><span class=\"line\">val wordCounts = text</span><br><span class=\"line\">    .flatMap&#123; _.split(&quot; &quot;) map &#123; (_, 1) &#125; &#125;</span><br><span class=\"line\">    .keyBy(0)</span><br><span class=\"line\">    .timeWindow(Time.seconds(5))</span><br><span class=\"line\">    .sum(1)</span><br><span class=\"line\">wordCounts.print()</span><br><span class=\"line\"></span><br><span class=\"line\">env.execute(&quot;Word Count Example&quot;)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-3-客户端级别\"><a href=\"#1-3-客户端级别\" class=\"headerlink\" title=\"1.3 客户端级别\"></a>1.3 客户端级别</h4><p>在向<code>Flink</code>提交作业时，可以在客户端设置并发度。客户端可以是<code>Java</code>或<code>Scala</code>程序。<code>Flink</code>的命令行接口(<code>CLI</code>)就是一种客户端。</p>\n<p>对于<code>CLI</code>客户端，可以使用<code>-p</code>指定并发度参数。 例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">./bin/flink run -p 10 ../examples/*WordCount-java*.jar</span><br></pre></td></tr></table></figure></p>\n<p>在<code>Java</code>/<code>Scala</code>程序中，并发度设置如下：</p>\n<p>Java版本:<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">    PackagedProgram program = <span class=\"keyword\">new</span> PackagedProgram(file, args);</span><br><span class=\"line\">    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(<span class=\"string\">\"localhost:6123\"</span>);</span><br><span class=\"line\">    Configuration config = <span class=\"keyword\">new</span> Configuration();</span><br><span class=\"line\"></span><br><span class=\"line\">    Client client = <span class=\"keyword\">new</span> Client(jobManagerAddress, config, program.getUserCodeClassLoader());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// set the parallelism to 10 here</span></span><br><span class=\"line\">    client.run(program, <span class=\"number\">10</span>, <span class=\"keyword\">true</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125; <span class=\"keyword\">catch</span> (ProgramInvocationException e) &#123;</span><br><span class=\"line\">    e.printStackTrace();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Scala版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">try &#123;</span><br><span class=\"line\">    PackagedProgram program = new PackagedProgram(file, args)</span><br><span class=\"line\">    InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(&quot;localhost:6123&quot;)</span><br><span class=\"line\">    Configuration config = new Configuration()</span><br><span class=\"line\"></span><br><span class=\"line\">    Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader())</span><br><span class=\"line\"></span><br><span class=\"line\">    // set the parallelism to 10 here</span><br><span class=\"line\">    client.run(program, 10, true)</span><br><span class=\"line\"></span><br><span class=\"line\">&#125; catch &#123;</span><br><span class=\"line\">    case e: Exception =&gt; e.printStackTrace</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-4-系统级别\"><a href=\"#1-4-系统级别\" class=\"headerlink\" title=\"1.4 系统级别\"></a>1.4 系统级别</h4><p>可以通过在<code>./conf/flink-conf.yaml</code>中设置<code>parallelism.default</code>属性来为所有执行环境定义全系统默认并发度。详细信息请参阅<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/config.html\" target=\"_blank\" rel=\"noopener\">配置文档</a>。</p>\n<h3 id=\"2-设置最大并发度\"><a href=\"#2-设置最大并发度\" class=\"headerlink\" title=\"2. 设置最大并发度\"></a>2. 设置最大并发度</h3><p>最大并发度可以在可以设置并发度的地方设置(客户端级别和系统级别除外)。你可以调用<code>setMaxParallelism()</code>取代<code>setParallelism()</code>方法来设置最大并发度。</p>\n<p>最大并发度的默认设置大致为<code>operatorParallelism +（operatorParallelism / 2）</code>，下限为<code>127</code>，上限为<code>32768</code>。</p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">将最大并发度设置为非常大的数值可能会对性能造成不利影响，因为一些后端状态必须保持在内部数据结构，而这些内部数据结构随key-groups(这是可扩展状态的内部实现机制)的数量进行扩展。(some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state).)</span><br></pre></td></tr></table></figure></p>\n<p>备注:<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><span class=\"line\">Flink版本:1.4</span><br></pre></td></tr></table></figure></p>\n<p>原文:<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/parallel.html</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cje58tipj0009ordbuj9qwen5","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tipv000hordbpmb97q28"},{"post_id":"cje58tip10004ordbpy7pt5dk","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiq1000mordby9shhtw8"},{"post_id":"cje58tipk000aordbfael9wpz","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiq4000pordbghafzgxm"},{"post_id":"cje58tipq000eordbzcvp4s4q","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiq8000tordbnomtwcku"},{"post_id":"cje58tipa0005ordby9rg5od4","category_id":"cje58tipp000bordbnpm0gfj6","_id":"cje58tiqb000wordb6qcfivmk"},{"post_id":"cje58tipt000gordbmtolalil","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiqf0010ordbn0udua7n"},{"post_id":"cje58tipz000lordbcpq61bqe","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiqh0013ordb4am524wx"},{"post_id":"cje58tipg0008ordbq03d3sc9","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiqm0017ordb6qwa5xrf"},{"post_id":"cje58tiq1000oordbpsdlu8xk","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiqq001aordb5n005kgd"},{"post_id":"cje58tiq6000sordbptk4ztaj","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiqt001dordbxez4le2e"},{"post_id":"cje58tiq9000vordbrxspx5fh","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiqx001hordb19m4ioy7"},{"post_id":"cje58tiqd000zordbfo0knnrj","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tir0001kordblmsklwxr"},{"post_id":"cje58tiqf0012ordb5t2eqiy5","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tir5001oordb7cmklevk"},{"post_id":"cje58tiqi0016ordbmf99oy04","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tir8001rordbxg8xkdmy"},{"post_id":"cje58tiqn0019ordbbvcmboih","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tird001uordbzva1i7ds"},{"post_id":"cje58tiqq001cordb5r3i1xt1","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tirh001xordbdheq4tuy"},{"post_id":"cje58tiqu001gordbcqwwdbo5","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tirl0020ordb5c3vq0q8"},{"post_id":"cje58tiqy001jordb3takyx2y","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tirp0023ordbv5fxg6t2"},{"post_id":"cje58tir1001nordbtcxfz2qj","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tiru0026ordb19yvhh8x"},{"post_id":"cje58tir6001qordbqdp1dmpx","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tirx0029ordbeylxbxhs"},{"post_id":"cje58tir9001tordbhchvxtcq","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tis0002eordbd1prs1pk"},{"post_id":"cje58tire001wordbwfd9i0ao","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tis7002hordb8xi0n14j"},{"post_id":"cje58tiri001zordbpi9gw85k","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tisb002mordbhmqnhm32"},{"post_id":"cje58tirn0022ordb9miemp4y","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tise002pordb0jm6tgf3"},{"post_id":"cje58tirv0028ordb3f4uzpox","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tisg002sordbe26lyyrp"},{"post_id":"cje58tis1002gordbqh7saefo","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tisi002vordblwoq8qse"},{"post_id":"cje58tirr0025ordbqx49f6i3","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tism002yordbt36gdi8g"},{"post_id":"cje58tis9002jordbjs4n3jjw","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tisp0031ordbg2fu7vvb"},{"post_id":"cje58tisc002oordb1x4bnbxp","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tisr0034ordbi31wemhz"},{"post_id":"cje58tirx002dordbaowcte91","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tist0037ordbf001f92i"},{"post_id":"cje58tisf002rordbw5aqdh54","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tisx003aordbd867pdxd"},{"post_id":"cje58tish002uordb7j5p8vx6","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tit1003dordbu9ft7uu1"},{"post_id":"cje58tisj002xordbqtlikuuk","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tit3003gordb98z06496"},{"post_id":"cje58tisn0030ordbz8fea2qg","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tit5003jordb3wx7idxc"},{"post_id":"cje58tisp0033ordbfbgq50t5","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tit8003mordbu6kjh5d6"},{"post_id":"cje58tisr0036ordb4eqke0ex","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58titf003pordbe8im4zpp"},{"post_id":"cje58tisv0039ordbguxkoksj","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58titj003tordbcle2qr9w"},{"post_id":"cje58tisz003cordblp7vaf4p","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58titm003wordbh62xvlii"},{"post_id":"cje58tit2003fordb4muv5js7","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58titp0040ordb5gs0faw4"},{"post_id":"cje58tit4003iordb18fe4hvr","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58titr0043ordbed2rjw6a"},{"post_id":"cje58tita003oordb3gzguiv6","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58titu0048ordbn3kcpwem"},{"post_id":"cje58titn003yordbokrt3vou","category_id":"cje58tits0044ordbfn3s2um8","_id":"cje58tiud004jordbun390ek6"},{"post_id":"cje58tits0046ordbvo3w0lz5","category_id":"cje58tity004dordbbi1m5gmv","_id":"cje58tiui004rordba2pdx07r"},{"post_id":"cje58titv004aordbcgy2y0xk","category_id":"cje58tity004dordbbi1m5gmv","_id":"cje58tiuo004yordbvbfx2tbc"},{"post_id":"cje58titx004cordb6c8s6dju","category_id":"cje58tity004dordbbi1m5gmv","_id":"cje58tiuu0054ordb2wdo2ych"},{"post_id":"cje58titz004gordbuxz3q6lr","category_id":"cje58tiuo004xordbf3g5lt7c","_id":"cje58tiuz005bordbkxbqai5b"},{"post_id":"cje58tius0053ordbab212m3f","category_id":"cje58tirx002aordb2zggsnwh","_id":"cje58tiv2005gordb18ic5t9m"},{"post_id":"cje58tiuu0058ordbfw9540d5","category_id":"cje58tiuu0055ordbbjn8erb4","_id":"cje58tiv5005jordbtipym1u6"},{"post_id":"cje58tiu9004iordbjhpc5kqp","category_id":"cje58tiuu0055ordbbjn8erb4","_id":"cje58tiv9005oordbibqkdel1"},{"post_id":"cje58tiud004nordbi7g4ofng","category_id":"cje58tiuz005cordbxdyjmmts","_id":"cje58tive005uordbq7dfcfrr"},{"post_id":"cje58tiug004pordbf7oyjtuf","category_id":"cje58tiuu0055ordbbjn8erb4","_id":"cje58tivl0061ordbkew6x4bt"},{"post_id":"cje58tiuj004uordbl8bwi1be","category_id":"cje58tiuu0055ordbbjn8erb4","_id":"cje58tivs0068ordbmn84lxa3"},{"post_id":"cje58tiul004wordbsd8fiua4","category_id":"cje58tiuu0055ordbbjn8erb4","_id":"cje58tivw006dordbgtufp4zc"},{"post_id":"cje58tivt006aordbtlun2efp","category_id":"cje58tipd0006ordbb65xsuz9","_id":"cje58tivx006hordbnpqf7drh"},{"post_id":"cje58tiup0051ordbe8ybz85v","category_id":"cje58tiuu0055ordbbjn8erb4","_id":"cje58tivz006kordbpaw39uo4"},{"post_id":"cje58tiux005aordbey3c35r3","category_id":"cje58tivw006cordbrfuow4nm","_id":"cje58tivz006nordbeanbpm0z"},{"post_id":"cje58tiv0005fordbd65kjp5z","category_id":"cje58tivw006cordbrfuow4nm","_id":"cje58tiw0006qordbx53qd3nn"},{"post_id":"cje58tiv3005iordbprnpkcy8","category_id":"cje58tivz006mordboq1ic8ih","_id":"cje58tiw1006uordbqcaztmkl"},{"post_id":"cje58tiv5005lordbrdfoccag","category_id":"cje58tiw0006rordbroq8htmm","_id":"cje58tiw2006zordb2vas3dco"},{"post_id":"cje58tiv9005qordbznkg4nkv","category_id":"cje58tivw006cordbrfuow4nm","_id":"cje58tiw30073ordbl4kn3smo"},{"post_id":"cje58tivb005sordb5nefohxg","category_id":"cje58tivw006cordbrfuow4nm","_id":"cje58tiw40076ordbpgwt49e3"},{"post_id":"cje58tivf005xordbkfguvodv","category_id":"cje58tivw006cordbrfuow4nm","_id":"cje58tiw5007aordbv8sv20c6"},{"post_id":"cje58tivh005zordbh3horot4","category_id":"cje58tiw0006rordbroq8htmm","_id":"cje58tiw6007fordby9cbe26c"},{"post_id":"cje58tivn0064ordbd3iptop6","category_id":"cje58tiw0006rordbroq8htmm","_id":"cje58tiw7007iordb1a1a8bkn"},{"post_id":"cje58tivq0066ordbl8vm0gjz","category_id":"cje58tiw0006rordbroq8htmm","_id":"cje58tiw8007lordb6vhkyoee"}],"PostTag":[{"post_id":"cje58tip10004ordbpy7pt5dk","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tipq000dordbzggd2f6a"},{"post_id":"cje58tipk000aordbfael9wpz","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tips000fordbvny9tkg4"},{"post_id":"cje58tipq000eordbzcvp4s4q","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tipy000kordbgzwiqio2"},{"post_id":"cje58tipa0005ordby9rg5od4","tag_id":"cje58tipp000cordbg77akjf7","_id":"cje58tiq1000nordb91e6w745"},{"post_id":"cje58tipt000gordbmtolalil","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiq6000rordb3t4cm36y"},{"post_id":"cje58tipg0008ordbq03d3sc9","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiq9000uordbrdbtweqh"},{"post_id":"cje58tiq1000oordbpsdlu8xk","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiqc000yordbcjmp9s15"},{"post_id":"cje58tiq6000sordbptk4ztaj","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiqf0011ordb1l4drl6w"},{"post_id":"cje58tipj0009ordbuj9qwen5","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiqh0015ordbbnwco2x5"},{"post_id":"cje58tipj0009ordbuj9qwen5","tag_id":"cje58tiq4000qordbn06yup0d","_id":"cje58tiqn0018ordbs2y18ybm"},{"post_id":"cje58tipz000lordbcpq61bqe","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiqq001bordbm9ma55tf"},{"post_id":"cje58tipz000lordbcpq61bqe","tag_id":"cje58tiqc000xordbymlmmns4","_id":"cje58tiqt001fordbtws33sig"},{"post_id":"cje58tiqf0012ordb5t2eqiy5","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tiqy001iordbxigif1er"},{"post_id":"cje58tiqn0019ordbbvcmboih","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tir0001mordbzy0egjad"},{"post_id":"cje58tiqn0019ordbbvcmboih","tag_id":"cje58tiqh0014ordbyimxh7wz","_id":"cje58tir6001pordbcns0reof"},{"post_id":"cje58tiq9000vordbrxspx5fh","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tir9001sordbia2m6txv"},{"post_id":"cje58tiq9000vordbrxspx5fh","tag_id":"cje58tiqh0014ordbyimxh7wz","_id":"cje58tire001vordbw4ilgyjj"},{"post_id":"cje58tiqq001cordb5r3i1xt1","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tirh001yordbnb6mkzi0"},{"post_id":"cje58tiqu001gordbcqwwdbo5","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tirn0021ordbqf4l6cl2"},{"post_id":"cje58tiqd000zordbfo0knnrj","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tirr0024ordb6i7rblh6"},{"post_id":"cje58tiqd000zordbfo0knnrj","tag_id":"cje58tiqt001eordb9rd45cwr","_id":"cje58tiru0027ordbxaevfpel"},{"post_id":"cje58tiqy001jordb3takyx2y","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tirx002cordbiwe5uoli"},{"post_id":"cje58tiqy001jordb3takyx2y","tag_id":"cje58tiqh0014ordbyimxh7wz","_id":"cje58tis1002fordbyy2pjib1"},{"post_id":"cje58tir1001nordbtcxfz2qj","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tis8002iordbuskqmkfj"},{"post_id":"cje58tiqi0016ordbmf99oy04","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisc002nordbgb83feul"},{"post_id":"cje58tiqi0016ordbmf99oy04","tag_id":"cje58tir0001lordbn3mvw720","_id":"cje58tise002qordbjpqo4xqy"},{"post_id":"cje58tir6001qordbqdp1dmpx","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisg002tordb2b1mh904"},{"post_id":"cje58tir9001tordbhchvxtcq","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisi002wordbgojpsdwa"},{"post_id":"cje58tire001wordbwfd9i0ao","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisn002zordbhy5a8q1e"},{"post_id":"cje58tire001wordbwfd9i0ao","tag_id":"cje58tiqh0014ordbyimxh7wz","_id":"cje58tisp0032ordb4y1lxge6"},{"post_id":"cje58tiri001zordbpi9gw85k","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisr0035ordbjtk54d2b"},{"post_id":"cje58tirn0022ordb9miemp4y","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisu0038ordbpjg0cwda"},{"post_id":"cje58tirv0028ordb3f4uzpox","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tisy003bordbl2sce669"},{"post_id":"cje58tis1002gordbqh7saefo","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tit1003eordbxu0703g6"},{"post_id":"cje58tirr0025ordbqx49f6i3","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tit3003hordb1c7zs27i"},{"post_id":"cje58tis9002jordbjs4n3jjw","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tit5003kordbpm6gcjcx"},{"post_id":"cje58tisc002oordb1x4bnbxp","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tit9003nordbstzbf1mj"},{"post_id":"cje58tirx002dordbaowcte91","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titg003rordbh0xy9wzn"},{"post_id":"cje58tisf002rordbw5aqdh54","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titj003uordbztifovy0"},{"post_id":"cje58tish002uordb7j5p8vx6","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titm003xordb2rfrivts"},{"post_id":"cje58tisj002xordbqtlikuuk","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titq0041ordby3nd1h6g"},{"post_id":"cje58tisn0030ordbz8fea2qg","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tits0045ordbp2ei4376"},{"post_id":"cje58tisp0033ordbfbgq50t5","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titu0049ordbmaixyci4"},{"post_id":"cje58tisr0036ordb4eqke0ex","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titx004bordbbtrd8c96"},{"post_id":"cje58tisv0039ordbguxkoksj","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58titz004fordbvxmqm7v4"},{"post_id":"cje58tisz003cordblp7vaf4p","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tiu9004hordbn06ts51h"},{"post_id":"cje58tit2003fordb4muv5js7","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tiud004mordbkxn5urpn"},{"post_id":"cje58tit4003iordb18fe4hvr","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tiuf004oordbuig8s0ko"},{"post_id":"cje58tita003oordb3gzguiv6","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tiui004tordb4s0qz6px"},{"post_id":"cje58titk003vordboecagrrf","tag_id":"cje58titg003qordb35in7c4t","_id":"cje58tiul004vordbbilrxzj8"},{"post_id":"cje58tit6003lordbfxl6u1f9","tag_id":"cje58titg003qordb35in7c4t","_id":"cje58tiuo0050ordb6uzr2oxi"},{"post_id":"cje58titq0042ordb1phechi4","tag_id":"cje58titg003qordb35in7c4t","_id":"cje58tiur0052ordbqua99duw"},{"post_id":"cje58titg003sordbiuj5kvb6","tag_id":"cje58titg003qordb35in7c4t","_id":"cje58tiuu0057ordbbjpiajzv"},{"post_id":"cje58titn003yordbokrt3vou","tag_id":"cje58titu0047ordbw91hc6a8","_id":"cje58tiux0059ordbgukxnuv0"},{"post_id":"cje58tits0046ordbvo3w0lz5","tag_id":"cje58titz004eordbkdh53cgm","_id":"cje58tiuz005eordbrb9wbzt3"},{"post_id":"cje58tits0046ordbvo3w0lz5","tag_id":"cje58tiud004lordbout2o8ix","_id":"cje58tiv3005hordbabgejpqh"},{"post_id":"cje58titv004aordbcgy2y0xk","tag_id":"cje58titz004eordbkdh53cgm","_id":"cje58tiv5005kordbxi8e541v"},{"post_id":"cje58titv004aordbcgy2y0xk","tag_id":"cje58tiud004lordbout2o8ix","_id":"cje58tiv9005pordbsmarn70v"},{"post_id":"cje58tius0053ordbab212m3f","tag_id":"cje58tirx002bordbukeersdy","_id":"cje58tivb005rordblsbvera5"},{"post_id":"cje58titx004cordb6c8s6dju","tag_id":"cje58titz004eordbkdh53cgm","_id":"cje58tive005wordb47vz5hv2"},{"post_id":"cje58titz004gordbuxz3q6lr","tag_id":"cje58tiuz005dordb4sxx28hv","_id":"cje58tivh005yordbzriy7tnc"},{"post_id":"cje58tiu9004iordbjhpc5kqp","tag_id":"cje58tiv8005nordb91gktwvr","_id":"cje58tivm0063ordbuh5wup6c"},{"post_id":"cje58tiud004nordbi7g4ofng","tag_id":"cje58tive005vordbdja6yaez","_id":"cje58tivq0065ordbn43la4ou"},{"post_id":"cje58tiug004pordbf7oyjtuf","tag_id":"cje58tiv8005nordb91gktwvr","_id":"cje58tivv006bordbhe0j37ug"},{"post_id":"cje58tivt006aordbtlun2efp","tag_id":"cje58tipg0007ordbeigxs8kr","_id":"cje58tivw006fordbcpynb1o2"},{"post_id":"cje58tiuj004uordbl8bwi1be","tag_id":"cje58tiv8005nordb91gktwvr","_id":"cje58tivx006gordbejtymq40"},{"post_id":"cje58tiul004wordbsd8fiua4","tag_id":"cje58tiv8005nordb91gktwvr","_id":"cje58tivz006lordb4fk3plcg"},{"post_id":"cje58tiup0051ordbe8ybz85v","tag_id":"cje58tiv8005nordb91gktwvr","_id":"cje58tiw0006pordb11ec8tmw"},{"post_id":"cje58tiuu0058ordbfw9540d5","tag_id":"cje58tiv8005nordb91gktwvr","_id":"cje58tiw1006tordb59qloh0i"},{"post_id":"cje58tiux005aordbey3c35r3","tag_id":"cje58tiw0006sordbckh8sf8b","_id":"cje58tiw1006xordbprex5ael"},{"post_id":"cje58tiv0005fordbd65kjp5z","tag_id":"cje58tiw0006sordbckh8sf8b","_id":"cje58tiw30071ordbsx794uxl"},{"post_id":"cje58tiv3005iordbprnpkcy8","tag_id":"cje58tiw20070ordb4t0t0jwq","_id":"cje58tiw50078ordbu0pkw3hn"},{"post_id":"cje58tiv3005iordbprnpkcy8","tag_id":"cje58tiw30074ordbxr8c6hn3","_id":"cje58tiw6007bordbm2fjz29x"},{"post_id":"cje58tiv5005lordbrdfoccag","tag_id":"cje58tiw40077ordbzsbsacof","_id":"cje58tiw6007dordblzjt2ply"},{"post_id":"cje58tiv9005qordbznkg4nkv","tag_id":"cje58tiw0006sordbckh8sf8b","_id":"cje58tiw7007hordbr8su4qx6"},{"post_id":"cje58tivb005sordb5nefohxg","tag_id":"cje58tiw0006sordbckh8sf8b","_id":"cje58tiw8007kordblih7l7dx"},{"post_id":"cje58tivf005xordbkfguvodv","tag_id":"cje58tiw0006sordbckh8sf8b","_id":"cje58tiw9007nordb0blsbj2v"},{"post_id":"cje58tivh005zordbh3horot4","tag_id":"cje58tiw40077ordbzsbsacof","_id":"cje58tiw9007pordb6hfd8d62"},{"post_id":"cje58tivn0064ordbd3iptop6","tag_id":"cje58tiw40077ordbzsbsacof","_id":"cje58tiwa007rordbrhkjzbix"},{"post_id":"cje58tivq0066ordbl8vm0gjz","tag_id":"cje58tiw40077ordbzsbsacof","_id":"cje58tiwa007sordbm6zai6qy"}],"Tag":[{"name":"Flink","_id":"cje58tipg0007ordbeigxs8kr"},{"name":"Airflow","_id":"cje58tipp000cordbg77akjf7"},{"name":"Flink 优化","_id":"cje58tiq4000qordbn06yup0d"},{"name":"Flink SQL","_id":"cje58tiqc000xordbymlmmns4"},{"name":"Flink 容错","_id":"cje58tiqh0014ordbyimxh7wz"},{"name":"Flink内部原理","_id":"cje58tiqt001eordb9rd45cwr"},{"name":"Flink 基础","_id":"cje58tir0001lordbn3mvw720"},{"name":"Hadoop","_id":"cje58tirx002bordbukeersdy"},{"name":"Hexo","_id":"cje58titg003qordb35in7c4t"},{"name":"Hive","_id":"cje58titu0047ordbw91hc6a8"},{"name":"Java","_id":"cje58titz004eordbkdh53cgm"},{"name":"JVM","_id":"cje58tiud004lordbout2o8ix"},{"name":"MySQL","_id":"cje58tiuz005dordb4sxx28hv"},{"name":"Python","_id":"cje58tiv8005nordb91gktwvr"},{"name":"CaChe","_id":"cje58tive005vordbdja6yaez"},{"name":"Scala","_id":"cje58tiw0006sordbckh8sf8b"},{"name":"Spark","_id":"cje58tiw20070ordb4t0t0jwq"},{"name":"Spark 优化","_id":"cje58tiw30074ordbxr8c6hn3"},{"name":"Stream","_id":"cje58tiw40077ordbzsbsacof"}]}}